<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>PySpark-核心编程 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="PySpark-核心编程" />
<meta property="og:description" content="2. PySpark——RDD编程入门 文章目录 2. PySpark——RDD编程入门2.1 程序执行入口SparkContext对象2.2 RDD的创建2.2.1 并行化创建2.2.2 获取RDD分区数2.2.3 读取文件创建 2.3 RDD算子2.4 常用Transformation算子2.4.1 map算子2.4.2 flatMap算子2.4.3 reduceByKey算子2.4.4 WordCount回顾2.4.5 groupBy算子2.4.6 Filter算子2.4.7 distinct算子2.4.8 union算子2.4.9 join算子2.4.10 intersection 算子2.4.11 glom算子2.4.12 groupByKey算子2.4.13 sortBy算子2.4.14 sortByKey2.4.15 综合案例2.4.16 将案例提交到yarn运行 2.5 常用Action算子2.5.1 countByKey算子2.5.2 collect算子2.5.3 reduce算子2.5.4 fold算子2.5.5 first算子2.5.6 take算子2.5.7 top算子2.5.8 count算子2.5.9 takeSample算子2.5.10 takeOrdered2.5.11 foreach算子2.5.12 saveAsTextFile2.5.13 注意点 2.6 分区操作算子2.6.1 mapPartitions算子2.6.2 foreachPartition算子2.6.3 partitionBy算子2.6.4 repartition算子2.6.5 coalesce算子2.6.6 mapValues算子2.6.7 join算子 2.7 面试题2.8 总结 3. RDD的持久化3.1 RDD的数据是过程数据3.2 RDD的缓存3.2.1 缓存3.2.2 缓存特点3.2.3 缓存是如何保存的 3.3 RDD的CheckPoint3.3.1 RDD CheckPoint3.3.2 CheckPoint是如何保存数据的3.3.3 缓存和CheckPoint的对比3." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/19c5988e1a4b6105d2abff11aae15e9d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-22T15:22:03+08:00" />
<meta property="article:modified_time" content="2023-08-22T15:22:03+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">PySpark-核心编程</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="2_PySparkRDD_1"></a>2. PySpark——RDD编程入门</h2> 
<p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#2_PySparkRDD_1" rel="nofollow">2. PySpark——RDD编程入门</a></li><li><ul><li><a href="#21_SparkContext_8" rel="nofollow">2.1 程序执行入口SparkContext对象</a></li><li><a href="#22_RDD_36" rel="nofollow">2.2 RDD的创建</a></li><li><ul><li><a href="#221__46" rel="nofollow">2.2.1 并行化创建</a></li><li><a href="#222_RDD_88" rel="nofollow">2.2.2 获取RDD分区数</a></li><li><a href="#223__142" rel="nofollow">2.2.3 读取文件创建</a></li></ul> 
   </li><li><a href="#23_RDD_251" rel="nofollow">2.3 RDD算子</a></li><li><a href="#24_Transformation_288" rel="nofollow">2.4 常用Transformation算子</a></li><li><ul><li><a href="#241_map_290" rel="nofollow">2.4.1 map算子</a></li><li><a href="#242_flatMap_337" rel="nofollow">2.4.2 flatMap算子</a></li><li><a href="#243_reduceByKey_375" rel="nofollow">2.4.3 reduceByKey算子</a></li><li><a href="#244_WordCount_438" rel="nofollow">2.4.4 WordCount回顾</a></li><li><a href="#245_groupBy_475" rel="nofollow">2.4.5 groupBy算子</a></li><li><a href="#246_Filter_520" rel="nofollow">2.4.6 Filter算子</a></li><li><a href="#247_distinct_558" rel="nofollow">2.4.7 distinct算子</a></li><li><a href="#248_union_597" rel="nofollow">2.4.8 union算子</a></li><li><a href="#249_join_637" rel="nofollow">2.4.9 join算子</a></li><li><a href="#2410_intersection__687" rel="nofollow">2.4.10 intersection 算子</a></li><li><a href="#2411_glom_719" rel="nofollow">2.4.11 glom算子</a></li><li><a href="#2412_groupByKey_756" rel="nofollow">2.4.12 groupByKey算子</a></li><li><a href="#2413_sortBy_786" rel="nofollow">2.4.13 sortBy算子</a></li><li><a href="#2414_sortByKey_835" rel="nofollow">2.4.14 sortByKey</a></li><li><a href="#2415__869" rel="nofollow">2.4.15 综合案例</a></li><li><a href="#2416_yarn_914" rel="nofollow">2.4.16 将案例提交到yarn运行</a></li></ul> 
   </li><li><a href="#25_Action_1026" rel="nofollow">2.5 常用Action算子</a></li><li><ul><li><a href="#251_countByKey_1028" rel="nofollow">2.5.1 countByKey算子</a></li><li><a href="#252_collect_1066" rel="nofollow">2.5.2 collect算子</a></li><li><a href="#253_reduce_1080" rel="nofollow">2.5.3 reduce算子</a></li><li><a href="#254_fold_1116" rel="nofollow">2.5.4 fold算子</a></li><li><a href="#255_first_1164" rel="nofollow">2.5.5 first算子</a></li><li><a href="#256_take_1177" rel="nofollow">2.5.6 take算子</a></li><li><a href="#257_top_1190" rel="nofollow">2.5.7 top算子</a></li><li><a href="#258_count_1203" rel="nofollow">2.5.8 count算子</a></li><li><a href="#259_takeSample_1216" rel="nofollow">2.5.9 takeSample算子</a></li><li><a href="#2510_takeOrdered_1267" rel="nofollow">2.5.10 takeOrdered</a></li><li><a href="#2511_foreach_1307" rel="nofollow">2.5.11 foreach算子</a></li><li><a href="#2512_saveAsTextFile_1348" rel="nofollow">2.5.12 saveAsTextFile</a></li><li><a href="#2513__1390" rel="nofollow">2.5.13 注意点</a></li></ul> 
   </li><li><a href="#26__1401" rel="nofollow">2.6 分区操作算子</a></li><li><ul><li><a href="#261_mapPartitions_1403" rel="nofollow">2.6.1 mapPartitions算子</a></li><li><a href="#262_foreachPartition_1463" rel="nofollow">2.6.2 foreachPartition算子</a></li><li><a href="#263_partitionBy_1504" rel="nofollow">2.6.3 partitionBy算子</a></li><li><a href="#264_repartition_1560" rel="nofollow">2.6.4 repartition算子</a></li><li><a href="#265_coalesce_1621" rel="nofollow">2.6.5 coalesce算子</a></li><li><a href="#266_mapValues_1643" rel="nofollow">2.6.6 mapValues算子</a></li><li><a href="#267_join_1682" rel="nofollow">2.6.7 join算子</a></li></ul> 
   </li><li><a href="#27__1692" rel="nofollow">2.7 面试题</a></li><li><a href="#28__1729" rel="nofollow">2.8 总结</a></li></ul> 
  </li><li><a href="#3_RDD_1764" rel="nofollow">3. RDD的持久化</a></li><li><ul><li><a href="#31_RDD_1766" rel="nofollow">3.1 RDD的数据是过程数据</a></li><li><a href="#32_RDD_1780" rel="nofollow">3.2 RDD的缓存</a></li><li><ul><li><a href="#321__1782" rel="nofollow">3.2.1 缓存</a></li><li><a href="#322__1812" rel="nofollow">3.2.2 缓存特点</a></li><li><a href="#323__1873" rel="nofollow">3.2.3 缓存是如何保存的</a></li></ul> 
   </li><li><a href="#33_RDDCheckPoint_1883" rel="nofollow">3.3 RDD的CheckPoint</a></li><li><ul><li><a href="#331_RDD_CheckPoint_1885" rel="nofollow">3.3.1 RDD CheckPoint</a></li><li><a href="#332_CheckPoint_1896" rel="nofollow">3.3.2 CheckPoint是如何保存数据的</a></li><li><a href="#333_CheckPoint_1902" rel="nofollow">3.3.3 缓存和CheckPoint的对比</a></li><li><a href="#334__1909" rel="nofollow">3.3.4 代码</a></li><li><a href="#335__1968" rel="nofollow">3.3.5 注意</a></li><li><a href="#336__1982" rel="nofollow">3.3.6 总结</a></li></ul> 
  </li></ul> 
  </li><li><a href="#4_Spark_1994" rel="nofollow">4. Spark案例练习</a></li><li><ul><li><a href="#41__1996" rel="nofollow">4.1 搜索引擎日志分析案例</a></li><li><a href="#42__2140" rel="nofollow">4.2 提交到集群运行</a></li><li><a href="#43__2160" rel="nofollow">4.3 作业</a></li></ul> 
  </li><li><a href="#5__2218" rel="nofollow">5. 共享变量</a></li><li><ul><li><a href="#51__2220" rel="nofollow">5.1 广播变量</a></li><li><ul><li><a href="#511__2222" rel="nofollow">5.1.1 问题引出</a></li><li><a href="#512__2236" rel="nofollow">5.1.2 解决方案-广播变量</a></li></ul> 
   </li><li><a href="#52__2335" rel="nofollow">5.2 累加器</a></li><li><ul><li><a href="#521__2337" rel="nofollow">5.2.1 需求</a></li><li><a href="#522__2341" rel="nofollow">5.2.2 没有累加器的代码演示</a></li><li><a href="#523__2388" rel="nofollow">5.2.3 解决方法-累加器</a></li><li><a href="#524__2462" rel="nofollow">5.2.4 累加器的注意事项</a></li></ul> 
   </li><li><a href="#53__2486" rel="nofollow">5.3 综合案例</a></li><li><ul><li><a href="#531__2488" rel="nofollow">5.3.1 需求</a></li></ul> 
   </li><li><a href="#54__2578" rel="nofollow">5.4 总结</a></li></ul> 
  </li><li><a href="#6Spark_2588" rel="nofollow">6.Spark内核调度（重点理解）</a></li><li><ul><li><a href="#61_DAG_2590" rel="nofollow">6.1 DAG</a></li><li><ul><li><a href="#611_DAG_2592" rel="nofollow">6.1.1 DAG</a></li><li><a href="#612_JobAction_2628" rel="nofollow">6.1.2 Job和Action</a></li><li><a href="#613_DAG_2654" rel="nofollow">6.1.3 DAG和分区</a></li></ul> 
   </li><li><a href="#62_DAG_2674" rel="nofollow">6.2 DAG的宽窄依赖和阶段划分</a></li><li><ul><li><a href="#621__2687" rel="nofollow">6.2.1 窄依赖</a></li><li><a href="#622__2691" rel="nofollow">6.2.2 宽依赖</a></li><li><a href="#623__2695" rel="nofollow">6.2.3 阶段划分</a></li></ul> 
   </li><li><a href="#63__2707" rel="nofollow">6.3 内存迭代计算</a></li><li><ul><li><a href="#631__2729" rel="nofollow">6.3.1 面试题</a></li></ul> 
   </li><li><a href="#64_Spark_2751" rel="nofollow">6.4 Spark并行度</a></li><li><ul><li><a href="#641__2763" rel="nofollow">6.4.1 如何设置并行度</a></li><li><a href="#642__2778" rel="nofollow">6.4.2 全局并行度-推荐</a></li><li><a href="#643_RDD_2802" rel="nofollow">6.4.3 针对RDD的并行度设置-不推荐</a></li><li><a href="#644__2810" rel="nofollow">6.4.4 集群中如何规划并行度</a></li></ul> 
   </li><li><a href="#65_Spark_2834" rel="nofollow">6.5 Spark任务调度</a></li><li><ul><li><a href="#651_Drivcer_2859" rel="nofollow">6.5.1 Drivcer内的两个组件</a></li></ul> 
   </li><li><a href="#66_Spark_2869" rel="nofollow">6.6 拓展-Spark概念名词大全</a></li><li><ul><li><a href="#661_Spark_2871" rel="nofollow">6.6.1 Spark运行中的概念名词大全</a></li></ul> 
   </li><li><a href="#67_SparkShuffle_2885" rel="nofollow">6.7 SparkShuffle</a></li><li><ul><li><a href="#671MR_Shuffle_2887" rel="nofollow">6.7.1MR Shuffle回顾</a></li><li><a href="#672__2893" rel="nofollow">6.7.2 简介</a></li><li><a href="#673_Sort_Shuffle_bypass_2953" rel="nofollow">6.7.3 Sort Shuffle bypass机制</a></li><li><a href="#674_Shuflle_3004" rel="nofollow">6.7.4 Shuflle的配置选项</a></li></ul> 
   </li><li><a href="#68__3062" rel="nofollow">6.8 总结</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<p>gitee仓库：<a href="https://gitee.com/sethli/PySpark" rel="nofollow">gitee仓库</a><br> 觉得有用的话，点个赞，点个收藏呗<br> 给人点赞，手留余香</p> 
<h3><a id="21_SparkContext_8"></a>2.1 程序执行入口SparkContext对象</h3> 
<p>Spark RDD 编程的程序入口对象是<code>SparkContext</code>对象(不论何种编程语言)</p> 
<p>只有构建出<code>SparkContext</code>, 基于它才能执行后续的API调用和计算</p> 
<p>本质上, <code>SparkContext</code>对编程来说, 主要功能就是创建第一个RDD出来</p> 
<p>代码演示：</p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token comment"># 导入Spark相关包</span>
<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext
<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    <span class="token comment"># 构建SparkConf对象</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName <span class="token punctuation">(</span><span class="token string">"helloSpark"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    <span class="token comment"># 构建SparkContext执行环境入口对象</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>
</code></pre> 
<blockquote> 
 <p>master的种类：</p> 
 <ol><li>local：local[N]:表示以N核CPU执行，local[*]:给予local进程 所有CPU核心的使用权</li><li>standlone：spark：//node1:7077</li><li>yarn 模式</li></ol> 
</blockquote> 
<h3><a id="22_RDD_36"></a>2.2 RDD的创建</h3> 
<p>RDD的创建主要有2种方式:</p> 
<p>• <strong>通过并行化集合创建</strong> ( 本地对象 转 分布式RDD )</p> 
<p>• <strong>读取外部数据源</strong> ( 读取文件 )</p> 
<p><img src="https://images2.imgbox.com/50/bf/rimpaYK8_o.png" alt="image-20230728224639737"></p> 
<h4><a id="221__46"></a>2.2.1 并行化创建</h4> 
<p>概念：并行化创建，是指将本地集合转向分布式RDD，这一步就是分布式的开端：本地转分布式</p> 
<p><strong>API</strong>：</p> 
<blockquote> 
 <p>rdd = spakcontext.parallelize(参数1，参数2)</p> 
 <p>参数1 集合对象即可，比如list</p> 
 <p>参数2 分区数</p> 
</blockquote> 
<p>完整代码：</p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>
<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span>SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    <span class="token comment"># 0. 构建Spark执行环境</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"create rdd"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    <span class="token comment"># sc 对象的parallelize方法，可以将本地集合转换成RDD返回给你</span>
    data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">]</span>
    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span>data<span class="token punctuation">,</span>numSlices<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>执行结果：</p> 
<pre><code>Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[1, 2, 3, 4, 5, 6, 7, 8, 9]

Process finished with exit code 0
</code></pre> 
<h4><a id="222_RDD_88"></a>2.2.2 获取RDD分区数</h4> 
<p><code>getNumPartitions</code> API :获取RDD分区数量，返回值是<code>Int</code>数字</p> 
<p>用法：<code>rdd.getNumPartitions()</code></p> 
<p>例如，基于上述代码设置了3为分区数，调用以下代码</p> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span>getNumPartitions<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>则会输出结果：3</p> 
<p>完整案例代码：<code>01_create_parallelize.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token comment"># 导入Spark相关包</span>
<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    <span class="token comment"># 0. 初始化执行环境 构建SparkContext对象,本地集合--&gt; 分布式对象（RDD）</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName <span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    <span class="token comment"># 演示通过并行化集合的方式去创建RDD</span>
    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># parallelize方法，没有给定分区数，默认分区数是多少？ 根据CPU核心来定</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"默认分区数："</span><span class="token punctuation">,</span> rdd<span class="token punctuation">.</span>getNumPartitions<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"分区数："</span><span class="token punctuation">,</span> rdd<span class="token punctuation">.</span>getNumPartitions<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># collect方法，是将RDD（分布式对象）中每个分区的数据，都发送到Driver中，形成一个Python List对象</span>
    <span class="token comment"># collect：分布式 转--&gt; 本地集合</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"rdd的内容是："</span><span class="token punctuation">,</span> rdd<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre> 
<p>输出结果：</p> 
<pre><code>默认分区数： 8
分区数： 3
rdd的内容是： [1, 2, 3]
&lt;class 'list'&gt;
</code></pre> 
<h4><a id="223__142"></a>2.2.3 读取文件创建</h4> 
<p><code>textFile</code>API</p> 
<p>这个API可以读取本地数据，也可以读取hdfs数据</p> 
<p><strong>使用方法</strong> ：</p> 
<blockquote> 
 <p>sparkcontext.textFile(参数1,参数2)</p> 
 <p>参数1，必填，文件路径 支持本地文件 支持HDFS 也支持一些比如S3协议</p> 
 <p>参数2 可选，表示最小分区数量</p> 
 <p>注意：参数2 话语权不足，spark有自己的判断，在它允许的范围内，参数2有效果，超出spark允许的范围，参数2失效</p> 
</blockquote> 
<p>案例代码：<code>02_create_textFile.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding : utf8</span>
<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"02_create_textFile"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    <span class="token comment"># 通过textFile API 读取数据</span>

    <span class="token comment"># 读取本地文件数据</span>
    file_rdd1 <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"../data/input/words.txt"</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"默认读取分区数："</span><span class="token punctuation">,</span> file_rdd1<span class="token punctuation">.</span>getNumPartitions<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"file_rdd1 内容："</span><span class="token punctuation">,</span> file_rdd1<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment">#</span>
    <span class="token comment"># # 加最小分区数的测试</span>
    file_rdd2 <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"../data/input/words.txt"</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>
    file_rdd3 <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"../data/input/words.txt"</span><span class="token punctuation">,</span><span class="token number">100</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"file_rdd2 分区数："</span><span class="token punctuation">,</span> file_rdd2<span class="token punctuation">.</span>getNumPartitions<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"file_rdd3 分区数："</span><span class="token punctuation">,</span> file_rdd3<span class="token punctuation">.</span>getNumPartitions<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 读取hdfs文件数据测试</span>
    hdfs_rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"hdfs://Tnode1:8020/input/words.txt"</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"hdfs_rdd 分区数："</span><span class="token punctuation">,</span> hdfs_rdd<span class="token punctuation">.</span>getNumPartitions<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"hdfs_rdd 内容："</span><span class="token punctuation">,</span> hdfs_rdd<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>默认读取分区数： 2
file_rdd1 内容： ['hello spark', 'hello hadoop', 'hello flink']
file_rdd2 内容： 4
file_rdd3 内容： 38
hdfs_rdd 分区： 2
hdfs_rdd 内容： ['hello spark', 'hello hadoop', 'hello flink']
</code></pre> 
<hr> 
<p><code>wholeTextFile</code> 读取文件的API，有个适用场景：适合读取一堆小文件</p> 
<blockquote> 
 <p>这个API是小文件读取专用</p> 
</blockquote> 
<p>用法：</p> 
<pre><code class="prism language-python">sparkcontext<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span>参数<span class="token number">1</span><span class="token punctuation">,</span>参数<span class="token number">2</span><span class="token punctuation">)</span>

<span class="token comment"># 参数1，必填，文件路径 支持本地文件 支持HDFS 也支持一些比如S3协议</span>

<span class="token comment"># 参数2 可选，表示最小分区数量</span>

<span class="token comment"># 注意：参数2 话语权不足，spark有自己的判断，在它允许的范围内，参数2有效果，超出spark允许的范围，参数2失效</span>
</code></pre> 
<blockquote> 
 <p>这个API偏向于少量分区读取数据</p> 
 <p>因为，这个API表明了自己是小文件读取专用，那么文件的数据很小、分区很多，</p> 
 <p>导致shuffle的几率更高，所以尽量少分区读取数据</p> 
</blockquote> 
<p>案例代码：<code>03_create_wholeTextFile.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>
<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    <span class="token comment"># 读取小文件文件夹</span>
    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>wholeTextFiles<span class="token punctuation">(</span><span class="token string">"../data/input/tiny_files"</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre> 
<p>输出结果：</p> 
<pre><code>[('file:/tmp/pycharm_project_937/PySpark01/data/input/tiny_files/1.txt', 'hello spark\r\nhello hadoop\r\nhello flink'), ('file:/tmp/pycharm_project_937/PySpark01/data/input/tiny_files/2.txt', 'hello spark\r\nhello hadoop\r\nhello flink'), ('file:/tmp/pycharm_project_937/PySpark01/data/input/tiny_files/3.txt', 'hello spark\r\nhello hadoop\r\nhello flink'), ('file:/tmp/pycharm_project_937/PySpark01/data/input/tiny_files/4.txt', 'hello spark\r\nhello hadoop\r\nhello flink'), ('file:/tmp/pycharm_project_937/PySpark01/data/input/tiny_files/5.txt', 'hello spark\r\nhello hadoop\r\nhello flink')]
['hello spark\r\nhello hadoop\r\nhello flink', 'hello spark\r\nhello hadoop\r\nhello flink', 'hello spark\r\nhello hadoop\r\nhello flink', 'hello spark\r\nhello hadoop\r\nhello flink', 'hello spark\r\nhello hadoop\r\nhello flink']

</code></pre> 
<h3><a id="23_RDD_251"></a>2.3 RDD算子</h3> 
<p><strong>算子是什么？</strong></p> 
<p>​ <strong>算子</strong>：分布式集合对象上的API称之为算子</p> 
<p>​ 方法、函数：本地对象的API，叫做方法、函数</p> 
<p>​ 算子：分布式对象的API，叫做算子</p> 
<p><strong>算子分类</strong></p> 
<p>​ RDD的算子 分成2类</p> 
<ul><li>​ Transformation：转换算子</li><li>​ Action：动作（行动）算子</li></ul> 
<p><strong>Transformation 算子：</strong></p> 
<p>​ 定义：RDD的算子，返回值任然是一个RDD的，称之为转换算子</p> 
<p>​ 特性：这类算子lazy 懒加载的，如果没有action算子，Transformation算子是不工作的</p> 
<p><strong>Action算子</strong></p> 
<p>​ 定义：返回值不是rdd的就是action算子</p> 
<blockquote> 
 <p>对于这两类算子来说，<code>Transformation</code>算子，相当于在构建执行计划，<code>action</code>是一个指令让这个执行计划开始工作。</p> 
 <p>如果没有<code>action</code>，<code>Transformation</code>算子之间的迭代关系，就是一个没有通电的流水线，</p> 
 <p>只有<code>action</code>到来，这个数据处理的流水线才开始工作</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/31/79/bDAtnQCv_o.png" alt="image-20230729153842438"></p> 
<p><img src="https://images2.imgbox.com/de/65/HBL2Ldhr_o.png" alt="image-20230729153933547"></p> 
<h3><a id="24_Transformation_288"></a>2.4 常用Transformation算子</h3> 
<h4><a id="241_map_290"></a>2.4.1 map算子</h4> 
<p>演示代码：<code>04_operators_map.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>
<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext


<span class="token keyword">def</span> <span class="token function">addNum</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> data <span class="token operator">*</span> <span class="token number">10</span>


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd1 <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>

    rdd2 <span class="token operator">=</span> rdd1<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x <span class="token operator">*</span> <span class="token number">10</span><span class="token punctuation">)</span>
    rdd3 <span class="token operator">=</span> rdd1<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>addNum<span class="token punctuation">)</span>

    result <span class="token operator">=</span> rdd2<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd3<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre> 
<p>输出结果：</p> 
<pre><code>[10, 20, 30, 40, 50, 60, 70, 80, 90]
[10, 20, 30, 40, 50, 60, 70, 80, 90]
</code></pre> 
<p>对于传入参数的lambda表达式</p> 
<blockquote> 
 <p>传入方法作为传参的时候，可以选择</p> 
 <ol><li>定义方法，传入其方法名</li><li>使用lambda 匿名方法的方式</li></ol> 
 <p>一般，如果方法体可以一行写完，用lambda方便。</p> 
 <p>如果方法体复杂，就直接定义方法更方便</p> 
</blockquote> 
<h4><a id="242_flatMap_337"></a>2.4.2 flatMap算子</h4> 
<p>功能：对rdd执行map操作，然后进行<code>解除嵌套</code>操作</p> 
<p><code>解除嵌套</code>：</p> 
<p><img src="https://images2.imgbox.com/3d/8d/eXCwelDi_o.png" alt="image-20230729164647747"></p> 
<p>演示代码：<code>05_operators_flatMap.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>
<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext


<span class="token keyword">def</span> <span class="token function">addNum</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> data <span class="token operator">*</span> <span class="token number">10</span>


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd1 <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"hadoop spark hadoop"</span><span class="token punctuation">,</span> <span class="token string">"spark hadoop hadoop"</span><span class="token punctuation">,</span> <span class="token string">"hadoop flink spark"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token comment"># 得到所有的单词，组成rdd,flatMap的传入参数和map一致，就是给map逻辑用的，解除嵌套无需逻辑（传参）</span>
    rdd2 <span class="token operator">=</span> rdd1<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span><span class="token keyword">lambda</span> line<span class="token punctuation">:</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd2<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>['hadoop', 'spark', 'hadoop', 'spark', 'hadoop', 'hadoop', 'hadoop', 'flink', 'spark']
</code></pre> 
<blockquote> 
 <p>注意：flatMap只适合用于有“嵌套”的rdd，直接用于没有嵌套的rdd会报错</p> 
</blockquote> 
<h4><a id="243_reduceByKey_375"></a>2.4.3 reduceByKey算子</h4> 
<p>功能：针对KV型的RDD，自动按照key分组，然后根据你提供的聚合逻辑，完成组内数据（value）的聚合操作。</p> 
<p>用法：</p> 
<pre><code class="prism language-python">rdd<span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span>func<span class="token punctuation">)</span>
<span class="token comment"># func:(V,V) ——&gt;V</span>
<span class="token comment"># 接收2个传入参数（类型要一致），返回一个返回值，类型和传入要求一致。</span>
</code></pre> 
<p><code>reduceByKey</code>的聚合逻辑是：</p> 
<p>比如，有<code>[1,2,3,4,5]</code>,然后聚合函数是：<code>lambda a,b: a+ b</code></p> 
<p><img src="https://images2.imgbox.com/6b/00/liThonBh_o.png" alt="image-20230729165607077"></p> 
<blockquote> 
 <p>注意：reduceByKey中接收的函数，只负责聚合，不理会分组</p> 
 <p>分组是自动 <code>byKey</code>来分组的。</p> 
</blockquote> 
<p>代码演示：<code>06_operators_reduceByKey.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>
<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext


<span class="token keyword">def</span> <span class="token function">addNum</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> data <span class="token operator">*</span> <span class="token number">10</span>


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    rdd2 <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    rdd3 <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    rdd <span class="token operator">=</span> rdd<span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span><span class="token keyword">lambda</span> a<span class="token punctuation">,</span> b<span class="token punctuation">:</span> a <span class="token operator">+</span> b<span class="token punctuation">)</span>

    rdd2 <span class="token operator">=</span> rdd2<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># 只操作value的算子</span>
    rdd3 <span class="token operator">=</span> rdd3<span class="token punctuation">.</span>mapValues<span class="token punctuation">(</span><span class="token keyword">lambda</span> value<span class="token punctuation">:</span> value <span class="token operator">*</span> <span class="token number">10</span><span class="token punctuation">)</span>

    <span class="token comment"># recudeByKey 对相同key的数据执行聚合相加</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd2<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd3<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>[('a', 3), ('b', 2)]
[('a', 10), ('a', 110), ('b', 30), ('b', 10), ('a', 50)]
[('a', 10), ('a', 110), ('b', 30), ('b', 10), ('a', 50)]
</code></pre> 
<h4><a id="244_WordCount_438"></a>2.4.4 WordCount回顾</h4> 
<p>代码演示：<code>07_wordcount_example.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkContext<span class="token punctuation">,</span> SparkConf

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
	<span class="token comment"># 构建SparkConf对象</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    <span class="token comment"># 构建SparkContext执行环境入口对象</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    <span class="token comment"># 1.读取文件获取数据 构建RDD</span>
    file_rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">r"../data/input/words.txt"</span><span class="token punctuation">)</span>

    <span class="token comment"># 2. 通过flatMap API取出所有的单词</span>
    word_rdd <span class="token operator">=</span> file_rdd<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 3.将单词转换成元组，key是单词，value是1</span>
    word_with_one_rdd <span class="token operator">=</span> word_rdd<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> word<span class="token punctuation">:</span><span class="token punctuation">(</span>word<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 4. 用reduceByKey 对单词进行分组并进行value的聚合</span>
    result_rdd <span class="token operator">=</span> word_with_one_rdd<span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span><span class="token keyword">lambda</span> a<span class="token punctuation">,</span>b<span class="token punctuation">:</span>a<span class="token operator">+</span>b<span class="token punctuation">)</span>

    <span class="token comment"># 5. 通过collect算子，将rdd的数据收集到Driver中，打印输出</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>result_rdd<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>[('hadoop', 1), ('hello', 3), ('spark', 1), ('flink', 1)]
</code></pre> 
<h4><a id="245_groupBy_475"></a>2.4.5 groupBy算子</h4> 
<p>功能：将rdd的数据进行分组</p> 
<p>语法：</p> 
<pre><code>rdd.groupBy(func)
# func 函数
# func:(T)——&gt;k
# 函数要求传入一个参数，返回一个返回值，类型无所谓
# 这个函数是 拿到你返回值后，将所有相同返回值的放入一个组中
# 分组完成后，每一个组是一个二元元组，key就是返回值，所有同组的数据放入一个迭代器对象中作为value
</code></pre> 
<p>代码演示：<code>08_oprators_groupBy.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token comment"># 通过groupBy对数据进行分组</span>
    <span class="token comment"># groupBy传入的函数的意思是：通过这个函数，确定按照谁来分组(返回谁即可)</span>
    <span class="token comment"># 分组规则和SQL是一致的，也就是相同的在一个组（Hash分组）</span>
    result <span class="token operator">=</span> rdd<span class="token punctuation">.</span>groupBy<span class="token punctuation">(</span><span class="token keyword">lambda</span> t<span class="token punctuation">:</span> t<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"hello"</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> t<span class="token punctuation">:</span> <span class="token punctuation">(</span>t<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">list</span><span class="token punctuation">(</span>t<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>[('a', &lt;pyspark.resultiterable.ResultIterable object at 0x7f85fa80eca0&gt;), ('b', &lt;pyspark.resultiterable.ResultIterable object at 0x7f85fa80ebb0&gt;)]
hello
[('a', [('a', 1), ('a', 1)]), ('b', [('b', 1), ('b', 1), ('b', 1)])]
</code></pre> 
<h4><a id="246_Filter_520"></a>2.4.6 Filter算子</h4> 
<p>功能：过滤，把想要的数据进行保留</p> 
<p>语法：</p> 
<pre><code class="prism language-python">rdd<span class="token punctuation">.</span><span class="token builtin">filter</span><span class="token punctuation">(</span>func<span class="token punctuation">)</span>
<span class="token comment"># func:(T)——&gt;bool 传入1个随意类型参数进来，返回值必须是True or False</span>
</code></pre> 
<blockquote> 
 <p>返回值是True的数据被保留，False的数据被丢弃</p> 
</blockquote> 
<p>代码演示：<code>09_operators_filter.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token comment"># 通过Filter算子，过滤奇数,filter 只返回true的值</span>
    result <span class="token operator">=</span> rdd<span class="token punctuation">.</span><span class="token builtin">filter</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>[1, 3, 5]
</code></pre> 
<h4><a id="247_distinct_558"></a>2.4.7 distinct算子</h4> 
<p>功能：对RDD数据进行去重，返回新的RDD</p> 
<p>语法：</p> 
<pre><code class="prism language-python">rdd<span class="token punctuation">.</span>distinct<span class="token punctuation">(</span>参数<span class="token number">1</span><span class="token punctuation">)</span>
<span class="token comment"># 参数1，去重分区数量，一般不用传</span>
</code></pre> 
<p>演示代码：<code>10_operators_distinct.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token comment"># distinct 进行RDD数据去重操作</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span>distinct<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    rdd2 <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd2<span class="token punctuation">.</span>distinct<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>[1, 2, 3]
[('a', 3), ('a', 1)]
</code></pre> 
<h4><a id="248_union_597"></a>2.4.8 union算子</h4> 
<p>功能：2个rdd合并成1个rdd返回</p> 
<p>用法：<code>rdd.union(other_rdd)</code></p> 
<blockquote> 
 <p>注意：只合并，不会去重</p> 
</blockquote> 
<p>代码演示：<code>11_operators_union.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd1 <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    rdd2 <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"a"</span><span class="token punctuation">,</span><span class="token string">"b"</span><span class="token punctuation">,</span><span class="token string">"a"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    rdd3 <span class="token operator">=</span> rdd1<span class="token punctuation">.</span>union<span class="token punctuation">(</span>rdd2<span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd3<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd3<span class="token punctuation">.</span>distinct<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">"""
1. 可以看到union算子是不会去重的
2. RDD的类型不同也是可以合并的
"""</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>[1, 1, 3, 3, 'a', 'b', 'a']
[1, 3, 'b', 'a']
</code></pre> 
<h4><a id="249_join_637"></a>2.4.9 join算子</h4> 
<p>功能：对两个RDD执行JOIN操作（可实现SQL的内、外连接）</p> 
<blockquote> 
 <p>注意：join算子只能用于二元元组</p> 
</blockquote> 
<p>语法：</p> 
<pre><code class="prism language-python">rdd<span class="token punctuation">.</span>join<span class="token punctuation">(</span>other_rdd<span class="token punctuation">)</span> <span class="token comment">#内连接</span>
rdd<span class="token punctuation">.</span>leftOuterJoin<span class="token punctuation">(</span>other_rdd<span class="token punctuation">)</span> <span class="token comment"># 左外</span>
rdd<span class="token punctuation">.</span>rightOuterJoin<span class="token punctuation">(</span>other_rdd<span class="token punctuation">)</span> <span class="token comment"># 右外</span>
</code></pre> 
<p>代码演示：<code>12_operators_join.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd1 <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1001</span><span class="token punctuation">,</span> <span class="token string">"张三"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1002</span><span class="token punctuation">,</span> <span class="token string">'李四'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1003</span><span class="token punctuation">,</span> <span class="token string">'王五'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1004</span><span class="token punctuation">,</span> <span class="token string">'赵六'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    rdd2 <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1001</span><span class="token punctuation">,</span> <span class="token string">"销售部"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1002</span><span class="token punctuation">,</span> <span class="token string">'科技部'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token comment"># 通过join算子来进行rdd之间的关联</span>
    <span class="token comment"># 对于join算子来说 关联条件 按照二元元组的key来进行关联</span>

    <span class="token comment"># 内连接</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd1<span class="token punctuation">.</span>join<span class="token punctuation">(</span>rdd2<span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 左外连接</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd1<span class="token punctuation">.</span>leftOuterJoin<span class="token punctuation">(</span>rdd2<span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 右外连接</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd1<span class="token punctuation">.</span>rightOuterJoin<span class="token punctuation">(</span>rdd2<span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
</code></pre> 
<p>输出结果：</p> 
<pre><code>[(1001, ('张三', '销售部')), (1002, ('李四', '科技部'))]
[(1001, ('张三', '销售部')), (1002, ('李四', '科技部')), (1003, ('王五', None)), (1004, ('赵六', None))]
[(1001, ('张三', '销售部')), (1002, ('李四', '科技部'))]
</code></pre> 
<h4><a id="2410_intersection__687"></a>2.4.10 intersection 算子</h4> 
<p>功能：求2个rdd的交集，返回一个新rdd</p> 
<p>用法：<code>rdd.intersection(other_rdd)</code></p> 
<p>代码演示：<code>13_operators_intersection.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd1 <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    rdd2 <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token comment"># 通过intersection算子求RDD之间的交集，将交集取出，返回新RDD</span>
    rdd3 <span class="token operator">=</span> rdd1<span class="token punctuation">.</span>intersection<span class="token punctuation">(</span>rdd2<span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd3<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>[('a', 1)]
</code></pre> 
<h4><a id="2411_glom_719"></a>2.4.11 glom算子</h4> 
<p>功能：将RDD的数据，加上嵌套，这个嵌套按照分区来进行</p> 
<p>比如RDD数据<code>[1,2,3,4,5]</code>有两个分区</p> 
<p>那么，被glom后，数据变成：<code>[[1,2,3],[4,5]]</code></p> 
<p>使用方法：<code>rdd.glom()</code></p> 
<p>代码演示：<code>14_operators_glom.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>
    rdd2 <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span>glom<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span>glom<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 用flatMap解嵌套</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd2<span class="token punctuation">.</span>glom<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
[[1], [2], [3], [4, 5], [6], [7], [8], [9, 10]]
</code></pre> 
<h4><a id="2412_groupByKey_756"></a>2.4.12 groupByKey算子</h4> 
<p>功能：针对KV型RDD，自动按照key分组</p> 
<p>用法：<code>rdd.groupByKey()</code> 自动按照key分组</p> 
<p>代码演示：<code>15_operators_groupByKey.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    rdd2 <span class="token operator">=</span> rdd<span class="token punctuation">.</span>groupByKey<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd2<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span><span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token builtin">list</span><span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>[('a', [1, 1, 1]), ('b', [1, 1])]
</code></pre> 
<h4><a id="2413_sortBy_786"></a>2.4.13 sortBy算子</h4> 
<p>功能：对RDD数据进行排序，基于你指定的排序依据</p> 
<p>语法：</p> 
<pre><code class="prism language-python">rdd<span class="token punctuation">.</span>sortBy<span class="token punctuation">(</span>func<span class="token punctuation">,</span>ascending<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>numPartitions<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token comment"># func:(T)——&gt;U:告知按照rdd中的哪个数据进行排序，比如lambda x:x[1] 表示按照rdd中的第二列元素进行排序</span>
<span class="token comment"># ascending = True升序；False 降序</span>
<span class="token comment"># numPartition：用多少分区来排序</span>
</code></pre> 
<blockquote> 
 <p>注意：如果要全局有序，排序分区数请设置为1，因为生产环境下，分区数大于1，很可能只得到局部有序的结果</p> 
</blockquote> 
<p>代码演示：<code>16_operators_sortBy.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'g'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'c'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'h'</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'i'</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'l'</span><span class="token punctuation">,</span> <span class="token number">26</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'o'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'d'</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token comment"># 使用sortBy对rdd进行排序</span>
    <span class="token comment"># 参数1 函数，表示的是，告诉spark按照数据的哪个列进行排序</span>
    <span class="token comment"># 参数2 bool，True表示升序，False表示降序</span>
    <span class="token comment"># 参数3 分区数设置</span>

    <span class="token triple-quoted-string string">"""注意：如果要全局有序，排序分区数请设置为1，因为生产环境下，分区数大于1，很可能只得到局部有序的结果"""</span>
    rdd2 <span class="token operator">=</span> rdd<span class="token punctuation">.</span>sortBy<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span>x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>ascending<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>numPartitions<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>
    rdd3 <span class="token operator">=</span> rdd<span class="token punctuation">.</span>sortBy<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span>x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>ascending<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>numPartitions<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span>


    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd2<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd3<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>[('c', 1), ('o', 1), ('b', 2), ('g', 3), ('i', 4), ('d', 7), ('a', 9), ('h', 10), ('l', 26)]
[('a', 9), ('b', 2), ('c', 1), ('d', 7), ('g', 3), ('h', 10), ('i', 4), ('l', 26), ('o', 1)]
</code></pre> 
<h4><a id="2414_sortByKey_835"></a>2.4.14 sortByKey</h4> 
<p>功能：针对KV型RDD，按照key进行排序</p> 
<p>语法：</p> 
<p><code>sortByKey(ascending=True,numPartitions=None,keyfunc=&lt;function RDD,&lt;lambda&gt;&gt;)</code></p> 
<ul><li>ascending:升序或降序，True升序，False降序，默认是升序</li><li>numPartitions：按照几个分区进行排序，如果全局有序，设置为1</li><li>keyfunc：在排序前对key进行处理，语法是：(k)——&gt;U,一个参数传入，返回一个值</li></ul> 
<p>代码演示：<code>17_operators_sortByKey.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'g'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'A'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'B'</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'A'</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'h'</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'i'</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'l'</span><span class="token punctuation">,</span> <span class="token number">26</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'o'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'d'</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
	<span class="token comment"># 调用了忽略大小写的函数</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span>sortByKey<span class="token punctuation">(</span>ascending<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> numPartitions<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keyfunc<span class="token operator">=</span><span class="token keyword">lambda</span> key<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">(</span>key<span class="token punctuation">)</span><span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>[('A', 1), ('A', 9), ('B', 2), ('d', 7), ('g', 3), ('h', 10), ('i', 4), ('l', 26), ('o', 1)]
</code></pre> 
<h4><a id="2415__869"></a>2.4.15 综合案例</h4> 
<p><img src="https://images2.imgbox.com/92/e7/hq3kL9Rr_o.png" alt="image-20230729223615036"></p> 
<p>代码演示：<code>18_operators_demo.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">import</span> json

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    <span class="token comment"># 读取数据文件</span>
    file_rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"../data/input/order.text"</span><span class="token punctuation">)</span>

    <span class="token comment"># 进行rdd数据的split 按照|符号进行，得到一个json数据</span>
    jsons_rdd <span class="token operator">=</span> file_rdd<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span><span class="token keyword">lambda</span> line<span class="token punctuation">:</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"|"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 通过python内置的json库，完成json字符串到字典对象的转换</span>
    dict_rdd <span class="token operator">=</span> jsons_rdd<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> json_str<span class="token punctuation">:</span> json<span class="token punctuation">.</span>loads<span class="token punctuation">(</span>json_str<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 过滤数据，只保留北京的数据</span>
    beijing_rdd <span class="token operator">=</span> dict_rdd<span class="token punctuation">.</span><span class="token builtin">filter</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> d<span class="token punctuation">:</span> d<span class="token punctuation">[</span><span class="token string">'areaName'</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'北京'</span><span class="token punctuation">)</span>

    <span class="token comment"># 组合北京和商品类型形成的字符串</span>
    category_rdd <span class="token operator">=</span> beijing_rdd<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token string">'areaName'</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">'_'</span> <span class="token operator">+</span> x<span class="token punctuation">[</span><span class="token string">'category'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token comment"># 对结果集进行去重操作</span>
    result_rdd <span class="token operator">=</span> category_rdd<span class="token punctuation">.</span>distinct<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 输出</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>result_rdd<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>['北京_平板电脑', '北京_家具', '北京_书籍', '北京_食品', '北京_服饰', '北京_手机', '北京_家电', '北京_电脑']
</code></pre> 
<h4><a id="2416_yarn_914"></a>2.4.16 将案例提交到yarn运行</h4> 
<p><strong>改动1</strong>：加入环境变量，让pycharm运行yarn的时候，知道hadoop的配置在哪，可以去读取yarn的信息</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> os
<span class="token keyword">from</span> defs_19 <span class="token keyword">import</span> city_with_category
<span class="token comment"># 导入自己写的函数时，把文件夹设置为SourceRoot就不会报错了</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'HADOOP_CONF_DIR'</span><span class="token punctuation">]</span><span class="token operator">=</span> <span class="token string">"/export/server/hadoop/etc/hadoop"</span>
</code></pre> 
<p><strong>改动2</strong>：在集群上运行，本地文件就不可以用了，需要用hdfs文件</p> 
<pre><code class="prism language-python">    <span class="token comment"># 在集群中运行，我们需要用HDFS路径，不能用本地路径</span>
    file_rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"hdfs://Tnode1:8020/input/order.text"</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>改动3</strong>：</p> 
<pre><code class="prism language-python">    <span class="token triple-quoted-string string">"""
     如果提交到集群运行，除了主代码以外，还依赖了其它的代码文件
     需要设置一个参数，来告知spark，还有依赖文件要同步上传到集群中
     参数叫做：spark.submit.pyFiles
     参数的值可以是单个.py文件，也可以是.zip压缩包（有多个依赖文件的时候可以用zip压缩后上传）
    """</span>
    conf<span class="token punctuation">.</span><span class="token builtin">set</span><span class="token punctuation">(</span><span class="token string">"spark.submit.pyFiles"</span><span class="token punctuation">,</span><span class="token string">"defs_19.py"</span><span class="token punctuation">)</span>
</code></pre> 
<p>完整代码：<code>19_operators_runOnYarn.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">import</span> json

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext
<span class="token keyword">import</span> os
<span class="token keyword">from</span> defs_19 <span class="token keyword">import</span> city_with_category
<span class="token comment"># 导入自己写的函数时，把文件夹设置为SourceRoot就不会报错了</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'HADOOP_CONF_DIR'</span><span class="token punctuation">]</span><span class="token operator">=</span> <span class="token string">"/export/server/hadoop/etc/hadoop"</span>
<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    <span class="token comment"># 提交到yarn集群，master设置为yarn</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"SparkDemo01"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"yarn"</span><span class="token punctuation">)</span>

    <span class="token triple-quoted-string string">"""
     如果提交到集群运行，除了主代码以外，还依赖了其它的代码文件
     需要设置一个参数，来告知spark，还有依赖文件要同步上传到集群中
     参数叫做：spark.submit.pyFiles
     参数的值可以是单个.py文件，也可以是.zip压缩包（有多个依赖文件的时候可以用zip压缩后上传）
    """</span>
    conf<span class="token punctuation">.</span><span class="token builtin">set</span><span class="token punctuation">(</span><span class="token string">"spark.submit.pyFiles"</span><span class="token punctuation">,</span><span class="token string">"defs_19.py"</span><span class="token punctuation">)</span>

    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    <span class="token comment"># 在集群中运行，我们需要用HDFS路径，不能用本地路径</span>
    file_rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"hdfs://Tnode1:8020/input/order.text"</span><span class="token punctuation">)</span>

    <span class="token comment"># 进行rdd数据的split 按照|符号进行，得到一个json数据</span>
    jsons_rdd <span class="token operator">=</span> file_rdd<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span><span class="token keyword">lambda</span> line<span class="token punctuation">:</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"|"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 通过python内置的json库，完成json字符串到字典对象的转换</span>
    dict_rdd <span class="token operator">=</span> jsons_rdd<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> json_str<span class="token punctuation">:</span> json<span class="token punctuation">.</span>loads<span class="token punctuation">(</span>json_str<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 过滤数据，只保留北京的数据</span>
    beijing_rdd <span class="token operator">=</span> dict_rdd<span class="token punctuation">.</span><span class="token builtin">filter</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> d<span class="token punctuation">:</span> d<span class="token punctuation">[</span><span class="token string">'areaName'</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'北京'</span><span class="token punctuation">)</span>

    <span class="token comment"># 组合北京和商品类型形成的字符串</span>
    category_rdd <span class="token operator">=</span> beijing_rdd<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>city_with_category<span class="token punctuation">)</span>

    <span class="token comment"># 对结果集进行去重操作</span>
    result_rdd <span class="token operator">=</span> category_rdd<span class="token punctuation">.</span>distinct<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 输出</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>result_rdd<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>依赖代码：<code>defs_19.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">def</span> <span class="token function">city_with_category</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> data<span class="token punctuation">[</span><span class="token string">'areaName'</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">'_'</span> <span class="token operator">+</span>data<span class="token punctuation">[</span><span class="token string">'category'</span><span class="token punctuation">]</span>

</code></pre> 
<p>输出结果：</p> 
<pre><code>['北京_书籍', '北京_食品', '北京_服饰', '北京_平板电脑', '北京_家具', '北京_手机', '北京_家电', '北京_电脑']
</code></pre> 
<p>在服务器上通过spark-submit 提交到集群运行</p> 
<pre><code class="prism language-python"><span class="token comment"># --py-files 可以帮你指定你依赖的其它python代码，支持.zip(一堆)，也可以单个.py文件都行。</span>
<span class="token operator">/</span>export<span class="token operator">/</span>server<span class="token operator">/</span>spark<span class="token operator">/</span><span class="token builtin">bin</span><span class="token operator">/</span>spark<span class="token operator">-</span>submit <span class="token operator">-</span><span class="token operator">-</span>master yarn <span class="token operator">-</span><span class="token operator">-</span>py<span class="token operator">-</span>files <span class="token punctuation">.</span><span class="token operator">/</span>defs<span class="token punctuation">.</span>py <span class="token punctuation">.</span><span class="token operator">/</span>main<span class="token punctuation">.</span>py

</code></pre> 
<p>服务器上程序运行结果：</p> 
<p><img src="https://images2.imgbox.com/8c/7d/saXT7SZM_o.png" alt="image-20230729230827582"></p> 
<blockquote> 
 <p>注意，在服务器上跑时，需要把conf中的setMaster去掉</p> 
 <p>即conf = SparkConf().setAppName(“SparkDemo01”).setMaster(“yarn”)改为：</p> 
 <p>conf = SparkConf().setAppName(“SparkDemo01”)</p> 
</blockquote> 
<h3><a id="25_Action_1026"></a>2.5 常用Action算子</h3> 
<h4><a id="251_countByKey_1028"></a>2.5.1 countByKey算子</h4> 
<p>功能：统计key出现的次数（一般适用于KV型的RDD）</p> 
<p>代码演示：<code>20_operators_countByKey.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">import</span> json

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"../data/input/words.txt"</span><span class="token punctuation">)</span>
    rdd2 <span class="token operator">=</span> rdd<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span>x<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 通过countByKey来对key进行计数，这是一个Action算子</span>
    result <span class="token operator">=</span> rdd2<span class="token punctuation">.</span>countByKey<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">[</span><span class="token string">"hello"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>defaultdict(&lt;class 'int'&gt;, {'hello': 3, 'spark': 1, 'hadoop': 1, 'flink': 1})
['hello', 'spark', 'hadoop', 'flink']
3
&lt;class 'collections.defaultdict'&gt;
</code></pre> 
<h4><a id="252_collect_1066"></a>2.5.2 collect算子</h4> 
<p>功能：将RDD各个分区内的数据，统一收集到Driver中，形成一个List对象</p> 
<p>用法：<code>rdd.collect()</code></p> 
<blockquote> 
 <p>这个算子，是将RDD各个分区数据都拉取到Driver</p> 
 <p>注意的是，RDD是分布式对象，其数据量可以很大，</p> 
 <p>所以用这个算子之前要心知肚明地了解 结果数据集不会太大。</p> 
 <p>不然，会把Driver内存撑爆</p> 
</blockquote> 
<h4><a id="253_reduce_1080"></a>2.5.3 reduce算子</h4> 
<p>功能：对RDD数据集按照你传入的逻辑进行聚合</p> 
<p>语法：</p> 
<pre><code class="prism language-scala">rdd<span class="token punctuation">.</span>reduce<span class="token punctuation">(</span>func<span class="token punctuation">)</span>
# func<span class="token operator">:</span><span class="token punctuation">(</span>T<span class="token punctuation">,</span>T<span class="token punctuation">)</span>——<span class="token operator">&gt;</span>T
# <span class="token number">2</span>参数传入<span class="token number">1</span>个返回值，返回值要和参数要求类型一致
</code></pre> 
<p>代码演示：<code>21_operators_reduce.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">import</span> json

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span><span class="token builtin">reduce</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> a<span class="token punctuation">,</span> b<span class="token punctuation">:</span> a <span class="token operator">+</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>15
</code></pre> 
<h4><a id="254_fold_1116"></a>2.5.4 fold算子</h4> 
<p>功能：和reduce一样，接收传入逻辑进行聚合，聚合是带有初始值的，</p> 
<p>这个初始值聚合会作用在：</p> 
<ul><li>分区内聚合</li><li>分区间聚合</li></ul> 
<p>比如：<code>[[1,2,3],[4,5,6],[7,8,9]]</code></p> 
<p>数据量分布在3个分区</p> 
<p>分区1： 1、2、3 聚合的时候带上10作为初始值得到16</p> 
<p>分区3： 4、5、6 聚合的时候带上10作为初始值得到25</p> 
<p>分区4： 7、8、9 聚合的时候带上10作为初始值得到34</p> 
<p>3个分区的结果做聚合也带上初始值10，所以结果是10+16+25+34 = 85</p> 
<p>代码演示：<code>22_operators_fold.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">import</span> json

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span>glom<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span>fold<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token keyword">lambda</span> a<span class="token punctuation">,</span> b<span class="token punctuation">:</span> a <span class="token operator">+</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>[[1, 2, 3], [4, 5, 6], [7, 8, 9]]
85
</code></pre> 
<h4><a id="255_first_1164"></a>2.5.5 first算子</h4> 
<p>功能：取出RDD的第一个元素</p> 
<p>用法</p> 
<pre><code class="prism language-scala">sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>first<span class="token punctuation">(</span><span class="token punctuation">)</span>
输出：<span class="token number">3</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/de/e6/YQULVfw2_o.png" alt="image-20230730174705629"></p> 
<h4><a id="256_take_1177"></a>2.5.6 take算子</h4> 
<p>功能：取RDD的前N个元素。组合成list返回给你</p> 
<p>用法：</p> 
<pre><code>&gt;&gt;&gt; sc.parallelize([3,2,1,4,5,6]).take(5)
[3, 2, 1, 4, 5]
</code></pre> 
<p><img src="https://images2.imgbox.com/4a/a5/2Qp5c7dX_o.png" alt="image-20230730210309986"></p> 
<h4><a id="257_top_1190"></a>2.5.7 top算子</h4> 
<p>功能：对RDD数据集进行降序排序，取前N个</p> 
<p>用法：</p> 
<pre><code>&gt;&gt;&gt; sc.parallelize([3,2,1,4,5,6]).top(3) # 表示取降序前3个
[6, 5, 4]
</code></pre> 
<p><img src="https://images2.imgbox.com/ac/f8/YwMi6cT2_o.png" alt="image-20230730220000263"></p> 
<h4><a id="258_count_1203"></a>2.5.8 count算子</h4> 
<p>功能：计算RDD有多少条数据，返回值是一个数字</p> 
<p>用法：</p> 
<pre><code>&gt;&gt;&gt; sc.parallelize([3,2,1,4,5,6]).count()
6
</code></pre> 
<p><img src="https://images2.imgbox.com/63/5d/xLdPVsyr_o.png" alt="image-20230730220111487"></p> 
<h4><a id="259_takeSample_1216"></a>2.5.9 takeSample算子</h4> 
<p>功能：随机抽样RDD的数据</p> 
<p>用法：</p> 
<pre><code>takeSample(参数1：True or False，参数2：采样数，参数3：随机数种子)
- 参数1：True表示允许取同一个数据，False表示不允许取同一个数据，和数据内容无关，是否重复表示的是同一个位置的数据（有、无放回抽样）
- 参数2：抽样要几个
- 参数3：随机数种子，这个参数传入一个数字即可，随意给
</code></pre> 
<blockquote> 
 <p>随机数种子 数字可以随便传，如果传同一个数字 那么取出的结果是一致的。</p> 
 <p>一般参数3 我们不传，Spark会自动给与随机的种子。</p> 
</blockquote> 
<p>代码演示：<code>23_operators_takeSample.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">import</span> json

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
    result <span class="token operator">=</span> rdd<span class="token punctuation">.</span>takeSample<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token comment"># 随机抽样可以抽出相同的数据，只是位置不同而已</span>
    <span class="token comment"># 随机数种子能让随机数不再继续发生变化</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>[2, 7, 6, 6, 3]
</code></pre> 
<blockquote> 
 <p>注意：</p> 
 <p>随机抽样可以抽出相同的数据，只是位置不同而已</p> 
 <p>随机数种子能让随机数不再继续发生变化</p> 
</blockquote> 
<h4><a id="2510_takeOrdered_1267"></a>2.5.10 takeOrdered</h4> 
<p>功能：对RDD进行排序取前N个</p> 
<p>用法：</p> 
<pre><code>rdd.takeOrdered(参数1，参数2)
- 参数1 要几个数据
- 参数2 对排序的数据进行更改（不会更改数据本身，只是在排序的时候换个样子）
这个方法按照元素自然顺序升序排序，如果你想玩倒叙，需要参数2 来对排序的数据进行处理
</code></pre> 
<p>代码演示：<code>24_operators_takeOrdered.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">import</span> json

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span>takeOrdered<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span>takeOrdered<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>[1, 2, 3]
[9, 7, 6]
</code></pre> 
<h4><a id="2511_foreach_1307"></a>2.5.11 foreach算子</h4> 
<p>功能：对RDD的每一个元素，执行你提供的逻辑的操作（和map一个思想），但是这个方法没有返回值</p> 
<p>用法：</p> 
<pre><code>rdd.foreach(func)
# func:(T) ——&gt; None
</code></pre> 
<p>代码演示：<code>25_operators_foreach.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">import</span> json

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

    rdd<span class="token punctuation">.</span>foreach<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token keyword">print</span><span class="token punctuation">(</span>x <span class="token operator">*</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>10
30
20
40
70
90
60
</code></pre> 
<h4><a id="2512_saveAsTextFile_1348"></a>2.5.12 saveAsTextFile</h4> 
<p>功能：将RDD的数据写入文本文件中</p> 
<p>支持本地写出，hdfs等文件系统</p> 
<p>代码演示：</p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">import</span> json

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>

    rdd<span class="token punctuation">.</span>saveAsTextFile<span class="token punctuation">(</span><span class="token string">"hdfs://Tnode1:8020/test/output/out1"</span><span class="token punctuation">)</span>
</code></pre> 
<p>运行结果：</p> 
<p><img src="https://images2.imgbox.com/3c/6a/35346kjQ_o.png" alt="image-20230730223526498"></p> 
<p><img src="https://images2.imgbox.com/57/96/D1Aio75Q_o.png" alt="image-20230730223606509"></p> 
<p><img src="https://images2.imgbox.com/6a/78/KITSWbsh_o.png" alt="image-20230730223545347"></p> 
<blockquote> 
 <p>注意：保存文件API，是分布式执行的</p> 
 <p>这个API的执行数据是不经过driver的</p> 
 <p><img src="https://images2.imgbox.com/49/60/9z9bZAZC_o.png" alt="image-20230730223742342"></p> 
 <p>如图，写出的时候，每个分区所在的Executor直接控制数据写出到目标文件系统中</p> 
 <p>所有才会一个分区产生一个结果文件</p> 
</blockquote> 
<h4><a id="2513__1390"></a>2.5.13 注意点</h4> 
<p>我们学习的action中：</p> 
<ul><li>foreach</li><li>saveAsTextFile</li></ul> 
<p>这两个算子是分区（Executor）直接执行的，跳过Driver，由分区所在的Executor直接执行</p> 
<p>反之：其余的Action算子都会将结果发送至Driver</p> 
<h3><a id="26__1401"></a>2.6 分区操作算子</h3> 
<h4><a id="261_mapPartitions_1403"></a>2.6.1 mapPartitions算子</h4> 
<p><strong>transformation算子</strong></p> 
<p>图解：</p> 
<p><img src="https://images2.imgbox.com/2f/ff/nToLKhZE_o.png" alt="image-20230730224303769"></p> 
<p>如图，mapPartition一次被传递的是一整个分区的数据</p> 
<p>作为一个迭代器（一次性list）对象传入过来。</p> 
<p>代码演示：<code>27_operators_mapPartitions.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">import</span> json

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">import</span> time

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    start_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>

    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    <span class="token comment"># 效果和map一样，但是性能比map好，cpu计算没有省，但是网络IO少很多</span>
    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">process</span><span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        result <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> it <span class="token keyword">in</span> <span class="token builtin">iter</span><span class="token punctuation">:</span>
            result<span class="token punctuation">.</span>append<span class="token punctuation">(</span>it<span class="token operator">*</span><span class="token number">10</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> result


    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span>mapPartitions<span class="token punctuation">(</span>process<span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

   <span class="token comment"># print(rdd.map(lambda x:x*10).collect())</span>

    end_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>
    gap_time <span class="token operator">=</span> <span class="token punctuation">(</span>end_time <span class="token operator">-</span> start_time<span class="token punctuation">)</span>
    gap_time <span class="token operator">=</span> <span class="token builtin">round</span><span class="token punctuation">(</span>gap_time<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>  <span class="token comment"># 保留四位小数</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"执行本程序共耗时："</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>gap_time<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"s"</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>[10, 30, 20, 40, 70, 90, 60]
执行本程序共耗时：8.0515s
</code></pre> 
<blockquote> 
 <p>注意：效果和map一样，但是性能比map好，cpu计算没有省，但是网络IO少很多</p> 
</blockquote> 
<h4><a id="262_foreachPartition_1463"></a>2.6.2 foreachPartition算子</h4> 
<p><strong>Action</strong>算子</p> 
<p>功能：和普通foreach一致，一次处理的是一整个分区数据</p> 
<p>代码演示：<code>28_operators_foreachPartitions.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">import</span> json

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">process</span><span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        result <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> it <span class="token keyword">in</span> <span class="token builtin">iter</span><span class="token punctuation">:</span>
            result<span class="token punctuation">.</span>append<span class="token punctuation">(</span>it<span class="token operator">*</span><span class="token number">10</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span>


    rdd<span class="token punctuation">.</span>foreachPartition<span class="token punctuation">(</span>process<span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>[70, 90, 60]
[10, 30]
[20, 40]
</code></pre> 
<blockquote> 
 <p>foreachPartition 就是一个没有返回值的mapPartitions</p> 
</blockquote> 
<h4><a id="263_partitionBy_1504"></a>2.6.3 partitionBy算子</h4> 
<p><strong>transformation算子</strong></p> 
<p>功能：对RDD进行自定义分区操作</p> 
<p>用法：</p> 
<pre><code class="prism language-python">rdd<span class="token punctuation">.</span>partitionBy<span class="token punctuation">(</span>参数<span class="token number">1</span>，参数<span class="token number">2</span><span class="token punctuation">)</span>
<span class="token operator">-</span> 参数<span class="token number">1</span> 重新分区后有几个分区
<span class="token operator">-</span> 参数<span class="token number">2</span> 自定义分区规则，函数传入

参数<span class="token number">2</span>：<span class="token punctuation">(</span>K<span class="token punctuation">)</span>——<span class="token operator">&gt;</span><span class="token builtin">int</span>
一个传入参数进来，类型无所谓，但是返回值一定是<span class="token builtin">int</span>类型，
将key传给这个函数，你自己写逻辑，决定返回一个分区编号

分区编号从<span class="token number">0</span>开始，不要超出分区数<span class="token operator">-</span><span class="token number">1</span>
</code></pre> 
<p>代码演示：<code>29_operators_partitionBy.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">import</span> json

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'hadoop'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'spark'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"hello"</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"flink"</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"hadoop"</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"spark"</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>


    <span class="token comment"># 使用partitionBy 自定义 分区</span>
    <span class="token keyword">def</span> <span class="token function">process</span><span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token string">'hadoop'</span> <span class="token operator">==</span> k <span class="token keyword">or</span> <span class="token string">'hello'</span> <span class="token operator">==</span> k<span class="token punctuation">:</span> <span class="token keyword">return</span> <span class="token number">0</span>
        <span class="token keyword">if</span> <span class="token string">'spark'</span> <span class="token operator">==</span> k<span class="token punctuation">:</span> <span class="token keyword">return</span> <span class="token number">1</span>
        <span class="token keyword">return</span> <span class="token number">2</span>


    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span>partitionBy<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> process<span class="token punctuation">)</span><span class="token punctuation">.</span>glom<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：分区依次为0、1、2</p> 
<pre><code>[[('hadoop', 1), ('hello', 1), ('hadoop', 1)], [('spark', 1), ('spark', 1)], [('flink', 1)]]
</code></pre> 
<blockquote> 
 <p>分区号不要超标，你设置3个分区，分区号只能是0 1 2</p> 
 <p>设置5个分区 分区号只能是0 1 2 3 4</p> 
</blockquote> 
<h4><a id="264_repartition_1560"></a>2.6.4 repartition算子</h4> 
<p><strong>transformation算子</strong></p> 
<p>功能：对RDD的分区执行重新分区（仅数量）</p> 
<p>用法：</p> 
<pre><code>rdd.repartition(N)
传入N 决定新的分区数
</code></pre> 
<p>代码演示：<code>30_operators_repartition_and_coalesce.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">import</span> json

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>


    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>

    <span class="token comment"># repartition 修改分区</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span>repartition<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getNumPartitions<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span>repartition<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getNumPartitions<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># coalesce 修改分区</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span>coalesce<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getNumPartitions<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span>coalesce<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span>shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getNumPartitions<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>1
5
1
5
</code></pre> 
<blockquote> 
 <p>注意：对分区的数量进行操作，一定要慎重</p> 
 <p>一般情况下，我们写spark代码除了要求全局排序设置为1个分区外，</p> 
 <p>多数时候，所有API中关于分区相关的代码我们都不太理会</p> 
</blockquote> 
<blockquote> 
 <p>因为，如果你改分区了</p> 
 <ol><li>会影响并行计算（内存迭代的并行管道数量）<code>后面学</code></li><li>分区如果增加，极大可能导致shuffle</li></ol> 
</blockquote> 
<h4><a id="265_coalesce_1621"></a>2.6.5 coalesce算子</h4> 
<p><strong>transformation算子</strong></p> 
<p>功能：对分区进行数量增减</p> 
<p>用法：</p> 
<pre><code class="prism language-python">rdd<span class="token punctuation">.</span>coalesce<span class="token punctuation">(</span>参数<span class="token number">1</span>，参数<span class="token number">2</span><span class="token punctuation">)</span>
<span class="token operator">-</span> 参数<span class="token number">1</span>，分区数
<span class="token operator">-</span> 参数<span class="token number">2</span>，<span class="token boolean">True</span> <span class="token keyword">or</span> <span class="token boolean">False</span>
<span class="token boolean">True</span>表示允许shuffle，也就是可以加分区
<span class="token boolean">False</span>表示不允许shuffle，也就是不能加分区，<span class="token boolean">False</span>是默认
</code></pre> 
<p>代码见2.6.4</p> 
<blockquote> 
 <p>对比repartition，一般使用coalesce较多，因为加分区要写参数2</p> 
 <p>这样避免写repartition的时候手抖了加分区了</p> 
</blockquote> 
<h4><a id="266_mapValues_1643"></a>2.6.6 mapValues算子</h4> 
<p><strong>Transformation算子</strong></p> 
<p>功能：针对二元元组RDD，对其内部的二元元组的Value执行map操作</p> 
<p>语法：</p> 
<pre><code class="prism language-python">rdd<span class="token punctuation">.</span>mapValues<span class="token punctuation">(</span>func<span class="token punctuation">)</span>
<span class="token comment"># func: (V)——&gt; U</span>
<span class="token comment"># 注意，传入的参数，是二元元组的 value值</span>
<span class="token comment"># 我们这个传入的方法，只对value进行处理</span>
</code></pre> 
<p>代码演示：</p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>
<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"create rdd"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># rdd.map(lambda x:(x[0],x[1]*10))</span>
    <span class="token comment"># 将二元元组的所有value都乘以10进行处理</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span>mapValues<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x <span class="token operator">*</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>[('a', 10), ('a', 110), ('a', 60), ('b', 30), ('b', 50)]
</code></pre> 
<h4><a id="267_join_1682"></a>2.6.7 join算子</h4> 
<p><strong>Transformation算子</strong></p> 
<p>功能：对两个RDD执行join操作（可以实现SQL的内、外连接）</p> 
<blockquote> 
 <p>注意：join算子只能用于二元元组</p> 
</blockquote> 
<p>代码见 2.4.9</p> 
<h3><a id="27__1692"></a>2.7 面试题</h3> 
<p><strong>groupByKey和reduceByKey的区别</strong></p> 
<p>在功能上的区别：</p> 
<ul><li><code>groupByKey</code>仅仅只有分组功能而已</li><li><code>reduceByKey</code>除了有<code>ByKey</code>的分组功能外，还有<code>reduce</code>聚合功能，所以是一个分组+聚合一体化的算子</li></ul> 
<blockquote> 
 <p>如果对数据执行分组+聚合，那么使用这2个算子的性能差别是很大的</p> 
 <p><code>reduceByKey</code>的性能是远大于：<code>groupByKey</code>+聚合逻辑的</p> 
</blockquote> 
<p>因为：</p> 
<p><img src="https://images2.imgbox.com/ce/83/Z41QucwU_o.png" alt="image-20230731161204010"></p> 
<p>如图，这是<code>groupByKey</code>+聚合逻辑的执行流程。</p> 
<p>因为，<code>groupByKey</code>只能分组，所以，执行上是先分组（shuffle）后聚合</p> 
<p>再来看<code>reduceByKey</code>：</p> 
<p><img src="https://images2.imgbox.com/35/59/CztRScAL_o.png" alt="image-20230731161424909"></p> 
<p>如图，reduceByKey由于自带聚合逻辑，所以可以完成：</p> 
<ol><li>先在分区内做预聚合</li><li>然后再走分组流程（shuffle）</li><li>分组后再做最终聚合</li></ol> 
<p>对于groupByKey，reduceByKey最大的提升在于，分组前进行了预聚合，那么在shuffle分组节点，被shuffle的数据可以极大地减少</p> 
<p>这就极大地提升了性能</p> 
<blockquote> 
 <p>分组+聚合，首选<code>reduceByKey</code>，数据越大，对groupByKey的优势就越高</p> 
</blockquote> 
<h3><a id="28__1729"></a>2.8 总结</h3> 
<ol><li><strong>RDD创建方式有哪几种方法？</strong></li></ol> 
<p>​ 通过并行化集合的方式（本地集合转分布式集合）</p> 
<p>​ 或者读取数据的方式创建（TextFile、WholeTextFile）</p> 
<ol start="2"><li><strong>RDD分区数如何查看？</strong></li></ol> 
<p>​ 通过getNumPartitions API查看，返回值Int</p> 
<ol start="3"><li> <p><strong>Transformation和Action的区别？</strong><br> 转换算子的返回值100%是RDD，而Action算子的返回值100%不是RDD</p> <p>转换算子是懒加载的，只有遇到Action才会执行，Action就是转换算子处理链条的开关。</p> </li><li> <p><strong>哪两个Action算子的结果不经过Driver，直接输出？</strong></p> </li></ol> 
<p>​ <code>foreach</code>和<code>saveAsTextFile</code> 直接由Executor执行后输出，不会将结果发送到Driver上去</p> 
<ol start="5"><li> <p><strong>reduceByKey和groupByKey的区别？</strong></p> <p>reduceByKey自带聚合逻辑，groupByKey不带</p> <p>如果做数据聚合reduceByKey的效率更好，因为可以先聚合后shuffle在最终聚合，传输的IO小</p> </li><li> <p><strong>mapPartitions和foreachPartition的区别？</strong></p> <p>mapPartitions带有返回值 foreachPartition不带</p> </li><li> <p><strong>对于分区操作有什么要注意的地方？</strong></p> <p>尽量不要增加分区，可能破坏内存迭代的计算管道</p> </li></ol> 
<h2><a id="3_RDD_1764"></a>3. RDD的持久化</h2> 
<h3><a id="31_RDD_1766"></a>3.1 RDD的数据是过程数据</h3> 
<p>RDD之间进行相互迭代计算（Transformation的转换），当执行开启后，新的RDD生成，代表老RDD的消失。</p> 
<p>RDD的数据是过程数据，只在处理的过程中存在，一旦处理完成，就不见了。</p> 
<blockquote> 
 <p>这个特性可以最大化地利用资源，老旧RDD没用了 就从内存中清理，给后续的计算腾出内存空间。</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/82/cd/w6P7WdXq_o.png" alt="image-20230731205816341"></p> 
<p>如上图，rdd3被2次使用，第一次使用之后，其实RDD3就不存在了。</p> 
<p>第二次使用的时候，只能基于RDD的血缘关系，从RDD1重新执行，构建出来RDD3，供RDD5使用。</p> 
<h3><a id="32_RDD_1780"></a>3.2 RDD的缓存</h3> 
<h4><a id="321__1782"></a>3.2.1 缓存</h4> 
<p>对于上述的场景，肯定要执行优化，优化就是：</p> 
<p>RDD3如果不消失，那么RDD1——&gt;RDD2——&gt;RDD3这个链条就不会执行2次，或者更多次</p> 
<p>RDD的缓存技术：Spark提供了缓存API，可以让我们通过调用APi，将指定的RDD数据保留在<code>内存或者硬盘</code>上</p> 
<p>缓存的API</p> 
<pre><code class="prism language-python"><span class="token comment"># RDD3 被2次使用，可以加入缓存进行优化</span>
rdd3<span class="token punctuation">.</span>cache<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 缓存到内存中</span>
rdd3<span class="token punctuation">.</span>persist<span class="token punctuation">(</span>StorageLevel<span class="token punctuation">.</span>MEMORY_ONLY<span class="token punctuation">)</span> <span class="token comment"># 仅内存缓存</span>
rdd3<span class="token punctuation">.</span>persist<span class="token punctuation">(</span>StorageLevel<span class="token punctuation">.</span>MEMORY_ONLY_2<span class="token punctuation">)</span> <span class="token comment"># 仅内存缓存,2个副本</span>
rdd3<span class="token punctuation">.</span>persist<span class="token punctuation">(</span>StorageLevel<span class="token punctuation">.</span>DISK_ONLY<span class="token punctuation">)</span> <span class="token comment"># 仅缓存硬盘上</span>
rdd3<span class="token punctuation">.</span>persist<span class="token punctuation">(</span>StorageLevel<span class="token punctuation">.</span>DISK_ONLY_2<span class="token punctuation">)</span> <span class="token comment"># 仅缓存硬盘上,2个副本</span>
rdd3<span class="token punctuation">.</span>persist<span class="token punctuation">(</span>StorageLevel<span class="token punctuation">.</span>DISK_ONLY_3<span class="token punctuation">)</span> <span class="token comment"># 仅缓存硬盘上,3个副本</span>
rdd3<span class="token punctuation">.</span>persist<span class="token punctuation">(</span>StorageLevel<span class="token punctuation">.</span>MEMORY_AND_DISK<span class="token punctuation">)</span> <span class="token comment"># 先放内存，不够放硬盘</span>
rdd3<span class="token punctuation">.</span>persist<span class="token punctuation">(</span>StorageLevel<span class="token punctuation">.</span>MEMORY_AND_DISK_2<span class="token punctuation">)</span> <span class="token comment"># 先放内存，不够放硬盘，2个副本</span>
rdd3<span class="token punctuation">.</span>persist<span class="token punctuation">(</span>StorageLevel<span class="token punctuation">.</span>OFF_HEAP<span class="token punctuation">)</span> <span class="token comment"># 堆外内存（系统内存）</span>

<span class="token comment"># 如上API，自行选择使用即可</span>
<span class="token comment"># 一般建议使用rdd3.persist(StorageLevel.MEMORY_AND_DISK)</span>
<span class="token comment"># 如果内存比较小的集群，建议使用rdd3.persist(StorageLevel.DISK_ONLY)或者就别用缓存了 用CheckPoint</span>

<span class="token comment"># 主动清理缓存的API</span>
rdd<span class="token punctuation">.</span>unpersist<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="322__1812"></a>3.2.2 缓存特点</h4> 
<ul><li>缓存技术可以将过程RDD数据，持久化保存到内存或者硬盘上</li><li>但是，这个保存在设定上是认为不安全的。</li></ul> 
<blockquote> 
 <p>缓存的数据在设计上是认为有丢失风险的。</p> 
</blockquote> 
<p>所以，缓存有一个特点就是：其保留RDD之间的<code>血缘（依赖）关系</code></p> 
<p>一旦缓存丢失，可以基于血缘关系的记录，重新计算这个RDD的数据</p> 
<blockquote> 
 <p>缓存如何丢失：</p> 
 <ol><li>在内存中的缓存是不安全的，比如断电、计算任务内存不足，把缓存清理给计算让路</li><li>硬盘中因为硬盘损坏也是可能丢失的。</li></ol> 
</blockquote> 
<p>代码演示：<code>31_cache.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>storagelevel <span class="token keyword">import</span> StorageLevel

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">import</span> time

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd1 <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"../data/input/words.txt"</span><span class="token punctuation">)</span>
    rdd2 <span class="token operator">=</span> rdd1<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    rdd3 <span class="token operator">=</span> rdd2<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 给rdd3加缓存</span>
    <span class="token comment"># rdd3.cache()</span>
    rdd3<span class="token punctuation">.</span>persist<span class="token punctuation">(</span>StorageLevel<span class="token punctuation">.</span>MEMORY_AND_DISK_2<span class="token punctuation">)</span> <span class="token comment"># 设置缓存级别</span>

    rdd4 <span class="token operator">=</span> rdd3<span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span><span class="token keyword">lambda</span> a<span class="token punctuation">,</span> b<span class="token punctuation">:</span> a <span class="token operator">+</span> b<span class="token punctuation">)</span>
    result <span class="token operator">=</span> rdd4<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span>

    rdd5 <span class="token operator">=</span> rdd3<span class="token punctuation">.</span>groupByKey<span class="token punctuation">(</span><span class="token punctuation">)</span>
    rdd6 <span class="token operator">=</span> rdd5<span class="token punctuation">.</span>mapValues<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span><span class="token builtin">sum</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd6<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 取消缓存</span>
    rdd3<span class="token punctuation">.</span>unpersist<span class="token punctuation">(</span><span class="token punctuation">)</span>
    time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">10000000</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>[('hadoop', 1), ('hello', 3), ('spark', 1), ('flink', 1)]
[('hadoop', 1), ('hello', 3), ('spark', 1), ('flink', 1)]
</code></pre> 
<h4><a id="323__1873"></a>3.2.3 缓存是如何保存的</h4> 
<p><img src="https://images2.imgbox.com/03/50/jXQ9dYLn_o.png" alt=""></p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-EP3ykOn3-1692435339403)(https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/%E7%BC%93%E5%AD%98%E6%98%AF%E5%A6%82%E4%BD%95%E4%BF%9D%E5%AD%98%E7%9A%842.png)]</p> 
<p>如图，RDD是将自己分区的数据，每个分区自行将其数据保存在其所在的Executor内存和硬盘上。</p> 
<p>这是<code>分散存储</code></p> 
<h3><a id="33_RDDCheckPoint_1883"></a>3.3 RDD的CheckPoint</h3> 
<h4><a id="331_RDD_CheckPoint_1885"></a>3.3.1 RDD CheckPoint</h4> 
<p>CheckPoint技术，也是将RDD的数据，保存起来。</p> 
<p>但是它<code>仅支持硬盘存储</code></p> 
<p>并且：</p> 
<ol><li>它被设计认为是安全的</li><li>不保留<code>血缘关系</code></li></ol> 
<h4><a id="332_CheckPoint_1896"></a>3.3.2 CheckPoint是如何保存数据的</h4> 
<p><img src="https://images2.imgbox.com/ae/f2/PmTXXgS9_o.png" alt="image-20230731213729297"></p> 
<p>如图：CheckPoint存储RDD数据，是集中收<code>集各个分区数据进行存储</code>。而缓存是<code>分散存储</code></p> 
<h4><a id="333_CheckPoint_1902"></a>3.3.3 缓存和CheckPoint的对比</h4> 
<ul><li>CheckPoint不管分区数量多少，风险是一样的，缓存分区越多，风险越高</li><li>CheckPoint支持写入HDFS，缓存不行，HDFS是高可靠存储，CheckPoint被认为是安全的</li><li>CheckPoint不支持内存，缓存可以，缓存如果写内存，性能比CheckPoint要好一些</li><li>CheckPoint因为设计是安全的，所以不保留血缘关系，而缓存因为设计上认为不安全，所以保留</li></ul> 
<h4><a id="334__1909"></a>3.3.4 代码</h4> 
<pre><code class="prism language-python"><span class="token comment"># 设置CheckPoint第一件事情，选择CP的保存路径</span>
<span class="token comment"># 如果是Local模式，可以支持本地文件系统，如果在集群运行，千万要用HDFS</span>
sc<span class="token punctuation">.</span>setCheckpointDir<span class="token punctuation">(</span><span class="token string">"hdfs://node1:8020/output/bj52ckp"</span><span class="token punctuation">)</span>
<span class="token comment"># 用的时候，直接调用checkPoint算子即可。</span>
rdd<span class="token punctuation">.</span>checkpoint<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>完整代码演示：<code>32_checkPoint.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">import</span> json

<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>storagelevel <span class="token keyword">import</span> StorageLevel

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">import</span> time

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    <span class="token comment"># 1.告知spark，开启checkPoint功能</span>
    sc<span class="token punctuation">.</span>setCheckpointDir<span class="token punctuation">(</span><span class="token string">"hdfs://Tnode1:8020/output/ckp"</span><span class="token punctuation">)</span>

    rdd1 <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"../data/input/words.txt"</span><span class="token punctuation">)</span>
    rdd2 <span class="token operator">=</span> rdd1<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    rdd3 <span class="token operator">=</span> rdd2<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 调用checkPoint API 保存数据即可</span>
    rdd3<span class="token punctuation">.</span>checkpoint<span class="token punctuation">(</span><span class="token punctuation">)</span>

    rdd4 <span class="token operator">=</span> rdd3<span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span><span class="token keyword">lambda</span> a<span class="token punctuation">,</span> b<span class="token punctuation">:</span> a <span class="token operator">+</span> b<span class="token punctuation">)</span>
    result <span class="token operator">=</span> rdd4<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span>

    rdd5 <span class="token operator">=</span> rdd3<span class="token punctuation">.</span>groupByKey<span class="token punctuation">(</span><span class="token punctuation">)</span>
    rdd6 <span class="token operator">=</span> rdd5<span class="token punctuation">.</span>mapValues<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span><span class="token builtin">sum</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>rdd6<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 取消缓存</span>
    rdd3<span class="token punctuation">.</span>unpersist<span class="token punctuation">(</span><span class="token punctuation">)</span>
    time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">10000000</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>[('hadoop', 1), ('hello', 3), ('spark', 1), ('flink', 1)]
[('hadoop', 1), ('hello', 3), ('spark', 1), ('flink', 1)]
</code></pre> 
<h4><a id="335__1968"></a>3.3.5 注意</h4> 
<p>CheckPoint是一种重量级的使用，也就是RDD的重新计算成本很高的时候，我们采用CheckPoint比较合适。</p> 
<p>或者数据量很大，用CheckPoint比较合适。</p> 
<p>如果数据量小，或者RDD重新计算是非常快的，用CheckPoint没啥必要</p> 
<blockquote> 
 <p>Cache和CheckPoint两个API都不是Action类型</p> 
 <p>所以，想要它俩工作，必须在后面接上Action</p> 
 <p>接上Action的目的，是让RDD有数据，而不是为了CheckPoint和Cache工作。</p> 
</blockquote> 
<h4><a id="336__1982"></a>3.3.6 总结</h4> 
<p><strong>1.Cache和CheckPoint的区别</strong></p> 
<ul><li>Cache是轻量化保存RDD数据，可存储在内存和硬盘，是分散存储，设计上数据是不安全的（保留RDD血缘关系）</li><li>CheckPoint是重量级保存RDD数据，是集中存储，只能存储在硬盘（HDFS）上，设计上是安全的（不保留RDD血缘关系）</li></ul> 
<p><strong>2.Cache和CheckPoint的性能对比？</strong></p> 
<ul><li>Cache性能更好，因为是分散存储，各个Executor并行执行，效率高，可以保存到内存中（占内存），更快</li><li>CheckPoint比较慢，因为是集中存储，涉及到网络IO，但是存储到HDFS上更加安全（多副本）</li></ul> 
<h2><a id="4_Spark_1994"></a>4. Spark案例练习</h2> 
<h3><a id="41__1996"></a>4.1 搜索引擎日志分析案例</h3> 
<p>数据格式：</p> 
<p><img src="https://images2.imgbox.com/5d/5a/5xLnlGM4_o.png" alt="image-20230731223032751"></p> 
<p>需求：</p> 
<p><img src="https://images2.imgbox.com/c7/19/rpHL7XpY_o.png" alt=""></p> 
<ul><li>用户搜索的关键词分析</li><li>用户和关键词组合分析</li><li>热门搜索时间段分析</li></ul> 
<p>案例实现代码：</p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>


<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>storagelevel <span class="token keyword">import</span> StorageLevel
<span class="token keyword">import</span> jieba

<span class="token keyword">from</span> operator <span class="token keyword">import</span> add


<span class="token keyword">def</span> <span class="token function">context_jieba</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""通过jieba分词工具 进行分词操作"""</span>
    seg <span class="token operator">=</span> jieba<span class="token punctuation">.</span>cut_for_search<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
    l <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> word <span class="token keyword">in</span> seg<span class="token punctuation">:</span>
        l<span class="token punctuation">.</span>append<span class="token punctuation">(</span>word<span class="token punctuation">)</span>
    <span class="token keyword">return</span> l


<span class="token keyword">def</span> <span class="token function">filter_words</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""过滤不要的 谷、帮、客 湖"""</span>
    <span class="token keyword">return</span> data <span class="token keyword">not</span> <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'谷'</span><span class="token punctuation">,</span> <span class="token string">'帮'</span><span class="token punctuation">,</span> <span class="token string">'客'</span><span class="token punctuation">,</span> <span class="token string">'湖'</span><span class="token punctuation">]</span>


<span class="token keyword">def</span> <span class="token function">append_words</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""修订某些关键词的内容"""</span>
    <span class="token keyword">if</span> data <span class="token operator">==</span> <span class="token string">'传智播'</span><span class="token punctuation">:</span> data <span class="token operator">=</span> <span class="token string">'传智播客'</span>
    <span class="token keyword">if</span> data <span class="token operator">==</span> <span class="token string">'院校'</span><span class="token punctuation">:</span> data <span class="token operator">=</span> <span class="token string">'院校帮'</span>
    <span class="token keyword">if</span> data <span class="token operator">==</span> <span class="token string">'博学'</span><span class="token punctuation">:</span> data <span class="token operator">=</span> <span class="token string">'博学谷'</span>
    <span class="token keyword">if</span> data <span class="token operator">==</span> <span class="token string">'数据'</span><span class="token punctuation">:</span> data <span class="token operator">=</span> <span class="token string">'数据湖'</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>data<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">extract_user_and_word</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""传入数据是 元组(1,我喜欢传智播客)"""</span>
    user_id <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    content <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
    <span class="token comment"># 对content进行分词</span>
    words <span class="token operator">=</span> context_jieba<span class="token punctuation">(</span>content<span class="token punctuation">)</span>

    return_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

    <span class="token keyword">for</span> word <span class="token keyword">in</span> words<span class="token punctuation">:</span>
        <span class="token comment"># 不要忘记过滤 \谷\帮\客\湖</span>
        <span class="token keyword">if</span> filter_words<span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">:</span>
            return_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>user_id <span class="token operator">+</span> <span class="token string">'_'</span> <span class="token operator">+</span> append_words<span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> return_list


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"SparkDemo2"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    <span class="token comment"># 1.读取文件</span>
    file_rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"hdfs://Tnode1/input/SogouQ.txt"</span><span class="token punctuation">)</span>

    <span class="token comment"># 2. 对数据进行切分 \t</span>
    split_rdd <span class="token operator">=</span> file_rdd<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"\t"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 3. 因为要做多个需求，split_rdd 作为基础的rdd 会被多次使用</span>
    split_rdd<span class="token punctuation">.</span>persist<span class="token punctuation">(</span>StorageLevel<span class="token punctuation">.</span>DISK_ONLY<span class="token punctuation">)</span>

    <span class="token comment"># TODO:需求1：用户搜索的关键‘词’分析</span>
    <span class="token comment"># 主要分析热点词</span>
    <span class="token comment"># 将所有的搜索内容取出</span>
    <span class="token comment"># print(split_rdd.takeSample(True, 3))</span>

    context_rdd <span class="token operator">=</span> split_rdd<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token comment"># 对搜索的内容进行分词分析</span>
    words_rdd <span class="token operator">=</span> context_rdd<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span>context_jieba<span class="token punctuation">)</span>

    <span class="token comment"># print(words_rdd.collect())</span>

    <span class="token comment"># 异常的数据：</span>
    <span class="token comment"># 数据 湖 ——&gt; 数据湖</span>
    <span class="token comment"># 院校 帮 ——&gt; 院校帮</span>
    <span class="token comment"># 博学 谷 ——&gt; 博学谷</span>
    <span class="token comment"># 传智播 客——&gt; 传智播客</span>

    filtered_rdd <span class="token operator">=</span> words_rdd<span class="token punctuation">.</span><span class="token builtin">filter</span><span class="token punctuation">(</span>filter_words<span class="token punctuation">)</span>

    <span class="token comment"># 将关键词转换：传智播 --&gt; 传智播客</span>
    final_words_rdd <span class="token operator">=</span> filtered_rdd<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>append_words<span class="token punctuation">)</span>

    <span class="token comment"># 对单词进行分组、聚合、排序 求出前五名</span>
    result1 <span class="token operator">=</span> final_words_rdd<span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span><span class="token keyword">lambda</span> a<span class="token punctuation">,</span> b<span class="token punctuation">:</span> a <span class="token operator">+</span> b<span class="token punctuation">)</span><span class="token punctuation">.</span> \
        sortBy<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> ascending<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> numPartitions<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span> \
        take<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"需求1结果："</span><span class="token punctuation">,</span> result1<span class="token punctuation">)</span>

    <span class="token comment"># TODO：需求2：用户和关键词组合分析</span>
    <span class="token comment"># 1，我喜欢传智播客</span>
    <span class="token comment"># 1 + 我 1+喜欢 1+传智播客</span>
    user_content_rdd <span class="token operator">=</span> split_rdd<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> x<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 对用户的搜索内容进行分词，分词后和用户ID再次组合</span>
    user_word_with_one_rdd <span class="token operator">=</span> user_content_rdd<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span>extract_user_and_word<span class="token punctuation">)</span>

    <span class="token comment"># 对内容进行分组、聚合、排序、求前5</span>
    result2 <span class="token operator">=</span> user_word_with_one_rdd<span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span><span class="token keyword">lambda</span> a<span class="token punctuation">,</span> b<span class="token punctuation">:</span> a <span class="token operator">+</span> b<span class="token punctuation">)</span><span class="token punctuation">.</span> \
        sortBy<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> ascending<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> numPartitions<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span> \
        take<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"需求2结果："</span><span class="token punctuation">,</span> result2<span class="token punctuation">)</span>

    <span class="token comment"># TODO:需求3：热门搜索时间段分析</span>
    <span class="token comment"># 取出来所有的时间</span>
    time_rdd <span class="token operator">=</span> split_rdd<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># 对时间进行处理，只保留小时精度即可</span>
    hour_with_one_rdd <span class="token operator">=</span> time_rdd<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token punctuation">(</span>x<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">":"</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># 分组、聚合、排序</span>
    result3 <span class="token operator">=</span> hour_with_one_rdd<span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span>add<span class="token punctuation">)</span><span class="token punctuation">.</span> \
        sortBy<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> ascending<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> numPartitions<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span> \
        collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"需求3结果："</span><span class="token punctuation">,</span> result3<span class="token punctuation">)</span>

</code></pre> 
<p>输出结果：</p> 
<pre><code>需求1结果： [('scala', 2310), ('hadoop', 2268), ('博学谷', 2002), ('传智汇', 1918), ('itheima', 1680)]
需求2结果： [('6185822016522959_scala', 2016), ('41641664258866384_博学谷', 1372), ('44801909258572364_hadoop', 1260), ('7044693659960919_仓库', 1120), ('15984948747597305_传智汇', 1120)]
需求3结果： [('20', 3479), ('23', 3087), ('21', 2989), ('22', 2499), ('01', 1365)
</code></pre> 
<h3><a id="42__2140"></a>4.2 提交到集群运行</h3> 
<pre><code class="prism language-shell"><span class="token comment"># 普通提交</span>
/export/server/spark/bin/spark-submit <span class="token parameter variable">--master</span> <span class="token function">yarn</span> SparkDemo2.py

<span class="token comment"># 压榨集群式提交</span>
<span class="token comment"># 每个executor吃14g内存，8核cpu，总共3个executor</span>
/export/server/spark/bin/spark-submit <span class="token parameter variable">--master</span> <span class="token function">yarn</span> --executor-memory 14g --executor-cores <span class="token number">8</span> --num-executors <span class="token number">3</span> ./SparkDemo2.py
</code></pre> 
<p>输出结果：</p> 
<p><img src="https://images2.imgbox.com/33/0f/dlspbGJ9_o.png" alt=""></p> 
<p>要注意代码中：</p> 
<blockquote> 
 <ol><li>master部分删除</li><li>读取的文件路径改为hdfs才可以</li></ol> 
</blockquote> 
<h3><a id="43__2160"></a>4.3 作业</h3> 
<p><img src="https://images2.imgbox.com/44/57/7hgTcqbw_o.png" alt="image-20230801222631776"></p> 
<p>代码演示：</p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkContext<span class="token punctuation">,</span> StorageLevel
<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"sparkHomeWork01"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    file_rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"../../data/input/apache.log"</span><span class="token punctuation">)</span>

    file_rdd<span class="token punctuation">.</span>persist<span class="token punctuation">(</span>StorageLevel<span class="token punctuation">.</span>MEMORY_AND_DISK_2<span class="token punctuation">)</span>

    <span class="token comment"># 需求1：TODO：计算当前网站访问的PV（被访问次数）</span>
    visit_Num <span class="token operator">=</span> file_rdd<span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"当前网站的被访问次数："</span><span class="token punctuation">,</span> visit_Num<span class="token punctuation">)</span>  <span class="token comment"># 14</span>

    <span class="token comment"># 需求2：TODO：当前网站访问的用户数</span>
    userNum <span class="token operator">=</span> file_rdd<span class="token punctuation">.</span>distinct<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"当前网站的访问用户数："</span><span class="token punctuation">,</span> userNum<span class="token punctuation">)</span>  <span class="token comment">#</span>

    <span class="token comment"># 需求3：TODO：有哪些IP访问了本网站？</span>
    Ip_rdd1 <span class="token operator">=</span> file_rdd<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    Ip_rdd1<span class="token punctuation">.</span>cache<span class="token punctuation">(</span><span class="token punctuation">)</span>

    Ip_rdd2 <span class="token operator">=</span> Ip_rdd1<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>distinct<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># print(IP_rdd2.collect())</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"有哪些IP访问了本网站："</span><span class="token punctuation">,</span> Ip_rdd2<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 需求4 TODO：哪个页面访问量最高</span>
    page_rdd1 <span class="token operator">=</span> Ip_rdd1<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span>x<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    page_rdd2 <span class="token operator">=</span> page_rdd1<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    page_rdd3 <span class="token operator">=</span> page_rdd2<span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span><span class="token keyword">lambda</span> a<span class="token punctuation">,</span>b<span class="token punctuation">:</span>a<span class="token operator">+</span>b<span class="token punctuation">)</span>
    <span class="token comment"># page = page_rdd3.sortBy(lambda x:x[1],ascending=False,numPartitions=1).take(1)</span>
    page <span class="token operator">=</span> page_rdd3<span class="token punctuation">.</span>takeOrdered<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span><span class="token operator">-</span>x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    page <span class="token operator">=</span> page<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>page<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"访问量最高的页面是："</span><span class="token punctuation">,</span>page<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">"共被访问："</span><span class="token punctuation">,</span>page<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">"次"</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：<code>sparkHomeWork01.py</code></p> 
<pre><code>当前网站的被访问次数： 14
当前网站的访问用户数： 9
有哪些IP访问了本网站： ['83.149.9.216', '10.0.0.1', '86.149.9.216']
('/presentations/logstash-monitorama-2013/css/print/paper.css', 13)
访问量最高的页面是： /presentations/logstash-monitorama-2013/css/print/paper.css 共被访问： 13 次
</code></pre> 
<h2><a id="5__2218"></a>5. 共享变量</h2> 
<h3><a id="51__2220"></a>5.1 广播变量</h3> 
<h4><a id="511__2222"></a>5.1.1 问题引出</h4> 
<p>有如下代码：</p> 
<p><img src="https://images2.imgbox.com/99/ba/WRnlgHk3_o.png" alt="image-20230802220002517"><img src="https://images2.imgbox.com/01/d4/6A1fk9lN_o.png" alt="image-20230802220014522"></p> 
<p>上述代码，本地list对象和分布式对象RDD有了关联。如下图：</p> 
<p><img src="https://images2.imgbox.com/46/e1/W6ZafczM_o.png" alt="image-20230802220536267"></p> 
<p>本地list对象，被发送到每个分区的处理线程上使用，也就是一个executor内，其实存放了2份一样的数据。</p> 
<p>executor是进程，进程内资源共享，这2份数据没有必要，造成了内存浪费。</p> 
<h4><a id="512__2236"></a>5.1.2 解决方案-广播变量</h4> 
<p>如果本地list对象标记为<code>广播变量对象</code>，那么</p> 
<p>当上述场景出现的时候，Spark只会：</p> 
<ul><li>给每个Executor来一份数据，而不像原本那样，每一个分区的处理线程都来一份，节省内存。</li></ul> 
<p><img src="https://images2.imgbox.com/c0/ec/zSarHyfI_o.png" alt="image-20230802221109858"></p> 
<p>如图，使用广播变量后，每个Executor只会收到一份数据集。</p> 
<p>内部的各个线程（分区）共享这一份数据集。</p> 
<p>使用方式：</p> 
<pre><code class="prism language-python"><span class="token comment"># 1. 将本地list 标记成广播变量即可</span>
broadcast <span class="token operator">=</span> sc<span class="token punctuation">.</span>broadcast<span class="token punctuation">(</span>stu_info_list<span class="token punctuation">)</span>

<span class="token comment"># 2. 使用广播变量，从broadcast对象中取出本地list对象即可</span>
value <span class="token operator">=</span> broadcast<span class="token punctuation">.</span>value

<span class="token comment"># 也就是 先放进去broadcast内部，然后从broadcast内部在取出来用，中间传输的是broadcast这个对象了</span>
<span class="token comment"># 只要中间传输的是broadcast对象，spark就会留意，只会给每个Executor发一份了，而不是傻傻的哪个分区都要给</span>
</code></pre> 
<p>代码演示：<code>33_broadcast.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">import</span> json

<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>storagelevel <span class="token keyword">import</span> StorageLevel

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">import</span> time

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"33_broadcast.py"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    stu_info_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'张大仙'</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                     <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'王晓晓'</span><span class="token punctuation">,</span> <span class="token number">13</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                     <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'张甜甜'</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                     <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token string">'王大力'</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">)</span><span class="token punctuation">]</span>

    <span class="token comment"># 1.将本地Python List对象标记为广播变量</span>
    broadcast <span class="token operator">=</span> sc<span class="token punctuation">.</span>broadcast<span class="token punctuation">(</span>stu_info_list<span class="token punctuation">)</span>

    score_info_rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span>
        <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'语文'</span><span class="token punctuation">,</span> <span class="token number">99</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'数学'</span><span class="token punctuation">,</span> <span class="token number">99</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'英语'</span><span class="token punctuation">,</span> <span class="token number">99</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token string">'编程'</span><span class="token punctuation">,</span> <span class="token number">99</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'语文'</span><span class="token punctuation">,</span> <span class="token number">99</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'编程'</span><span class="token punctuation">,</span> <span class="token number">99</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'语文'</span><span class="token punctuation">,</span> <span class="token number">99</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token string">'英语'</span><span class="token punctuation">,</span> <span class="token number">99</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'语文'</span><span class="token punctuation">,</span> <span class="token number">99</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'英语'</span><span class="token punctuation">,</span> <span class="token number">99</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'编程'</span><span class="token punctuation">,</span> <span class="token number">99</span><span class="token punctuation">)</span>

    <span class="token punctuation">]</span><span class="token punctuation">)</span>


    <span class="token keyword">def</span> <span class="token function">map_func</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>
        name <span class="token operator">=</span> <span class="token string">''</span>
        <span class="token builtin">id</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        <span class="token comment"># 匹配本地list和分布式rdd中的学生ID 匹配成功后 即可获得当前学生的姓名</span>
        <span class="token comment"># 2.在使用到本地集合对象的地方，从广播变量中取出来用即可</span>
        <span class="token keyword">for</span> stu_info <span class="token keyword">in</span> broadcast<span class="token punctuation">.</span>value<span class="token punctuation">:</span>
            <span class="token keyword">if</span> stu_info<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token builtin">id</span><span class="token punctuation">:</span>
                name <span class="token operator">=</span> stu_info<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
                <span class="token keyword">break</span>
        <span class="token keyword">return</span> <span class="token punctuation">(</span>name<span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>


    <span class="token keyword">print</span><span class="token punctuation">(</span>score_info_rdd<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>map_func<span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">"""
广播变量使用场景：本地集合对象和 分布式集合对象(RDD) 进行关联的时候
需要将本地集合对象封装为广播变量
可以节省：
1. 网络IO的次数
2. Executor的内存占用
"""</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>[('张大仙', '语文', 99), ('王晓晓', '数学', 99), ('张甜甜', '英语', 99), ('王大力', '编程', 99), ('张大仙', '语文', 99), ('王晓晓', '编程', 99), ('张甜甜', '语文', 99), ('王大力', '英语', 99), ('张大仙', '语文', 99), ('张甜甜', '英语', 99), ('王晓晓', '编程', 99)]
</code></pre> 
<h3><a id="52__2335"></a>5.2 累加器</h3> 
<h4><a id="521__2337"></a>5.2.1 需求</h4> 
<p>想要对<code>map</code>算子计算中的数据，进行数据累加，得到全部数据计算完后的累加结果</p> 
<h4><a id="522__2341"></a>5.2.2 没有累加器的代码演示</h4> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>
<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token comment"># 演示spark的accumulator累加器</span>
<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"create rdd"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>

    count <span class="token operator">=</span> <span class="token number">0</span>

    <span class="token keyword">def</span> <span class="token function">map_func</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">global</span> count
        count <span class="token operator">+=</span> <span class="token number">1</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>count<span class="token punctuation">)</span>
    rdd<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>map_func<span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>count<span class="token punctuation">)</span>

    <span class="token comment"># 代码中count 最后打印结果是0</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>1
2
3
4
5
1
2
3
4
5
0
</code></pre> 
<p>代码的问题在于：</p> 
<blockquote> 
 <ol><li>count来自driver对象，当在分布式的map算子中需要count对象的时候，driver会将count对象发送给每一个executor一份（复制发送）</li><li>每个executor各自收到一个，在最后执行print(count) 的时候，这个被打印的count依旧是driver那个</li><li>所以，不管executor中累加到多少，都和driver这个count无关</li></ol> 
</blockquote> 
<h4><a id="523__2388"></a>5.2.3 解决方法-累加器</h4> 
<p>代码演示：</p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">import</span> json

<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>storagelevel <span class="token keyword">import</span> StorageLevel

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">import</span> time

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"33_broadcast.py"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    <span class="token comment"># 10条数据 2个分区</span>
    rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>

    <span class="token comment"># count = 0</span>

    <span class="token comment"># Spark 提供的累加器变量，参数是初始值</span>
    acmlt <span class="token operator">=</span> sc<span class="token punctuation">.</span>accumulator<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>


    <span class="token keyword">def</span> <span class="token function">map_func</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">global</span> acmlt
        acmlt <span class="token operator">+=</span> <span class="token number">1</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>acmlt<span class="token punctuation">)</span>


    rdd<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>map_func<span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># rdd2 = rdd.map(map_func)</span>
    <span class="token comment"># rdd2.cache()</span>
    <span class="token comment"># rdd2.collect()</span>

    <span class="token comment"># rdd3 = rdd2.map(lambda x:x)</span>
    <span class="token comment"># rdd3.collect()</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span>acmlt<span class="token punctuation">)</span>  <span class="token comment"># 结果是10</span>

    <span class="token triple-quoted-string string">"""
    累加器使用的注意点：
    某个rdd使用完后，再被新的rdd重新调用，有可能会产生和想象中不一样的结果
    避免方法：给需要重用的rdd加缓存
    """</span>
</code></pre> 
<p>如上代码，将全部的<code>count</code>对象，都替换成<code>acmlt</code>对象即可</p> 
<p>这个对象就是累加器对象，构建方式：<code>sc.accumulator(初始值)</code>即可构建。</p> 
<p>这个对象唯一和前面提到的count不同的是，这个对象可以从各个Executor中收集它们的执行结果，作用回自己身上。</p> 
<p>输出结果：</p> 
<pre><code>1
2
3
4
5
1
2
3
4
5
10
</code></pre> 
<h4><a id="524__2462"></a>5.2.4 累加器的注意事项</h4> 
<p><img src="https://images2.imgbox.com/15/41/Op9BmQRs_o.png" alt="image-20230804113018115"></p> 
<p>如上代码，第一次rdd2被action后，累加器值是10，然后rdd2就没有了（没数据了）</p> 
<p>当rdd3构建出来的时候，是依赖rdd2，rdd2没数据，那么rdd2就要重新生成</p> 
<p>重新生成就导致累加器累加数据的代码再次被执行，</p> 
<p>所以代码的结果是20</p> 
<blockquote> 
 <p>也就是说，使用累加器的时候，要注意，因为rdd是过程数据，如果rdd被多次使用</p> 
 <p>可能重新构建此rdd</p> 
 <p>如果累加器累加代码，存在重新构建的步骤中</p> 
 <p>累加器累加代码就可能被多次执行。</p> 
</blockquote> 
<p>如何解决：加缓存或者CheckPoint即可</p> 
<h3><a id="53__2486"></a>5.3 综合案例</h3> 
<h4><a id="531__2488"></a>5.3.1 需求</h4> 
<p><img src="https://images2.imgbox.com/6a/6a/Hu7MMVSv_o.png" alt="image-20230804145316497"></p> 
<p>对上面的数据执行：</p> 
<ol><li>正常的单词进行单词计数</li><li>特殊字符统计出现有多少个</li></ol> 
<p>特殊字符定义如下：</p> 
<pre><code class="prism language-python">abnormal_char <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">","</span><span class="token punctuation">,</span><span class="token string">"."</span><span class="token punctuation">,</span><span class="token string">"!"</span><span class="token punctuation">,</span><span class="token string">"#"</span><span class="token punctuation">,</span><span class="token string">"$"</span><span class="token punctuation">,</span><span class="token string">"%"</span><span class="token punctuation">]</span>
</code></pre> 
<p>代码演示：</p> 
<pre><code class="prism language-python"><span class="token comment"># coding:utf8</span>

<span class="token keyword">import</span> json

<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>storagelevel <span class="token keyword">import</span> StorageLevel

<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext

<span class="token keyword">import</span> time
<span class="token keyword">import</span> re

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"35_demo.py"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>conf<span class="token punctuation">)</span>

    <span class="token comment"># 1.读取数据文件</span>
    file_rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"../data/input/accumulator_broadcast_data.txt"</span><span class="token punctuation">)</span>


    <span class="token comment"># 特殊字符的List定义</span>
    abnormal_char <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">','</span><span class="token punctuation">,</span> <span class="token string">'.'</span><span class="token punctuation">,</span> <span class="token string">'!'</span><span class="token punctuation">,</span> <span class="token string">'#'</span><span class="token punctuation">,</span> <span class="token string">'$'</span><span class="token punctuation">,</span> <span class="token string">'%'</span><span class="token punctuation">]</span>

    <span class="token comment"># 2. 特殊字符list 包装成广播变量</span>
    broadcast <span class="token operator">=</span> sc<span class="token punctuation">.</span>broadcast<span class="token punctuation">(</span>abnormal_char<span class="token punctuation">)</span>

    <span class="token comment"># 3.对特殊字符出现次数做累加，累加使用累加器最好</span>
    acmlt <span class="token operator">=</span> sc<span class="token punctuation">.</span>accumulator<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>

    <span class="token comment"># 4.数据处理，先处理数据的空行,在Python中有内容就是True None就是False</span>
    lines_rdd <span class="token operator">=</span> file_rdd<span class="token punctuation">.</span><span class="token builtin">filter</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> line<span class="token punctuation">:</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 5.去除前后的空格</span>
    data_rdd <span class="token operator">=</span> lines_rdd<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> line<span class="token punctuation">:</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 6.对数据进行切分，按照正则表达式切分，因为空格分隔符某些单词之间是两个或多个空格</span>
    <span class="token comment"># 正则表达式 \s+ 表示 不确定多少个空格，最少一个空格</span>
    words_rdd <span class="token operator">=</span> data_rdd<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span><span class="token keyword">lambda</span> line<span class="token punctuation">:</span> re<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"\s+"</span><span class="token punctuation">,</span> line<span class="token punctuation">)</span><span class="token punctuation">)</span>


    <span class="token comment"># 7. 当前words_rdd中有正常单词，也有特殊符号</span>
    <span class="token comment"># 现在需要过滤数据，保留正常单词用于单词计数，在过滤的过程中 对特殊符号做计数</span>

    <span class="token keyword">def</span> <span class="token function">filter_func</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">global</span> acmlt
        <span class="token comment"># 取出广播变量中存储的特殊符号lsit</span>
        abnormal_chars <span class="token operator">=</span> broadcast<span class="token punctuation">.</span>value

        <span class="token keyword">if</span> data <span class="token keyword">in</span> abnormal_chars<span class="token punctuation">:</span>
            <span class="token comment"># 表示这个是特殊字符</span>
            acmlt <span class="token operator">+=</span> <span class="token number">1</span>
            <span class="token keyword">return</span> <span class="token boolean">False</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> <span class="token boolean">True</span>


    normal_words_rdd <span class="token operator">=</span> words_rdd<span class="token punctuation">.</span><span class="token builtin">filter</span><span class="token punctuation">(</span>filter_func<span class="token punctuation">)</span>

    <span class="token comment"># 8. 正常单词的单词计数逻辑</span>
    result_rdd <span class="token operator">=</span> normal_words_rdd<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span> \
        reduceByKey<span class="token punctuation">(</span><span class="token keyword">lambda</span> a<span class="token punctuation">,</span> b<span class="token punctuation">:</span> a <span class="token operator">+</span> b<span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"正常单词计数结果："</span><span class="token punctuation">,</span>result_rdd<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"特殊字符数量："</span><span class="token punctuation">,</span>acmlt<span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code>正常单词计数结果： [('hadoop', 3), ('hive', 6), ('hdfs', 2), ('spark', 11), ('mapreduce', 4), ('sql', 2)]
特殊字符数量： 8
</code></pre> 
<h3><a id="54__2578"></a>5.4 总结</h3> 
<ol><li> <p>广播变量解决了什么问题？</p> <p>分布式集合RDD和本地集合进行关联使用的时候，降低内存占用以及减少网络IO传输，提高性能。</p> </li><li> <p>累加器解决了什么问题</p> <p>分布式代码执行中，进行全局累加</p> </li></ol> 
<h2><a id="6Spark_2588"></a>6.Spark内核调度（重点理解）</h2> 
<h3><a id="61_DAG_2590"></a>6.1 DAG</h3> 
<h4><a id="611_DAG_2592"></a>6.1.1 DAG</h4> 
<p>Spark的核心是根据RDD来实现的，Spark Scheduler则为Spark核心实现的重要一环，其作用就是任务调度。Spark</p> 
<p>的任务调度就是如何组织任务去处理RDD中每个分区的数据，根据RDD的依赖关系构建DAG，基于DAG划分Stage，</p> 
<p>将每个Stage中的任务发到指定节点运行。基于Spark的任务调度原理，可以合理规划资源利用，做到尽可能用最少的</p> 
<p>资源高效地完成任务计算。</p> 
<p>以词频统计WordCount程序为例，DAG图：</p> 
<p><img src="https://images2.imgbox.com/e2/f0/KW9zP2vL_o.png" alt="image-20230812105010262"></p> 
<p>DAG：有向无环图（拓扑结构）</p> 
<p>有向：有方向</p> 
<p>无环：没有闭环</p> 
<p>DAG：有方向没有形成闭环的一个执行流程图</p> 
<p>比如：</p> 
<p><img src="https://images2.imgbox.com/f8/0e/1VxWBiFE_o.png" alt="image-20230812105437155"></p> 
<p><img src="https://images2.imgbox.com/01/7d/TP0juOtC_o.png" alt="image-20230812105455134"></p> 
<p>此图，就是一个典型的DAG图。</p> 
<p>有方向：RDD1——&gt;RDD2——&gt;…——&gt;collect结束</p> 
<p>无闭环：以action(collect) 结束了，没有形成闭环循环</p> 
<p>作用：标识代码的<code>逻辑</code>执行流程</p> 
<h4><a id="612_JobAction_2628"></a>6.1.2 Job和Action</h4> 
<p>Action：返回值不是RDD的算子</p> 
<p>它的作用是一个触发开关，会将action算子之前的一串rdd依赖执行起来</p> 
<p><img src="https://images2.imgbox.com/91/a5/PC9hk8Vi_o.png" alt="image-20230812110007451"></p> 
<p>如图，我们前面写的搜索引擎日志分析案例中，前两个需求就是2个action，就产生了2个DAG</p> 
<p><strong>结论：</strong></p> 
<blockquote> 
 <p>1个Action会产生1个DAG，如果在代码中有3个Action就产生3个DAG</p> 
 <p>一个Action产生的一个DAG，会在程序运行中产生一个<code>JOB</code></p> 
 <p>所以：<code>1个ACTION = 1个DAG = 1个JOB</code></p> 
 <p>如果一个代码中，写了3个Action，那么这个代码运行起来产生3个JOB，每个JOB有自己的DAG</p> 
 <p>一个代码运行起来，在Spark中称之为：<code>Application</code></p> 
 <p>层级关系：</p> 
 <p>1个Application中，可以有多个JOB，每一个JOB内含一个DAG，同时每一个JOB都是由一个Action产生的。</p> 
</blockquote> 
<h4><a id="613_DAG_2654"></a>6.1.3 DAG和分区</h4> 
<p>DAG是Spark代码的逻辑执行图，这个DAG的最终作用是；为了构建物理上的Spark详细执行计划而生。</p> 
<p>所以，由于Spark是分布式（多分区）的，那么DAG和分区之间也是有关联的。</p> 
<pre><code>rdd1 = sc.textFile()
rdd2 = rdd1.flatMap()
rdd3 = rdd2.map()
rdd4 = rdd3.reduceByKey()
rdd4.action()
</code></pre> 
<p>假设，全部RDD都是3个分区在执行</p> 
<p><img src="https://images2.imgbox.com/8a/38/jzb2RKSL_o.png" alt="image-20230812112219158"></p> 
<p>如图，就得到了带有分区关系的DAG图</p> 
<h3><a id="62_DAG_2674"></a>6.2 DAG的宽窄依赖和阶段划分</h3> 
<p>在Spark RDD前后之间的关系为：</p> 
<ul><li>窄依赖</li><li>宽依赖</li></ul> 
<p>窄依赖：父RDD的一个分区，全部将数据发送给子RDD的一个分区</p> 
<p>宽依赖：父RDD的一个分区，将数据发送给子RDD的多个分区</p> 
<p>宽依赖还有一个别名：<code>shuffle</code></p> 
<h4><a id="621__2687"></a>6.2.1 窄依赖</h4> 
<p><img src="https://images2.imgbox.com/40/7a/HQPpre8d_o.png" alt="image-20230812170638380"></p> 
<h4><a id="622__2691"></a>6.2.2 宽依赖</h4> 
<p><img src="https://images2.imgbox.com/27/21/7EggERaN_o.png" alt="image-20230812171009615"></p> 
<h4><a id="623__2695"></a>6.2.3 阶段划分</h4> 
<p>对于Spark来说，会根据DAG，按照宽依赖，划分不同的DAG阶段</p> 
<p>划分依据：从后向前，遇到宽依赖就划分出一个阶段，称之为stage</p> 
<p><img src="https://images2.imgbox.com/aa/07/9kCbtAVA_o.png" alt="image-20230812170837184"></p> 
<p>如图，可以看到，在DAG中，基于宽依赖，将DAG划分成了2个stage</p> 
<p>在stage的内部，一定都是：窄依赖</p> 
<h3><a id="63__2707"></a>6.3 内存迭代计算</h3> 
<p><img src="https://images2.imgbox.com/a1/c9/NNAaxkTI_o.png" alt="image-20230812171632168"></p> 
<p>如图，基于带有分区的DAG以及阶段划分。可以从图中得到 逻辑上最优的task分配，一个task是一个线程来具体执行</p> 
<p>那么如上图，task1中rdd1 rdd2 rdd3的迭代计算，都是由一个task(线程完成)，这一阶段的这一条线，是纯内存计算。</p> 
<p>如上图，task1 task2 task3，就形成了三个并行的 内存计算管道。</p> 
<p>Spark默认受到全局并行度的限制，除了个别算子有特殊分区情况，大部分的算子，都会遵循全局并行度的要求，来规划自己的分区数。</p> 
<p>如果全局并行度是3，其实大部分算子分区都是3</p> 
<blockquote> 
 <p>注意：Spark ，我们一般推荐只设置全局并行度，不要在算子上设置并行度。</p> 
 <p>除了一些排序算子外，计算算子就让他默认开分区就可以了</p> 
</blockquote> 
<hr> 
<h4><a id="631__2729"></a>6.3.1 面试题</h4> 
<blockquote> 
 <p>面试题1：Spark是怎么做内存计算的？DAG的作用？Stage阶段划分的作用？</p> 
</blockquote> 
<blockquote> 
 <ol><li>Spark会产生DAG图</li><li>DAG图会基于分区和宽窄依赖关系划分阶段</li><li>一个阶段的内部都是窄依赖，窄依赖内，如果形成前后1:1的分区对应关系，就可以产生许多内存迭代计算的管道</li><li>这些内存迭代计算的管道，就是一个个具体的执行Task</li><li>一个Task是一个具体的线程，任务跑在一个线程内，就是走内存计算了。</li></ol> 
</blockquote> 
<hr> 
<blockquote> 
 <p>面试题2：Spark为什么比MapReduce快</p> 
</blockquote> 
<blockquote> 
 <ol><li>Spark的算子丰富，MapReduce算子匮乏（Map和Reduce），MapReduce这个编程模型，很难在一套MR中处理复杂的任务。很多复杂任务，是需要写多个MapReduce进行串联。多个MR串联通过磁盘交互数据。</li><li>Spark可以执行内存迭代，算子之间形成DAG，基于依赖划分阶段后，在阶段内形成内存迭代管道。但是MapReduce的Map和Reduce之间的交互依旧是通过硬盘来交互的。</li></ol> 
</blockquote> 
<p>总结：</p> 
<ol><li>编程模型上Spark占优（算子够多）</li><li>算子交互上，和计算行可以尽量多的内存计算而非磁盘迭代</li></ol> 
<h3><a id="64_Spark_2751"></a>6.4 Spark并行度</h3> 
<p>Spark的并行度：在同一时间内，有多少个task在同时运行</p> 
<p>并行度：并行能力的设置</p> 
<p>比如设置并行度6，其实就是要6个task并行在跑</p> 
<p>在有了6个task并行的前提下，rdd的分区就被规划成6个分区了。</p> 
<h4><a id="641__2763"></a>6.4.1 如何设置并行度</h4> 
<p>可以在代码中配置文件中以及提交程序的客户端参数中设置</p> 
<p>优先级从高到低：</p> 
<ol><li>代码中</li><li>客户端提交参数中</li><li>配置文件中</li><li>默认（1，但是不会全部以1来跑，多数时候基于读取文件的分片数量来作为默认并行度）</li></ol> 
<p>全局并行度配置的参数：<code>spark.default.parallelism</code></p> 
<h4><a id="642__2778"></a>6.4.2 全局并行度-推荐</h4> 
<p>配置文件中：</p> 
<pre><code>conf/spark-defaults.conf 中设置
spark.default.parallelism 100
</code></pre> 
<p>在客户端提交参数中：</p> 
<pre><code>bin/spark-submit --conf "spark.default.parallelism=100"
</code></pre> 
<p>在代码中设置：</p> 
<pre><code>conf = SparkConf()
conf.set("spark.default.parallelism",100)
</code></pre> 
<blockquote> 
 <p>全局并行度是推荐设置，不要针对RDD改分区，可能会影响内存迭代管道的构建，或者会产生额外的shuffle</p> 
</blockquote> 
<h4><a id="643_RDD_2802"></a>6.4.3 针对RDD的并行度设置-不推荐</h4> 
<p>只能在代码中写，算子：</p> 
<ul><li>repartition算子</li><li>coalesce算子</li><li>partitionBy算子</li></ul> 
<h4><a id="644__2810"></a>6.4.4 集群中如何规划并行度</h4> 
<p>结论：设置为CPU总核心的2~10倍</p> 
<p>比如集群可用CPU核心是100个，我们建议并行度是200~1000</p> 
<blockquote> 
 <p>确保是CPU核心的整数倍即可，最小是2倍，最大一般10倍或者更高（适量）均可</p> 
</blockquote> 
<p><code>为什么要设置最少2倍？</code></p> 
<p>CPU的一个核心同一时间只能干一件事。</p> 
<p>所以，在100个核心的情况下，设置100个并行，就能让CPU100%出力。</p> 
<p>这种设置下，如果task的压力不均衡，某个task先执行完了，就导致某个CPU核心空闲</p> 
<p>所以，我们将Task（并行）分配的数量变多，比如100个并行，同一时间只有100个在运行，700个在等待，</p> 
<p>但是可以确保，某个task运行完了，后续有task补上，不让cpu闲下来，最大程度利用集群的资源。</p> 
<blockquote> 
 <p>规划并行度，只看集群总CPU核数</p> 
</blockquote> 
<h3><a id="65_Spark_2834"></a>6.5 Spark任务调度</h3> 
<p>Spark的任务，由Driver进行调度，这个工作包含：</p> 
<ol><li>逻辑DAG产生</li><li>分区DAG产生</li><li>Task划分</li><li>将Task分配给Executor并监控其工作</li></ol> 
<p><img src="https://images2.imgbox.com/36/a4/LxJtJNSN_o.png" alt="image-20230812200120293"></p> 
<p>如图，Spark程序的调度流程如图：</p> 
<ol><li>Driver被构建出来</li><li>构建SparkContext（执行环境入口对象）</li><li>基于DAG Scheduler（DAG调度器）构建逻辑Task分配</li><li>基于TaskSchedule（Task调度器）将逻辑Task分配到各个Executor上干活，并监控它们。</li><li>Worker（Executor），被TaskScheduler管理监控，听从它们的指令干活，并定期汇报进度。</li></ol> 
<blockquote> 
 <p>1，2,3,4都是Driver的工作</p> 
 <p>5是Worker的工作</p> 
</blockquote> 
<h4><a id="651_Drivcer_2859"></a>6.5.1 Drivcer内的两个组件</h4> 
<p><strong>DAG调度器</strong>：</p> 
<p>工作内容：将逻辑的DAG图进行处理，最终得到逻辑上的Task划分</p> 
<p><strong>Task调度器</strong>：</p> 
<p>工作内容：基于DAG Scheduler的产出，来规划这些逻辑的task，应该在哪些物理的executor上运行，以及监控管理它们的运行。</p> 
<h3><a id="66_Spark_2869"></a>6.6 拓展-Spark概念名词大全</h3> 
<h4><a id="661_Spark_2871"></a>6.6.1 Spark运行中的概念名词大全</h4> 
<p><img src="https://images2.imgbox.com/4f/95/hC953P5j_o.png" alt="image-20230812201554418"></p> 
<p>层级关系梳理：</p> 
<ol><li>一个Spark环境可以运行多个Application</li><li>一个代码运行起来，会成为一个Application</li><li>Application内部可以有多个Job</li><li>每个Job由一个Action产生，并且每个Job有自己的DAG执行图</li><li>一个Job的DAG图会基于宽窄依赖划分成不同的阶段</li><li>不同阶段内基于分区数量，形成多个并行的内存迭代管道</li><li>每一个内存迭代管道形成一个Task（DAG调度器划分将Job内划分出具体的task任务，一个Job被划分出来的task在逻辑上称之为这个job的taskset）</li></ol> 
<h3><a id="67_SparkShuffle_2885"></a>6.7 SparkShuffle</h3> 
<h4><a id="671MR_Shuffle_2887"></a>6.7.1MR Shuffle回顾</h4> 
<p>首先回顾MapReduce框架中Shuffle过程，整体流程图如下：</p> 
<p><img src="https://images2.imgbox.com/c7/0a/KORnF5Jb_o.png" alt="image-20230812202841363"></p> 
<h4><a id="672__2893"></a>6.7.2 简介</h4> 
<p>Spark在DAG调度阶段会将一个Job划分为多个Stage，上游Stage做map工作，下游Stage做reduce工作，其本质上</p> 
<p>还是MapReduce计算框架。Shuffle是连接map和reduce之间的桥梁，它将<strong>map的输出对应到reduce输入中</strong>，涉及</p> 
<p>到序列化反序列化、跨节点网络IO以及磁盘读写IO等。</p> 
<p><img src="https://images2.imgbox.com/8a/e8/tSViU6Pt_o.png" alt="image-20230812202900693"></p> 
<p>Spark的Shuffle分为Write和Read两个阶段，分属于两个不同的Stage，前者是Parent Stage的最后一步，后者是</p> 
<p>Child Stage的第一步。</p> 
<hr> 
<p>执行Shuffle的主体是Stage中的并发任务，这些任务分ShuffleMapTask和ResultTask两种，ShuffleMapTask要进行</p> 
<p>Shuffle，ResultTask负责返回计算结果，一个Job中只有最后的Stage采用ResultTask，其他的均为ShuffleMapTask</p> 
<p>。如果要按照map端和reduce端来分析的话，ShuffleMapTask可以即是map端任务，又是reduce端任务，因为</p> 
<p>Spark中的Shuffle是可以串行的；ResultTask则只能充当reduce端任务的角色。</p> 
<p><img src="https://images2.imgbox.com/d5/76/N8u6obhf_o.png" alt="image-20230812202945614"></p> 
<hr> 
<p>Spark在1.1以前的版本一直是采用Hash Shuffle的实现的方式，到1.1版本时参考Hadoop MapReduce的实现开始引</p> 
<p>入Sort Shuffle，在1.5版本时开始Tungsten钨丝计划，引入UnSafe Shuffle优化内存及CPU的使用，在1.6中将</p> 
<p>Tungsten统一到Sort Shuffle中，实现自我感知选择最佳Shuffle方式，到的2.0版本，Hash Shuffle已被删除，所有</p> 
<p>Shuffle方式全部统一到Sort Shuffle一个实现中。</p> 
<p><img src="https://images2.imgbox.com/1e/f2/mPAuI9tU_o.png" alt="image-20230812203021649"></p> 
<hr> 
<p>在Spark的中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。ShuffleManager随着</p> 
<p>Spark的发展有两种实现的方式，分别为HashShuffleManager和SortShuffleManager，因此spark的Shuffle有Hash Shuffle和Sort</p> 
<p>Shuffle两种。</p> 
<p>在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。该ShuffleManager而HashShuffleManager有着一个非常严重</p> 
<p>的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。</p> 
<p>因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了SortShuffleManager。SortShuffleManager相较于</p> 
<p>HashShuffleManager来说，有了一定的改进。主要就在于，每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但</p> 
<p>是最后会将所有的临时文件合并(merge)成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉</p> 
<p>取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。</p> 
<p><img src="https://images2.imgbox.com/d5/0c/Eizzq0E3_o.png" alt="image-20230812203120271"></p> 
<h4><a id="673_Sort_Shuffle_bypass_2953"></a>6.7.3 Sort Shuffle bypass机制</h4> 
<p>bypass运行机制的触发条件如下：</p> 
<ol><li>shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold=200参数的值。</li><li>不是map combine聚合的shuffle算子(比如reduceByKey有map combie)。</li></ol> 
<p><img src="https://images2.imgbox.com/9b/6a/jHqFUDz2_o.png" alt="image-20230812203715048"></p> 
<p>bypass运行机制的触发条件如下：</p> 
<p>1)shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold=200参数的值。</p> 
<p>2)不是map combine聚合的shuffle算子(比如reduceByKey有map combie)。</p> 
<ul><li>此时task会为每个reduce端的task都创建一个临时磁盘文件，并将数据按key进行hash，然后根据key的hash值，</li></ul> 
<p>将key写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的</p> 
<p>。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。</p> 
<ul><li>该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一模一样的，因为都要创建数量惊人的磁盘文件，</li></ul> 
<p>只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的</p> 
<p>HashShuffleManager来说，shuffle read的性能会更好。</p> 
<p>而该机制与普通SortShuffleManager运行机制的不同在于：</p> 
<p>第一，磁盘写机制不同;</p> 
<p>第二，不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，</p> 
<p>也就节省掉了这部分的性能开销</p> 
<hr> 
<p>总结：</p> 
<ul><li>SortShuffle也分为普通机制和bypass机制</li><li>普通机制在内存数据结构(默认为5M)完成排序，会产生2M个磁盘小文件。</li><li>而当shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值。或者算子不是聚合类的</li></ul> 
<p>shuffle算子(比如reduceByKey)的时候会触发SortShuffle的bypass机制，SortShuffle的bypass机制不会进行排序</p> 
<p>，极大的提高了其性能。</p> 
<h4><a id="674_Shuflle_3004"></a>6.7.4 Shuflle的配置选项</h4> 
<p>Shuffle阶段划分：</p> 
<p>shuffle write：mapper阶段，上一个stage得到最后的结果写出</p> 
<p>shuffle read ：reduce阶段，下一个stage拉取上一个stage进行合并</p> 
<hr> 
<p>spark 的shuffle调优：主要是调整缓冲的大小，拉取次数重试重试次数与等待时间，内存比例分配，是否进行排序操作等等</p> 
<p><strong>spark.shuffle.file.buffer</strong></p> 
<p>参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小（默认是32K）。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写</p> 
<p>到磁盘。</p> 
<p>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性</p> 
<p>能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</p> 
<p><strong>spark.reducer.maxSizeInFlight</strong>：</p> 
<p>参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。(默认48M)</p> 
<p>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现</p> 
<p>，合理调节该参数，性能会有1%~5%的提升。</p> 
<p><strong>spark.shuffle.io.maxRetries and spark.shuffle.io.retryWait</strong>：</p> 
<p>spark.shuffle.io.maxRetries ：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试</p> 
<p>的最大次数。（默认是3次）</p> 
<p>spark.shuffle.io.retryWait：该参数代表了每次重试拉取数据的等待间隔。（默认为5s）</p> 
<p>调优建议：一般的调优都是将重试次数调高，不调整时间间隔。</p> 
<p><strong>spark.shuffle.memoryFraction</strong>：</p> 
<p>参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作内存比例。</p> 
<p>spark.shuffle.manager</p> 
<p>参数说明：该参数用于设置shufflemanager的类型（默认为sort）.Spark1.5x以后有三个可选项：</p> 
<p>Hash：spark1.x版本的默认值，HashShuffleManager</p> 
<p>Sort：spark2.x版本的默认值，普通机制，当shuffle read task 的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数，自动开启bypass 机制</p> 
<p><strong>spark.shuffle.sort.bypassMergeThreshold</strong></p> 
<p>参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作。</p> 
<p>调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些</p> 
<h3><a id="68__3062"></a>6.8 总结</h3> 
<blockquote> 
 <ol><li>DAG是什么有什么用？</li></ol> 
 <p>DAG是有向无环图，用以描述任务执行流程，主要作用就是协助DAG调度器构建Task分配用以做任务管理</p> 
 <ol start="2"><li>内存迭代、阶段划分？</li></ol> 
 <p>基于DAG的宽窄依赖划分阶段，阶段内部都是窄依赖可以构建内存迭代的管道</p> 
 <ol start="3"><li>DAG调度器是？</li></ol> 
 <p>构建Task分配以做任务管理</p> 
</blockquote>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9aa78c48d714e0d82e95d07e927eb6d4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">阿里10年IT老兵亲码《SpringCloud开发从入门到实战》，带你玩转SpringCloud</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/18744d35f93133d3fcddc9699f8118a8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">视频分类之数据集介绍</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>