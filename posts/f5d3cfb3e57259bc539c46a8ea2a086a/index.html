<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【机器学习】数据挖掘神器LightGBM详解 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【机器学习】数据挖掘神器LightGBM详解" />
<meta property="og:description" content="LightGBM 是微软开发的 boosting 集成模型，和 XGBoost 一样是对 GBDT 的优化和高效实现，原理有一些相似之处，但它很多方面比 XGBoost 有着更为优秀的表现。
1.LightGBM安装 LightGBM作为常见的强大Python机器学习工具库，安装也比较简单。
这些系统下的 XGBoost 安装，大家只要基于 pip 就可以轻松完成了，在命令行端输入命令如下命令即可等待安装完成。
pip install lightgbm 大家也可以选择国内的pip源，以获得更好的安装速度：
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple lightgbm Windows系统 对于 Windows 系统而言，比较高效便捷的安装方式是：在网址http://www.lfd.uci.edu/~gohlke/pythonlibs/ 中去下载对应版本的的LightGBM安装包，再通过如下命令安装。
pip install lightgbm‑3.3.2‑cp310‑cp310‑win_amd64.whl 2.LightGBM参数手册 在ShowMeAI的前一篇内容 XGBoost工具库建模应用详解[3] 中，我们讲解到了 Xgboost 的三类参数:通用参数，学习目标参数，Booster参数。
而 LightGBM 可调参数更加丰富，包含核心参数，学习控制参数，IO参数，目标参数，度量参数，网络参数，GPU参数，模型参数。这里我常修改的便是核心参数，学习控制参数，度量参数等。下面我们对这些模型参数做展开讲解，更多的细节可以参考 LightGBM中文文档[4]。
(1) 核心参数 config或者config_file：一个字符串，给出了配置文件的路径。默认为空字符串。
task：一个字符串，给出了要执行的任务。可以为：
train或者training：表示是训练任务。默认为train。
predict或者prediction或者test：表示是预测任务。
convert_model：表示是模型转换任务。将模型文件转换成if-else格式。
application或者objective或者app：一个字符串，表示问题类型。可以为：
regression 或 regression_l2 或 mean_squared_error 或 mse或l2_root 或 root_mean_squred_error 或 rmse：表示回归任务，但是使用L2损失函数。默认为regression。
regression_l1或者mae或者mean_absolute_error：表示回归任务，但是使用L1损失函数。
huber：表示回归任务，但是使用huber损失函数。
fair：表示回归任务，但是使用fair损失函数。
poisson：表示Poisson回归任务。
quantile：表示quantile回归任务。
quantile_l2：表示quantile回归任务，但是使用了L2损失函数。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/f5d3cfb3e57259bc539c46a8ea2a086a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-26T12:03:03+08:00" />
<meta property="article:modified_time" content="2023-12-26T12:03:03+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【机器学习】数据挖掘神器LightGBM详解</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p><strong>LightGBM</strong> 是微软开发的 boosting 集成模型，和 XGBoost 一样是对 GBDT 的优化和高效实现，原理有一些相似之处，但它很多方面比 XGBoost 有着更为优秀的表现。<img src="https://images2.imgbox.com/97/b9/NUzebEuD_o.png" alt="7496c814e07d650fdbd21ac6affbc422.png"></p> 
 <h2><strong>1.LightGBM安装</strong></h2> 
 <p>LightGBM作为常见的强大Python机器学习工具库，安装也比较简单。</p> 
 <p>这些系统下的 XGBoost 安装，大家只要基于 <code>pip</code> 就可以轻松完成了，在命令行端输入命令如下命令即可等待安装完成。</p> 
 <pre class="has"><code class="language-go">pip install lightgbm</code></pre> 
 <p>大家也可以选择国内的pip源，以获得更好的安装速度：</p> 
 <pre class="has"><code class="language-go">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple lightgbm</code></pre> 
 <h3><strong> Windows系统</strong></h3> 
 <p>对于 Windows 系统而言，比较高效便捷的安装方式是：在网址http://www.lfd.uci.edu/~gohlke/pythonlibs/ 中去下载对应版本的的LightGBM安装包，再通过如下命令安装。</p> 
 <pre class="has"><code class="language-go">pip install lightgbm‑3.3.2‑cp310‑cp310‑win_amd64.whl</code></pre> 
 <h2><strong>2.LightGBM参数手册</strong></h2> 
 <p>在ShowMeAI的前一篇内容 <strong>XGBoost工具库建模应用详解</strong><sup><strong>[3]</strong></sup> 中，我们讲解到了 Xgboost 的三类参数:<strong>通用参数</strong>，<strong>学习目标参数</strong>，<strong>Booster参数</strong>。</p> 
 <p>而 LightGBM 可调参数更加丰富，包含<strong>核心参数</strong>，<strong>学习控制参数</strong>，<strong>IO参数</strong>，<strong>目标参数</strong>，<strong>度量参数</strong>，<strong>网络参数</strong>，<strong>GPU参数</strong>，<strong>模型参数</strong>。这里我常修改的便是核心参数，学习控制参数，度量参数等。下面我们对这些模型参数做展开讲解，更多的细节可以参考 <strong>LightGBM中文文档</strong><sup><strong>[4]</strong></sup>。</p> 
 <h3><strong>(1) 核心参数</strong></h3> 
 <blockquote> 
  <p><code>config</code>或者<code>config_file</code>：一个字符串，给出了配置文件的路径。默认为空字符串。</p> 
 </blockquote> 
 <br> 
 <blockquote> 
  <p><code>task</code>：一个字符串，给出了要执行的任务。可以为：</p> 
 </blockquote> 
 <ul><li><p><code><strong>train</strong></code><strong>或者</strong><code><strong>training</strong></code><strong>：表示是训练任务。默认为</strong><code><strong>train</strong></code><strong>。</strong></p></li><li><p><code><strong>predict</strong></code><strong>或者</strong><code><strong>prediction</strong></code><strong>或者</strong><code><strong>test</strong></code><strong>：表示是预测任务。</strong></p></li><li><p><code><strong>convert_model</strong></code><strong>：表示是模型转换任务。将模型文件转换成if-else格式。</strong></p></li></ul> 
 <br> 
 <blockquote> 
  <p><code>application</code>或者<code>objective</code>或者<code>app</code>：一个字符串，表示问题类型。可以为：</p> 
 </blockquote> 
 <ul><li><p><code><strong>regression</strong></code><strong> 或 </strong><code><strong>regression_l2</strong></code><strong> 或 </strong><code><strong>mean_squared_error</strong></code><strong> 或 </strong><code><strong>mse</strong></code><strong>或</strong><code><strong>l2_root</strong></code><strong> 或 </strong><code><strong>root_mean_squred_error</strong></code><strong> 或 </strong><code><strong>rmse</strong></code><strong>：表示回归任务，但是使用L2损失函数。默认为</strong><code><strong>regression</strong></code><strong>。</strong></p></li><li><p><code><strong>regression_l1</strong></code><strong>或者</strong><code><strong>mae</strong></code><strong>或者</strong><code><strong>mean_absolute_error</strong></code><strong>：表示回归任务，但是使用L1损失函数。</strong></p></li><li><p><code><strong>huber</strong></code><strong>：表示回归任务，但是使用huber损失函数。</strong></p></li><li><p><code><strong>fair</strong></code><strong>：表示回归任务，但是使用fair损失函数。</strong></p></li><li><p><code><strong>poisson</strong></code><strong>：表示Poisson回归任务。</strong></p></li><li><p><code><strong>quantile</strong></code><strong>：表示quantile回归任务。</strong></p></li><li><p><code><strong>quantile_l2</strong></code><strong>：表示quantile回归任务，但是使用了L2损失函数。</strong></p></li><li><p><code><strong>mape</strong></code><strong>或者</strong><code><strong>mean_absolute_precentage_error</strong></code><strong>：表示回归任务，但是使用MAPE损失函数</strong></p></li><li><p><code><strong>gamma</strong></code><strong>：表示gamma回归任务。</strong></p></li><li><p><code><strong>tweedie</strong></code><strong>：表示tweedie回归任务。</strong></p></li><li><p><code><strong>binary</strong></code><strong>：表示二分类任务，使用对数损失函数作为目标函数。</strong></p></li><li><p><code><strong>multiclass</strong></code><strong>：表示多分类任务，使用softmax函数作为目标函数。必须设置</strong><code><strong>num_class</strong></code><strong>参数</strong></p></li><li><p><code><strong>multiclassova</strong></code><strong>或者</strong><code><strong>multiclass_ova</strong></code><strong>或者</strong><code><strong>ova</strong></code><strong>或者</strong><code><strong>ovr</strong></code><strong>：表示多分类任务，使用</strong><code><strong>one-vs-all</strong></code><strong>的二分类目标函数。必须设置</strong><code><strong>num_class</strong></code><strong>参数。</strong></p></li><li><p><code><strong>xentropy</strong></code><strong>或者</strong><code><strong>cross_entropy</strong></code><strong>：目标函数为交叉熵(同时具有可选择的线性权重)。要求标签是[0,1]之间的数值。</strong></p></li><li><p><code><strong>xentlambda</strong></code><strong>或者</strong><code><strong>cross_entropy_lambda</strong></code><strong>：替代了参数化的</strong><code><strong>cross_entropy</strong></code><strong>。要求标签是[0,1]之间的数值。</strong></p></li><li><p><code><strong>lambdarank</strong></code><strong>：表示排序任务。在</strong><code><strong>lambdarank</strong></code><strong>任务中，标签应该为整数类型，数值越大表示相关性越高。</strong><code><strong>label_gain</strong></code><strong>参数可以用于设置整数标签的增益(权重)。</strong></p></li></ul> 
 <br> 
 <blockquote> 
  <p><code>boosting</code>或者<code>boost</code>或者<code>boosting_type</code>：一个字符串，给出了基学习器模型算法。可以为：</p> 
 </blockquote> 
 <ul><li><p><code><strong>gbdt</strong></code><strong>：表示传统的梯度提升决策树。默认值为</strong><code><strong>gbdt</strong></code><strong>。</strong></p></li><li><p><code><strong>rf</strong></code><strong>：表示随机森林。</strong></p></li><li><p><code><strong>dart</strong></code><strong>：表示带dropout的gbdt。</strong></p></li><li><p><code><strong>goss</strong></code><strong>：表示Gradient-based One-Side Sampling 的gbdt。</strong></p></li></ul> 
 <br> 
 <blockquote> 
  <p><code>data</code>或者<code>train</code>或者<code>train_data</code>：一个字符串，给出了训练数据所在的文件的文件名。默认为空字符串。LightGBM将使用它来训练模型。</p> 
 </blockquote> 
 <blockquote> 
  <p><code>valid</code>或者<code>test</code>或者<code>valid_data</code>或者<code>test_data</code>：一个字符串，表示验证集所在的文件的文件名。默认为空字符串。LightGBM将输出该数据集的度量。如果有多个验证集，则用逗号分隔。</p> 
 </blockquote> 
 <br> 
 <blockquote> 
  <p><code>num_iterations</code>或者<code>num_iteration</code>或者<code>num_tree</code>或者<code>num_trees</code>或者<code>num_round</code>或者<code>num_rounds</code>或者<code>num_boost_round</code>一个整数，给出了<code>boosting</code>的迭代次数。默认为100。</p> 
 </blockquote> 
 <ul><li><p><strong>对于Python/R包，该参数是被忽略的。对于Python，使用</strong><code><strong>train()/cv()</strong></code><strong>的输入参数</strong><code><strong>num_boost_round</strong></code><strong>来代替。</strong></p></li><li><p><strong>在内部，LightGBM对于multiclass问题设置了</strong><code><strong>num_class*num_iterations</strong></code><strong>棵树。</strong></p></li></ul> 
 <br> 
 <blockquote> 
  <p><code>learning_rate</code>或者<code>shrinkage_rate</code>：个浮点数，给出了学习率。默认为1。在dart中，它还会影响dropped trees的归一化权重。</p> 
 </blockquote> 
 <blockquote> 
  <p><code>num_leaves</code>或者<code>num_leaf</code>：一个整数，给出了一棵树上的叶子数。默认为31。</p> 
 </blockquote> 
 <br> 
 <blockquote> 
  <p><code>tree_learner</code>或者<code>tree</code>：一个字符串，给出了tree learner，主要用于并行学习。默认为<code>serial</code>。可以为：</p> 
 </blockquote> 
 <ul><li><p><code><strong>serial</strong></code><strong>：单台机器的tree learner</strong></p></li><li><p><code><strong>feature</strong></code><strong>：特征并行的tree learner</strong></p></li><li><p><code><strong>data</strong></code><strong>：数据并行的tree learner</strong></p></li><li><p><code><strong>voting</strong></code><strong>：投票并行的tree learner</strong></p></li></ul> 
 <br> 
 <blockquote> 
  <p><code>num_threads</code>或者<code>num_thread</code>或者<code>nthread</code>：一个整数，给出了LightGBM的线程数。默认为<code>OpenMP_default</code>。</p> 
 </blockquote> 
 <ul><li><p><strong>为了更快的速度，应该将它设置为真正的CPU内核数，而不是线程的数量(大多数CPU使用超线程来使每个CPU内核生成2个线程)。</strong></p></li><li><p><strong>当数据集较小的时候，不要将它设置的过大。</strong></p></li><li><p><strong>对于并行学习，不应该使用全部的CPU核心，因为这会使得网络性能不佳。</strong></p></li></ul> 
 <br> 
 <blockquote> 
  <p><code>device</code>：一个字符串，指定计算设备。默认为<code>cpu</code>。可以为<code>gpu</code>、<code>cpu</code>。</p> 
 </blockquote> 
 <ul><li><p><strong>建议使用较小的</strong><code><strong>max_bin</strong></code><strong>来获得更快的计算速度。</strong></p></li><li><p><strong>为了加快学习速度，GPU默认使用32位浮点数来求和。你可以设置</strong><code><strong>gpu_use_dp=True</strong></code><strong>来启动64位浮点数，但是它会使得训练速度降低。</strong></p></li></ul> 
 <h3><strong>(2) 学习控制参数</strong></h3> 
 <ul><li><p><code><strong>max_depth</strong></code><strong>：一个整数，限制了树模型的最大深度，默认值为-1。如果小于0，则表示没有限制。</strong></p></li><li><p><code><strong>min_data_in_leaf</strong></code><strong>或者</strong><code><strong>min_data_per_leaf</strong></code><strong>或者</strong><code><strong>min_data</strong></code><strong>或者</strong><code><strong>min_child_samples</strong></code><strong>：一个整数，表示一个叶子节点上包含的最少样本数量。默认值为20。</strong></p></li><li><p><code><strong>min_sum_hessian_in_leaf</strong></code><strong>或者</strong><code><strong>min_sum_hessian_per_leaf</strong></code><strong>或者</strong><code><strong>min_sum_hessian</strong></code><strong>或者</strong><code><strong>min_hessian</strong></code><strong>或者</strong><code><strong>min_child_weight</strong></code><strong>：一个浮点数，表示一个叶子节点上的最小hessian之和。(也就是叶节点样本权重之和的最小值)默认为1e-3。</strong></p></li><li><p><code><strong>feature_fraction</strong></code><strong>或者</strong><code><strong>sub_feature</strong></code><strong>或者</strong><code><strong>colsample_bytree</strong></code><strong>：一个浮点数，取值范围为[0.0,1.0]，默认值为0。如果小于1.0，则LightGBM会在每次迭代中随机选择部分特征。如0.8表示：在每棵树训练之前选择80%的特征来训练。</strong></p></li><li><p><code><strong>feature_fraction_seed</strong></code><strong>：一个整数，表示</strong><code><strong>feature_fraction</strong></code><strong>的随机数种子，默认为2。</strong></p></li><li><p><code><strong>bagging_fraction</strong></code><strong>或者</strong><code><strong>sub_row</strong></code><strong>或者</strong><code><strong>subsample</strong></code><strong>：一个浮点数，取值范围为[0.0,1.0]，默认值为0。如果小于1.0，则LightGBM会在每次迭代中随机选择部分样本来训练(非重复采样)。如0.8表示：在每棵树训练之前选择80%的样本(非重复采样)来训练。</strong></p></li><li><p><code><strong>bagging_freq</strong></code><strong>或者</strong><code><strong>subsample_freq</strong></code><strong>：一个整数，表示每</strong><code><strong>bagging_freq</strong></code><strong>次执行bagging。如果该参数为0，表示禁用bagging。</strong></p></li><li><p><code><strong>bagging_seed</strong></code><strong>或者</strong><code><strong>bagging_fraction_seed</strong></code><strong>：一个整数，表示bagging的随机数种子，默认为3。</strong></p></li><li><p><code><strong>early_stopping_round</strong></code><strong>或者</strong><code><strong>early_stopping_rounds</strong></code><strong>或者</strong><code><strong>early_stopping</strong></code><strong>：一个整数，默认为0。如果一个验证集的度量在</strong><code><strong>early_stopping_round</strong></code><strong>循环中没有提升，则停止训练。如果为0则表示不开启早停。</strong></p></li><li><p><code><strong>lambda_l1</strong></code><strong>或者</strong><code><strong>reg_alpha</strong></code><strong>：一个浮点数，表示L1正则化系数。默认为0。</strong></p></li><li><p><code><strong>lambda_l2</strong></code><strong>或者</strong><code><strong>reg_lambda</strong></code><strong>：一个浮点数，表示L2正则化系数。默认为0。</strong></p></li><li><p><code><strong>min_split_gain</strong></code><strong>或者</strong><code><strong>min_gain_to_split</strong></code><strong>：一个浮点数，表示执行切分的最小增益，默认为0。</strong></p></li><li><p><code><strong>drop_rate</strong></code><strong>：一个浮点数，取值范围为[0.0,1.0]，表示dropout的比例，默认为1。该参数仅在dart中使用。</strong></p></li><li><p><code><strong>skip_drop</strong></code><strong>：一个浮点数，取值范围为[0.0,1.0]，表示跳过dropout的概率，默认为5。该参数仅在dart中使用。</strong></p></li><li><p><code><strong>max_drop</strong></code><strong>：一个整数，表示一次迭代中删除树的最大数量，默认为50。如果小于等于0，则表示没有限制。该参数仅在dart中使用。</strong></p></li><li><p><code><strong>uniform_drop</strong></code><strong>：一个布尔值，表示是否想要均匀的删除树，默认值为False。该参数仅在dart中使用。</strong></p></li><li><p><code><strong>xgboost_dart_mode</strong></code><strong>：一个布尔值，表示是否使用xgboost dart模式，默认值为False。该参数仅在dart中使用。</strong></p></li><li><p><code><strong>drop_seed</strong></code><strong>：一个整数，表示dropout的随机数种子，默认值为4。该参数仅在dart中使用。</strong></p></li><li><p><code><strong>top_rate</strong></code><strong>：一个浮点数，取值范围为[0.0,1.0]，表示在goss中，大梯度数据的保留比例，默认值为2。该参数仅在goss中使用。</strong></p></li><li><p><code><strong>other_rate</strong></code><strong>：一个浮点数，取值范围为[0.0,1.0]，表示在goss中，小梯度数据的保留比例，默认值为1。该参数仅在goss中使用。</strong></p></li><li><p><code><strong>min_data_per_group</strong></code><strong>：一个整数，表示每个分类组的最小数据量，默认值为100。用于排序任务</strong></p></li><li><p><code><strong>max_cat_threshold</strong></code><strong>：一个整数，表示category特征的取值集合的最大大小。默认为32。</strong></p></li><li><p><code><strong>cat_smooth</strong></code><strong>：一个浮点数，用于category特征的概率平滑。默认值为10。它可以降低噪声在category特征中的影响，尤其是对于数据很少的类。</strong></p></li><li><p><code><strong>cat_l2</strong></code><strong>：一个浮点数，用于category切分中的L2正则化系数。默认为10。</strong></p></li><li><p><code><strong>top_k</strong></code><strong>或者</strong><code><strong>topk</strong></code><strong>：一个整数，用于投票并行中。默认为20。将它设置为更大的值可以获得更精确的结果，但是会降低训练速度。</strong></p></li></ul> 
 <h3><strong>(3) IO参数</strong></h3> 
 <ul><li><p><code><strong>max_bin</strong></code><strong>：一个整数，表示最大的桶的数量。默认值为255。LightGBM会根据它来自动压缩内存。如</strong><code><strong>max_bin=255</strong></code><strong>时，则LightGBM将使用uint8来表示特征的每一个值。</strong></p></li><li><p><code><strong>min_data_in_bin</strong></code><strong>：一个整数，表示每个桶的最小样本数。默认为3。该方法可以避免出现一个桶只有一个样本的情况。</strong></p></li><li><p><code><strong>data_random_seed</strong></code><strong>：一个整数，表示并行学习数据分隔中的随机数种子。默认为1它不包括特征并行。</strong></p></li><li><p><code><strong>output_model</strong></code><strong>或者</strong><code><strong>model_output</strong></code><strong>或者</strong><code><strong>model_out</strong></code><strong>：一个字符串，表示训练中输出的模型被保存的文件的文件名。默认txt。</strong></p></li><li><p><code><strong>input_model</strong></code><strong>或者</strong><code><strong>model_input</strong></code><strong>或者</strong><code><strong>model_in</strong></code><strong>：一个字符串，表示输入模型的文件的文件名。默认空字符串。对于prediction任务，该模型将用于预测数据，对于train任务，训练将从该模型继续</strong></p></li><li><p><code><strong>output_result</strong></code><strong>或者</strong><code><strong>predict_result</strong></code><strong>或者</strong><code><strong>prediction_result</strong></code><strong>：一个字符串，给出了prediction结果存放的文件名。默认为txt。</strong></p></li><li><p><code><strong>pre_partition</strong></code><strong>或者</strong><code><strong>is_pre_partition</strong></code><strong>：一个布尔值，指示数据是否已经被划分。默认值为False。如果为true，则不同的机器使用不同的partition来训练。它用于并行学习(不包括特征并行)</strong></p></li><li><p><code><strong>is_sparse</strong></code><strong>或者</strong><code><strong>is_enable_sparse</strong></code><strong>或者</strong><code><strong>enable_sparse</strong></code><strong>：一个布尔值，表示是否开启稀疏优化，默认为True。如果为True则启用稀疏优化。</strong></p></li><li><p><code><strong>two_round</strong></code><strong>或者</strong><code><strong>two_round_loading</strong></code><strong>或者</strong><code><strong>use_two_round_loading</strong></code><strong>：一个布尔值，指示是否启动两次加载。默认值为False，表示只需要进行一次加载。默认情况下，LightGBM会将数据文件映射到内存，然后从内存加载特征，这将提供更快的数据加载速度。但是当数据文件很大时，内存可能会被耗尽。如果数据文件太大，则将它设置为True</strong></p></li><li><p><code><strong>save_binary</strong></code><strong>或者</strong><code><strong>is_save_binary</strong></code><strong>或者</strong><code><strong>is_save_binary_file</strong></code><strong>：一个布尔值，表示是否将数据集(包括验证集)保存到二进制文件中。默认值为False。如果为True，则可以加快数据的加载速度。</strong></p></li><li><p><code><strong>verbosity</strong></code><strong>或者</strong><code><strong>verbose</strong></code><strong>：一个整数，表示是否输出中间信息。默认值为1。如果小于0，则仅仅输出critical信息；如果等于0，则还会输出error,warning信息；如果大于0，则还会输出info信息。</strong></p></li><li><p><code><strong>header</strong></code><strong>或者</strong><code><strong>has_header</strong></code><strong>：一个布尔值，表示输入数据是否有头部。默认为False。</strong></p></li><li><p><code><strong>label</strong></code><strong>或者</strong><code><strong>label_column</strong></code><strong>：一个字符串，表示标签列。默认为空字符串。你也可以指定一个整数，如label=0表示第0列是标签列。你也可以为列名添加前缀，如</strong><code><strong>label=prefix:label_name</strong></code><strong>。</strong></p></li><li><p><code><strong>weight</strong></code><strong>或者</strong><code><strong>weight_column</strong></code><strong>：一个字符串，表示样本权重列。默认为空字符串。你也可以指定一个整数，如weight=0表示第0列是权重列。注意：它是剔除了标签列之后的索引。假如标签列为0，权重列为1，则这里weight=0。你也可以为列名添加前缀，如</strong><code><strong>weight=prefix:weight_name</strong></code><strong>。</strong></p></li><li><p><code><strong>query</strong></code><strong>或者</strong><code><strong>query_column</strong></code><strong>或者</strong><code><strong>gourp</strong></code><strong>或者</strong><code><strong>group_column</strong></code><strong>：一个字符串，query/groupID列。默认为空字符串。你也可以指定一个整数，如query=0表示第0列是query列。注意：它是剔除了标签列之后的索引。假如标签列为0，query列为1，则这里query=0。你也可以为列名添加前缀，如</strong><code><strong>query=prefix:query_name</strong></code><strong>。</strong></p></li><li><p><code><strong>ignore_column</strong></code><strong>或者</strong><code><strong>ignore_feature</strong></code><strong>或者</strong><code><strong>blacklist</strong></code><strong>：一个字符串，表示训练中忽略的一些列，默认为空字符串。可以用数字做索引，如</strong><code><strong>ignore_column=0,1,2</strong></code><strong>表示第0,1,2列将被忽略。注意：它是剔除了标签列之后的索引。</strong></p></li><li><p><strong>你也可以为列名添加前缀，如</strong><code><strong>ignore_column=prefix:ign_name1,ign_name2</strong></code><strong>。</strong></p></li><li><p><code><strong>categorical_feature</strong></code><strong>或者</strong><code><strong>categorical_column</strong></code><strong>或者</strong><code><strong>cat_feature</strong></code><strong>或者</strong><code><strong>cat_column</strong></code><strong>：一个字符串，指定category特征的列。默认为空字符串。可以用数字做索引，如</strong><code><strong>categorical_feature=0,1,2</strong></code><strong>表示第0,1,2列将作为category特征。注意：它是剔除了标签列之后的索引。你也可以为列名添加前缀，如</strong><code><strong>categorical_feature=prefix:cat_name1,cat_name2</strong></code><strong>在categorycal特征中，负的取值被视作缺失值。</strong></p></li><li><p><code><strong>predict_raw_score</strong></code><strong>或者</strong><code><strong>raw_score</strong></code><strong>或者</strong><code><strong>is_predict_raw_score</strong></code><strong>：一个布尔值，表示是否预测原始得分。默认为False。如果为True则仅预测原始得分。该参数只用于prediction任务。</strong></p></li><li><p><code><strong>predict_leaf_index</strong></code><strong>或者</strong><code><strong>leaf_index</strong></code><strong>或者</strong><code><strong>is_predict_leaf_index</strong></code><strong>：一个布尔值，表示是否预测每个样本在每棵树上的叶节点编号。默认为False。在预测时，每个样本都会被分配到每棵树的某个叶子节点上。该参数就是要输出这些叶子节点的编号。该参数只用于prediction任务。</strong></p></li><li><p><code><strong>predict_contrib</strong></code><strong>或者</strong><code><strong>contrib</strong></code><strong>或者</strong><code><strong>is_predict_contrib</strong></code><strong>：一个布尔值，表示是否输出每个特征对于每个样本的预测的贡献。默认为False。输出的结果形状为[nsamples,nfeatures+1]，之所以+1是考虑到bais的贡献。所有的贡献加起来就是该样本的预测结果。该参数只用于prediction任务。</strong></p></li><li><p><code><strong>bin_construct_sample_cnt</strong></code><strong>或者</strong><code><strong>subsample_for_bin</strong></code><strong>：一个整数，表示用来构建直方图的样本的数量。默认为200000。如果数据非常稀疏，则可以设置为一个更大的值，如果设置更大的值，则会提供更好的训练效果，但是会增加数据加载时间。</strong></p></li><li><p><code><strong>num_iteration_predict</strong></code><strong>：一个整数，表示在预测中使用多少棵子树。默认为-1。小于等于0表示使用模型的所有子树。该参数只用于prediction任务。</strong></p></li><li><p><code><strong>pred_early_stop</strong></code><strong>：一个布尔值，表示是否使用早停来加速预测。默认为False。如果为True，则可能影响精度。</strong></p></li><li><p><code><strong>pred_early_stop_freq</strong></code><strong>：一个整数，表示检查早停的频率。默认为10</strong></p></li><li><p><code><strong>pred_early_stop_margin</strong></code><strong>：一个浮点数，表示早停的边际阈值。默认为0</strong></p></li><li><p><code><strong>use_missing</strong></code><strong>：一个布尔值，表示是否使用缺失值功能。默认为True如果为False则禁用缺失值功能。</strong></p></li><li><p><code><strong>zero_as_missing</strong></code><strong>：一个布尔值，表示是否将所有的零(包括在libsvm/sparse矩阵中未显示的值)都视为缺失值。默认为False。如果为False，则将nan视作缺失值。如果为True，则</strong><code><strong>np.nan</strong></code><strong>和零都将视作缺失值。</strong></p></li><li><p><code><strong>init_score_file</strong></code><strong>：一个字符串，表示训练时的初始化分数文件的路径。默认为空字符串，表示train_data_file+”.init”(如果存在)</strong></p></li><li><p><code><strong>valid_init_score_file</strong></code><strong>：一个字符串，表示验证时的初始化分数文件的路径。默认为空字符串，表示valid_data_file+”.init”(如果存在)。如果有多个(对应于多个验证集)，则可以用逗号</strong><code><strong>,</strong></code><strong>来分隔。</strong></p></li></ul> 
 <h3><strong>(4) 目标参数</strong></h3> 
 <ul><li><p><code><strong>sigmoid</strong></code><strong>：一个浮点数，用sigmoid函数的参数，默认为0。它用于二分类任务和lambdarank任务。</strong></p></li><li><p><code><strong>alpha</strong></code><strong>：一个浮点数，用于Huber损失函数和Quantileregression，默认值为0。它用于huber回归任务和Quantile回归任务。</strong></p></li><li><p><code><strong>fair_c</strong></code><strong>：一个浮点数，用于Fair损失函数，默认值为0。它用于fair回归任务。</strong></p></li><li><p><code><strong>gaussian_eta</strong></code><strong>：一个浮点数，用于控制高斯函数的宽度，默认值为0。它用于regression_l1回归任务和huber回归任务。</strong></p></li><li><p><code><strong>posson_max_delta_step</strong></code><strong>：一个浮点数，用于Poisson regression的参数，默认值为7。它用于poisson回归任务。</strong></p></li><li><p><code><strong>scale_pos_weight</strong></code><strong>：一个浮点数，用于调整正样本的权重，默认值为0它用于二分类任务。</strong></p></li><li><p><code><strong>boost_from_average</strong></code><strong>：一个布尔值，指示是否将初始得分调整为平均值(它可以使得收敛速度更快)。默认为True。它用于回归任务。</strong></p></li><li><p><code><strong>is_unbalance</strong></code><strong>或者</strong><code><strong>unbalanced_set</strong></code><strong>：一个布尔值，指示训练数据是否均衡的。默认为True。它用于二分类任务。</strong></p></li><li><p><code><strong>max_position</strong></code><strong>：一个整数，指示将在这个NDCG位置优化。默认为20。它用于lambdarank任务。</strong></p></li><li><p><code><strong>label_gain</strong></code><strong>：一个浮点数序列，给出了每个标签的增益。默认值为0,1,3,7,15,….它用于lambdarank任务。</strong></p></li><li><p><code><strong>num_class</strong></code><strong>或者</strong><code><strong>num_classes</strong></code><strong>：一个整数，指示了多分类任务中的类别数量。默认为1它用于多分类任务。</strong></p></li><li><p><code><strong>reg_sqrt</strong></code><strong>：一个布尔值，默认为False。如果为True，则拟合的结果为：\sqrt{label}。同时预测的结果被自动转换为：{pred}^2。它用于回归任务。</strong></p></li></ul> 
 <h3><strong>(5) 度量参数</strong></h3> 
 <blockquote> 
  <p><code>metric</code>：一个字符串，指定了度量的指标，默认为：对于回归问题，使用l2；对于二分类问题，使用<code>binary_logloss</code>；对于lambdarank问题，使用ndcg。如果有多个度量指标，则用逗号<code>,</code>分隔。</p> 
 </blockquote> 
 <ul><li><p><code><strong>l1</strong></code><strong>或者</strong><code><strong>mean_absolute_error</strong></code><strong>或者</strong><code><strong>mae</strong></code><strong>或者</strong><code><strong>regression_l1</strong></code><strong>：表示绝对值损失。</strong></p></li><li><p><code><strong>l2</strong></code><strong>或者</strong><code><strong>mean_squared_error</strong></code><strong>或者</strong><code><strong>mse</strong></code><strong>或者</strong><code><strong>regression_l2</strong></code><strong>或者</strong><code><strong>regression</strong></code><strong>：表示平方损失。</strong></p></li><li><p><code><strong>l2_root</strong></code><strong>或者</strong><code><strong>root_mean_squared_error</strong></code><strong>或者</strong><code><strong>rmse</strong></code><strong>：表示开方损失。</strong></p></li><li><p><code><strong>quantile</strong></code><strong>：表示Quantile回归中的损失。</strong></p></li><li><p><code><strong>mape</strong></code><strong>或者</strong><code><strong>mean_absolute_percentage_error</strong></code><strong>：表示MAPE损失。</strong></p></li><li><p><code><strong>huber</strong></code><strong>：表示huber损失。</strong></p></li><li><p><code><strong>fair</strong></code><strong>：表示fair损失。</strong></p></li><li><p><code><strong>poisson</strong></code><strong>：表示poisson回归的负对数似然。</strong></p></li><li><p><code><strong>gamma</strong></code><strong>：表示gamma回归的负对数似然。</strong></p></li><li><p><code><strong>gamma_deviance</strong></code><strong>：表示gamma回归的残差的方差。</strong></p></li><li><p><code><strong>tweedie</strong></code><strong>：表示Tweedie回归的负对数似然。</strong></p></li><li><p><code><strong>ndcg</strong></code><strong>：表示NDCG。</strong></p></li><li><p><code><strong>map</strong></code><strong>或者</strong><code><strong>mean_average_precision</strong></code><strong>：表示平均的精度。</strong></p></li><li><p><code><strong>auc</strong></code><strong>：表示AUC。</strong></p></li><li><p><code><strong>binary_logloss</strong></code><strong>或者</strong><code><strong>binary</strong></code><strong>：表示二类分类中的对数损失函数。</strong></p></li><li><p><code><strong>binary_error</strong></code><strong>：表示二类分类中的分类错误率。</strong></p></li><li><p><code><strong>multi_logloss</strong></code><strong>或者</strong><code><strong>multiclass</strong></code><strong>或者</strong><code><strong>softmax</strong></code><strong>或者‘multiclassova</strong><code><strong>或者</strong></code><strong>multiclass_ova</strong><code><strong>，或者</strong></code><strong>ova</strong><code><strong>或者</strong></code><strong>ovr`：表示多类分类中的对数损失函数。</strong></p></li><li><p><code><strong>multi_error</strong></code><strong>：表示多分类中的分类错误率。</strong></p></li><li><p><code><strong>xentropy</strong></code><strong>或者</strong><code><strong>cross_entropy</strong></code><strong>：表示交叉熵。</strong></p></li><li><p><code><strong>xentlambda</strong></code><strong>或者</strong><code><strong>cross_entropy_lambda</strong></code><strong>：表示intensity加权的交叉熵。</strong></p></li><li><p><code><strong>kldiv</strong></code><strong>或者</strong><code><strong>kullback_leibler</strong></code><strong>：表示KL散度。</strong></p></li></ul> 
 <br> 
 <blockquote> 
  <p><code>metric_freq</code>或者<code>output_freq</code>：一个正式，表示每隔多少次输出一次度量结果。默认为1。</p> 
 </blockquote> 
 <blockquote> 
  <p><code>train_metric</code>或者<code>training_metric</code>或者<code>is_training_metric</code>：一个布尔值，默认为False。如果为True，则在训练时就输出度量结果。</p> 
 </blockquote> 
 <blockquote> 
  <p><code>ndcg_at</code>或者<code>ndcg_eval_at</code>或者<code>eval_at</code>：一个整数列表，指定了NDCG评估点的位置。默认为1、2、3、4、5。</p> 
 </blockquote> 
 <h3><strong>参数影响与调参建议</strong></h3> 
 <p>以下为总结的核心参数对模型的影响，及与之对应的调参建议。</p> 
 <h4><strong>(1) 对树生长控制</strong></h4> 
 <img src="https://images2.imgbox.com/f2/2e/mYFicz19_o.png" alt="16b257a64146ce372f71fc824537222b.png"> 
 <blockquote> 
  <p><code>num_leaves</code>：叶节点的数目。它是控制树模型复杂度的主要参数。</p> 
 </blockquote> 
 <ul><li><p><strong>如果是</strong><code><strong>level-wise</strong></code><strong>，则该参数为</strong><strong>，其中depth为树的深度。但是当叶子数量相同时，leaf-wise的树要远远深过level-wise树，非常容易导致过拟合。因此应该让num_leaves小于</strong><strong>。在leaf-wise树中，并不存在depth的概念。因为不存在一个从leaves到depth的合理映射。</strong></p></li></ul> 
 <br> 
 <blockquote> 
  <p><code>min_data_in_leaf</code>：每个叶节点的最少样本数量。</p> 
 </blockquote> 
 <ul><li><p><strong>它是处理</strong><code><strong>leaf-wise</strong></code><strong>树的过拟合的重要参数。将它设为较大的值，可以避免生成一个过深的树。但是也可能导致欠拟合。</strong></p></li></ul> 
 <br> 
 <blockquote> 
  <p><code>max_depth</code>：树的最大深度。该参数可以显式的限制树的深度。</p> 
 </blockquote> 
 <h4><strong>(2) 更快的训练速度</strong></h4> 
 <img src="https://images2.imgbox.com/d8/9e/DPTiE415_o.png" alt="0209817b10c3c187af14a32e82b450fd.png"> 
 <ul><li><p><strong>通过设置</strong><code><strong>bagging_fraction</strong></code><strong>和</strong><code><strong>bagging_freq</strong></code><strong>参数来使用bagging方法。</strong></p></li><li><p><strong>通过设置</strong><code><strong>feature_fraction</strong></code><strong>参数来使用特征的子抽样。</strong></p></li><li><p><strong>使用较小的</strong><code><strong>max_bin</strong></code><strong>。</strong></p></li><li><p><strong>使用</strong><code><strong>save_binary</strong></code><strong>在未来的学习过程对数据加载进行加速。</strong></p></li></ul> 
 <h4><strong>(3) 更好的模型效果</strong></h4> 
 <img src="https://images2.imgbox.com/ae/13/Eee4FHrg_o.png" alt="43ee7b60d4a0912a20e958e432b247c1.png"> 
 <ul><li><p><strong>使用较大的</strong><code><strong>max_bin</strong></code><strong>(学习速度可能变慢)。</strong></p></li><li><p><strong>使用较小的</strong><code><strong>learning_rate</strong></code><strong>和较大的</strong><code><strong>num_iterations</strong></code><strong>。</strong></p></li><li><p><strong>使用较大的</strong><code><strong>num_leaves</strong></code><strong>(可能导致过拟合)。</strong></p></li><li><p><strong>使用更大的训练数据。</strong></p></li><li><p><strong>尝试</strong><code><strong>dart</strong></code><strong>。</strong></p></li></ul> 
 <h4><strong>(4) 缓解过拟合问题</strong></h4> 
 <img src="https://images2.imgbox.com/df/34/vsVgWe4z_o.png" alt="8206d32f674d02cad4fa68724032a3a8.png"> 
 <ul><li><p><strong>使用较小的</strong><code><strong>max_bin</strong></code><strong>。</strong></p></li><li><p><strong>使用较小的</strong><code><strong>num_leaves</strong></code><strong>。</strong></p></li><li><p><strong>使用</strong><code><strong>min_data_in_leaf</strong></code><strong>和</strong><code><strong>min_sum_hessian_in_leaf</strong></code><strong>。</strong></p></li><li><p><strong>通过设置</strong><code><strong>bagging_fraction</strong></code><strong>和</strong><code><strong>bagging_freq</strong></code><strong>来使用</strong><code><strong>bagging</strong></code><strong>。</strong></p></li><li><p><strong>通过设置</strong><code><strong>feature_fraction</strong></code><strong>来使用特征子抽样。</strong></p></li><li><p><strong>使用更大的训练数据。</strong></p></li><li><p><strong>使用</strong><code><strong>lambda_l1</strong></code><strong>、</strong><code><strong>lambda_l2</strong></code><strong>和</strong><code><strong>min_gain_to_split</strong></code><strong>来使用正则。</strong></p></li><li><p><strong>尝试</strong><code><strong>max_depth</strong></code><strong>来避免生成过深的树。</strong></p></li></ul> 
 <h2><strong>3.LightGBM内置建模方式</strong></h2> 
 <h3><strong>内置建模方式</strong></h3> 
 <p>LightGBM内置了建模方式，有如下的数据格式与核心训练方法：</p> 
 <ul><li><p><strong>基于</strong><code><strong>lightgbm.Dataset</strong></code><strong>格式的数据。</strong></p></li><li><p><strong>基于</strong><code><strong>lightgbm.train</strong></code><strong>接口训练。</strong></p></li></ul> 
 <p>下面是官方的一个简单示例，演示了读取libsvm格式数据(成<code>Dataset</code>格式)并指定参数建模的过程。</p> 
 <pre class="has"><code class="language-go"># coding: utf-8
import json
import lightgbm as lgb
import pandas as pd
from sklearn.metrics import mean_squared_error


# 加载数据集合
print('加载数据...')
df_train = pd.read_csv('./data/regression.train.txt', header=None, sep='\t')
df_test = pd.read_csv('./data/regression.test.txt', header=None, sep='\t')

# 设定训练集和测试集
y_train = df_train[0].values
y_test = df_test[0].values
X_train = df_train.drop(0, axis=1).values
X_test = df_test.drop(0, axis=1).values

# 构建lgb中的Dataset格式
lgb_train = lgb.Dataset(X_train, y_train)
lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)

# 敲定好一组参数
params = {
    'task': 'train',
    'boosting_type': 'gbdt',
    'objective': 'regression',
    'metric': {'l2', 'auc'},
    'num_leaves': 31,
    'learning_rate': 0.05,
    'feature_fraction': 0.9,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'verbose': 0
}

print('开始训练...')
# 训练
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=20,
                valid_sets=lgb_eval,
                early_stopping_rounds=5)

# 保存模型
print('保存模型...')
# 保存模型到文件中
gbm.save_model('model.txt')

print('开始预测...')
# 预测
y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)
# 评估
print('预估结果的rmse为:')
print(mean_squared_error(y_test, y_pred) ** 0.5)</code></pre> 
 <img src="https://images2.imgbox.com/49/19/M6ckqwcL_o.png" alt="a1b6c664dcb17e91661ef7b141a89f38.png"> 
 <pre class="has"><code class="language-go">加载数据...
开始训练...
[1]  valid_0's l2: 0.24288   valid_0's auc: 0.764496
Training until validation scores don't improve for 5 rounds.
[2]  valid_0's l2: 0.239307  valid_0's auc: 0.766173
[3]  valid_0's l2: 0.235559  valid_0's auc: 0.785547
[4]  valid_0's l2: 0.230771  valid_0's auc: 0.797786
[5]  valid_0's l2: 0.226297  valid_0's auc: 0.805155
[6]  valid_0's l2: 0.223692  valid_0's auc: 0.800979
[7]  valid_0's l2: 0.220941  valid_0's auc: 0.806566
[8]  valid_0's l2: 0.217982  valid_0's auc: 0.808566
[9]  valid_0's l2: 0.215351  valid_0's auc: 0.809041
[10] valid_0's l2: 0.213064  valid_0's auc: 0.805953
[11] valid_0's l2: 0.211053  valid_0's auc: 0.804631
[12] valid_0's l2: 0.209336  valid_0's auc: 0.802922
[13] valid_0's l2: 0.207492  valid_0's auc: 0.802011
[14] valid_0's l2: 0.206016  valid_0's auc: 0.80193
Early stopping, best iteration is:
[9]  valid_0's l2: 0.215351  valid_0's auc: 0.809041
保存模型...
开始预测...
预估结果的rmse为:
0.4640593794679212</code></pre> 
 <h3><strong>设置样本权重</strong></h3> 
 <p>LightGBM的建模非常灵活，它可以支持我们对于每个样本设置不同的权重学习，设置的方式也非常简单，我们需要提供给模型一组权重数组数据，长度和样本数一致。</p> 
 <p>如下是一个典型的例子，其中<code>binary.train</code>和<code>binary.test</code>读取后加载为<code>lightgbm.Dataset</code>格式的输入，而在<code>lightgbm.Dataset</code>的构建参数中可以设置样本权重(这个例子中是numpy array的形态)。再基于<code>lightgbm.train</code>接口使用内置建模方式训练。</p> 
 <pre class="has"><code class="language-go"># coding: utf-8
import json
import lightgbm as lgb
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error
import warnings
warnings.filterwarnings("ignore")

# 加载数据集
print('加载数据...')
df_train = pd.read_csv('./data/binary.train', header=None, sep='\t')
df_test = pd.read_csv('./data/binary.test', header=None, sep='\t')
W_train = pd.read_csv('./data/binary.train.weight', header=None)[0]
W_test = pd.read_csv('./data/binary.test.weight', header=None)[0]

y_train = df_train[0].values
y_test = df_test[0].values
X_train = df_train.drop(0, axis=1).values
X_test = df_test.drop(0, axis=1).values

num_train, num_feature = X_train.shape

# 加载数据的同时加载权重
lgb_train = lgb.Dataset(X_train, y_train,
                        weight=W_train, free_raw_data=False)
lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train,
                       weight=W_test, free_raw_data=False)

# 设定参数
params = {
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'metric': 'binary_logloss',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'feature_fraction': 0.9,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'verbose': 0
}

# 产出特征名称
feature_name = ['feature_' + str(col) for col in range(num_feature)]

print('开始训练...')
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                valid_sets=lgb_train,  # 评估训练集
                feature_name=feature_name,
                categorical_feature=[21])</code></pre> 
 <pre class="has"><code class="language-go">加载数据...
开始训练...
[1]  training's binary_logloss: 0.68205
[2]  training's binary_logloss: 0.673618
[3]  training's binary_logloss: 0.665891
[4]  training's binary_logloss: 0.656874
[5]  training's binary_logloss: 0.648523
[6]  training's binary_logloss: 0.641874
[7]  training's binary_logloss: 0.636029
[8]  training's binary_logloss: 0.629427
[9]  training's binary_logloss: 0.623354
[10] training's binary_logloss: 0.617593</code></pre> 
 <h3><strong>模型存储与加载</strong></h3> 
 <p>上述建模过程得到的模型对象，可以通过save_model成员函数进行保存。保存好的模型可以通过<code>lgb.Booster</code>加载回内存，并对测试集进行预测。</p> 
 <p>具体示例代码如下：</p> 
 <pre class="has"><code class="language-go"># 查看特征名称
print('完成10轮训练...')
print('第7个特征为:')
print(repr(lgb_train.feature_name[6]))

# 存储模型
gbm.save_model('./model/lgb_model.txt')

# 特征名称
print('特征名称:')
print(gbm.feature_name())

# 特征重要度
print('特征重要度:')
print(list(gbm.feature_importance()))

# 加载模型
print('加载模型用于预测')
bst = lgb.Booster(model_file='./model/lgb_model.txt')

# 预测
y_pred = bst.predict(X_test)

# 在测试集评估效果
print('在测试集上的rmse为:')
print(mean_squared_error(y_test, y_pred) ** 0.5)</code></pre> 
 <img src="https://images2.imgbox.com/dc/ab/wIXMi9uX_o.png" alt="879d8c18d2ab6c314716a12bd2d23c75.png"> 
 <pre class="has"><code class="language-go">完成10轮训练...
第7个特征为:
'feature_6'
特征名称:
['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27']
特征重要度:
[8, 5, 1, 19, 7, 33, 2, 0, 2, 10, 5, 2, 0, 9, 3, 3, 0, 2, 2, 5, 1, 0, 36, 3, 33, 45, 29, 35]
加载模型用于预测
在测试集上的rmse为:
0.4629245607636925</code></pre> 
 <h3><strong>继续训练</strong></h3> 
 <p>LightGBM 为 boosting模型，每一轮训练会增加新的基学习器，LightGBM 还支持基于现有模型和参数继续训练，无需每次从头训练。</p> 
 <p>如下是典型的示例，我们加载已经训练10轮(即10颗树集成)的lgb模型，在此基础上继续训练(在参数层面做了一些改变，调整了学习率，增加了一些 bagging 等缓解过拟合的处理方法)</p> 
 <pre class="has"><code class="language-go"># 继续训练
# 从./model/model.txt中加载模型初始化
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                init_model='./model/lgb_model.txt',
                valid_sets=lgb_eval)

print('以旧模型为初始化，完成第 10-20 轮训练...')

# 在训练的过程中调整超参数
# 比如这里调整的是学习率
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                init_model=gbm,
                learning_rates=lambda iter: 0.05 * (0.99 ** iter),
                valid_sets=lgb_eval)

print('逐步调整学习率完成第 20-30 轮训练...')

# 调整其他超参数
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                init_model=gbm,
                valid_sets=lgb_eval,
                callbacks=[lgb.reset_parameter(bagging_fraction=[0.7] * 5 + [0.6] * 5)])

print('逐步调整bagging比率完成第 30-40 轮训练...')</code></pre> 
 <img src="https://images2.imgbox.com/53/1c/WpHjvHfW_o.png" alt="8d3d0178ed70fc6006324d78240c7571.png"> 
 <pre class="has"><code class="language-go">[11] valid_0's binary_logloss: 0.616177
[12] valid_0's binary_logloss: 0.611792
[13] valid_0's binary_logloss: 0.607043
[14] valid_0's binary_logloss: 0.602314
[15] valid_0's binary_logloss: 0.598433
[16] valid_0's binary_logloss: 0.595238
[17] valid_0's binary_logloss: 0.592047
[18] valid_0's binary_logloss: 0.588673
[19] valid_0's binary_logloss: 0.586084
[20] valid_0's binary_logloss: 0.584033
以旧模型为初始化，完成第 10-20 轮训练...
[21] valid_0's binary_logloss: 0.616177
[22] valid_0's binary_logloss: 0.611834
[23] valid_0's binary_logloss: 0.607177
[24] valid_0's binary_logloss: 0.602577
[25] valid_0's binary_logloss: 0.59831
[26] valid_0's binary_logloss: 0.595259
[27] valid_0's binary_logloss: 0.592201
[28] valid_0's binary_logloss: 0.589017
[29] valid_0's binary_logloss: 0.586597
[30] valid_0's binary_logloss: 0.584454
逐步调整学习率完成第 20-30 轮训练...
[31] valid_0's binary_logloss: 0.616053
[32] valid_0's binary_logloss: 0.612291
[33] valid_0's binary_logloss: 0.60856
[34] valid_0's binary_logloss: 0.605387
[35] valid_0's binary_logloss: 0.601744
[36] valid_0's binary_logloss: 0.598556
[37] valid_0's binary_logloss: 0.595585
[38] valid_0's binary_logloss: 0.593228
[39] valid_0's binary_logloss: 0.59018
[40] valid_0's binary_logloss: 0.588391
逐步调整bagging比率完成第 30-40 轮训练...</code></pre> 
 <h3><strong>自定义损失函数</strong></h3> 
 <p>LightGBM 支持在训练过程中，自定义损失函数和评估准则，其中损失函数的定义需要返回损失函数一阶和二阶导数的计算方法，评估准则部分需要对数据的 label 和预估值进行计算。其中损失函数用于训练过程中的树结构学习，而评估准则很多时候是用在验证集上进行效果评估。</p> 
 <pre class="has"><code class="language-go"># 自定义损失函数需要提供损失函数的一阶和二阶导数形式
def loglikelood(preds, train_data):
    labels = train_data.get_label()
    preds = 1. / (1. + np.exp(-preds))
    grad = preds - labels
    hess = preds * (1. - preds)
    return grad, hess


# 自定义评估函数
def binary_error(preds, train_data):
    labels = train_data.get_label()
    return 'error', np.mean(labels != (preds &gt; 0.5)), False


gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                init_model=gbm,
                fobj=loglikelood,
                feval=binary_error,
                valid_sets=lgb_eval)

print('用自定义的损失函数与评估标准完成第40-50轮...')</code></pre> 
 <img src="https://images2.imgbox.com/54/68/Vmn15HD5_o.png" alt="c0f4c08b95a0653a7bf4b6740a070683.png"> 
 <pre class="has"><code class="language-go">[41] valid_0's binary_logloss: 0.614429  valid_0's error: 0.268
[42] valid_0's binary_logloss: 0.610689  valid_0's error: 0.26
[43] valid_0's binary_logloss: 0.606267  valid_0's error: 0.264
[44] valid_0's binary_logloss: 0.601949  valid_0's error: 0.258
[45] valid_0's binary_logloss: 0.597271  valid_0's error: 0.266
[46] valid_0's binary_logloss: 0.593971  valid_0's error: 0.276
[47] valid_0's binary_logloss: 0.591427  valid_0's error: 0.278
[48] valid_0's binary_logloss: 0.588301  valid_0's error: 0.284
[49] valid_0's binary_logloss: 0.586562  valid_0's error: 0.288
[50] valid_0's binary_logloss: 0.584056  valid_0's error: 0.288
用自定义的损失函数与评估标准完成第40-50轮...</code></pre> 
 <h2><strong>4.LightGBM预估器形态接口</strong></h2> 
 <h3><strong>SKLearn形态预估器接口</strong></h3> 
 <p>和 XGBoost 一样，LightGBM 也支持用 SKLearn 中统一的预估器形态接口进行建模，如下为典型的参考案例，对于读取为 Dataframe 格式的训练集和测试集，可以直接使用 LightGBM 初始化<code>LGBMRegressor</code>进行 fit 拟合训练。使用方法与接口，和 SKLearn 中其他预估器一致。</p> 
 <pre class="has"><code class="language-go"># coding: utf-8
import lightgbm as lgb
import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV

# 加载数据
print('加载数据...')
df_train = pd.read_csv('./data/regression.train.txt', header=None, sep='\t')
df_test = pd.read_csv('./data/regression.test.txt', header=None, sep='\t')

# 取出特征和标签
y_train = df_train[0].values
y_test = df_test[0].values
X_train = df_train.drop(0, axis=1).values
X_test = df_test.drop(0, axis=1).values

print('开始训练...')
# 初始化LGBMRegressor
gbm = lgb.LGBMRegressor(objective='regression',
                        num_leaves=31,
                        learning_rate=0.05,
                        n_estimators=20)

# 使用fit函数拟合
gbm.fit(X_train, y_train,
        eval_set=[(X_test, y_test)],
        eval_metric='l1',
        early_stopping_rounds=5)

# 预测
print('开始预测...')
y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)
# 评估预测结果
print('预测结果的rmse是:')
print(mean_squared_error(y_test, y_pred) ** 0.5)</code></pre> 
 <img src="https://images2.imgbox.com/7a/ed/CSCzhxQN_o.png" alt="9010aea86a9b549b4ea2e7c4107556e3.png"> 
 <pre class="has"><code class="language-go">加载数据...
开始训练...
[1]  valid_0's l1: 0.491735
Training until validation scores don't improve for 5 rounds.
[2]  valid_0's l1: 0.486563
[3]  valid_0's l1: 0.481489
[4]  valid_0's l1: 0.476848
[5]  valid_0's l1: 0.47305
[6]  valid_0's l1: 0.469049
[7]  valid_0's l1: 0.465556
[8]  valid_0's l1: 0.462208
[9]  valid_0's l1: 0.458676
[10] valid_0's l1: 0.454998
[11] valid_0's l1: 0.452047
[12] valid_0's l1: 0.449158
[13] valid_0's l1: 0.44608
[14] valid_0's l1: 0.443554
[15] valid_0's l1: 0.440643
[16] valid_0's l1: 0.437687
[17] valid_0's l1: 0.435454
[18] valid_0's l1: 0.433288
[19] valid_0's l1: 0.431297
[20] valid_0's l1: 0.428946
Did not meet early stopping. Best iteration is:
[20] valid_0's l1: 0.428946
开始预测...
预测结果的rmse是:
0.4441153344254208</code></pre> 
 <h3><strong>网格搜索调参</strong></h3> 
 <p>上面提到 LightGBM 的预估器接口，整体使用方法和 SKLearn 中其他预估器一致，所以我们也可以使用 SKLearn 中的超参数调优方法来进行模型调优。</p> 
 <p>如下是一个典型的网格搜索交法调优超参数的代码示例，我们会给出候选参数列表字典，通过<code>GridSearchCV</code>进行交叉验证实验评估，选出 LightGBM 在候选参数中最优的超参数。</p> 
 <pre class="has"><code class="language-go"># 配合scikit-learn的网格搜索交叉验证选择最优超参数
estimator = lgb.LGBMRegressor(num_leaves=31)

param_grid = {
    'learning_rate': [0.01, 0.1, 1],
    'n_estimators': [20, 40]
}

gbm = GridSearchCV(estimator, param_grid)

gbm.fit(X_train, y_train)

print('用网格搜索找到的最优超参数为:')
print(gbm.best_params_)</code></pre> 
 <img src="https://images2.imgbox.com/43/8e/RWR3zxpn_o.png" alt="26b7fc71aca8c880e734e101bc79580b.png"> 
 <pre class="has"><code class="language-go">用网格搜索找到的最优超参数为:
{'learning_rate': 0.1, 'n_estimators': 40}</code></pre> 
 <h3><strong>绘图解释</strong></h3> 
 <p>LightGBM 支持对模型训练进行可视化呈现与解释，包括对于训练过程中的损失函数取值与评估准则结果的可视化、训练完成后特征重要度的排序与可视化、基学习器(比如决策树)的可视化。</p> 
 <p>以下为参考代码：</p> 
 <pre class="has"><code class="language-go"># coding: utf-8
import lightgbm as lgb
import pandas as pd

try:
    import matplotlib.pyplot as plt
except ImportError:
    raise ImportError('You need to install matplotlib for plotting.')

# 加载数据集
print('加载数据...')
df_train = pd.read_csv('./data/regression.train.txt', header=None, sep='\t')
df_test = pd.read_csv('./data/regression.test.txt', header=None, sep='\t')

# 取出特征和标签
y_train = df_train[0].values
y_test = df_test[0].values
X_train = df_train.drop(0, axis=1).values
X_test = df_test.drop(0, axis=1).values

# 构建lgb中的Dataset数据格式
lgb_train = lgb.Dataset(X_train, y_train)
lgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)

# 设定参数
params = {
    'num_leaves': 5,
    'metric': ('l1', 'l2'),
    'verbose': 0
}

evals_result = {}  # to record eval results for plotting

print('开始训练...')
# 训练
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=100,
                valid_sets=[lgb_train, lgb_test],
                feature_name=['f' + str(i + 1) for i in range(28)],
                categorical_feature=[21],
                evals_result=evals_result,
                verbose_eval=10)

print('在训练过程中绘图...')
ax = lgb.plot_metric(evals_result, metric='l1')
plt.show()

print('画出特征重要度...')
ax = lgb.plot_importance(gbm, max_num_features=10)
plt.show()

print('画出第84颗树...')
ax = lgb.plot_tree(gbm, tree_index=83, figsize=(20, 8), show_info=['split_gain'])
plt.show()

#print('用graphviz画出第84颗树...')
#graph = lgb.create_tree_digraph(gbm, tree_index=83, name='Tree84')
#graph.render(view=True)</code></pre> 
 <img src="https://images2.imgbox.com/f8/45/xrb0P3NV_o.png" alt="34016059d4b3f15a3c96184e5a58998d.png"> 
 <pre class="has"><code class="language-go">加载数据...
开始训练...
[10] training's l2: 0.217995 training's l1: 0.457448 valid_1's l2: 0.21641   valid_1's l1: 0.456464
[20] training's l2: 0.205099 training's l1: 0.436869 valid_1's l2: 0.201616  valid_1's l1: 0.434057
[30] training's l2: 0.197421 training's l1: 0.421302 valid_1's l2: 0.192514  valid_1's l1: 0.417019
[40] training's l2: 0.192856 training's l1: 0.411107 valid_1's l2: 0.187258  valid_1's l1: 0.406303
[50] training's l2: 0.189593 training's l1: 0.403695 valid_1's l2: 0.183688  valid_1's l1: 0.398997
[60] training's l2: 0.187043 training's l1: 0.398704 valid_1's l2: 0.181009  valid_1's l1: 0.393977
[70] training's l2: 0.184982 training's l1: 0.394876 valid_1's l2: 0.178803  valid_1's l1: 0.389805
[80] training's l2: 0.1828   training's l1: 0.391147 valid_1's l2: 0.176799  valid_1's l1: 0.386476
[90] training's l2: 0.180817 training's l1: 0.388101 valid_1's l2: 0.175775  valid_1's l1: 0.384404
[100]   training's l2: 0.179171 training's l1: 0.385174 valid_1's l2: 0.175321  valid_1's l1: 0.382929</code></pre> 
 <img src="https://images2.imgbox.com/2c/d6/kJ0m8zfi_o.png" alt="e265f16faaa797363acb94ca4790ba24.png"> 
 <img src="https://images2.imgbox.com/9f/c3/QVh2slVg_o.png" alt="cda85355b8505739ebb49a788b9ba5ec.png"> 
 <h4><strong>参考资料</strong></h4> 
 <p>[1]</p> 
 <p>图解机器学习 | LightGBM模型详解: <em>https://www.showmeai.tech/article-detail/195</em></p> 
 [2] 
  
 <p>图解python | 安装与环境设置](https://www.showmeai.tech/article-detail/65: <em>https://www.showmeai.tech/article-detail/65</em></p> 
 [3] 
  
 <p>XGBoost工具库建模应用详解: <em>https://www.showmeai.tech/article-detail/204</em></p> 
 [4] 
  
 <p>LightGBM中文文档: <em>https://lightgbm.apachecn.org/#/</em></p> 
 <pre></pre> 
 <pre></pre> 
 <p><img src="https://images2.imgbox.com/49/10/5ODnG1cc_o.jpg" alt="37631c81f04a82adff853d4864ae5320.jpeg"></p> 
 <pre></pre> 
 <pre></pre> 
 <pre></pre> 
 <pre></pre> 
 <pre class="has"><code class="language-go">往期精彩回顾




适合初学者入门人工智能的路线及资料下载(图文+视频)机器学习入门系列下载机器学习及深度学习笔记等资料打印《统计学习方法》的代码复现专辑</code></pre> 
 <ul><li><pre class="has"><code class="language-go">交流群</code></pre></li></ul> 
 <p><strong>欢迎加入机器学习爱好者微信群一起和同行交流，目前有机器学习交流群、博士群、博士申报交流、CV、NLP等微信群，请扫描下面的微信号加群，备注：”昵称-学校/公司-研究方向“，例如：”张小明-浙大-CV“。请按照格式备注，否则不予通过。添加成功后会根据研究方向邀请进入相关微信群。请勿在群内发送广告，否则会请出群，谢谢理解~（</strong>也可以加入机器学习交流qq群772479961）</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/2b/9b/y8TBKYIl_o.png" alt="5bdbbbcdf0b8170c786874dbe6af51e8.png"></p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b3598c1b567712af569b7e8e6c47aa11/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Interesting！只需上传照片，GPT-4V精准识别食物的卡路里和摄入热量</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2439e7f9ec9db5d40464e483518d06ef/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Led驱动模块加载与测试</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>