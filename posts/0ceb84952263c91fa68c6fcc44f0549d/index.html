<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Deep Learning论文笔记之（四）CNN卷积神经网络推导和实现 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Deep Learning论文笔记之（四）CNN卷积神经网络推导和实现" />
<meta property="og:description" content="Deep Learning论文笔记之（四）CNN卷积神经网络推导和实现
zouxy09@qq.com
http://blog.csdn.net/zouxy09
自己平时看了一些论文，但老感觉看完过后就会慢慢的淡忘，某一天重新拾起来的时候又好像没有看过一样。所以想习惯地把一些感觉有用的论文中的知识点总结整理一下，一方面在整理过程中，自己的理解也会更深，另一方面也方便未来自己的勘察。更好的还可以放到博客上面与大家交流。因为基础有限，所以对论文的一些理解可能不太正确，还望大家不吝指正交流，谢谢。
本文的论文来自：
Notes on Convolutional Neural Networks, Jake Bouvrie。
这个主要是CNN的推导和实现的一些笔记，再看懂这个笔记之前，最好具有CNN的一些基础。这里也先列出一个资料供参考：
[1] Deep Learning（深度学习）学习笔记整理系列之（七）
[2] LeNet-5, convolutional neural networks
[3]卷积神经网络
[4] Neural Network for Recognition of Handwritten Digits
[5] Deep learning：三十八(Stacked CNN简单介绍)
[6] Gradient-based learning applied to document recognition.
[7]Imagenet classification with deep convolutional neural networks.
[8] UFLDL中的“卷积特征提取”和“池化”。
另外，这里有个matlab的Deep Learning的toolbox，里面包含了CNN的代码，在下一个博文中，我将会详细注释这个代码。这个笔记对这个代码的理解非常重要。
下面是自己对其中的一些知识点的理解：
《Notes on Convolutional Neural Networks》
一、介绍
这个文档讨论的是CNNs的推导和实现。CNN架构的连接比权值要多很多，这实际上就隐含着实现了某种形式的规则化。这种特别的网络假定了我们希望通过数据驱动的方式学习到一些滤波器，作为提取输入的特征的一种方法。
本文中，我们先对训练全连接网络的经典BP算法做一个描述，然后推导2D CNN网络的卷积层和子采样层的BP权值更新方法。在推导过程中，我们更强调实现的效率，所以会给出一些Matlab代码。最后，我们转向讨论如何自动地学习组合前一层的特征maps，特别地，我们还学习特征maps的稀疏组合。
二、全连接的反向传播算法
典型的CNN中，开始几层都是卷积和下采样的交替，然后在最后一些层（靠近输出层的），都是全连接的一维网络。这时候我们已经将所有两维2D的特征maps转化为全连接的一维网络的输入。这样，当你准备好将最终的2D特征maps输入到1D网络中时，一个非常方便的方法就是把所有输出的特征maps连接成一个长的输入向量。然后我们回到BP算法的讨论。（更详细的基础推导可以参考UFLDL中“反向传导算法”）。
2.1、Feedforward Pass前向传播
在下面的推导中，我们采用平方误差代价函数。我们讨论的是多类问题，共c类，共N个训练样本。
这里表示第n个样本对应的标签的第k维。表示第n个样本对应的网络输出的第k个输出。对于多类问题，输出一般组织为“one-of-c”的形式，也就是只有该输入对应的类的输出节点输出为正，其他类的位或者节点为0或者负数，这个取决于你输出层的激活函数。sigmoid就是0，tanh就是-1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/0ceb84952263c91fa68c6fcc44f0549d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2013-08-16T00:40:14+08:00" />
<meta property="article:modified_time" content="2013-08-16T00:40:14+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Deep Learning论文笔记之（四）CNN卷积神经网络推导和实现</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p align="center"><strong><span style="font-size:18px;">Deep Learning论文笔记之（四）CNN卷积神经网络推导和实现</span></strong></p> 
<p align="center"><a target="_blank" href="mailto:zouxy09@qq.com" rel="nofollow noopener noreferrer"><span style="font-size:18px;color:#0000ff;">zouxy09@qq.com</span></a></p> 
<p align="center"><a target="_blank" href="http://blog.csdn.net/zouxy09" rel="noopener noreferrer"><span style="font-size:18px;color:#0000ff;">http://blog.csdn.net/zouxy09</span></a></p> 
<p><span style="font-size:18px;"> </span></p> 
<p><span style="font-size:18px;"><strong><span style="font-family:Calibri;">         </span></strong>自己平时看了一些论文，但老感觉看完过后就会慢慢的淡忘，某一天重新拾起来的时候又好像没有看过一样。所以想习惯地把一些感觉有用的论文中的知识点总结整理一下，一方面在整理过程中，自己的理解也会更深，另一方面也方便未来自己的勘察。更好的还可以放到博客上面与大家交流。因为基础有限，所以对论文的一些理解可能不太正确，还望大家不吝指正交流，谢谢。</span></p> 
<p><span style="font-family:Calibri;font-size:18px;"> </span></p> 
<p><span style="font-size:18px;">       本文的论文来自：</span></p> 
<p><a target="_blank" href="http://cogprints.org/5869/1/cnn_tutorial.pdf" rel="nofollow noopener noreferrer"><span style="font-family:Calibri;font-size:18px;color:#0000ff;">Notes on Convolutional Neural Networks,</span></a><span style="font-size:18px;"><span style="font-family:Calibri;"> Jake Bouvrie</span>。</span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>这个主要是<span style="font-family:Calibri;">CNN</span>的推导和实现的一些笔记，再看懂这个笔记之前，最好具有<span style="font-family:Calibri;">CNN</span>的一些基础。这里也先列出一个资料供参考：</span></p> 
<p><span style="font-family:Calibri;font-size:18px;">[1] </span><a target="_blank" href="http://blog.csdn.net/zouxy09/article/details/8781543" rel="noopener noreferrer"><span style="font-size:18px;"><span style="font-family:Calibri;color:#0000ff;">Deep Learning</span>（深度学习）学习笔记整理系列之（七）</span></a></p> 
<p><span style="font-family:Calibri;font-size:18px;">[2] </span><a target="_blank" href="http://yann.lecun.com/exdb/lenet/index.html" rel="nofollow noopener noreferrer"><span style="font-family:Calibri;font-size:18px;color:#0000ff;">LeNet-5, convolutional neural networks</span></a></p> 
<p><span style="font-family:Calibri;font-size:18px;">[3]</span><a target="_blank" href="http://wenku.baidu.com/view/cd16fb8302d276a200292e22.html" rel="nofollow noopener noreferrer"><span style="font-size:18px;color:#0000ff;">卷积神经网络</span></a></p> 
<p><span style="font-family:Calibri;font-size:18px;">[4] </span><a target="_blank" href="http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi" rel="nofollow noopener noreferrer"><span style="font-family:Calibri;font-size:18px;color:#0000ff;">Neural Network for Recognition of Handwritten Digits</span></a></p> 
<p><span style="font-family:Calibri;font-size:18px;">[5] </span><a target="_blank" href="http://www.cnblogs.com/tornadomeet/archive/2013/05/05/3061457.html" rel="nofollow noopener noreferrer"><span style="font-size:18px;"><span style="font-family:Calibri;color:#0000ff;">Deep learning</span>：三十八<span style="font-family:Calibri;">(Stacked CNN</span>简单介绍<span style="font-family:Calibri;">)</span></span></a></p> 
<p><span style="font-family:Calibri;font-size:18px;">[6] Gradient-based learning applied to document recognition.</span></p> 
<p><span style="font-family:Calibri;font-size:18px;">[7]Imagenet classification with deep convolutional neural networks.</span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">[8] UFLDL</span>中的“</span><a target="_blank" href="http://deeplearning.stanford.edu/wiki/index.php/%E5%8D%B7%E7%A7%AF%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96" rel="nofollow noopener noreferrer"><span style="font-size:18px;color:#0000ff;">卷积特征提取</span></a><span style="font-size:18px;">”和“</span><a target="_blank" href="http://deeplearning.stanford.edu/wiki/index.php/%E6%B1%A0%E5%8C%96" rel="nofollow noopener noreferrer"><span style="font-size:18px;color:#0000ff;">池化</span></a><span style="font-size:18px;">”。</span></p> 
<p><span style="font-size:18px;">        另外，这里有个<span style="font-family:Calibri;">matlab</span>的</span><a target="_blank" href="https://github.com/rasmusbergpalm/DeepLearnToolbox" rel="noopener noreferrer"><span style="font-size:18px;"><span style="font-family:Calibri;color:#0000ff;">Deep Learning</span>的<span style="font-family:Calibri;">toolbox</span></span></a><span style="font-size:18px;">，里面包含了<span style="font-family:Calibri;">CNN</span>的代码，在下一个博文中，我将会详细注释这个代码。这个笔记对这个代码的理解非常重要。</span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>下面是自己对其中的一些知识点的理解：</span></p> 
<p><span style="font-family:Calibri;font-size:18px;"> </span></p> 
<p align="center"><strong><span style="font-size:18px;">《<span style="font-family:Calibri;">Notes on Convolutional Neural Networks</span>》</span></strong></p> 
<p><strong><span style="font-size:18px;">一、介绍</span></strong></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>这个文档讨论的是<span style="font-family:Calibri;">CNNs</span>的推导和实现。<span style="font-family:Calibri;">CNN</span>架构的连接比权值要多很多，这实际上就隐含着实现了某种形式的规则化。这种特别的网络假定了我们希望通过数据驱动的方式学习到一些滤波器，作为提取输入的特征的一种方法。</span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>本文中，我们先对训练全连接网络的经典<span style="font-family:Calibri;">BP</span>算法做一个描述，然后推导<span style="font-family:Calibri;">2D CNN</span>网络的卷积层和子采样层的<span style="font-family:Calibri;">BP</span>权值更新方法。在推导过程中，我们更强调实现的效率，所以会给出一些<span style="font-family:Calibri;">Matlab</span>代码。最后，我们转向讨论如何自动地学习组合前一层的特征<span style="font-family:Calibri;">maps</span>，特别地，我们还学习特征<span style="font-family:Calibri;">maps</span>的稀疏组合。</span></p> 
<p><span style="font-family:Calibri;font-size:18px;"> </span></p> 
<p><strong><span style="font-size:18px;">二、全连接的反向传播算法</span></strong></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>典型的<span style="font-family:Calibri;">CNN</span>中，开始几层都是卷积和下采样的交替，然后在最后一些层（靠近输出层的），都是全连接的一维网络。这时候我们已经将所有两维<span style="font-family:Calibri;">2D</span>的特征<span style="font-family:Calibri;">maps</span>转化为全连接的一维网络的输入。这样，当你准备好将最终的<span style="font-family:Calibri;">2D</span>特征<span style="font-family:Calibri;">maps</span>输入到<span style="font-family:Calibri;">1D</span>网络中时，一个非常方便的方法就是把所有输出的特征<span style="font-family:Calibri;">maps</span>连接成一个长的输入向量。然后我们回到<span style="font-family:Calibri;">BP</span>算法的讨论。（更详细的基础推导可以参考<span style="font-family:Calibri;">UFLDL</span>中“</span><a target="_blank" href="http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95" rel="nofollow noopener noreferrer"><span style="font-size:18px;color:#0000ff;">反向传导算法</span></a><span style="font-size:18px;">”）。</span></p> 
<p><strong><span style="font-size:18px;"><span style="font-family:Calibri;">2.1</span>、<span style="font-family:Calibri;">Feedforward Pass</span>前向传播</span></strong></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>在下面的推导中，我们采用平方误差代价函数。我们讨论的是多类问题，共<span style="font-family:Calibri;">c</span>类，共<span style="font-family:Calibri;">N</span>个训练样本。</span></p> 
<p align="center"><img src="https://images2.imgbox.com/a6/7b/7TMq5jCX_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>这里<img src="https://images2.imgbox.com/02/de/zEefvXlw_o.jpg" alt="">表示第<span style="font-family:Calibri;">n</span>个样本对应的标签的第<span style="font-family:Calibri;">k</span>维。<img src="https://images2.imgbox.com/c2/b6/rRUG5NlF_o.jpg" alt="">表示第<span style="font-family:Calibri;">n</span>个样本对应的网络输出的第<span style="font-family:Calibri;">k</span>个输出。对于多类问题，输出一般组织为“<span style="font-family:Calibri;">one-of-c</span>”的形式，也就是只有该输入对应的类的输出节点输出为正，其他类的位或者节点为<span style="font-family:Calibri;">0</span>或者负数，这个取决于你输出层的激活函数。<span style="font-family:Calibri;">sigmoid</span>就是<span style="font-family:Calibri;">0</span>，<span style="font-family:Calibri;">tanh</span>就是<span style="font-family:Calibri;">-1.</span></span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>因为在全部训练集上的误差只是每个训练样本的误差的总和，所以这里我们先考虑对于一个样本的<span style="font-family:Calibri;">BP</span>。对于第<span style="font-family:Calibri;">n</span>个样本的误差，表示为：</span></p> 
<p align="center"><img src="https://images2.imgbox.com/9b/37/Ae7H4gZE_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-size:18px;">       传统的全连接神经网络中，我们需要根据<span style="font-family:Calibri;">BP</span>规则计算代价函数<span style="font-family:Calibri;">E</span>关于网络每一个权值的偏导数。我们用<span style="font-family:Calibri;">l</span>来表示当前层，那么当前层的输出可以表示为：</span></p> 
<p align="center"><img src="https://images2.imgbox.com/e1/c7/QSztCJMQ_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-size:18px;">       输出激活函数<span style="font-family:Calibri;">f(.)</span>可以有很多种，一般是<span style="font-family:Calibri;">sigmoid</span>函数或者双曲线正切函数。<span style="font-family:Calibri;">sigmoid</span>将输出压缩到<span style="font-family:Calibri;">[0, 1]</span>，所以最后的输出平均值一般趋于<span style="font-family:Calibri;">0 </span>。所以如果将我们的训练数据归一化为零均值和方差为<span style="font-family:Calibri;">1</span>，可以在梯度下降的过程中增加收敛性。对于归一化的数据集来说，双曲线正切函数也是不错的选择。</span></p> 
<p><span style="font-family:Calibri;font-size:18px;"> </span></p> 
<p><strong><span style="font-size:18px;"><span style="font-family:Calibri;">2.2</span>、<span style="font-family:Calibri;">Backpropagation Pass</span>反向传播</span></strong></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>反向传播回来的误差可以看做是每个神经元的基的灵敏度<span style="font-family:Calibri;">sensitivities</span>（灵敏度的意思就是我们的基<span style="font-family:Calibri;">b</span>变化多少，误差会变化多少，也就是误差对基的变化率，也就是导数了），定义如下：（第二个等号是根据求导的链式法则得到的）</span></p> 
<p align="center"><img src="https://images2.imgbox.com/4d/c6/EN7mRu1q_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>因为<span style="font-family:Calibri;">∂u/∂b=1</span>，所以<span style="font-family:Calibri;">∂E/∂b=∂E/∂u=</span>δ，也就是说<span style="color:red;"><span style="font-family:Calibri;">bias</span></span><span style="color:red;">基的灵敏度</span><span style="color:red;"><span style="font-family:Calibri;">∂E/∂b=</span></span><span style="color:red;">δ</span><span style="color:red;">和误差</span><span style="color:red;"><span style="font-family:Calibri;">E</span></span><span style="color:red;">对一个节点全部输入</span><span style="color:red;"><span style="font-family:Calibri;">u</span></span><span style="color:red;">的导数</span><span style="color:red;"><span style="font-family:Calibri;">∂E/∂u</span></span><span style="color:red;">是相等的</span>。这个导数就是让高层误差反向传播到底层的神来之笔。反向传播就是用下面这条关系式：（下面这条式子表达的就是第<span style="font-family:Calibri;">l</span>层的灵敏度，就是）</span></p> 
<p align="center"><span style="font-size:18px;"><span style="font-family:Calibri;"><img src="https://images2.imgbox.com/99/8e/GFxWolMm_o.jpg" alt=""> </span><span style="color:red;">公式（</span><span style="color:red;"><span style="font-family:Calibri;">1</span></span><span style="color:red;">）</span></span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>这里的“<span style="font-family:Calibri;">◦</span>”表示每个元素相乘。输出层的神经元的灵敏度是不一样的：</span></p> 
<p align="center"><img src="https://images2.imgbox.com/a1/29/nSvkHnt4_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>最后，对每个神经元运用<span style="font-family:Calibri;">delta</span>（即δ）规则进行权值更新。具体来说就是，对一个给定的神经元，得到它的输入，然后用这个神经元的<span style="font-family:Calibri;">delta</span>（即δ）来进行缩放。用向量的形式表述就是，<span style="color:red;">对于第</span><span style="color:red;"><span style="font-family:Calibri;">l</span></span><span style="color:red;">层，误差对于该层每一个权值（组合为矩阵）的导数是该层的输入（等于上一层的输出）与该层的灵敏度（该层每个神经元的</span><span style="color:red;">δ组合成一个向量的形式</span><span style="color:red;">）的叉乘</span>。然后得到的偏导数乘以一个负学习率就是该层的神经元的权值的更新了：</span></p> 
<p align="center"><span style="font-size:18px;"><span style="font-family:Calibri;"> <img src="https://images2.imgbox.com/9d/e8/Ae4jCatG_o.jpg" alt=""></span><span style="color:red;">公式（</span><span style="color:red;"><span style="font-family:Calibri;">2</span></span><span style="color:red;">）</span></span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>对于<span style="font-family:Calibri;">bias</span>基的更新表达式差不多。实际上，对于每一个权值<span style="font-family:Calibri;">(W)<sub>ij</sub></span>都有一个特定的学习率η<sub><span style="font-family:Calibri;">Ij</span></sub>。</span></p> 
<p><span style="font-family:Calibri;font-size:18px;"> </span></p> 
<p><strong><span style="font-size:18px;">三、<span style="font-family:Calibri;">Convolutional Neural Networks </span>卷积神经网络</span></strong></p> 
<p><strong><span style="font-size:18px;"><span style="font-family:Calibri;">3.1</span>、<span style="font-family:Calibri;">Convolution Layers </span>卷积层</span></strong></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>我们现在关注网络中卷积层的<span style="font-family:Calibri;">BP</span>更新。在一个卷积层，上一层的特征<span style="font-family:Calibri;">maps</span>被一个可学习的卷积核进行卷积，然后通过一个激活函数，就可以得到输出特征<span style="font-family:Calibri;">map</span>。每一个输出<span style="font-family:Calibri;">map</span>可能是组合卷积多个输入<span style="font-family:Calibri;">maps</span>的值：</span></p> 
<p align="center"><img src="https://images2.imgbox.com/ab/16/N1MYRFHB_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-size:18px;">       这里<span style="font-family:Calibri;">M<sub>j</sub></span>表示选择的输入<span style="font-family:Calibri;">maps</span>的集合，那么到底选择哪些输入<span style="font-family:Calibri;">maps</span>呢？有选择一对的或者三个的。但下面我们会讨论如何去自动选择需要组合的特征<span style="font-family:Calibri;">maps</span>。每一个输出<span style="font-family:Calibri;">map</span>会给一个额外的偏置<span style="font-family:Calibri;">b</span>，但是对于一个特定的输出<span style="font-family:Calibri;">map</span>，卷积每个输入<span style="font-family:Calibri;">maps</span>的卷积核是不一样的。也就是说，如果输出特征<span style="font-family:Calibri;">map j</span>和输出特征<span style="font-family:Calibri;">map k</span>都是从输入<span style="font-family:Calibri;">map i</span>中卷积求和得到，那么对应的卷积核是不一样的。</span></p> 
<p><strong><span style="font-size:18px;"><span style="font-family:Calibri;">3.1.1</span>、<span style="font-family:Calibri;">Computing the Gradients</span>梯度计算</span></strong></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>我们假定每个卷积层<span style="font-family:Calibri;">l</span>都会接一个下采样层<span style="font-family:Calibri;">l+1 </span>。对于<span style="font-family:Calibri;">BP</span>来说，根据上文我们知道，要想求得层<span style="font-family:Calibri;">l</span>的每个神经元对应的权值的权值更新，就需要先求层<span style="font-family:Calibri;">l</span>的每一个神经节点的灵敏度δ（也就是权值更新的<span style="color:red;">公式（2</span>））。为了求这个灵敏度我们就需要先对下一层的节点（连接到当前层<span style="font-family:Calibri;">l</span>的感兴趣节点的第<span style="font-family:Calibri;">l+1</span>层的节点）的灵敏度求和（得到δ<sup>l+1</sup>），然后乘以这些连接对应的权值（连接第<span style="font-family:Calibri;">l</span>层感兴趣节点和第<span style="font-family:Calibri;">l+1</span>层节点的权值）<span style="font-family:Calibri;">W</span>。再乘以当前层<span style="font-family:Calibri;">l</span>的该神经元节点的输入<span style="font-family:Calibri;">u</span>的激活函数<span style="font-family:Calibri;">f</span>的导数值（也就是那个灵敏度反向传播的<span style="color:red;">公式（1</span>）的δ<sup>l</sup>的求解），这样就可以得到当前层<span style="font-family:Calibri;">l</span>每个神经节点对应的灵敏度δ<sup>l</sup>了。</span></p> 
<p><span style="font-size:18px;">      然而，因为下采样的存在，采样层的一个像素（神经元节点）对应的灵敏度δ对应于卷积层（上一层）的输出<span style="font-family:Calibri;">map</span>的一块像素（采样窗口大小）。因此，层<span style="font-family:Calibri;">l</span>中的一个<span style="font-family:Calibri;">map</span>的每个节点只与<span style="font-family:Calibri;">l+1</span>层中相应<span style="font-family:Calibri;">map</span>的一个节点连接。</span></p> 
<p><span style="font-size:18px;">     为了有效计算层<span style="font-family:Calibri;">l</span>的灵敏度，我们需要上采样<span style="font-family:Calibri;">upsample </span>这个下采样<span style="font-family:Calibri;">downsample</span>层对应的灵敏度<span style="font-family:Calibri;">map</span>（特征<span style="font-family:Calibri;">map</span>中每个像素对应一个灵敏度，所以也组成一个<span style="font-family:Calibri;">map</span>），这样才使得这个灵敏度<span style="font-family:Calibri;">map</span>大小与卷积层的<span style="font-family:Calibri;">map</span>大小一致，然后再将层<span style="font-family:Calibri;">l</span>的<span style="font-family:Calibri;">map</span>的激活值的偏导数与从第<span style="font-family:Calibri;">l+1</span>层的上采样得到的灵敏度<span style="font-family:Calibri;">map</span>逐元素相乘（也就是<span style="color:red;">公式（</span><span style="color:red;"><span style="font-family:Calibri;">1</span></span><span style="color:red;">）</span>）。</span></p> 
<p><span style="font-size:18px;">        在下采样层<span style="font-family:Calibri;">map</span>的权值都取一个相同值β，而且是一个常数。所以我们只需要将上一个步骤得到的结果乘以一个β就可以完成第<span style="font-family:Calibri;">l</span>层灵敏度δ的计算。</span></p> 
<p><span style="font-size:18px;">       我们可以对卷积层中每一个特征<span style="font-family:Calibri;">map j</span>重复相同的计算过程。但很明显需要匹配相应的子采样层的<span style="font-family:Calibri;">map</span>（参考<span style="color:red;">公式（</span><span style="color:red;"><span style="font-family:Calibri;">1</span></span><span style="color:red;">））</span>：</span></p> 
<p align="center"><img src="https://images2.imgbox.com/0a/ed/FCBQnbhK_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">        up(.)</span>表示一个上采样操作。如果下采样的采样因子是<span style="font-family:Calibri;">n</span>的话，它简单的将每个像素水平和垂直方向上拷贝<span style="font-family:Calibri;">n</span>次。这样就可以恢复原来的大小了。实际上，这个函数可以用<span style="font-family:Calibri;">Kronecker</span>乘积来实现：</span></p> 
<p align="center"><img src="https://images2.imgbox.com/7e/ce/QjR1qlT3_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-size:18px;">       好，到这里，对于一个给定的<span style="font-family:Calibri;">map</span>，我们就可以计算得到其灵敏度<span style="font-family:Calibri;">map</span>了。然后我们就可以通过简单的对层<span style="font-family:Calibri;">l</span>中的灵敏度<span style="font-family:Calibri;">map</span>中所有节点进行求和快速的计算<span style="color:red;"><span style="font-family:Calibri;">bias</span></span><span style="color:red;">基的梯度</span>了：</span></p> 
<p align="center"><span style="font-size:18px;"><span style="font-family:Calibri;"><img src="https://images2.imgbox.com/1a/23/RIu4K4PN_o.jpg" alt=""> </span><span style="color:red;">公式（</span><span style="color:red;"><span style="font-family:Calibri;">3</span></span><span style="color:red;">）</span></span></p> 
<p><span style="font-size:18px;">       最后，对卷积核的<span style="color:red;">权值的梯度</span>就可以用<span style="font-family:Calibri;">BP</span>算法来计算了（<span style="color:red;">公式（</span><span style="color:red;"><span style="font-family:Calibri;">2</span></span><span style="color:red;">）</span>）。另外，很多连接的权值是共享的，因此，对于一个给定的权值，我们需要对所有与该权值有联系（权值共享的连接）的连接对该点求梯度，然后对这些梯度进行求和，就像上面对<span style="font-family:Calibri;">bias</span>基的梯度计算一样：</span></p> 
<p align="center"><img src="https://images2.imgbox.com/c1/a8/nI8Y9I3m_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-size:18px;">       这里，<img src="https://images2.imgbox.com/35/d6/96SExbbK_o.jpg" alt="">是<img src="https://images2.imgbox.com/5f/9f/ebzsA0j9_o.jpg" alt="">中的在卷积的时候与<img src="https://images2.imgbox.com/0e/9d/OUETSNGl_o.jpg" alt="">逐元素相乘的<span style="font-family:Calibri;">patch</span>，输出卷积<span style="font-family:Calibri;">map</span>的<span style="font-family:Calibri;">(u, v)</span>位置的值是由上一层的<span style="font-family:Calibri;">(u, v)</span>位置的<span style="font-family:Calibri;">patch</span>与卷积核<span style="font-family:Calibri;">k_ij</span>逐元素相乘的结果。</span></p> 
<p><span style="font-size:18px;">      咋一看，好像我们需要煞费苦心地记住输出<span style="font-family:Calibri;">map</span>（和对应的灵敏度<span style="font-family:Calibri;">map</span>）每个像素对应于输入<span style="font-family:Calibri;">map</span>的哪个<span style="font-family:Calibri;">patch</span>。但实际上，在<span style="font-family:Calibri;">Matlab</span>中，可以通过一个代码就实现。对于上面的公式，可以用<span style="font-family:Calibri;">Matlab</span>的卷积函数来实现：</span></p> 
<p><img src="https://images2.imgbox.com/8c/db/f3EX2nYt_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-size:18px;">       我们先对<span style="font-family:Calibri;">delta</span>灵敏度<span style="font-family:Calibri;">map</span>进行旋转，这样就可以进行互相关计算，而不是卷积（在卷积的数学定义中，特征矩阵（卷积核）在传递给<span style="font-family:Calibri;">conv2</span>时需要先翻转（<span style="font-family:Calibri;">flipped</span>）一下。也就是颠倒下特征矩阵的行和列）。然后把输出反旋转回来，这样我们在前向传播进行卷积的时候，卷积核才是我们想要的方向。</span></p> 
<p><span style="font-family:Calibri;font-size:18px;"> </span></p> 
<p><strong><span style="font-size:18px;"><span style="font-family:Calibri;">3.2</span>、<span style="font-family:Calibri;">Sub-sampling Layers </span>子采样层</span></strong></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>对于子采样层来说，有<span style="font-family:Calibri;">N</span>个输入<span style="font-family:Calibri;">maps</span>，就有<span style="font-family:Calibri;">N</span>个输出<span style="font-family:Calibri;">maps</span>，只是每个输出<span style="font-family:Calibri;">map</span>都变小了。</span></p> 
<p align="center"><img src="https://images2.imgbox.com/3f/7d/OxruS8Od_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">        down(.)</span>表示一个下采样函数。典型的操作一般是对输入图像的不同<span style="font-family:Calibri;">nxn</span>的块的所有像素进行求和。这样输出图像在两个维度上都缩小了<span style="font-family:Calibri;">n</span>倍。每个输出<span style="font-family:Calibri;">map</span>都对应一个属于自己的乘性偏置β和一个加性偏置<span style="font-family:Calibri;">b</span>。</span></p> 
<p><span style="font-family:Calibri;font-size:18px;"> </span></p> 
<p><strong><span style="font-size:18px;"><span style="font-family:Calibri;">3.2.1</span>、<span style="font-family:Calibri;">Computing the Gradients </span>梯度计算</span></strong></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>这里最困难的是计算灵敏度<span style="font-family:Calibri;">map</span>。一旦我们得到这个了，那我们唯一需要更新的偏置参数β和<span style="font-family:Calibri;">b</span>就可以轻而易举了（<span style="color:red;">公式（</span><span style="color:red;"><span style="font-family:Calibri;">3</span></span><span style="color:red;">）</span>）。如果下一个卷积层与这个子采样层是全连接的，那么就可以通过<span style="font-family:Calibri;">BP</span>来计算子采样层的灵敏度<span style="font-family:Calibri;">maps</span>。</span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>我们需要计算卷积核的梯度，所以我们必须找到输入<span style="font-family:Calibri;">map</span>中哪个<span style="font-family:Calibri;">patch</span>对应输出<span style="font-family:Calibri;">map</span>的哪个像素。这里，就是必须找到当前层的灵敏度<span style="font-family:Calibri;">map</span>中哪个<span style="font-family:Calibri;">patch</span>对应与下一层的灵敏度<span style="font-family:Calibri;">map</span>的给定像素，这样才可以利用<span style="color:red;">公式（</span><span style="color:red;"><span style="font-family:Calibri;">1</span></span><span style="color:red;">）</span>那样的δ递推，也就是灵敏度反向传播回来。另外，需要乘以输入<span style="font-family:Calibri;">patch</span>与输出像素之间连接的权值，这个权值实际上就是卷积核的权值（已旋转的）。</span></p> 
<p align="center"><img src="https://images2.imgbox.com/5b/14/RitjDfqh_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-size:18px;">      在这之前，我们需要先将核旋转一下，让卷积函数可以实施互相关计算。另外，我们需要对卷积边界进行处理，但在<span style="font-family:Calibri;">Matlab</span>里面，就比较容易处理。<span style="font-family:Calibri;">Matlab</span>中全卷积会对缺少的输入像素补<span style="font-family:Calibri;">0 </span>。</span></p> 
<p><span style="font-size:18px;">      到这里，我们就可以对<span style="font-family:Calibri;">b</span>和β计算梯度了。首先，加性基<span style="font-family:Calibri;">b</span>的计算和上面卷积层的一样，对灵敏度<span style="font-family:Calibri;">map</span>中所有元素加起来就可以了：</span></p> 
<p align="center"><img src="https://images2.imgbox.com/c3/ea/foEiBuWD_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">       </span>而对于乘性偏置β，因为涉及到了在前向传播过程中下采样<span style="font-family:Calibri;">map</span>的计算，所以我们最好在前向的过程中保存好这些<span style="font-family:Calibri;">maps</span>，这样在反向的计算中就不用重新计算了。我们定义：</span></p> 
<p align="center"><img src="https://images2.imgbox.com/a8/19/2RPqRrdW_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-size:18px;">这样，对β的梯度就可以用下面的方式计算：</span></p> 
<p align="center"><img src="https://images2.imgbox.com/35/7d/fmHv7KWZ_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-family:Calibri;font-size:18px;"> </span></p> 
<p><strong><span style="font-size:18px;"><span style="font-family:Calibri;">3.3</span>、<span style="font-family:Calibri;">Learning Combinations of Feature Maps </span>学习特征<span style="font-family:Calibri;">map</span>的组合</span></strong></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>大部分时候，通过卷积多个输入<span style="font-family:Calibri;">maps</span>，然后再对这些卷积值求和得到一个输出<span style="font-family:Calibri;">map</span>，这样的效果往往是比较好的。在一些文献中，一般是人工选择哪些输入<span style="font-family:Calibri;">maps</span>去组合得到一个输出<span style="font-family:Calibri;">map</span>。但我们这里尝试去让<span style="font-family:Calibri;">CNN</span>在训练的过程中学习这些组合，也就是让网络自己学习挑选哪些输入<span style="font-family:Calibri;">maps</span>来计算得到输出<span style="font-family:Calibri;">map</span>才是最好的。我们用α<span style="font-family:Calibri;">ij</span>表示在得到第<span style="font-family:Calibri;">j</span>个输出<span style="font-family:Calibri;">map</span>的其中第<span style="font-family:Calibri;">i</span>个输入<span style="font-family:Calibri;">map</span>的权值或者贡献。这样，第<span style="font-family:Calibri;">j</span>个输出<span style="font-family:Calibri;">map</span>可以表示为：</span></p> 
<p align="center"><img src="https://images2.imgbox.com/48/8d/NSSUu026_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>需要满足约束：</span></p> 
<p align="center"><img src="https://images2.imgbox.com/0b/71/ssHUwfXs_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>这些对变量α<span style="font-family:Calibri;">ij</span>的约束可以通过将变量α<span style="font-family:Calibri;">ij</span>表示为一个组无约束的隐含权值<span style="font-family:Calibri;">c<sub>ij</sub></span>的<span style="font-family:Calibri;">softmax</span>函数来加强。（因为<span style="font-family:Calibri;">softmax</span>的因变量是自变量的指数函数，他们的变化率会不同）。</span></p> 
<p align="center"><img src="https://images2.imgbox.com/ef/5f/TwvPImzs_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>因为对于一个固定的<span style="font-family:Calibri;">j</span>来说，每组权值<span style="font-family:Calibri;">c<sub>ij</sub></span>都是和其他组的权值独立的，所以为了方面描述，我们把下标<span style="font-family:Calibri;">j</span>去掉，只考虑一个<span style="font-family:Calibri;">map</span>的更新，其他<span style="font-family:Calibri;">map</span>的更新是一样的过程，只是<span style="font-family:Calibri;">map</span>的索引<span style="font-family:Calibri;">j</span>不同而已。</span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         Softmax</span>函数的导数表示为：</span></p> 
<p align="center"><img src="https://images2.imgbox.com/c1/d6/oGm1kSg5_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-size:18px;">        这里的δ是<span style="font-family:Calibri;">Kronecker delta</span>。对于误差对于第<span style="font-family:Calibri;">l</span>层变量α<span style="font-family:Calibri;">i</span>的导数为：</span></p> 
<p align="center"><img src="https://images2.imgbox.com/32/3e/RoaIDDP6_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>最后就可以通过链式规则去求得代价函数关于权值<span style="font-family:Calibri;">c<sub>i</sub></span>的偏导数了：</span></p> 
<p align="center"><img src="https://images2.imgbox.com/6a/ee/oPDA1WU3_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-family:Calibri;font-size:18px;"> </span></p> 
<p><strong><span style="font-size:18px;"><span style="font-family:Calibri;">3.3.1</span>、<span style="font-family:Calibri;">Enforcing Sparse Combinations </span>加强稀疏性组合</span></strong></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>为了限制α<span style="font-family:Calibri;">i</span>是稀疏的，也就是限制一个输出<span style="font-family:Calibri;">map</span>只与某些而不是全部的输入<span style="font-family:Calibri;">maps</span>相连。我们在整体代价函数里增加稀疏约束项<span style="font-family:Calibri;">Ω(α)</span>。对于单个样本，重写代价函数为：</span></p> 
<p align="center"><img src="https://images2.imgbox.com/ab/cc/hWH52WPC_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-size:18px;">然后寻找这个规则化约束项对权值<span style="font-family:Calibri;">ci</span>求导的贡献。规则化项<span style="font-family:Calibri;">Ω(α)</span>对<span style="font-family:Calibri;">αi</span>求导是：</span></p> 
<p align="center"><img src="https://images2.imgbox.com/c6/d1/BG23caRj_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>然后，通过链式法则，对<span style="font-family:Calibri;">ci</span>的求导是：</span></p> 
<p align="center"><img src="https://images2.imgbox.com/98/74/d7ABHzzV_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>所以，权值<span style="font-family:Calibri;">ci</span>最后的梯度是：</span></p> 
<p align="center"><img src="https://images2.imgbox.com/43/50/FtyQJMyP_o.jpg" alt=""><span style="font-size:18px;"></span></p> 
<p><span style="font-family:Calibri;font-size:18px;"> </span></p> 
<p><strong><span style="font-family:Calibri;font-size:18px;">3.4、Making it Fast with MATLAB</span></strong></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">        CNN</span>的训练主要是在卷积层和子采样层的交互上，其主要的计算瓶颈是：</span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">1</span>）前向传播过程：下采样每个卷积层的<span style="font-family:Calibri;">maps</span>；</span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">2</span>）反向传播过程：上采样高层子采样层的灵敏度<span style="font-family:Calibri;">map</span>，以匹配底层的卷积层输出<span style="font-family:Calibri;">maps</span>的大小；</span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">3</span>）<span style="font-family:Calibri;">sigmoid</span>的运用和求导。</span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>对于第一和第二个问题，我们考虑的是如何用<span style="font-family:Calibri;">Matlab</span>内置的图像处理函数去实现上采样和下采样的操作。对于上采样，<span style="font-family:Calibri;">imresize</span>函数可以搞定，但需要很大的开销。一个比较快速的版本是使用<span style="font-family:Calibri;">Kronecker</span>乘积函数<span style="font-family:Calibri;">kron</span>。通过一个全一矩阵<span style="font-family:Calibri;">ones</span>来和我们需要上采样的矩阵进行<span style="font-family:Calibri;">Kronecker</span>乘积，就可以实现上采样的效果。对于前向传播过程中的下采样，<span style="font-family:Calibri;">imresize</span>并没有提供在缩小图像的过程中还计算<span style="font-family:Calibri;">nxn</span>块内像素的和的功能，所以没法用。一个比较好和快速的方法是用一个全一的卷积核来卷积图像，然后简单的通过标准的索引方法来采样最后卷积结果。例如，如果下采样的域是<span style="font-family:Calibri;">2x2</span>的，那么我们可以用<span style="font-family:Calibri;">2x2</span>的元素全是<span style="font-family:Calibri;">1</span>的卷积核来卷积图像。然后再卷积后的图像中，我们每个<span style="font-family:Calibri;">2</span>个点采集一次数据，<span style="font-family:Calibri;">y=x(1:2:end,1:2:end)</span>，这样就可以得到了两倍下采样，同时执行求和的效果。</span></p> 
<p><span style="font-size:18px;"><span style="font-family:Calibri;">         </span>对于第三个问题，实际上有些人以为<span style="font-family:Calibri;">Matlab</span>中对<span style="font-family:Calibri;">sigmoid</span>函数进行<span style="font-family:Calibri;">inline</span>的定义会更快，其实不然，<span style="font-family:Calibri;">Matlab</span>与<span style="font-family:Calibri;">C/C++</span>等等语言不一样，<span style="font-family:Calibri;">Matlab</span>的<span style="font-family:Calibri;">inline</span>反而比普通的函数定义更非时间。所以，我们可以直接在代码中使用计算<span style="font-family:Calibri;">sigmoid</span>函数及其导数的真实代码。</span></p> 
<p><span style="font-family:Calibri;font-size:18px;"></span> </p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/81ec7ea3565d3f4d51bd509b41bf9b19/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">.NET 调用外部exe程序，出现已停止工作</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/11360bc6e9bc866ae9d9c550aa6d98e8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">安装Android Studio提示找不到JDK解决方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>