<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Wav2Lip---嘴型同步模型Wav2Lip - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Wav2Lip---嘴型同步模型Wav2Lip" />
<meta property="og:description" content="嘴型同步模型Wav2Lip 软硬件环境是 ubuntu 18.04 64bit
nvidia GeForce 3090Ti
cuda 11.4
anaconda with python 3.7
简介 2020年，来自印度海德拉巴大学和英国巴斯大学的团队，在ACM MM2020发表了的一篇论文《A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild 》，在文章中，他们提出一个叫做Wav2Lip的AI模型，只需要一段人物视频和一段目标语音，就能够让音频和视频合二为一，人物嘴型与音频完全匹配。
快速体验 可以先到作者提供的体验站体验一番，地址是：https://bhaasha.iiit.ac.in/lipsync/example3/
wav2lip
按照上图中的选择视频和音频上传即可同步。
实践 准备环境 首先使用conda创建新的虚拟环境，然后激活这个环境
conda create -n py_video python=3.7 conda activate py_video 接着来到官方网站，使用git克隆代码，或者直接下载源码压缩包解压，安装依赖
git clone https://github.com/Rudrabha/Wav2Lip.git pip install -r requirements.txt 在windows平台上，使用了阿里云的pip源，发现找不到torch 1.1的版本，
后来使用了最新稳定版1.11也没有问题，其它依赖库使用最新版也是ok的。
接下来需要安装ffmpeg，这是音视频处理的神器，ubuntu版本使用apt安装
sudo apt install ffmpeg windows用户的话，可以到 https://github.com/BtbN/FFmpeg-Builds/releases 下载，解压后将bin对应的路径添加到系统环境变量PATH中。
准备素材 下面开始准备素材，我们把官方体验站上的测试视频下载下来，使用下面的命令" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/d433abe2656e0a37e76fe0dc75727cf6/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-25T13:43:08+08:00" />
<meta property="article:modified_time" content="2022-07-25T13:43:08+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Wav2Lip---嘴型同步模型Wav2Lip</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-dracula">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="Wav2Lip_0"></a>嘴型同步模型Wav2Lip</h2> 
<h3><a id="_1"></a>软硬件环境是</h3> 
<ul><li> <p>ubuntu 18.04 64bit</p> </li><li> <p>nvidia GeForce 3090Ti</p> </li><li> <p>cuda 11.4</p> </li><li> <p>anaconda with python 3.7</p> </li></ul> 
<h3><a id="_10"></a>简介</h3> 
<p>2020年，来自印度海德拉巴大学和英国巴斯大学的团队，在ACM MM2020发表了的一篇论文《A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild 》，在文章中，他们提出一个叫做Wav2Lip的AI模型，只需要一段人物视频和一段目标语音，就能够让音频和视频合二为一，人物嘴型与音频完全匹配。</p> 
<h3><a id="_13"></a>快速体验</h3> 
<p>可以先到作者提供的体验站体验一番，地址是：https://bhaasha.iiit.ac.in/lipsync/example3/<br> <img src="https://images2.imgbox.com/e4/f1/dL4XjMss_o.png" alt="在这里插入图片描述"><br> wav2lip<br> 按照上图中的选择视频和音频上传即可同步。</p> 
<h3><a id="_19"></a>实践</h3> 
<h4><a id="_20"></a>准备环境</h4> 
<p>首先使用conda创建新的虚拟环境，然后激活这个环境</p> 
<pre><code class="prism language-python">conda create <span class="token operator">-</span>n py_video python<span class="token operator">=</span><span class="token number">3.7</span>
conda activate py_video
</code></pre> 
<p>接着来到官方网站，使用git克隆代码，或者直接下载源码压缩包解压，安装依赖</p> 
<pre><code class="prism language-python">git clone https<span class="token punctuation">:</span><span class="token operator">//</span>github<span class="token punctuation">.</span>com<span class="token operator">/</span>Rudrabha<span class="token operator">/</span>Wav2Lip<span class="token punctuation">.</span>git
pip install <span class="token operator">-</span>r requirements<span class="token punctuation">.</span>txt
</code></pre> 
<p>在windows平台上，使用了阿里云的pip源，发现找不到torch 1.1的版本，<br> 后来使用了最新稳定版1.11也没有问题，其它依赖库使用最新版也是ok的。</p> 
<p>接下来需要安装ffmpeg，这是音视频处理的神器，ubuntu版本使用apt安装</p> 
<pre><code class="prism language-python">sudo apt install ffmpeg
</code></pre> 
<p>windows用户的话，可以到 https://github.com/BtbN/FFmpeg-Builds/releases 下载，解压后将bin对应的路径添加到系统环境变量PATH中。</p> 
<h4><a id="_46"></a>准备素材</h4> 
<p>下面开始准备素材，我们把官方体验站上的测试视频下载下来，使用下面的命令</p> 
<pre><code class="prism language-python">wget <span class="token operator">-</span><span class="token operator">-</span>no<span class="token operator">-</span>check<span class="token operator">-</span>certificate https<span class="token punctuation">:</span><span class="token operator">//</span>bhaasha<span class="token punctuation">.</span>iiit<span class="token punctuation">.</span>ac<span class="token punctuation">.</span><span class="token keyword">in</span><span class="token operator">/</span>lipsync<span class="token operator">/</span>static<span class="token operator">/</span>samples<span class="token operator">/</span>game<span class="token punctuation">.</span>mp4
</code></pre> 
<p>这个测试视频只有 <strong>3秒</strong>，那接下来就去找个对应 <strong>3秒</strong> 的音频。如果能有现成的音频文件最好，如果没有的话，我的做法是这样的，从某个视频文件中提取音频，然后进行裁剪，这里需要的时长是三秒。这两步会使用到ffmpeg这个工具</p> 
<h4><a id="153_55"></a>视频裁剪，从第15秒开始，总时长是3秒，目标视频的音视频编码格式与原始的保持一致</h4> 
<pre><code class="prism language-python">ffmpeg <span class="token operator">-</span>ss <span class="token number">00</span><span class="token punctuation">:</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">15</span> <span class="token operator">-</span>t <span class="token number">00</span><span class="token punctuation">:</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">03</span> <span class="token operator">-</span>i <span class="token builtin">input</span><span class="token punctuation">.</span>mp4 <span class="token operator">-</span>vcodec copy <span class="token operator">-</span>acodec copy test<span class="token punctuation">.</span>mp4
</code></pre> 
<h3><a id="_62"></a>从视频中提取音频</h3> 
<pre><code class="prism language-python">ffmpeg <span class="token operator">-</span>i test<span class="token punctuation">.</span>mp4 <span class="token operator">-</span>vn test<span class="token punctuation">.</span>mp3
</code></pre> 
<h4><a id="_68"></a>准备模型文件</h4> 
<p>第一个需要的模型是脸部检测预训练模型，下载地址是：<br> https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth，<br> 下载后放到目录 <code>face_detection/detection/sfd</code> 中，并重命名为s3fd.pth。</p> 
<p>脸部模型：接下来去下载模型文件，这里作者提供了3个，可以任选一个，后两个优于第一个。</p> 
<p>它们的区别如下表所示，本文使用的是<strong>Wav2Lip + GAN</strong>，下载地址：<br> https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA?e=n9ljGW ，</p> 
<p>下载后放在项目 <strong>根目录</strong></p> 
<p>wav2lip</p> 
<h4><a id="_82"></a>运行代码</h4> 
<p>执行下面的命令将视频 test.mp4 和音频 test.mp3 进行合成</p> 
<pre><code class="prism language-python">python inference<span class="token punctuation">.</span>py <span class="token operator">-</span><span class="token operator">-</span>checkpoint_path wav2lip_gan<span class="token punctuation">.</span>pth <span class="token operator">-</span><span class="token operator">-</span>face test<span class="token punctuation">.</span>mp4 <span class="token operator">-</span><span class="token operator">-</span>audio test<span class="token punctuation">.</span>mp3
</code></pre> 
<p>执行下面的命令将图片1.jpg 和音频 test.mp3 进行合成</p> 
<pre><code class="prism language-python">python inference<span class="token punctuation">.</span>py <span class="token operator">-</span><span class="token operator">-</span>checkpoint_path wav2lip_gan<span class="token punctuation">.</span>pth <span class="token operator">-</span><span class="token operator">-</span>face <span class="token number">1</span><span class="token punctuation">.</span>jpg <span class="token operator">-</span><span class="token operator">-</span>audio test<span class="token punctuation">.</span>mp3
</code></pre> 
<p>最后，生成的新视频文件保存在 <code>results/result_voice.mp4</code> ，生成的中间文件存放在 <code>temp</code> 下，像单独处理后的音频<code>temp.wav</code>、视频 <code>result.avi</code> 等。</p> 
<p><img src="https://images2.imgbox.com/61/61/IekX1eod_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="FAQ_99"></a>FAQ</h3> 
<pre><code class="prism language-python">ValueError<span class="token punctuation">:</span> Face <span class="token keyword">not</span> detected! Ensure the video contains a face <span class="token keyword">in</span> <span class="token builtin">all</span> the frames<span class="token punctuation">.</span>
</code></pre> 
<p>这个一般是由于片头或者片尾没有脸，解决方法也很简单，使用ffmpeg或者剪辑软件，将片头或片尾相应的帧剪掉就可以了</p> 
<p>备注<br> 项目作者强调，基于此开放源代码的所有结果仅应用于学术研究和个人目的，由于模型是基于LRS2 (Lip Reading Sentences 2) 数据集进行训练，因此严禁任何形式的商业用途。</p> 
<p>资源下载<br> 打包了文中所使用的模型文件，下载地址：</p> 
<p>链接：https://pan.baidu.com/s/1uaOC3PEFRvTuC5Xwbj1KUw<br> 提取码：t9pg</p> 
<h3><a id="_117"></a>参考资料</h3> 
<p>http://arxiv.org/abs/2008.10010</p> 
<p>https://github.com/Rudrabha/Wav2Lip</p> 
<p>http://bhaasha.iiit.ac.in/lipsync/</p> 
<p>https://github.com/1adrianb/face-alignment</p> 
<p>https://xugaoxiang.com/2020/09/29/ffmpeg-cmd/</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b496bd55a98afe76d79f760e8187dd7a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">JavaScript语法简介</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9d57b7b54dbfad47ba3319d30821bc8c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">OAuth2和JWT</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>