<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>图神经网络基础学习 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="图神经网络基础学习" />
<meta property="og:description" content="注：本篇博客公式格式没有经过认真编辑，欢迎去我的博客：http://www.pinkman.tech/index.php/tech/2021/gnn-basic/ 或者知乎：https://zhuanlan.zhihu.com/p/356872702 获得更好的阅读体验。
本篇文章旨在通过最直白的语言解释一些GNN中的基础知识，涉及到的内容包括：
GNN在现实中的应用研究图问题的两个重要原则GNN是如何学习的GNN中信息如何传递？什么是卷积GNN，注意力GNN和信息传递GNN？关于GNN的一些观点 本篇文章参考于Petar Veličković的教程，讲义位于：petar-v.com/talks/GNN-W。由于我刚接触这个领域，所以在书写过程中难免出错，欢迎在评论中指出。
一、现实世界中GNN的应用 判断分子特性 生物分子就是一个图结构，其中每一个原子是一个节点，连接原子的化学键就是一个边。关于分子有一些比较有趣的研究任务，比如判断一个分子是不是一种强烈的药物。此时，可以将该分子通过GNN来进行二分类。
GNN应用于化学领域 这样训练以后，有一个好处是可以通过图神经网络发现各种各样的分子特性：将一个很大的候选分子库送入网络进行预测，可以选择GNN模型输出的候选100个分子集合，再进行化学测试。在这个领域还是非常有用的。
交通地图也是图结构 其中，每个十字路口可以是一个节点，每个道路就是一个边。
GNN应用在交通地图 通过在交通地图上构建的GNN，可以用来回归预计抵达的时间（time to arrival, ETA）。
二、从CNN中观察 CNN的思想 第一个思想：只要图中出现某个信息，无论这个信息出现在哪里，都是有用的。比如检测某个物体，无论在图像中的哪里被检测到，只要被检测到就是成功的。所以才可以对整张图像进行扫描。这个思想叫做translational invariance。
第二个思想：离得近的图像像素之间的影响，远比离得远的像素的影响要大，因此才能够使用一个提取局部信息的卷积核。
我们接下来同样会利用这些思想，应用在图上。
图的特性 由于在图中不会指定任何的节点顺序，所以下面其实是一模一样的两幅图。我们需要图神经网络对这个输入，能够输出同样的信息。
两个相同的图 三、排列不变性和排列相等性 我们暂时先假设图中没有边，此时图是一个节点的集合。令(\boldsymbol{x}_i \in R^k) 表示节点 i 的 k 维特征。可以将所有的节点特征堆叠到一起，形成一个(n\times k)的特征矩阵。需要注意的是，在这里为节点指定了顺序，但是想要图的输出不会依赖于这个顺序。
为了研究组合方式的操作，定义如下的 (n\times n) 矩阵叫做排列矩阵。其中，每一行和每一列都只有一个1，其余都为0。这个矩阵的作用就是当左乘上述特征矩阵时，就将上述特征矩阵换了一个排列组合的方式。这个排列矩阵对于研究排列组合来说非常有用。
排列不变性 Permutation Invariance 我们的目标是：定义一个不依赖于输入节点顺序的函数(f(\boldsymbol{X})) 。
数学定义：如果 (f(\boldsymbol{X})) 是排列不变的，那么对于所有的排列矩阵P ，都有：(f(\boldsymbol{PX}) = f(\boldsymbol{X}) ) 。
排列相等性 Permutation Equivariance 但是，如果想要获得某一个节点的特征呢？此时就需要排列相等性了。它研究的是一个节点层面的排列性质。目标是找到不会改变节点顺序的函数。也就是说，如果我们改变节点的顺序，无论我们经过函数前改变，还是经过函数后改变，结果应该是一致的。
数学定义：如果 (f(\boldsymbol{X})) 是排列相等的，那么对于所有的排列矩阵P，都有：(f(\boldsymbol{PX}) = \boldsymbol{P}f(\boldsymbol{X}) ) 。
集合研究的基础 说到这里，大家也可以发现，排列相等性要求我们不能进行每个节点的特征交互，即特征矩阵的行与行之间的交互，所以我们只能够对每一个节点进行一个特征的映射。
其中的(\psi)是一个映射函数，独立地作用于对每个节点。将所有的 (\boldsymbol{h}_i) 堆叠起来，就得到了(\boldsymbol{H} = f(\boldsymbol{X}))。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/ee9319d7134eab8a1af0b1c2e0d29910/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-03-14T10:29:45+08:00" />
<meta property="article:modified_time" content="2021-03-14T10:29:45+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">图神经网络基础学习</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p>注：本篇博客公式格式没有经过认真编辑，欢迎去我的博客：http://www.pinkman.tech/index.php/tech/2021/gnn-basic/ 或者知乎：https://zhuanlan.zhihu.com/p/356872702 获得更好的阅读体验。</p> 
</blockquote> 
<p>本篇文章旨在通过最直白的语言解释一些GNN中的基础知识，涉及到的内容包括：</p> 
<ul><li>GNN在现实中的应用</li><li>研究图问题的两个重要原则</li><li>GNN是如何学习的</li><li>GNN中信息如何传递？什么是卷积GNN，注意力GNN和信息传递GNN？</li><li>关于GNN的一些观点</li></ul> 
<p>本篇文章参考于Petar Veličković的教程，讲义位于：<a href="https://petar-v.com/talks/GNN-Wednesday.pdf" rel="nofollow">petar-v.com/talks/GNN-W</a>。由于我刚接触这个领域，所以在书写过程中难免出错，欢迎在评论中指出。</p> 
<h3><a id="GNN_12"></a>一、现实世界中GNN的应用</h3> 
<h4><a id="_14"></a>判断分子特性</h4> 
<p>生物分子就是一个图结构，其中每一个原子是一个<strong>节点</strong>，连接原子的化学键就是一个<strong>边</strong>。关于分子有一些比较有趣的研究任务，比如判断一个分子是不是一种强烈的药物。此时，可以将该分子通过GNN来进行二分类。</p> 
<p><img src="https://images2.imgbox.com/8f/9c/z8HdxMUr_o.png" alt="image"></p> 
<figcaption>
  GNN应用于化学领域 
</figcaption> 
<p>这样训练以后，有一个好处是可以通过图神经网络发现各种各样的分子特性：将一个很大的候选分子库送入网络进行预测，可以选择GNN模型输出的候选100个分子集合，再进行化学测试。在这个领域还是非常有用的。</p> 
<h4><a id="_24"></a>交通地图也是图结构</h4> 
<p>其中，每个十字路口可以是一个<strong>节点</strong>，每个道路就是一个<strong>边</strong>。</p> 
<p><img src="https://images2.imgbox.com/8d/24/D4EYkk4L_o.png" alt="image"></p> 
<figcaption>
  GNN应用在交通地图 
</figcaption> 
<p>通过在交通地图上构建的GNN，可以用来回归预计抵达的时间（time to arrival, ETA）。</p> 
<h3><a id="CNN_34"></a>二、从CNN中观察</h3> 
<h4><a id="CNN_36"></a>CNN的思想</h4> 
<p>第一个思想：只要图中出现某个信息，无论这个信息出现在哪里，都是有用的。比如检测某个物体，无论在图像中的哪里被检测到，只要被检测到就是成功的。所以才可以对整张图像进行扫描。这个思想叫做<strong>translational invariance</strong>。</p> 
<p>第二个思想：离得近的图像像素之间的影响，远比离得远的像素的影响要大，因此才能够使用一个提取局部信息的卷积核。</p> 
<p>我们接下来同样会利用这些思想，应用在图上。</p> 
<h4><a id="_44"></a>图的特性</h4> 
<p>由于在图中不会指定任何的节点顺序，所以下面其实是一模一样的两幅图。我们需要图神经网络对这个输入，能够输出同样的信息。</p> 
<p><img src="https://images2.imgbox.com/4e/02/HjfxkyQB_o.png" alt="image"></p> 
<figcaption>
  两个相同的图 
</figcaption> 
<h3><a id="_52"></a>三、排列不变性和排列相等性</h3> 
<p>我们<strong>暂时先假设图中没有边</strong>，此时图是一个节点的集合。令(\boldsymbol{x}_i \in R^k) 表示节点 i 的 k 维特征。可以将所有的节点特征堆叠到一起，形成一个(n\times k)的特征矩阵。需要注意的是，在这里为节点指定了顺序，但是想要图的输出不会依赖于这个顺序。</p> 
<p><img src="https://images2.imgbox.com/51/10/bZinHdv3_o.png" alt="image"></p> 
<p>为了研究组合方式的操作，定义如下的 (n\times n) 矩阵叫做<strong>排列矩阵</strong>。其中，每一行和每一列都只有一个1，其余都为0。这个矩阵的作用就是当左乘上述特征矩阵时，就将上述特征矩阵换了一个排列组合的方式。这个排列矩阵对于研究排列组合来说非常有用。</p> 
<p><img src="https://images2.imgbox.com/75/14/46APdPPX_o.png" alt="image"></p> 
<h4><a id="_Permutation_Invariance_62"></a>排列不变性 Permutation Invariance</h4> 
<p>我们的目标是：定义一个不依赖于输入节点顺序的函数(f(\boldsymbol{X})) 。</p> 
<p>数学定义：如果 (f(\boldsymbol{X})) 是排列不变的，那么对于所有的排列矩阵P ，都有：(f(\boldsymbol{PX}) = f(\boldsymbol{X}) ) 。</p> 
<h4><a id="_Permutation_Equivariance_68"></a>排列相等性 Permutation Equivariance</h4> 
<p>但是，如果想要获得某一个节点的特征呢？此时就需要排列相等性了。它研究的是一个<strong>节点层面</strong>的排列性质。目标是<strong>找到不会改变节点顺序的函数</strong>。也就是说，如果我们改变节点的顺序，无论我们经过函数前改变，还是经过函数后改变，结果应该是一致的。</p> 
<p>数学定义：如果 (f(\boldsymbol{X})) 是排列相等的，那么对于所有的排列矩阵P，都有：(f(\boldsymbol{PX}) = \boldsymbol{P}f(\boldsymbol{X}) ) 。</p> 
<h4><a id="_74"></a>集合研究的基础</h4> 
<p>说到这里，大家也可以发现，排列相等性要求我们不能进行每个节点的特征交互，即特征矩阵的行与行之间的交互，所以我们只能够对每一个节点进行一个特征的映射。</p> 
<p><img src="https://images2.imgbox.com/fe/e7/ar6c3y3w_o.png" alt="image"></p> 
<p>其中的(\psi)是一个映射函数，独立地作用于对每个节点。将所有的 (\boldsymbol{h}_i) 堆叠起来，就得到了(\boldsymbol{H} = f(\boldsymbol{X}))。</p> 
<p>另一方面，为了保证排列不变性，我们需要在最后添加一个<strong>聚集函数</strong>，例如求和，平均值，最大值等。这样我们就得到了最终的输出。</p> 
<p><img src="https://images2.imgbox.com/20/4b/vNxfirgA_o.png" alt="image"></p> 
<p>说了这么多，思想其实很简单，就是<strong>先单独对每个节点提取特征，然后将所有的特征整合起来，整合的过程要保证顺序不会影响到最终结果。</strong></p> 
<h3><a id="_88"></a>四、图中的学习</h3> 
<p>好了，现在可以研究图中有边的情况了。我们定义邻接矩阵 A，其中</p> 
<p><img src="https://images2.imgbox.com/0b/dd/uJFN18Z0_o.png" alt="image"></p> 
<p>这里为了便于表示，先忽略边的特征，比如距离，花费成本等。</p> 
<p>将图中的节点进行排列时，由于节点的顺序变化会同时影响到连接它的所有边，在邻接矩阵中表示，就是<strong>邻接矩阵的行和列都要受到影响</strong>。具体来说，假如节点经过了 (\boldsymbol{P}) 的排列组合转换，那么邻接矩阵就会变为 (\boldsymbol{PAP}^T) 。</p> 
<p>此时，排列不变性和排列相等性就变成了这样的表示：</p> 
<p><img src="https://images2.imgbox.com/1b/8a/pjxwiWY1_o.png" alt="image"></p> 
<h4><a id="_102"></a>图中的局部性：邻居节点</h4> 
<p>在前文提到的节点集合部分中，我们通过每个节点<strong>独立</strong>地提取特征保证了排列相等性。然而在图中，我们有一个非常有用的信息：一个节点的邻居节点。对于一个节点 i 来说，和它距离为1的节点集合可以表示为：</p> 
<p><img src="https://images2.imgbox.com/3e/b0/9yC7bcfx_o.png" alt="image"></p> 
<p>此时，我们可以在这个邻居节点的集合上提取特征，表示为</p> 
<p><img src="https://images2.imgbox.com/7f/e8/eTQDq5jy_o.png" alt="image"></p> 
<p>将这个提取特征的过程表示为函数 (g(\boldsymbol{x}<em>i, \boldsymbol{X}</em>{N_i})) 。</p> 
<h4><a id="_114"></a>图神经网络</h4> 
<p>此时，我们终于可以了解到图神经网络是什么样的了。将上述的函数g作用于每一个节点及其邻居节点，得到最终的特征，这就是一个图神经网络的基础形式。</p> 
<p><img src="https://images2.imgbox.com/79/d4/AprHU9zp_o.png" alt="image"></p> 
<p>为了确保<strong>排列相等</strong>性，我们需要保证这里的函数g不依赖于邻居节点的顺序信息，因此，g应该是<strong>排列不变</strong>的。</p> 
<p>下图中是一个图神经网络提取特征的图示，通过输入一个节点b，及其周围的邻居节点给函数g，提取得到的隐向量，标记为 (\boldsymbol{h}_b)。</p> 
<p><img src="https://images2.imgbox.com/10/1c/9AeTQo17_o.png" alt="image"></p> 
<figcaption>
  图神经网络提取特征图示 
</figcaption> 
<p>得到了隐含向量以后，如何利用它们呢？这里给出几种常见的使用方式：</p> 
<ul><li>节点分类。得到某一个节点的的隐向量以后，利用该向量去对一个节点进行分类。</li><li>图分类。将所有节点的隐向量整合到一起，得到整个图的特征，然后利用这个特征去分类。比如上文提到的分子特性分类。</li><li>链接预测。输入两个节点的隐向量以及他们之间的边的信息，能够输出这两个节点之间相互作用的结果。这一点也非常有用，比如预测从一个节点到另一个节点的抵达时间。</li></ul> 
<p>以下图示表示了这一过程：</p> 
<p><img src="https://images2.imgbox.com/c9/b2/06oYXf3V_o.png" alt="image"></p> 
<figcaption>
  如何使用隐向量 
</figcaption> 
<h3><a id="_140"></a>五、图中的信息传递</h3> 
<p>在GNN领域中，通常把前文中描述的 (f) 叫做<strong>GNN的一层</strong>， 叫做“扩散”、“传播”或者“信息传递”。<strong>如何定义函数</strong> (g)，是图神经网络中的重点，也是非常热门的研究领域。几乎所有提出的定义方式，可以按照空间偏好分为三类，分别是卷积（Convolutional），注意力（Attentional）和信息传递（Message-passing）。</p> 
<h4><a id="_144"></a>卷积图神经网络</h4> 
<p>通过一个<strong>固定的权重</strong> (c_{ij}) 来整合邻居节点的特征。通常来说，这里的权重通常直接依赖于节点之间边的信息（边的信息代表的是节点的相似度，比如两个节点距离越近，那么它们可能就越相似）。</p> 
<p><img src="https://images2.imgbox.com/b8/03/qy2WEO2w_o.png" alt="image"></p> 
<p><img src="https://images2.imgbox.com/b6/fc/Xd18hthi_o.png" alt="image"></p> 
<figcaption>
  卷积GNN示意图 
</figcaption> 
<p>这种结构对于同质图来说很有用（同质图：图中的节点类型和关系类型都仅有一种），并且网络可以轻易扩展到很大的规模。</p> 
<p>使用这类方法的经典模型包括：</p> 
<ul><li>ChebyNet (Defferrard , NeurIPS’16) et al.</li><li>GCN (Kipf &amp; Welling, ICLR’17)</li><li>SGC (Wu , ICML’19） et al.</li></ul> 
<h4><a id="_162"></a>注意力图卷积神经网络</h4> 
<p>注意力机制是近几年非常火的一个研究方向。应用在GNN中，就是将前文中<strong>固定的权重改为可学习的权重，以此来描述两个相邻节点之间的关系</strong>。用公式表示为：</p> 
<p><img src="https://images2.imgbox.com/d8/25/F5kPP7eU_o.png" alt="image"></p> 
<p><img src="https://images2.imgbox.com/3f/26/sPuI8F8c_o.png" alt="image"></p> 
<figcaption>
  注意力GNN示意图 
</figcaption> 
<p>这种结构对于一些边中没有蕴含节点关系的图来说非常有效，适用于规模不是特别大的图，因为注意力机制会增加比较大的显存和计算量。</p> 
<p>使用这类方法的经典模型包括：</p> 
<ul><li>MoNet (Monti , CVPR’17) et al.</li><li>GAT (Veličković , ICLR’18) et al.</li><li>GaAN (Zhang , UAI’18) et al.</li></ul> 
<h4><a id="_180"></a>信息传递图神经网络</h4> 
<p>计算出一个向量（“信息”），通过边传递给其余节点。</p> 
<p>用公式表示为：</p> 
<p><img src="https://images2.imgbox.com/eb/12/eg9Bl2DE_o.png" alt="image"></p> 
<p><img src="https://images2.imgbox.com/ec/90/SGktPhK2_o.png" alt="image"></p> 
<figcaption>
  信息传递GNN示意图 
</figcaption> 
<p>其中 ，(m_{ij} = \psi)就是传递的信息。</p> 
<p>因为有了信息的传递，这类模型可以拟合一些非常复杂的任务，但是可能会面临一些规模或者学习上的问题，因为需要计算和存储整个图中所有边计算出来的信息，而图中的边一般都比节点要多得多。</p> 
<p>使用这类方法的经典模型包括：</p> 
<ul><li>Interaction Networks (Battaglia ., NeurIPS’16) et al</li><li>MPNN (Gilmer ., ICML’17) et al</li><li>GraphNets (Battaglia ., 2018) et al</li></ul> 
<h3><a id="GNN_202"></a>六、关于GNN的观点</h3> 
<h4><a id="_204"></a>节点嵌入技术</h4> 
<p>这项技术的目的是，将图中的节点嵌入到隐空间中。</p> 
<p><img src="https://images2.imgbox.com/3f/31/SGGVllQj_o.png" alt="image"></p> 
<figcaption>
  节点嵌入 
</figcaption> 
<p>那么什么是一个好的嵌入表示呢？对于图来说，一个自然的想法就是，保留住图中的有用信息，而节点之间的有用信息，那就是边。所以就有了一个非常直接的目标：如果两个节点之间右边，那么他们在隐空间中应该是相近的，可以使用标准的二分类交叉熵损失函数来实现这个目标：</p> 
<p><img src="https://images2.imgbox.com/27/8d/FZVDOLv6_o.png" alt="image"></p> 
<p>这个其实是一个更加广泛的方法簇——随机行走的一个特例。将上述损失函数中的 ，重新定义为 和 <strong>共同出现在一个随机行走的过程</strong>中。</p> 
<p>这类方法在GNN面世前，是<strong>最主流</strong>的无监督图表示学习方法。有以下著名的算法：</p> 
<ul><li>DeepWalk (Perozzi , KDD’14) et al.</li><li>node2vec (Grover &amp; Leskovec, KDD’16)</li><li>LINE (Tang , WWW’15) et al.</li></ul> 
<p>值得注意的一点是，通过这种方法，能够学习到节点局部的相似性。因为在隐空间中距离比较近的节点，可能在原先的图结构中是存在边相连的，就是他们互相是邻居节点。这是随机行走方法的一个内在的特点。但是，这恰恰就是卷积GNN的设计思想：强迫邻居学习到相似的特征，这是因为两个图中相邻的节点，可能会有很多个共同的邻居节点，所以能够学习到相似的特征。关于此，有两个有用的推论：</p> 
<ul><li>随机行走的目标可能<strong>不能</strong>给GNN提供有用的信号</li><li>有时，DeepWalk能得到与<strong>未训练的卷积GNN</strong>相同的效果！（Veličković et al., ICLR’19）</li></ul> 
<h4><a id="NLP_229"></a>与NLP的关系</h4> 
<p>节点嵌入技术，其实和NLP中的词向量技术如出一辙：</p> 
<ul><li>节点可以和词对应</li><li>随机行走和一个句子对应</li><li>node2vec技术对应于word2vec技术</li><li>优化目标基本是一模一样的</li></ul> 
<p>并不单纯只有NLP可以嵌入到GNN的设计思想中，GNN其实也可以嵌入到NLP的设计思想中，两门技术是非常相通的。</p> 
<p>在NLP领域中，一个句子中的不同单词，是会互相交互的，比如出现一个词，另外一个词的含义可能完全变了。我们可以使用图来表示这种交互，但是这个交互信息并不是直接提供的，此时，一个常见的假设就是去初始化一个<strong>完全图</strong>（完全图是一个简单的无向图，其中每对不同的顶点之间都恰连有一条边相连），然后让网络去学习相互之间的关系。而这，<strong>就是Transformers的思想！</strong></p> 
<p>可以将Transformer视作GNN的一种，它是一个完全图，属于注意力图神经网络分类下。attention可以看做图中的一种<strong>软相邻</strong>。（Joshi (The Gradient; 2020).）</p> 
<p><img src="https://images2.imgbox.com/e2/5c/AIRM0Qcm_o.png" alt="image"></p> 
<figcaption>
  多头注意力机制 
</figcaption> 
<p>感谢您的阅读，如果觉得这篇文章对您有帮助，欢迎分享收藏！最后附上教程的原始视频链接。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/bf195e34b15596562021dfe8dc656d8e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">centos 开放指定端口</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a77396609792afc1ad944596e183f82f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">二极管电路的分析方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>