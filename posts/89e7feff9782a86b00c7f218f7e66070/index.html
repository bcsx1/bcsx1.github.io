<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>基于DnCNN的图像和视频去噪 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="基于DnCNN的图像和视频去噪" />
<meta property="og:description" content="点击上方“小白学视觉”，选择加&#34;星标&#34;或“置顶”
重磅干货，第一时间送达 简介
随着数字图像数量的增加，对高质量的图像需求也在增加。然而，现代相机拍摄的图像会因噪声而退化。图像中的噪声是图像中颜色信息的失真，噪声是指数字失真。当在夜间拍摄时，图像变得更嘈杂。该案例研究试图建立一个预测模型，该模型将带噪图像作为输入并输出去噪后的图像。
深度学习的使用
这个问题是基于计算机视觉的，CNN等深度学习技术的进步已经能够在图像去噪方面提供最先进的性能，用于执行图像去噪的模型是DnCNN（去噪卷积神经网络）。
数据集
BSD300和BSD500数据集均用作训练数据，BSD68用于验证数据。由于数据有限，每个图像使用了4次，即缩放到[1.0,0.7,0.8,0.7]。
每个缩放图像被分割成50x50的块，步幅为20。每个贴片都添加了一个标准偏差在[1,55]之间的高斯噪声。数据生成代码如下所示：
#Fix Noise stddevs = np.random.uniform(1, 55.0, 125000)[:, np.newaxis, np.newaxis, np.newaxis] noise = np.random.normal(loc = 0, scale=stddevs, size=(125000, 50, 50, 3)).astype(np.float16) def get_dataset(img_path): def image_generator(): patch_size = 50 stride = 20 index = 0 for scale in [1, 0.9, 0.8, 0.7]: for path in img_path: true_img = cv2.imread(path) for i in range(0, true_img.shape[0] - patch_size &#43; 1, stride): for j in range(0, true_img." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/89e7feff9782a86b00c7f218f7e66070/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-11T10:05:00+08:00" />
<meta property="article:modified_time" content="2022-04-11T10:05:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">基于DnCNN的图像和视频去噪</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p style="text-align:center;">点击上方“<strong><strong>小白学视觉</strong></strong>”，选择加"<strong>星标</strong>"或“<strong>置顶</strong>”</p> 
 <pre class="has"><code class="language-go">重磅干货，第一时间送达</code></pre> 
 <p><strong>简介</strong></p> 
 <p>随着数字图像数量的增加，对高质量的图像需求也在增加。然而，现代相机拍摄的图像会因噪声而退化。图像中的噪声是图像中颜色信息的失真，噪声是指数字失真。当在夜间拍摄时，图像变得更嘈杂。该案例研究试图建立一个预测模型，该模型将带噪图像作为输入并输出去噪后的图像。</p> 
 <p><strong>深度学习的使用</strong></p> 
 <p>这个问题是基于计算机视觉的，CNN等深度学习技术的进步已经能够在图像去噪方面提供最先进的性能，用于执行图像去噪的模型是DnCNN（去噪卷积神经网络）。</p> 
 <p><strong>数据集</strong></p> 
 <p>BSD300和BSD500数据集均用作训练数据，BSD68用于验证数据。由于数据有限，每个图像使用了4次，即缩放到[1.0,0.7,0.8,0.7]。</p> 
 <p>每个缩放图像被分割成50x50的块，步幅为20。每个贴片都添加了一个标准偏差在[1,55]之间的高斯噪声。数据生成代码如下所示：</p> 
 <pre class="has"><code class="language-python">#Fix Noise
stddevs = np.random.uniform(1, 55.0, 125000)[:, np.newaxis, np.newaxis, np.newaxis]
noise = np.random.normal(loc = 0, scale=stddevs, size=(125000, 50, 50, 3)).astype(np.float16)


def get_dataset(img_path):
def image_generator():
        patch_size = 50
        stride = 20
        index = 0
for scale in [1, 0.9, 0.8, 0.7]:
for path in img_path:
                true_img = cv2.imread(path)
for i in range(0, true_img.shape[0] - patch_size + 1, stride):
for j in range(0, true_img.shape[1] - patch_size + 1, stride):
                        Y = true_img[i:i+patch_size, j:j+patch_size]
                        gauss_noise = noise[index].astype(np.float32)
                        X = np.clip(Y + gauss_noise, 0, 255.0)
                        index = (index + 1)%125000
yield (X/255.0,),Y/255.0
return tf.data.Dataset.from_generator(image_generator, output_signature=((tf.TensorSpec(shape=(None, None, 3)),),
                                                                             (tf.TensorSpec(shape=(None, None, 3)))))</code></pre> 
 <p><strong>DnCNN体系结构</strong></p> 
 <p>DnCNN中有三种类型的层：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/7c/9c/lF8zWup4_o.png" alt="0e1224c8f4468429999cca68e64a6271.png"></p> 
 <ol><li><p>Conv+ReLU：过滤器大小为3，过滤器数量为64，跨步为1，使用零填充保持卷积后的输出形状，使用ReLU作为激活函数。输出为形状（批量大小，50、50、64）</p></li><li><p>Conv+批量归一化+ReLU：过滤器大小为3，过滤器数量为64，步长为1，使用零填充保持卷积后的输出形状，使用批量归一化层更好地收敛，ReLU作为激活函数。输出为形状（批次大小，50、50、64）。</p></li><li><p>Conv：滤镜大小为3，跨步为1，滤镜数量为c（彩色图像为3个，灰度图像为1个），使用零填充在卷积后保持输出形状。输出形状为（批次大小，50，50，c）。</p></li></ol> 
 <p>DnCNN模型的输出为残差图像。因此，原始图像=噪声图像-残差图像。</p> 
 <p>在DnCNN中，在每层卷积之前填充零，以确保中间层的每个特征贴图与输入图像具有相同的大小。根据本文，简单的零填充策略不会导致任何边界伪影。</p> 
 <p>本文建议深度为17，但本案例研究适用于深度为12和深度为8。</p> 
 <p><strong>评价指标</strong></p> 
 <p>评估指标是PSNR（峰值信噪比）分数。它只是一个数值，表示构造的去噪图像与原始图像相比有多好。</p> 
 <p><strong>模型训练</strong></p> 
 <pre class="has"><code class="language-makefile">def get_model(depth, channels):
noise_inp = tf.keras.layers.Input(shape = (50, 50, channels), dtype=tf.float32)
init = 'Orthogonal'


y = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, padding = 'same', kernel_initializer=init, 
use_bias=True)(noise_inp)
y = tf.keras.layers.ReLU()(y)
for i in range(1, depth-1):
y = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, padding = 'same', kernel_initializer=init, 
use_bias=True)(y)
bn = tf.keras.layers.BatchNormalization(axis=-1, epsilon=1e-5, momentum=0.9)
y = bn(y)
y = tf.keras.layers.ReLU()(y)
residual = tf.keras.layers.Conv2D(filters = channels, kernel_size = 3, padding = 'same', kernel_initializer=init, 
use_bias=True)(y)


true_img = tf.keras.layers.Subtract()([noise_inp, residual])
model = tf.keras.Model(inputs = [noise_inp], outputs=[true_img])
model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mse')


return model


def lr_decay(epoch):
lr = 1e-3
if epoch+1 &gt; 20:
lr/=30
elif epoch+1 &gt; 10:
lr /= 10
return lr
model = get_model(8, 3)
lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_decay)
dataset = get_dataset(bsd500).shuffle(1000).batch(128).prefetch(tf.data.experimental.AUTOTUNE).repeat(None)
model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mse')
history = model.fit(x = dataset, steps_per_epoch=2000, epochs=30, shuffle=True,verbose=1,
callbacks=[lr_callback])</code></pre> 
 <p>批量大小=128，每个历元的步数=2000，历元数=30。<br></p> 
 <p><strong>结果</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/0a/12/mxAW3wwK_o.png" alt="753b11f87370fda259555536cfda6302.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/23/bf/1P6511MW_o.png" alt="c84a68e997ece6d518a006a5a3da3ad6.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/33/c9/42nYyaRy_o.png" alt="16f740ce477a8e9a61fd39797d01e94d.png"></p> 
 <p>BSD68数据集上的峰值信噪比对于标准差25为~28，对于标准差50为~25。</p> 
 <p>如果深度=12，则BSD68数据集上的峰值信噪比对于标准差25为28.30，对于标准差50为26.13。</p> 
 <p><strong>应用：视频去噪</strong></p> 
 <p>我们可以将这个想法扩展到视频帧，每个帧作为输入传递给DnCNN模型，生成的帧传递给视频编写器。</p> 
 <pre class="has"><code class="language-python">import sys
import tensorflow as tf
import numpy as np
import cv2
import time
import matplotlib.pyplot as plt
import os
import glob
import seaborn as snb
import re
from skvideo.io import FFmpegWriter




class Denoiser:
def __init__(self, merge_outputs):
        self.model = tf.keras.models.load_model('./model')
        self.merge_outputs = merge_outputs


def get_patches(self, frame):
        patches = np.zeros(shape=(self.batch_size, 50, 50, 3))
        counter = 0
for i in range(0, self.SCALE_H, 50):
for j in range(0, self.SCALE_W, 50):
                patches[counter] = frame[i:i+50, j:j+50]
                counter+=1
return patches.astype(np.float32)


def reconstruct_from_patches(self, patches, h, w, true_h, true_w, patch_size):
        img = np.zeros((h,w, patches[0].shape[-1]))
        counter = 0
for i in range(0,h-patch_size+1,patch_size):
for j in range(0,w-patch_size+1,patch_size):
                img[i:i+patch_size, j:j+patch_size, :] = patches[counter]
                counter+=1
return cv2.resize(img, (true_w, true_h), cv2.INTER_CUBIC)


def denoise_video(self, PATH):
        self.cap = cv2.VideoCapture(PATH)
        self.H, self.W = int(self.cap.get(4)), int(self.cap.get(3))
        self.SCALE_H, self.SCALE_W = (self.H//50 * 50), (self.W//50 * 50)
        self.batch_size = ((self.SCALE_H * self.SCALE_W) // (50**2))


        outputFile = './denoise.mp4'
        writer = FFmpegWriter(
        outputFile,
            outputdict={
'-vcodec':'libx264',
'-crf':'0',
'-preset':'veryslow'
        }
        )


while True:
            success, img = self.cap.read()
if not success:
break
            resize_img = cv2.resize(img, (self.SCALE_W, self.SCALE_H), cv2.INTER_CUBIC).astype(np.float32)


            noise_img = resize_img/255.0
            patches = self.get_patches(noise_img).astype(np.float32)
            predictions = np.clip(self.model(patches), 0, 1)
            pred_img = (self.reconstruct_from_patches(predictions, self.SCALE_H, 
                                                     self.SCALE_W, self.H, self.W, 50)*255.0)
if self.merge_outputs:
                merge = np.vstack([img[:self.H//2,:,:], pred_img[:self.H//2,:,:]])
                writer.writeFrame(merge[:,:,::-1])
else:
                writer.writeFrame(pred_img[:,:,::-1])
        writer.close()


PATH = sys.argv[1]
print(f"Path is : {PATH}")
denoise = Denoiser(merge_outputs = True)
x = denoise.denoise_video(PATH)</code></pre> 
 <p>参考<br></p> 
 <ol><li><p>https://arxiv.org/pdf/1608.03981.pdf</p></li><li><p>https://www.appliedaicourse.com/</p></li></ol> 
 <p>GITHUB代码链接：https://github.com/saproovarun/DnCNN-Keras</p> 
 <pre class="has"><code class="language-go">小白团队出品：零基础精通语义分割↓↓↓

下载1：OpenCV-Contrib扩展模块中文版教程

在「小白学视觉」公众号后台回复：扩展模块中文教程，即可下载全网第一份OpenCV扩展模块教程中文版，涵盖扩展模块安装、SFM算法、立体视觉、目标跟踪、生物视觉、超分辨率处理等二十多章内容。


下载2：Python视觉实战项目52讲
在「小白学视觉」公众号后台回复：Python视觉实战项目，即可下载包括图像分割、口罩检测、车道线检测、车辆计数、添加眼线、车牌识别、字符识别、情绪检测、文本内容提取、面部识别等31个视觉实战项目，助力快速学校计算机视觉。


下载3：OpenCV实战项目20讲
在「小白学视觉」公众号后台回复：OpenCV实战项目20讲，即可下载含有20个基于OpenCV实现20个实战项目，实现OpenCV学习进阶。


交流群

欢迎加入公众号读者群一起和同行交流，目前有SLAM、三维视觉、传感器、自动驾驶、计算摄影、检测、分割、识别、医学影像、GAN、算法竞赛等微信群（以后会逐渐细分），请扫描下面微信号加群，备注：”昵称+学校/公司+研究方向“，例如：”张三 + 上海交大 + 视觉SLAM“。请按照格式备注，否则不予通过。添加成功后会根据研究方向邀请进入相关微信群。请勿在群内发送广告，否则会请出群，谢谢理解~</code></pre> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d165b11b970f92fb918ea719932877d8/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">实战案例：使用机器学习算法预测用户贷款是否违约?</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7cb6cb92fc64d93af582c8fe56023ef4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Altium Designer将原理图导出到PDF时部分引脚名称消失</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>