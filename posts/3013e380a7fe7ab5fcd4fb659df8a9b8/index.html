<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>神经网络与深度学习（四）线性分类 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="神经网络与深度学习（四）线性分类" />
<meta property="og:description" content="目录
3.1 基于Logistic回归的二分类任务
3.1.1 数据集构建
3.1.2 模型构建
3.1.3 损失函数
3.1.4 模型优化
3.1.4.1 梯度计算
3.1.4.2 参数更新
3.1.5 评价指标
3.1.6 完善Runner类
3.1.7 模型训练
3.1.8 模型评价
3.2 基于Softmax回归的多分类任务
3.2.1 数据集构建
3.2.2 模型构建
3.2.2.1 Softmax函数 3.2.2.2 Softmax回归算子 3.2.3 损失函数
3.2.4 模型优化
3.2.4.1 梯度计算 3.2.4.2 参数更新
3.2.5 模型训练
3.2.6 模型评价
3.3 实践：基于Softmax回归完成鸢尾花分类任务
3.3.1 数据处理 3.3.1.1 数据集介绍
3.3.1.2 数据清洗 3.3.1.3 数据读取 ​ 3.3.2 模型构建
3.3.4 模型评价 3.3.5 模型预测 3.4 小结 3.5 实验拓展
参考资料 3.1 基于Logistic回归的二分类任务 3." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/3013e380a7fe7ab5fcd4fb659df8a9b8/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-09-21T21:00:02+08:00" />
<meta property="article:modified_time" content="2022-09-21T21:00:02+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">神经网络与深度学习（四）线性分类</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="3.1%20%E5%9F%BA%E4%BA%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E4%BA%8C%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1-toc" style="margin-left:0px;"><a href="#3.1%20%E5%9F%BA%E4%BA%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E4%BA%8C%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1" rel="nofollow">3.1 基于Logistic回归的二分类任务</a></p> 
<p id="3.1.1%20%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E5%BB%BA-toc" style="margin-left:40px;"><a href="#3.1.1%20%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E5%BB%BA" rel="nofollow">3.1.1 数据集构建</a></p> 
<p id="312-模型构建-toc" style="margin-left:40px;"><a href="#312-%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA" rel="nofollow">3.1.2 模型构建</a></p> 
<p id="3.1.3%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-toc" style="margin-left:40px;"><a href="#3.1.3%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0" rel="nofollow">3.1.3 损失函数</a></p> 
<p id="3.1.4%20%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96-toc" style="margin-left:40px;"><a href="#3.1.4%20%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96" rel="nofollow">3.1.4 模型优化</a></p> 
<p id="%C2%A03.1.4.1%20%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97-toc" style="margin-left:80px;"><a href="#%C2%A03.1.4.1%20%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97" rel="nofollow"> 3.1.4.1 梯度计算</a></p> 
<p id="%C2%A03.1.4.2%20%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0-toc" style="margin-left:80px;"><a href="#%C2%A03.1.4.2%20%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0" rel="nofollow"> 3.1.4.2 参数更新</a></p> 
<p id="3.1.5%20%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87-toc" style="margin-left:40px;"><a href="#3.1.5%20%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87" rel="nofollow">3.1.5 评价指标</a></p> 
<p id="3.1.6%20%E5%AE%8C%E5%96%84Runner%E7%B1%BB-toc" style="margin-left:40px;"><a href="#3.1.6%20%E5%AE%8C%E5%96%84Runner%E7%B1%BB" rel="nofollow">3.1.6 完善Runner类</a></p> 
<p id="3.1.7%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-toc" style="margin-left:40px;"><a href="#3.1.7%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83" rel="nofollow">3.1.7 模型训练</a></p> 
<p id="3.1.8%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7-toc" style="margin-left:40px;"><a href="#3.1.8%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7" rel="nofollow">3.1.8 模型评价</a></p> 
<p id="3.2%20%E5%9F%BA%E4%BA%8ESoftmax%E5%9B%9E%E5%BD%92%E7%9A%84%E5%A4%9A%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1-toc" style="margin-left:0px;"><a href="#3.2%20%E5%9F%BA%E4%BA%8ESoftmax%E5%9B%9E%E5%BD%92%E7%9A%84%E5%A4%9A%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1" rel="nofollow">3.2 基于Softmax回归的多分类任务</a></p> 
<p id="3.2.1%20%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E5%BB%BA-toc" style="margin-left:40px;"><a href="#3.2.1%20%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E5%BB%BA" rel="nofollow">3.2.1 数据集构建</a></p> 
<p id="3.2.2%20%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA-toc" style="margin-left:40px;"><a href="#3.2.2%20%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA" rel="nofollow">3.2.2 模型构建</a></p> 
<p id="3.2.2.1%20Softmax%E5%87%BD%E6%95%B0%C2%A0-toc" style="margin-left:80px;"><a href="#3.2.2.1%20Softmax%E5%87%BD%E6%95%B0%C2%A0" rel="nofollow">3.2.2.1 Softmax函数 </a></p> 
<p id="3.2.2.2%20Softmax%E5%9B%9E%E5%BD%92%E7%AE%97%E5%AD%90%C2%A0-toc" style="margin-left:80px;"><a href="#3.2.2.2%20Softmax%E5%9B%9E%E5%BD%92%E7%AE%97%E5%AD%90%C2%A0" rel="nofollow">3.2.2.2 Softmax回归算子 </a></p> 
<p id="323-损失函数-toc" style="margin-left:40px;"><a href="#323-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0" rel="nofollow">3.2.3 损失函数</a></p> 
<p id="3.2.4%20%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96-toc" style="margin-left:40px;"><a href="#3.2.4%20%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96" rel="nofollow">3.2.4 模型优化</a></p> 
<p id="3.2.4.1%20%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97%C2%A0-toc" style="margin-left:80px;"><a href="#3.2.4.1%20%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97%C2%A0" rel="nofollow">3.2.4.1 梯度计算 </a></p> 
<p id="%C2%A03.2.4.2%20%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0-toc" style="margin-left:80px;"><a href="#%C2%A03.2.4.2%20%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0" rel="nofollow"> 3.2.4.2 参数更新</a></p> 
<p id="3.2.5%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-toc" style="margin-left:40px;"><a href="#3.2.5%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83" rel="nofollow">3.2.5 模型训练</a></p> 
<p id="3.2.6%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7-toc" style="margin-left:40px;"><a href="#3.2.6%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7" rel="nofollow">3.2.6 模型评价</a></p> 
<p id="3.3%20%E5%AE%9E%E8%B7%B5%EF%BC%9A%E5%9F%BA%E4%BA%8ESoftmax%E5%9B%9E%E5%BD%92%E5%AE%8C%E6%88%90%E9%B8%A2%E5%B0%BE%E8%8A%B1%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1-toc" style="margin-left:0px;"><a href="#3.3%20%E5%AE%9E%E8%B7%B5%EF%BC%9A%E5%9F%BA%E4%BA%8ESoftmax%E5%9B%9E%E5%BD%92%E5%AE%8C%E6%88%90%E9%B8%A2%E5%B0%BE%E8%8A%B1%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1" rel="nofollow">3.3 实践：基于Softmax回归完成鸢尾花分类任务</a></p> 
<p id="3.3.1%20%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%C2%A0-toc" style="margin-left:40px;"><a href="#3.3.1%20%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%C2%A0" rel="nofollow">3.3.1 数据处理 </a></p> 
<p id="3.3.1.1%20%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D-toc" style="margin-left:80px;"><a href="#3.3.1.1%20%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D" rel="nofollow">3.3.1.1 数据集介绍</a></p> 
<p id="3.3.1.2%20%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%C2%A0-toc" style="margin-left:80px;"><a href="#3.3.1.2%20%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%C2%A0" rel="nofollow">3.3.1.2 数据清洗 </a></p> 
<p id="3.3.1.3%20%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%C2%A0-toc" style="margin-left:80px;"><a href="#3.3.1.3%20%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%C2%A0" rel="nofollow">3.3.1.3 数据读取 </a></p> 
<p id="%E2%80%8B%C2%A03.3.2%20%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA-toc" style="margin-left:40px;"><a href="#%E2%80%8B%C2%A03.3.2%20%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA" rel="nofollow">​ 3.3.2 模型构建</a></p> 
<p id="3.3.4%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%C2%A0-toc" style="margin-left:40px;"><a href="#3.3.4%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%C2%A0" rel="nofollow">3.3.4 模型评价 </a></p> 
<p id="3.3.5%20%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%C2%A0-toc" style="margin-left:40px;"><a href="#3.3.5%20%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%C2%A0" rel="nofollow">3.3.5 模型预测 </a></p> 
<p id="3.4%20%E5%B0%8F%E7%BB%93%C2%A0-toc" style="margin-left:0px;"><a href="#3.4%20%E5%B0%8F%E7%BB%93%C2%A0" rel="nofollow">3.4 小结 </a></p> 
<p id="%C2%A03.5%20%E5%AE%9E%E9%AA%8C%E6%8B%93%E5%B1%95-toc" style="margin-left:0px;"><a href="#%C2%A03.5%20%E5%AE%9E%E9%AA%8C%E6%8B%93%E5%B1%95" rel="nofollow"> 3.5 实验拓展</a></p> 
<p id="%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99%C2%A0-toc" style="margin-left:0px;"><a href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99%C2%A0" rel="nofollow">参考资料 </a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="3.1%20%E5%9F%BA%E4%BA%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E4%BA%8C%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1">3.1 基于Logistic回归的二分类任务</h2> 
<h3 id="3.1.1%20%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E5%BB%BA">3.1.1 数据集构建</h3> 
<p>构建一个简单的分类任务，并构建训练集、验证集和测试集。</p> 
<p>本任务的数据来自带噪音的两个弯月形状函数，每个弯月对一个类别。我们采集1000条样本，每个样本包含2个特征。</p> 
<p>数据集的构建函数make_moons的代码实现如下：</p> 
<pre><code class="language-python">import math
import torch


def make_moons(n_samples=1000, shuffle=True, noise=None):
    n_samples_out = n_samples // 2  # 这里是只去除完之后的整数部分。
    n_samples_in = n_samples - n_samples_out

    # 采集第一类数据，特征为（X，y）
    # 使用'torch.linspace'在0到pi上均匀取n_samples_out个值
    # 使用'torch.cos'计算上述取值的余弦值作为特征1，使用'torch.sin'计算上述取值的正弦值作为特征2
    outer_circ_x = torch.cos(torch.linspace(0, math.pi, n_samples_out))
    outer_circ_y = torch.sin(torch.linspace(0, math.pi, n_samples_out))

    inner_circ_x = 1 - torch.cos(torch.linspace(0, math.pi, n_samples_in))
    inner_circ_y = 0.5 - torch.sin(torch.linspace(0, math.pi, n_samples_in))

    print('outer_circ_x.shape:', outer_circ_x.shape, 'outer_circ_y.shape:', outer_circ_y.shape)
    print('outer_circ_x.shape:', inner_circ_x.shape, 'inner_circ_y.shape:', inner_circ_y.shape)

    # 使用'torch.cat'将两类数据的特征1和特征2分别延维度0拼接在一起，得到全部特征1和特征2
    # 使用'torch.stack'将两类特征延维度1堆叠在一起
    X = torch.stack(
        [torch.cat([outer_circ_x, inner_circ_x]),
         torch.cat([outer_circ_y, inner_circ_y])],
        dim=1
    )

    print('after concat shape:', torch.cat([outer_circ_x, inner_circ_x]).shape)
    print('X shape:', X.shape)

    # 使用'torch. zeros'将第一类数据的标签全部设置为0
    # 使用'torch. ones'将第一类数据的标签全部设置为1
    y = torch.cat(
        [torch.zeros(size=[n_samples_out]), torch.ones(size=[n_samples_in])]
    )

    print('y shape:', y.shape)

    # 如果shuffle为True，将所有数据打乱
    if shuffle:
        # 使用'torch.randperm'生成一个数值在0到X.shape[0]，随机排列的一维Tensor做索引值，用于打乱数据
        idx = torch.randperm(X.shape[0])
        X = X[idx]
        y = y[idx]

    # 如果noise不为None，则给特征值加入噪声
    if noise is not None:
        # 使用'torch.normal'生成符合正态分布的随机Tensor作为噪声，并加到原始特征上
        X += torch.normal(mean=0.0, std=noise, size=X.shape)

    return X, y</code></pre> 
<p>随机采样1000个样本，并进行可视化。</p> 
<pre><code class="language-python">import matplotlib.pyplot as plt
# 采1000个样本
n_samples = 1000
X, y = make_moons(n_samples=n_samples, shuffle=True, noise=0.5)

plt.figure(figsize=(5, 5))
plt.scatter(x=X[:, 0].tolist(), y=X[:, 1].tolist(), marker='*', c=y.tolist())
plt.xlim(-3, 4)
plt.ylim(-3, 4)
plt.savefig('linear-dataset-vis.pdf')
plt.show()</code></pre> 
<p> 运行结果：</p> 
<p><img alt="" height="142" src="https://images2.imgbox.com/3d/3d/4PaVRWfR_o.png" width="773"></p> 
<p> <img alt="" height="542" src="https://images2.imgbox.com/6e/db/9zNeJURE_o.png" width="556"></p> 
<p>将1000条样本数据拆分成训练集、验证集和测试集，其中训练集640条、验证集160条、测试集200条。</p> 
<pre><code class="language-python">num_train = 640
num_dev = 160
num_test = 200
X_train, y_train = X[:num_train], y[:num_train]
X_dev, y_dev = X[num_train:num_train + num_dev], y[num_train:num_train + num_dev]
X_test, y_test = X[num_train + num_dev:], y[num_train + num_dev:]
y_train = y_train.reshape([-1, 1])
y_dev = y_dev.reshape([-1, 1])
y_test = y_test.reshape([-1, 1])
print("X_train shape: ", X_train.shape, "y_train shape: ", y_train.shape)
print(y_train[:5])</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="170" src="https://images2.imgbox.com/f6/9f/WyScQ0lV_o.png" width="747"></p> 
<h3 id="312-模型构建">3.1.2 模型构建</h3> 
<p>Logistic回归是一种常用的处理二分类问题的线性模型。与线性回归一样，Logistic回归也会将输入特征与权重做线性叠加。不同之处在于，Logistic回归引入了非线性函数<img alt="g:\mathbb{R}^{D} \overset{}{\rightarrow} (0,1)" class="mathcode" src="https://images2.imgbox.com/f7/05/RrhhMCMt_o.png">，预测类别标签的后验概率 p(y=1|x) ，从而解决连续的线性函数不适合进行分类的问题。</p> 
<p style="text-align:center;"><img alt="p(y=1|x)=\sigma (w^{T}x+b)" class="mathcode" src="https://images2.imgbox.com/ff/57/CbQ40Bne_o.png"></p> 
<p>其中判别函数σ(⋅)为Logistic函数，也称为激活函数，作用是将线性函数f(x;w,b)的输出从实数区间“挤压”到（0,1）之间，用来表示概率。Logistic函数定义为： </p> 
<p style="text-align:center;"><img alt="\sigma (x)=\frac{1}{1+exp(-x)}" class="mathcode" src="https://images2.imgbox.com/cd/e2/vlJybBPx_o.png"></p> 
<p><strong>Logistic函数</strong>的代码实现如下：</p> 
<pre><code class="language-python">def logistic(x):
    return 1 / (1 + torch.exp(-x))


# 在[-10,10]的范围内生成一系列的输入值
x = torch.linspace(-10, 10, 10000)
plt.figure()
plt.plot(x.tolist(), logistic(x).tolist(), color="#e4007f", label="Logistic Function")
# 设置坐标轴
ax = plt.gca()
# 取消右侧和上侧坐标轴
ax.spines['top'].set_color('none')
ax.spines['right'].set_color('none')
# 设置默认的x轴和y轴方向
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')
# 设置坐标原点为(0,0)
ax.spines['left'].set_position(('data', 0))
ax.spines['bottom'].set_position(('data', 0))
# 添加图例
plt.legend()
plt.savefig('linear-logistic.pdf')
plt.show()</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="507" src="https://images2.imgbox.com/07/fb/zey9Y2mG_o.png" width="673"></p> 
<p> 从输出结果看，当<strong>输入在0附近时</strong>，Logistic函数<strong>近似为线性函数</strong>；而当输入值非常大或非常小时，函数会对输入进行抑制。<strong>输入越小，则越接近0；输入越大，则越接近1。</strong>正因为Logistic函数具有这样的性质，使得其输出可以直接看作为<strong>概率分布</strong>。</p> 
<p><strong>Logistic回归算子 </strong></p> 
<p>Logistic回归模型其实就是线性层与Logistic函数的组合，通常会将 Logistic回归模型中的权重和偏置初始化为0，同时，为了提高预测样本的效率，我们将NN个样本归为一组进行成批地预测。 </p> 
<p style="text-align:center;"><img alt="\hat{y}=p(y|x)=\sigma (Xw+b)" class="mathcode" src="https://images2.imgbox.com/ec/cf/qw1tIPEn_o.png"></p> 
<p>其中<img alt="X\in \mathbb{R} ^{N\times D}" class="mathcode" src="https://images2.imgbox.com/10/ff/LcH0UCQI_o.png">为N个样本的特征矩阵，<img alt="\hat{y}" class="mathcode" src="https://images2.imgbox.com/7a/c6/HTIDHQG5_o.png">为N个样本的预测值构成的N维向量。</p> 
<p>这里，我们构建一个Logistic回归算子，代码实现如下：</p> 
<pre><code class="language-python">from nndl import op
class model_LR(op.Op):
    def __init__(self, input_dim):
        super(model_LR, self).__init__()
        self.params = {}
        # 将线性层的权重参数全部初始化为0
        self.params['w'] = torch.zeros(input_dim, 1)
        # 将线性层的偏置参数初始化为0
        self.params['b'] = torch.zeros(1)

    def __call__(self, inputs):
        return self.forward(inputs)

    def forward(self, inputs):
        # 线性计算
        score = torch.matmul(inputs, self.params['w']) + self.params['b']
        # Logistic 函数
        outputs = logistic(score)
        return outputs
</code></pre> 
<p>随机生成3条长度为4的数据输入Logistic回归模型，观察输出结果。</p> 
<pre><code class="language-python"># 固定随机种子，保持每次运行结果一致
torch.manual_seed(0)
# 随机生成3条长度为4的数据
inputs = torch.randn([3, 4])
print('Input is:\n', inputs)
# 实例化模型
model = model_LR(4)
outputs = model(inputs)
print('Output is:\n', outputs)</code></pre> 
<p>运行结果：</p> 
<p> <img alt="" height="237" src="https://images2.imgbox.com/ef/a7/rH0xmhxT_o.png" width="492"></p> 
<p>从输出结果看，模型最终的输出g(⋅)恒为0.5。这是由于采用全0初始化后，不论输入值的大小为多少，Logistic函数的输入值恒为0，因此输出恒为0.5。 </p> 
<p id="31-基于logistic回归的二分类任务"><strong><span style="color:#fe2c24;">问题1：</span></strong>Logistic回归在不同的书籍中，有许多其他的称呼，具体有哪些？你认为哪个称呼最好？</p> 
<p>对数几率回归、逻辑斯蒂回归。我认为对数几率回归这个称呼最好，能通过名字一目了然地知道其使用的大概方法内容。</p> 
<p><strong><span style="color:#fe2c24;">问题2：</span></strong>什么是激活函数？为什么要用激活函数？常见激活函数有哪些？</p> 
<p><strong><span style="background-color:#ffd900;">一、什么是激活函数？</span></strong></p> 
<p>简单地说，激活函数就是加入到人工神经网络中的一个函数，目的在于帮助神经网络从数据中学习复杂模式。相比于人类大脑中基于神经元的模型，激活函数是决定向下一个神经元传递何种信息的单元，这也正是激活函数在人工神经网络中的作用。激活函数接收前一个单元输出的信号，并将其转换成某种可以被下一个单元接收的形式。</p> 
<figure class="image"> 
 <img alt="" height="337" src="https://images2.imgbox.com/30/a6/xeRhsAjQ_o.png" width="886"> 
 <figcaption>
   图片来源：斯坦福大学的cs231n 课程题 
 </figcaption> 
</figure> 
<p>  </p> 
<p> <strong><span style="background-color:#ffd900;">二、为什么要用激活函数？</span></strong></p> 
<p>在神经网络中使用非线性激活函数的原因有很多。</p> 
<p>1. 除了前面讨论过的生物学方面的相似性外，激活函数还有助于我们根据要求将神经元的输出值限定在一定的范围内。这一点很重要，因为激活函数的输入是 W*x+b，其中 W 是单元的权重，x 是输入值，然后加上偏置 b。如果输出值不被限定在某个范围内，它可能会变得非常大，特别是在具有数百万个参数的深层神经网络中，从而导致计算量过大。例如，有一些激活函数对于不同的输入值（0 或 1）会输出特定的值。</p> 
<p>2. 激活函数最重要的特点是它具有在神经网络中加入非线性的能力，为了使模型能够学习非线性模式（或者说具有更高的复杂度），特定的非线性层（激活函数）被加入其中。</p> 
<p><strong><span style="background-color:#ffd900;">三、常见激活函数有哪些？</span></strong></p> 
<p>现在常听到或者用到的激活函数有Relu、sigmoid、tanh等</p> 
<p><img alt="" height="432" src="https://images2.imgbox.com/4b/2a/CTwWTtsg_o.png" width="908"></p> 
<h3 id="3.1.3%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0">3.1.3 损失函数</h3> 
<p>在模型训练过程中，需要使用损失函数来量化预测值和真实值之间的差异。</p> 
<p>给定一个分类任务，y表示样本x的标签的真实概率分布，向量<img alt="\hat{y} =p(y|x)" class="mathcode" src="https://images2.imgbox.com/47/5b/pM4Bqb5c_o.png">表示预测的标签概率分布。训练目标是使得<img alt="\hat{y}" class="mathcode" src="https://images2.imgbox.com/f0/52/RpSQR3fn_o.png">尽可能地接近y，通常可以使用<strong>交叉熵损失函数</strong>。</p> 
<p>在给定y的情况下，如果预测的概率分布<img alt="\hat{y}" class="mathcode" src="https://images2.imgbox.com/47/f3/jmtKo29g_o.png">与标签真实的分布y越接近，则交叉熵越小；如果p(x)和y越远，交叉熵就越大。</p> 
<p>对于二分类任务，我们只需要计算<img alt="\hat{y} =p(y=1|x)" class="mathcode" src="https://images2.imgbox.com/8e/58/Jui36Ajg_o.png">，用<img alt="1-\hat{y}" class="mathcode" src="https://images2.imgbox.com/db/24/frPSgpuj_o.png">来表示p(y=0|x)。<br> 给定有N个训练样本的训练集<img alt="\left \{ \left ( x^{\left ( n \right ) },y^{\left ( n \right ) } \right ) \right \}^{N} _{n=1}" class="mathcode" src="https://images2.imgbox.com/32/15/9ypA5BsB_o.png">，使用交叉熵损失函数，Logistic回归的风险函数计算方式为：</p> 
<p style="text-align:center;"><img alt="R(w,b)=-\frac{1}{N}\sum_{n=1}^{N}(y^{\left(n\right)}log\hat{y}^{\left(n\right)}+(1-y^{\left(n\right)})log(1-\hat{y}^{\left(n\right)}))" class="mathcode" src="https://images2.imgbox.com/29/ef/b4e6q63W_o.png"></p> 
<p>向量形式可以表示为：</p> 
<p style="text-align:center;"><img alt="R(w,b)=-\frac{1}{N}(y^{T}log\hat{y}+(1-y)^{T}log(1-\hat{y}))" class="mathcode" src="https://images2.imgbox.com/cc/fc/KWCFiWWt_o.png"></p> 
<p>其中<img alt="y\in [0,1]^{N}" class="mathcode" src="https://images2.imgbox.com/07/f5/HBfGotv9_o.png">为N个样本的真实标签构成的N维向量，<img alt="\hat{y}" class="mathcode" src="https://images2.imgbox.com/5f/e4/yX1KJ9dK_o.png">为N个样本标签为1的后验概率构成的N维向量。</p> 
<p>二分类任务的交叉熵损失函数的代码实现如下：</p> 
<pre><code class="language-python"># 实现交叉熵损失函数
class BinaryCrossEntropyLoss(op.Op):
    def __init__(self):
        self.predicts = None
        self.labels = None
        self.num = None

    def __call__(self, predicts, labels):
        return self.forward(predicts, labels)

    def forward(self, predicts, labels):
        self.predicts = predicts
        self.labels = labels
        self.num = self.predicts.shape[0]
        loss = -1. / self.num * (torch.matmul(self.labels.t(), torch.log(self.predicts)) + torch.matmul((1-self.labels.t()), torch.log(1-self.predicts)))
        loss = torch.squeeze(loss, dim=1)
        return loss


# 测试一下
# 生成一组长度为3，值为1的标签数据
labels = torch.ones([3, 1])
# 计算风险函数
bce_loss = BinaryCrossEntropyLoss()
print(bce_loss(outputs, labels))</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="47" src="https://images2.imgbox.com/fb/80/AEBGETY8_o.png" width="182"></p> 
<h3 id="3.1.4%20%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96">3.1.4 模型优化</h3> 
<p>不同于线性回归中直接使用最小二乘法即可进行模型参数的求解，Logistic回归需要使用优化算法对模型参数进行有限次地迭代来获取更优的模型，从而尽可能地降低风险函数的值。<br> 在机器学习任务中，最简单、常用的优化算法是梯度下降法。</p> 
<p>使用梯度下降法进行模型优化，首先需要初始化参数W和 b，然后不断地计算它们的梯度，并沿梯度的反方向更新参数。</p> 
<h4 id="%C2%A03.1.4.1%20%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"> 3.1.4.1 梯度计算</h4> 
<p>在Logistic回归中，风险函数R(w,b)关于参数w和b的偏导数为： </p> 
<p style="text-align:center;"><img alt="\frac{\partial R(w,b)}{\partial w}=-\frac{1}{N}\sum_{n=1}^{N}x^{(n)}(y^{(n)}-\hat{y}^{(n)})=-\frac{1}{N}X^{T}(y-\hat{y})" class="mathcode" src="https://images2.imgbox.com/66/13/aNLTGadn_o.png"></p> 
<p style="text-align:center;"><img alt="\frac{\partial R(w,b)}{\partial b}=-\frac{1}{N}\sum_{n=1}^{N}(y^{(n)}-\hat{y}^{(n)})=-\frac{1}{N}sum(y-\hat{y})" class="mathcode" src="https://images2.imgbox.com/76/10/a0NUYjKL_o.png"></p> 
<p>通常将偏导数的计算过程定义在Logistic回归算子的<code>backward</code>函数中，代码实现如下:</p> 
<pre><code class="language-python">class model_LR(op.Op):
    def __init__(self, input_dim):
        super(model_LR, self).__init__()
        # 存放线性层参数
        self.params = {}
        # 将线性层的权重参数全部初始化为0
        self.params['w'] = torch.zeros([input_dim, 1])
        # self.params['w'] = paddle.normal(mean=0, std=0.01, shape=[input_dim, 1])
        # 将线性层的偏置参数初始化为0
        self.params['b'] = torch.zeros([1])
        # 存放参数的梯度
        self.grads = {}
        self.X = None
        self.outputs = None

    def __call__(self, inputs):
        return self.forward(inputs)

    def forward(self, inputs):
        self.X = inputs
        # 线性计算
        score = torch.matmul(inputs, self.params['w']) + self.params['b']
        # Logistic 函数
        self.outputs = logistic(score)
        return self.outputs

    def backward(self, labels):
        N = labels.shape[0]
        # 计算偏导数
        self.grads['w'] = -1 / N * torch.matmul(self.X.t(), (labels - self.outputs))
        self.grads['b'] = -1 / N * torch.sum(labels - self.outputs)</code></pre> 
<h4 id="%C2%A03.1.4.2%20%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0"> 3.1.4.2 参数更新</h4> 
<p>在计算参数的梯度之后，我们按照下面公式更新参数：</p> 
<p style="text-align:center;"><img alt="w\overset{}{\leftarrow} w-\alpha \frac{\partial R(w,b)}{\partial w}" class="mathcode" src="https://images2.imgbox.com/3a/3d/WzjFXUUY_o.png"></p> 
<p style="text-align:center;"><img alt="b\overset{}{\leftarrow} b-\alpha \frac{\partial R(w,b)}{\partial b}" class="mathcode" src="https://images2.imgbox.com/e9/f4/vSumNgRj_o.png"></p> 
<p>其中α为学习率。</p> 
<p>将上面的参数更新过程包装为优化器，首先定义一个优化器基类<code>Optimizer</code>，方便后续所有的优化器调用。在这个基类中，需要初始化优化器的初始学习率<code>init_lr</code>，以及指定优化器需要优化的参数。代码实现如下：</p> 
<pre><code class="language-python">from abc import abstractmethod
# 优化器基类
class Optimizer(object):
    def __init__(self, init_lr, model):
        # 初始化学习率，用于参数更新的计算
        self.init_lr = init_lr
        # 指定优化器需要优化的模型
        self.model = model

    @abstractmethod
    def step(self):
        pass</code></pre> 
<p> 然后实现一个梯度下降法的优化器函数<code>SimpleBatchGD</code>来执行参数更新过程。其中<code>step</code>函数从模型的<code>grads</code>属性取出参数的梯度并更新。代码实现如下：</p> 
<pre><code class="language-python">class SimpleBatchGD(Optimizer):
    def __init__(self, init_lr, model):
        super(SimpleBatchGD, self).__init__(init_lr=init_lr, model=model)

    def step(self):
        # 参数更新
        # 遍历所有参数，按照公式更新参数
        if isinstance(self.model.params, dict):
            for key in self.model.params.keys():
                self.model.params[key] = self.model.params[key] - self.init_lr * self.model.grads[key]</code></pre> 
<h3 id="3.1.5%20%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87">3.1.5 评价指标</h3> 
<p> 在分类任务中，通常使用准确率（Accuracy）作为评价指标。如果模型预测的类别与真实类别一致，则说明模型预测正确。准确率即正确预测的数量与总的预测数量的比值：</p> 
<p style="text-align:center;"><img alt="A=\frac{1}{N}\sum_{n=1}^{N}I(y^{(n)}=\hat{y}^{(n)})" class="mathcode" src="https://images2.imgbox.com/e4/d1/vOHaTXmH_o.png"></p> 
<p>其中I(⋅)是指示函数。代码实现如下： </p> 
<pre><code class="language-python">def accuracy(preds, labels):
    # 判断是二分类任务还是多分类任务，preds.shape[1]=1时为二分类任务，preds.shape[1]&gt;1时为多分类任务
    if preds.shape[1] == 1:
        # 二分类时，判断每个概率值是否大于0.5，当大于0.5时，类别为1，否则类别为0
        preds = torch.as_tensor((preds &gt;= 0.5),dtype=torch.float32)
    else:
        # 多分类时，使用'torch.argmax'计算最大元素索引作为类别
        preds = torch.argmax(preds, dim=1).int()
    return torch.mean((preds == labels).float())


# 假设模型的预测值为[[0.],[1.],[1.],[0.]]，真实类别为[[1.],[1.],[0.],[0.]]，计算准确率
preds = torch.tensor([[0.], [1.], [1.], [0.]])
labels = torch.tensor([[1.], [1.], [0.], [0.]])
print("accuracy is:", accuracy(preds, labels))</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="47" src="https://images2.imgbox.com/a1/4c/wYKICWUy_o.png" width="287"></p> 
<h3 id="3.1.6%20%E5%AE%8C%E5%96%84Runner%E7%B1%BB">3.1.6 完善Runner类</h3> 
<p> 基于RunnerV1，本章的RunnerV2类在训练过程中使用梯度下降法进行网络优化，模型训练过程中计算在训练集和验证集上的损失及评估指标并打印，训练过程中保存最优模型。代码实现如下：</p> 
<pre><code class="language-python"># 用RunnerV2类封装整个训练过程
class RunnerV2(object):
    def __init__(self, model, optimizer, metric, loss_fn):
        self.model = model
        self.optimizer = optimizer
        self.loss_fn = loss_fn
        self.metric = metric
        # 记录训练过程中的评价指标变化情况
        self.train_scores = []
        self.dev_scores = []
        # 记录训练过程中的损失函数变化情况
        self.train_loss = []
        self.dev_loss = []

    def train(self, train_set, dev_set, **kwargs):
        # 传入训练轮数，如果没有传入值则默认为0
        num_epochs = kwargs.get("num_epochs", 0)
        # 传入log打印频率，如果没有传入值则默认为100
        log_epochs = kwargs.get("log_epochs", 100)
        # 传入模型保存路径，如果没有传入值则默认为"best_model.pdparams"
        save_path = kwargs.get("save_path", "best_model.pdparams")
        # 梯度打印函数，如果没有传入则默认为"None"
        print_grads = kwargs.get("print_grads", None)
        # 记录全局最优指标
        best_score = 0
        # 进行num_epochs轮训练
        for epoch in range(num_epochs):
            X, y = train_set
            # 获取模型预测
            logits = self.model(X)
            # 计算交叉熵损失
            trn_loss = self.loss_fn(logits, y).item()
            self.train_loss.append(trn_loss)
            # 计算评价指标
            trn_score = self.metric(logits, y).item()
            self.train_scores.append(trn_score)
            # 计算参数梯度
            self.model.backward(y)
            if print_grads is not None:
                # 打印每一层的梯度
                print_grads(self.model)
            # 更新模型参数
            self.optimizer.step()
            dev_score, dev_loss = self.evaluate(dev_set)
            # 如果当前指标为最优指标，保存该模型
            if dev_score &gt; best_score:
                self.save_model(save_path)
                print(f"best accuracy performence has been updated: {best_score:.5f} --&gt; {dev_score:.5f}")
                best_score = dev_score
            if epoch % log_epochs == 0:
                print(f"[Train] epoch: {epoch}, loss: {trn_loss}, score: {trn_score}")
                print(f"[Dev] epoch: {epoch}, loss: {dev_loss}, score: {dev_score}")

    def evaluate(self, data_set):
        X, y = data_set
        # 计算模型输出
        logits = self.model(X)
        # 计算损失函数
        loss = self.loss_fn(logits, y).item()
        self.dev_loss.append(loss)
        # 计算评价指标
        score = self.metric(logits, y).item()
        self.dev_scores.append(score)
        return score, loss

    def predict(self, X):
        return self.model(X)

    def save_model(self, save_path):
        torch.save(self.model.params, save_path)

    def load_model(self, model_path):
        self.model.params = torch.load(model_path)</code></pre> 
<h3 id="3.1.7%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83">3.1.7 模型训练</h3> 
<p> Logistic回归模型的训练，使用交叉熵损失函数和梯度下降法进行优化。<br> 使用训练集和验证集进行模型训练，共训练 500个epoch，每隔50个epoch打印出训练集上的指标。代码实现如下：</p> 
<pre><code class="language-python"># 固定随机种子，保持每次运行结果一致
torch.manual_seed(102)
input_dim = 2  # 特征维度
lr = 0.1
# 实例化模型
model = model_LR(input_dim=input_dim)
# 指定优化器
optimizer = SimpleBatchGD(init_lr=lr, model=model)
# 指定损失函数
loss_fn = BinaryCrossEntropyLoss()
# 指定评价方式
metric = accuracy
# 实例化RunnerV2类，并传入训练配置
runner = RunnerV2(model, optimizer, metric, loss_fn)
runner.train([X_train, y_train], [X_dev, y_dev], num_epochs=500, log_epochs=50, save_path="best_model.pdparams")
</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="662" src="https://images2.imgbox.com/39/56/cu5Dk7vP_o.png" width="747"></p> 
<p> 可视化观察训练集与验证集的准确率和损失的变化情况。</p> 
<pre><code class="language-python"># 可视化观察训练集与验证集的指标变化情况
def plot(runner,fig_name):
    plt.figure(figsize=(10,5))
    plt.subplot(1,2,1)
    epochs = [i for i in range(len(runner.train_scores))]
    # 绘制训练损失变化曲线
    plt.plot(epochs, runner.train_loss, color='#e4007f', label="Train loss")
    # 绘制评价损失变化曲线
    plt.plot(epochs, runner.dev_loss, color='#f19ec2', linestyle='--', label="Dev loss")
    # 绘制坐标轴和图例
    plt.ylabel("loss", fontsize='large')
    plt.xlabel("epoch", fontsize='large')
    plt.legend(loc='upper right', fontsize='x-large')
    plt.subplot(1,2,2)
    # 绘制训练准确率变化曲线
    plt.plot(epochs, runner.train_scores, color='#e4007f', label="Train accuracy")
    # 绘制评价准确率变化曲线
    plt.plot(epochs, runner.dev_scores, color='#f19ec2', linestyle='--', label="Dev accuracy")
    # 绘制坐标轴和图例
    plt.ylabel("score", fontsize='large')
    plt.xlabel("epoch", fontsize='large')
    plt.legend(loc='lower right', fontsize='x-large')
    plt.tight_layout()
    plt.savefig(fig_name)
    plt.show()


plot(runner, fig_name='linear-acc.pdf')</code></pre> 
<p> 运行结果：</p> 
<p><img alt="" height="608" src="https://images2.imgbox.com/16/40/RwbfL6m1_o.png" width="1200"></p> 
<p> 从输出结果可以看到，在训练集与验证集上，loss得到了收敛，同时准确率指标都达到了较高的水平，训练比较充分。</p> 
<h3 id="3.1.8%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7">3.1.8 模型评价</h3> 
<p> 使用测试集对训练完成后的最终模型进行评价，观察模型在测试集上的准确率和loss数据。</p> 
<p>代码实现如下：</p> 
<pre><code class="language-python">score, loss = runner.evaluate([X_test, y_test])
print("[Test] score/loss: {:.4f}/{:.4f}".format(score, loss))</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="42" src="https://images2.imgbox.com/49/71/czzSGR6b_o.png" width="333"></p> 
<p> 可视化观察拟合的决策边界 Xw+b=0。</p> 
<pre><code class="language-python">def decision_boundary(w, b, x1):
    w1, w2 = w
    x2 = (- w1 * x1 - b) / w2
    return x2


plt.figure(figsize=(5, 5))
# 绘制原始数据
plt.scatter(X[:, 0].tolist(), X[:, 1].tolist(), marker='*', c=y.tolist())

w = model.params['w']
b = model.params['b']
x1 = torch.linspace(-2, 3, 1000)
x2 = decision_boundary(w, b, x1)
# 绘制决策边界
plt.plot(x1.tolist(), x2.tolist(), color="red")
plt.show()</code></pre> 
<p>运行结果：</p> 
<p> <img alt="" height="552" src="https://images2.imgbox.com/ed/06/ZQ2rPR9Q_o.png" width="570"></p> 
<p></p> 
<h2 id="3.2%20%E5%9F%BA%E4%BA%8ESoftmax%E5%9B%9E%E5%BD%92%E7%9A%84%E5%A4%9A%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1">3.2 基于Softmax回归的多分类任务</h2> 
<p>Logistic回归可以有效地解决二分类问题。</p> 
<p>但在分类任务中，还有一类多分类问题，即类别数C大于2 的分类问题。</p> 
<p>Softmax回归就是Logistic回归在多分类问题上的推广。</p> 
<h3 id="3.2.1%20%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E5%BB%BA">3.2.1 数据集构建</h3> 
<p> 数据来自3个不同的簇，每个簇对一个类别。我们采集1000条样本，每个样本包含2个特征。</p> 
<p>数据集的构建函数<code>make_multi</code>的代码实现如下： </p> 
<pre><code class="language-python">import torch
import numpy as np


def make_multiclass_classification(n_samples=100, n_features=2, n_classes=3, shuffle=True, noise=0.1):
    # 计算每个类别的样本数量
    n_samples_per_class = [int(n_samples / n_classes) for k in range(n_classes)]
    for i in range(n_samples - sum(n_samples_per_class)):
        n_samples_per_class[i % n_classes] += 1
    # 将特征和标签初始化为0
    X = torch.zeros([n_samples, n_features])
    y = torch.zeros([n_samples], dtype=torch.int32)
    # 随机生成3个簇中心作为类别中心
    centroids = torch.randperm(2 ** n_features)[:n_classes]
    centroids_bin = np.unpackbits(centroids.numpy().astype('uint8')).reshape((-1, 8))[:, -n_features:]
    centroids = torch.tensor(centroids_bin, dtype=torch.float32)
    # 控制簇中心的分离程度
    centroids = 1.5 * centroids - 1
    # 随机生成特征值
    X[:, :n_features] = torch.randn([n_samples, n_features])

    stop = 0
    # 将每个类的特征值控制在簇中心附近
    for k, centroid in enumerate(centroids):
        start, stop = stop, stop + n_samples_per_class[k]
        # 指定标签值
        y[start:stop] = k % n_classes
        X_k = X[start:stop, :n_features]
        # 控制每个类别特征值的分散程度
        A = 2 * torch.rand([n_features, n_features]) - 1
        X_k[...] = torch.matmul(X_k, A)
        X_k += centroid
        X[start:stop, :n_features] = X_k

    # 如果noise不为None，则给特征加入噪声
    if noise &gt; 0.0:
        # 生成noise掩膜，用来指定给那些样本加入噪声
        noise_mask = torch.rand([n_samples]) &lt; noise
        for i in range(len(noise_mask)):
            if noise_mask[i]:
                # 给加噪声的样本随机赋标签值
                y[i] = torch.randint(n_classes, shape=[1]).astype('int32')
    # 如果shuffle为True，将所有数据打乱
    if shuffle:
        idx = torch.randperm(X.shape[0])
        X = X[idx]
        y = y[idx]

    return X, y</code></pre> 
<p> 随机采集1000个样本，并进行可视化。</p> 
<pre><code class="language-python"># 固定随机种子，保持每次运行结果一致
torch.manual_seed(102)
# 采样1000个样本
n_samples = 1000
X, y = make_multiclass_classification(n_samples=n_samples, n_features=2, n_classes=3, noise=0.2)

# 可视化生产的数据集，不同颜色代表不同类别
plt.figure(figsize=(5,5))
plt.scatter(x=X[:, 0].tolist(), y=X[:, 1].tolist(), marker='*', c=y.tolist())
plt.savefig('linear-dataset-vis2.pdf')
plt.show()</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="547" src="https://images2.imgbox.com/8f/a6/Qak1RH1S_o.png" width="541"></p> 
<p>将实验数据拆分成训练集、验证集和测试集。其中训练集640条、验证集160条、测试集200条。并打印前5个数据的标签。</p> 
<pre><code class="language-python">num_train = 640
num_dev = 160
num_test = 200

X_train, y_train = X[:num_train], y[:num_train]
X_dev, y_dev = X[num_train:num_train + num_dev], y[num_train:num_train + num_dev]
X_test, y_test = X[num_train + num_dev:], y[num_train + num_dev:]

# 打印X_train和y_train的维度
print("X_train shape: ", X_train.shape, "y_train shape: ", y_train.shape)
# 打印前5个数据的标签
print(y_train[:5])</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="58" src="https://images2.imgbox.com/1d/01/sNwbpESQ_o.png" width="717"> 这样，我们就完成了Multi1000数据集的构建。</p> 
<h3 id="3.2.2%20%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA">3.2.2 模型构建</h3> 
<h4 id="3.2.2.1%20Softmax%E5%87%BD%E6%95%B0%C2%A0">3.2.2.1 Softmax函数 </h4> 
<p>Softmax函数可以将多个标量映射为一个概率分布。对于一个K维向量，<img alt="x=[x_{1} ,...,x_{K}]" class="mathcode" src="https://images2.imgbox.com/20/cb/VFRoSEB9_o.png">，Softmax的计算公式为 :</p> 
<p style="text-align:center;"><img alt="softmax(x_{k})=\frac{exp(x_{k})}{ {\textstyle \sum_{i=1}^{K}}exp(x_{i}) }" class="mathcode" src="https://images2.imgbox.com/ad/da/8aYIb1ug_o.png"></p> 
<p>在Softmax函数的计算过程中，要注意<strong>上溢出</strong>和<strong>下溢出</strong>的问题。假设Softmax 函数中所有的<img alt="x_{k}" class="mathcode" src="https://images2.imgbox.com/1c/1a/JR1D3ZBI_o.png">都是相同大小的数值a，理论上，所有的输出都应该为<img alt="\frac{1}{k}" class="mathcode" src="https://images2.imgbox.com/09/d3/az5kgTX2_o.png">。但需要考虑如下两种特殊情况： </p> 
<ul><li>a为一个非常大的负数，此时exp(a) 会发生下溢出现象。计算机在进行数值计算时，当数值过小，会被四舍五入为0。此时，Softmax函数的分母会变为0，导致计算出现问题；</li><li>a为一个非常大的正数，此时会导致exp(a)发生上溢出现象，导致计算出现问题。</li></ul> 
<p>为了解决上溢出和下溢出的问题，在计算Softmax函数时，可以使用<img alt="x_{k}" class="mathcode" src="https://images2.imgbox.com/9a/e0/DE16HOcn_o.png">−max(x)代替<img alt="x_{k}" class="mathcode" src="https://images2.imgbox.com/84/4d/GUdij3HD_o.png">。 此时，通过减去最大值，<img alt="x_{k}" class="mathcode" src="https://images2.imgbox.com/12/11/aKfCT8LQ_o.png">最大为0，避免了上溢出的问题；同时，分母中至少会包含一个值为1的项，从而也避免了下溢出的问题。</p> 
<p>Softmax函数的代码实现如下： </p> 
<pre><code class="language-python">def softmax(X):
    x_max = torch.max(X, dim=1, keepdim=True)
    x_exp = torch.exp(X - x_max.values)
    partition = torch.sum(x_exp, dim=1, keepdim=True)
    return x_exp / partition


X = torch.tensor([[0.1, 0.2, 0.3, 0.4], [1, 2, 3, 4]])
predict = softmax(X)
print(predict)</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="76" src="https://images2.imgbox.com/19/20/OC9wbCqb_o.png" width="430"></p> 
<h4 id="3.2.2.2%20Softmax%E5%9B%9E%E5%BD%92%E7%AE%97%E5%AD%90%C2%A0">3.2.2.2 Softmax回归算子 </h4> 
<p>在Softmax回归中，类别标签y∈{1,2,…,C}。给定一个样本x，使用Softmax回归预测的属于类别c的条件概率为 :</p> 
<p style="text-align:center;"> <img alt="p(y=c|x)=softmax(w_{c}^{T}x+b_{c})" class="mathcode" src="https://images2.imgbox.com/08/b7/TOdnMGIl_o.png"></p> 
<p>其中<img alt="w_{c}" class="mathcode" src="https://images2.imgbox.com/7c/84/wXmZbhNu_o.png">是第 c 类的权重向量，<img alt="b_{c}" class="mathcode" src="https://images2.imgbox.com/34/ad/eAI8FeOY_o.png">是第 cc类的偏置。</p> 
<p>Softmax回归模型其实就是线性函数与Softmax函数的组合。</p> 
<p>将N个样本归为一组进行成批地预测。</p> 
<p style="text-align:center;"> <img alt="\hat{Y}=softmax(XW+b)" class="mathcode" src="https://images2.imgbox.com/82/3c/m4udbG41_o.png"></p> 
<p>其中<img alt="X\in \mathbb{R} ^{N\times D}" class="mathcode" src="https://images2.imgbox.com/41/56/K8Z7teOD_o.png">为N个样本的特征矩阵，<img alt="W=[w_{1} ,......,w_{C} ]" class="mathcode" src="https://images2.imgbox.com/a0/9d/keDAtUpY_o.png">为C个类的权重向量组成的矩阵，<img alt="\hat{Y}\in \mathbb{R}^{C}" class="mathcode" src="https://images2.imgbox.com/1c/79/CCwSK9ot_o.png">为所有类别的预测条件概率组成的矩阵。</p> 
<p>我们根据公式实现Softmax回归算子，代码实现如下：</p> 
<pre><code class="language-python">from nndl import op


class model_SR(op.Op):
    def __init__(self, input_dim, output_dim):
        super(model_SR, self).__init__()
        self.params = {}
        # 将线性层的权重参数全部初始化为0
        self.params['W'] = torch.zeros([input_dim, output_dim])
        # self.params['W'] = paddle.normal(mean=0, std=0.01, shape=[input_dim, output_dim])
        # 将线性层的偏置参数初始化为0
        self.params['b'] = torch.zeros([output_dim])
        self.outputs = None

    def __call__(self, inputs):
        return self.forward(inputs)

    def forward(self, inputs):
        # 线性计算
        score = torch.matmul(inputs, self.params['W']) + self.params['b']
        # Softmax 函数
        self.outputs = softmax(score)
        return self.outputs


# 随机生成1条长度为4的数据
inputs = torch.randn([1, 4])
print('Input is:', inputs)
# 实例化模型，这里令输入长度为4，输出类别数为3
model = model_SR(input_dim=4, output_dim=3)
outputs = model(inputs)
print('Output is:', outputs)</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="87" src="https://images2.imgbox.com/d2/8c/MUyNRClG_o.png" width="573"></p> 
<p>从输出结果可以看出，采用全0初始化后，属于每个类别的条件概率均为<img alt="\frac{1}{C}" class="mathcode" src="https://images2.imgbox.com/db/8e/C1xDDNHa_o.png">。这是因为，不论输入值的大小为多少，线性函数f(x;W,b)的输出值恒为0。此时，再经过Softmax函数的处理，每个类别的条件概率恒等。 </p> 
<p><span style="color:#fe2c24;"><strong>思考题：</strong></span>Logistic函数是激活函数。Softmax函数是激活函数么？谈谈你的看法。 </p> 
<p>Softmax函数是激活函数，用于多分类过程中，它作用在输出层将多个神经元的输出，映射到（0，1）区间，可以看成概率来理解，从而来进行多分类。</p> 
<p><img alt="" height="778" src="https://images2.imgbox.com/02/4f/EQvszDrK_o.png" width="1200"><span title="点击并拖拽以改变尺寸">​</span></p> 
<h3 id="323-损失函数">3.2.3 损失函数</h3> 
<p>Softmax回归同样使用交叉熵损失作为损失函数，并使用梯度下降法对参数进行优化。通常使用C维的one-hot类型向量<img alt="y\in \left \{ 0,1 \right \}^{C}" class="mathcode" src="https://images2.imgbox.com/e6/db/62WOTY1s_o.png"><span title="点击并拖拽以改变尺寸">​</span>来表示多分类任务中的类别标签。对于类别c，其向量表示为： </p> 
<p style="text-align:center;"><img alt="y\in \left \{ 0,1 \right \}^{C}" class="mathcode" src="https://images2.imgbox.com/6b/2c/ZEIhDA4N_o.png"><span title="点击并拖拽以改变尺寸">​</span> <img alt="y=[I(1=c),I(2=c),...,I(C=c)]^{T}" class="mathcode" src="https://images2.imgbox.com/14/05/WM6avqg1_o.png"><span title="点击并拖拽以改变尺寸">​</span></p> 
<p>其中I(⋅)是指示函数，即括号内的输入为“真”，I(⋅)=1；否则，I(⋅)=0。</p> 
<p>给定有N个训练样本的训练集，令<img alt="\hat{y}^{(n)}=softmax(W^{T}x^{(n)}+b)" class="mathcode" src="https://images2.imgbox.com/70/b1/2PY0CglY_o.png"><span title="点击并拖拽以改变尺寸">​</span>为样本x(n)在每个类别的后验概率。多分类问题的交叉熵损失函数定义为：</p> 
<p style="text-align:center;"> <img alt="R(W,b)=-\frac{1}{N}\sum_{n=1}^{N}(y^{(n)})^{T}log\hat{y}^{(n)}=-\frac{1}{N}\sum_{n=1}^{N}\sum_{c=1}^{C}y_{c}^{(n)}log\hat{y}_{c}^{(n)}" class="mathcode" src="https://images2.imgbox.com/33/08/hdXX5LJE_o.png"><span title="点击并拖拽以改变尺寸">​</span></p> 
<p>观察上式，<img alt="y_{c}^{(n)}" class="mathcode" src="https://images2.imgbox.com/cc/08/YLp3Glfn_o.png"><span title="点击并拖拽以改变尺寸">​</span>在c为真实类别时为1，其余都为0。也就是说，交叉熵损失只关心正确类别的预测概率，因此，上式又可以优化为： </p> 
<p style="text-align:center;"> <img alt="R(W,b)=-\frac{1}{N}\sum_{n=1}^{N}log[\hat{y}^{(n)}]_{y^{(n)} }" class="mathcode" src="https://images2.imgbox.com/8b/1f/4w98FNis_o.png"><span title="点击并拖拽以改变尺寸">​</span></p> 
<p>其中<img alt="y^{(n)}" class="mathcode" src="https://images2.imgbox.com/c0/5b/3051D7Nh_o.png"><span title="点击并拖拽以改变尺寸">​</span>是第n个样本的标签。</p> 
<p>因此，多类交叉熵损失函数的代码实现如下：</p> 
<div> 
 <pre><code class="language-python hljs"><span class="hljs-keyword">class</span> <span class="class_ hljs-title">MultiCrossEntropyLoss</span>(op.Op):
    <span class="hljs-keyword">def</span> <span class="function_ hljs-title">__init__</span>(<span class="hljs-params">self</span>):
        self.predicts = <span class="hljs-literal">None</span>
        self.labels = <span class="hljs-literal">None</span>
        self.num = <span class="hljs-literal">None</span>

    <span class="hljs-keyword">def</span> <span class="function_ hljs-title">__call__</span>(<span class="hljs-params">self, predicts, labels</span>):
        <span class="hljs-keyword">return</span> self.forward(predicts, labels)

    <span class="hljs-keyword">def</span> <span class="function_ hljs-title">forward</span>(<span class="hljs-params">self, predicts, labels</span>):
        <span class="hljs-string">"""
        输入：
            - predicts：预测值，shape=[N, 1]，N为样本数量
            - labels：真实标签，shape=[N, 1]
        输出：
            - 损失值：shape=[1]
        """</span>
        self.predicts = predicts
        self.labels = labels
        self.num = self.predicts.shape[<span class="hljs-number">0</span>]
        loss = <span class="hljs-number">0</span>
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, self.num):
            index = self.labels[i]
            loss -= torch.log(self.predicts[i][index])
        <span class="hljs-keyword">return</span> loss / self.num


<span class="hljs-comment"># 假设真实标签为第1类</span>
labels = torch.tensor([<span class="hljs-number">0</span>])
<span class="hljs-comment"># 计算风险函数</span>
mce_loss = MultiCrossEntropyLoss()
<span class="hljs-built_in">print</span>(mce_loss(outputs, labels))</code></pre> 
</div> 
<p>运行结果：</p> 
<p><img alt="" height="45" src="https://images2.imgbox.com/3d/d3/nc8eIupV_o.png" width="152"><span title="点击并拖拽以改变尺寸">​</span></p> 
<h3 id="3.2.4%20%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96">3.2.4 模型优化</h3> 
<h4 id="3.2.4.1%20%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97%C2%A0">3.2.4.1 梯度计算 </h4> 
<p>计算风险函数R(W,b)关于参数W和b的偏导数。在Softmax回归中，计算方法为：</p> 
<p style="text-align:center;"><img alt="\frac{\partial R(W,b)}{\partial W}=-\frac{1}{N}\sum_{n=1}^{N}x^{(n)}(y^{(n)}-\hat{y}^{(n)})^{T}=-\frac{1}{N}X^{T}(y-\hat{y})" class="mathcode" src="https://images2.imgbox.com/b6/af/oM9BNIJG_o.png"><span title="点击并拖拽以改变尺寸">​</span></p> 
<p style="text-align:center;"><img alt="\frac{\partial R(W,b)}{\partial b}=-\frac{1}{N}\sum_{n=1}^{N}(y^{(n)}-\hat{y}^{(n)})^{T}=-\frac{1}{N}1(y-\hat{y})" class="mathcode" src="https://images2.imgbox.com/4a/9f/thzr2bob_o.png"><span title="点击并拖拽以改变尺寸">​</span></p> 
<p> 其中<img alt="X\in \mathbb{R} ^{N\times D}" class="mathcode" src="https://images2.imgbox.com/d0/94/NOXrNqXY_o.png"><span title="点击并拖拽以改变尺寸">​</span>为N个样本组成的矩阵，<img alt="y\in \mathbb{R} ^{N}" class="mathcode" src="https://images2.imgbox.com/b0/5c/hBHGVL1N_o.png"><span title="点击并拖拽以改变尺寸">​</span>为N个样本标签组成的向量，<img alt="\hat{y} \in \mathbb{R} ^{N}" class="mathcode" src="https://images2.imgbox.com/96/6a/h3Sr2xXB_o.png"><span title="点击并拖拽以改变尺寸">​</span>为N个样本的预测标签组成的向量，1为N维的全1向量。</p> 
<p>将上述计算方法定义在模型的<code>backward</code>函数中，代码实现如下：</p> 
<div> 
 <pre><code class="language-python hljs"><span class="hljs-keyword">class</span> <span class="class_ hljs-title">model_SR</span>(op.Op):
    <span class="hljs-keyword">def</span> <span class="function_ hljs-title">__init__</span>(<span class="hljs-params">self, input_dim, output_dim</span>):
        <span class="hljs-built_in">super</span>(model_SR, self).__init__()
        self.params = {}
        <span class="hljs-comment"># 将线性层的权重参数全部初始化为0</span>
        self.params[<span class="hljs-string">'W'</span>] = torch.zeros([input_dim, output_dim])
        <span class="hljs-comment"># 将线性层的偏置参数初始化为0</span>
        self.params[<span class="hljs-string">'b'</span>] = torch.zeros([output_dim])
        <span class="hljs-comment"># 存放参数的梯度</span>
        self.grads = {}
        self.X = <span class="hljs-literal">None</span>
        self.outputs = <span class="hljs-literal">None</span>
        self.output_dim = output_dim

    <span class="hljs-keyword">def</span> <span class="function_ hljs-title">__call__</span>(<span class="hljs-params">self, inputs</span>):
        <span class="hljs-keyword">return</span> self.forward(inputs)

    <span class="hljs-keyword">def</span> <span class="function_ hljs-title">forward</span>(<span class="hljs-params">self, inputs</span>):
        self.X = inputs
        <span class="hljs-comment"># 线性计算</span>
        score = torch.matmul(self.X, self.params[<span class="hljs-string">'W'</span>]) + self.params[<span class="hljs-string">'b'</span>]
        <span class="hljs-comment"># Softmax 函数</span>
        self.outputs = softmax(score)
        <span class="hljs-keyword">return</span> self.outputs

    <span class="hljs-keyword">def</span> <span class="function_ hljs-title">backward</span>(<span class="hljs-params">self, labels</span>):
        <span class="hljs-comment"># 计算偏导数</span>
        N =labels.shape[<span class="hljs-number">0</span>]
        labels = torch.nn.functional.one_hot(labels, self.output_dim)
        self.grads[<span class="hljs-string">'W'</span>] = -<span class="hljs-number">1</span> / N * torch.matmul(self.X.t(), (labels-self.outputs))
        self.grads[<span class="hljs-string">'b'</span>] = -<span class="hljs-number">1</span> / N * torch.matmul(torch.ones([N]), (labels-self.outputs))
        </code></pre> 
</div> 
<h4 id="%C2%A03.2.4.2%20%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0"> 3.2.4.2 参数更新</h4> 
<p> 使用3.1.4.2中实现的梯度下降法进行参数更新 </p> 
<h3 id="3.2.5%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83">3.2.5 模型训练</h3> 
<p> 实例化RunnerV2类，并传入训练配置。使用训练集和验证集进行模型训练，共训练500个epoch。每隔50个epoch打印训练集上的指标。</p> 
<div> 
 <pre><code class="language-python hljs">torch.manual_seed(<span class="hljs-number">102</span>)
input_dim = <span class="hljs-number">2</span>  <span class="hljs-comment"># 特征维度</span>
output_dim = <span class="hljs-number">3</span>  <span class="hljs-comment"># 类别数</span>
lr = <span class="hljs-number">0.1</span>
<span class="hljs-comment"># 实例化模型</span>
model = model_SR(input_dim=input_dim, output_dim=output_dim)
<span class="hljs-comment"># 指定优化器</span>
optimizer = SimpleBatchGD(init_lr=lr, model=model)
<span class="hljs-comment"># 指定损失函数</span>
loss_fn = MultiCrossEntropyLoss()
<span class="hljs-comment"># 指定评价方式</span>
metric = accuracy
<span class="hljs-comment"># 实例化RunnerV2类</span>
runner = RunnerV2(model, optimizer, metric, loss_fn)
<span class="hljs-comment"># 模型训练</span>
runner.train([X_train, y_train], [X_dev, y_dev], num_epochs=<span class="hljs-number">500</span>, log_eopchs=<span class="hljs-number">50</span>, eval_epochs=<span class="hljs-number">1</span>, save_path=<span class="hljs-string">"best_model.pdparams"</span>)

<span class="hljs-comment"># 可视化观察训练集与验证集的准确率变化情况</span>
plot(runner,fig_name=<span class="hljs-string">'linear-acc2.pdf'</span>)</code></pre> 
</div> 
<p>运行结果： </p> 
<p><img alt="" height="737" src="https://images2.imgbox.com/eb/0c/qj1HZ2OB_o.png" width="725"><span title="点击并拖拽以改变尺寸">​</span></p> 
<p><img alt="" height="615" src="https://images2.imgbox.com/cf/46/lYa2ZqIO_o.png" width="1200"><span title="点击并拖拽以改变尺寸">​</span></p> 
<h3 id="3.2.6%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7">3.2.6 模型评价</h3> 
<p> 使用测试集对训练完成后的最终模型进行评价，观察模型在测试集上的准确率。</p> 
<div> 
 <pre><code class="language-python hljs">score, loss = runner.evaluate([X_test, y_test])
<span class="hljs-built_in">print</span>(<span class="hljs-string">"[Test] score/loss: {:.4f}/{:.4f}"</span>.<span class="hljs-built_in">format</span>(score, loss))
</code></pre> 
</div> 
<p>运行结果：</p> 
<p> <img alt="" height="53" src="https://images2.imgbox.com/b5/68/kkl5lLJv_o.png" width="340"><span title="点击并拖拽以改变尺寸">​</span></p> 
<p>可视化观察类别划分结果。</p> 
<div> 
 <pre><code class="language-python hljs"><span class="hljs-comment"># 均匀生成40000个数据点</span>
x1, x2 = torch.meshgrid(torch.linspace(-<span class="hljs-number">3.5</span>, <span class="hljs-number">2</span>, <span class="hljs-number">200</span>), torch.linspace(-<span class="hljs-number">4.5</span>, <span class="hljs-number">3.5</span>, <span class="hljs-number">200</span>),  indexing=<span class="hljs-string">'xy'</span>)
x = torch.stack([torch.flatten(x1), torch.flatten(x2)], dim=<span class="hljs-number">1</span>)
<span class="hljs-comment"># 预测对应类别</span>
y = runner.predict(x)
y = torch.argmax(y, dim=<span class="hljs-number">1</span>)
<span class="hljs-comment"># 绘制类别区域</span>
plt.ylabel(<span class="hljs-string">'x2'</span>)
plt.xlabel(<span class="hljs-string">'x1'</span>)
plt.scatter(x[:, <span class="hljs-number">0</span>].tolist(), x[:, <span class="hljs-number">1</span>].tolist(), c=y.tolist(), cmap=plt.cm.Spectral)

torch.manual_seed(<span class="hljs-number">102</span>)
n_samples = <span class="hljs-number">1000</span>
X, y = make_multiclass_classification(n_samples=n_samples, n_features=<span class="hljs-number">2</span>, n_classes=<span class="hljs-number">3</span>, noise=<span class="hljs-number">0.2</span>)
plt.scatter(X[:, <span class="hljs-number">0</span>].tolist(), X[:, <span class="hljs-number">1</span>].tolist(), marker=<span class="hljs-string">'*'</span>, c=y.tolist())
plt.show()</code></pre> 
</div> 
<p>运行结果：</p> 
<p><img alt="" height="546" src="https://images2.imgbox.com/58/0c/phpKh9z2_o.png" width="721"><span title="点击并拖拽以改变尺寸">​</span></p> 
<p> <span style="color:#fe2c24;">（注：提前停止是在使用梯度下降法进行模型优化时常用的正则化方法。对于某些拟合能力非常强的机器学习算法，当训练轮数较多时，容易发生过拟合现象。为了解决这一问题，通常会在模型优化时，使用验证集上的错误代替期望错误。当验证集上的错误率不在下降时，就停止迭代。）</span></p> 
<p></p> 
<h2 id="3.3%20%E5%AE%9E%E8%B7%B5%EF%BC%9A%E5%9F%BA%E4%BA%8ESoftmax%E5%9B%9E%E5%BD%92%E5%AE%8C%E6%88%90%E9%B8%A2%E5%B0%BE%E8%8A%B1%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1">3.3 实践：基于Softmax回归完成鸢尾花分类任务</h2> 
<p><strong>步骤：</strong>数据处理、模型构建、损失函数定义、优化器构建、模型训练、模型评价和模型预测等，</p> 
<ul><li>数据处理：根据网络接收的数据格式，完成相应的预处理操作，保证模型正常读取；</li><li>模型构建：定义Softmax回归模型类；</li><li>训练配置：训练相关的一些配置，如：优化算法、评价指标等；</li><li>组装Runner类：Runner用于管理模型训练和测试过程；</li><li>模型训练和测试：利用Runner进行模型训练、评价和测试。</li></ul> 
<p><strong>主要配置：</strong></p> 
<ul><li>数据：Iris数据集；</li><li>模型：Softmax回归模型；</li><li>损失函数：交叉熵损失；</li><li>优化器：梯度下降法；</li><li>评价指标：准确率。</li></ul> 
<h3 id="3.3.1%20%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%C2%A0">3.3.1 数据处理 </h3> 
<h4 id="3.3.1.1%20%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D">3.3.1.1 数据集介绍</h4> 
<p> Iris数据集，也称为鸢尾花数据集，包含了3种鸢尾花类别（Setosa、Versicolour、Virginica），每种类别有50个样本，共计150个样本。其中每个样本中包含了4个属性：花萼长度、花萼宽度、花瓣长度以及花瓣宽度，本实验通过鸢尾花这4个属性来判断该样本的类别。</p> 
<p> 鸢尾花属性类别对应预览：</p> 
<table border="1" cellpadding="1" cellspacing="1" style="width:500px;"><tbody><tr><td style="text-align:center;">Id</td><td style="text-align:center;width:99px;">SepalLength</td><td style="text-align:center;width:91px;">SepalWidth</td><td style="text-align:center;width:95px;">PetalLength</td><td style="text-align:center;width:95px;">PetalWidth</td><td style="text-align:center;">Species</td></tr><tr><td style="text-align:center;">1</td><td style="text-align:center;width:99px;">5.1</td><td style="text-align:center;width:91px;">3.5</td><td style="text-align:center;width:95px;">1.4</td><td style="text-align:center;width:95px;">0.2</td><td style="text-align:center;">Iris-setosa</td></tr><tr><td style="text-align:center;">2</td><td style="text-align:center;width:99px;">4.9</td><td style="text-align:center;width:91px;">3.0</td><td style="text-align:center;width:95px;">1.4</td><td style="text-align:center;width:95px;">0.2</td><td style="text-align:center;">Iris-setosa</td></tr><tr><td style="text-align:center;">3</td><td style="text-align:center;width:99px;">4.7</td><td style="text-align:center;width:91px;">3.2</td><td style="text-align:center;width:95px;">1.3</td><td style="text-align:center;width:95px;">0.2</td><td style="text-align:center;">Iris-setosa</td></tr><tr><td style="text-align:center;">4</td><td style="text-align:center;width:99px;">4.6</td><td style="text-align:center;width:91px;">3.1</td><td style="text-align:center;width:95px;">1.5</td><td style="text-align:center;width:95px;">0.2</td><td style="text-align:center;">Iris-setosa</td></tr><tr><td style="text-align:center;">...</td><td style="text-align:center;width:99px;">...</td><td style="text-align:center;width:91px;">...</td><td style="text-align:center;width:95px;">...</td><td style="text-align:center;width:95px;">...</td><td style="text-align:center;">...</td></tr></tbody></table> 
<h4 id="3.3.1.2%20%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%C2%A0">3.3.1.2 数据清洗 </h4> 
<ul><li><strong>缺失值分析:</strong>对数据集中的缺失值或异常值等情况进行分析和处理，保证数据可以被模型正常读取。代码实现如下：</li></ul> 
<div> 
 <pre><code class="language-python hljs"><span class="hljs-keyword">import</span> pandas
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris

iris_features = np.array(load_iris().data, dtype=np.float32)
iris_labels = np.array(load_iris().target, dtype=np.int32)
<span class="hljs-built_in">print</span>(pandas.isna(iris_features).<span class="hljs-built_in">sum</span>())
<span class="hljs-built_in">print</span>(pandas.isna(iris_labels).<span class="hljs-built_in">sum</span>())</code></pre> 
</div> 
<p>运行结果：<br>  <img alt="" height="71" src="https://images2.imgbox.com/51/07/XP9fpluC_o.png" width="152"><span title="点击并拖拽以改变尺寸">​</span></p> 
<p>输出结果为0，由此可知鸢尾花数据集中不存在缺失值的情况。 </p> 
<ul><li><strong>异常值处理：</strong>通过箱线图直观的显示数据分布，并观测数据中的异常值。</li></ul> 
<div> 
 <pre><code class="language-python hljs"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt

<span class="hljs-keyword">def</span> <span class="function_ hljs-title">boxplot</span>(<span class="hljs-params">features</span>):
    feature_names = [<span class="hljs-string">'sepal_length'</span>, <span class="hljs-string">'sepal_width'</span>, <span class="hljs-string">'petal_length'</span>, <span class="hljs-string">'petal_width'</span>]
    plt.figure(figsize=(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>), dpi=<span class="hljs-number">200</span>)
    plt.subplots_adjust(wspace=<span class="hljs-number">0.6</span>)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):  <span class="hljs-comment"># 每个特征画一个箱线图</span>
        plt.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, i+<span class="hljs-number">1</span>)
        <span class="hljs-comment"># 画箱线图</span>
        plt.boxplot(features[:, i],
                    showmeans=<span class="hljs-literal">True</span>,
                    whiskerprops={<!-- --><span class="hljs-string">"color"</span>: <span class="hljs-string">"#E20079"</span>, <span class="hljs-string">"linewidth"</span>: <span class="hljs-number">0.4</span>, <span class="hljs-string">'linestyle'</span>: <span class="hljs-string">"--"</span>},
                    flierprops={<!-- --><span class="hljs-string">"markersize"</span>: <span class="hljs-number">0.4</span>},
                    meanprops={<!-- --><span class="hljs-string">"markersize"</span>: <span class="hljs-number">1</span>})
        plt.title(feature_names[i], fontdict={<!-- --><span class="hljs-string">"size"</span>: <span class="hljs-number">5</span>}, pad=<span class="hljs-number">2</span>)
        <span class="hljs-comment"># y方向刻度</span>
        plt.yticks(fontsize=<span class="hljs-number">4</span>, rotation=<span class="hljs-number">90</span>)
        plt.tick_params(pad=<span class="hljs-number">0.5</span>)
        <span class="hljs-comment"># x方向刻度</span>
        plt.xticks([])
    plt.savefig(<span class="hljs-string">'ml-vis.pdf'</span>)
    plt.show()


boxplot(iris_features)</code></pre> 
</div> 
<p>运行结果：</p> 
<p><img alt="" height="845" src="https://images2.imgbox.com/4c/af/Cvr8zBKm_o.png" width="1085"><span title="点击并拖拽以改变尺寸">​</span></p> 
<p> 从输出结果看，数据中基本不存在异常值，不需要进行异常值处理。</p> 
<h4 id="3.3.1.3%20%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%C2%A0">3.3.1.3 数据读取 </h4> 
<p>本实验中将数据集划分为了三个部分：</p> 
<ul><li>训练集：用于确定模型参数；</li><li>验证集：与训练集独立的样本集合，用于使用提前停止策略选择最优模型；</li><li>测试集：用于估计应用效果。</li></ul> 
<p>在本实验中，将80%的数据用于模型训练，10%的数据用于模型验证，10%的数据用于模型测试。代码实现如下：</p> 
<div> 
 <pre><code class="language-python hljs"><span class="hljs-keyword">import</span> torch
<span class="hljs-comment"># 加载数据集</span>
<span class="hljs-keyword">def</span> <span class="function_ hljs-title">load_data</span>(<span class="hljs-params">shuffle=<span class="hljs-literal">True</span></span>):
    <span class="hljs-comment"># 加载原始数据</span>
    X = np.array(load_iris().data, dtype=np.float32)
    y = np.array(load_iris().target, dtype=np.int32)

    X = torch.tensor(X)
    y = torch.tensor(y)

    <span class="hljs-comment"># 数据归一化</span>
    X_min = torch.<span class="hljs-built_in">min</span>(X, dim=<span class="hljs-number">0</span>)
    X_max = torch.<span class="hljs-built_in">max</span>(X, dim=<span class="hljs-number">0</span>)
    X = (X-X_min.values) / (X_max.values-X_min.values)

    <span class="hljs-comment"># 如果shuffle为True，随机打乱数据</span>
    <span class="hljs-keyword">if</span> shuffle:
        idx = torch.randperm(X.shape[<span class="hljs-number">0</span>])
        X = X[idx]
        y = y[idx]
    <span class="hljs-keyword">return</span> X, y


torch.manual_seed(<span class="hljs-number">102</span>)
num_train = <span class="hljs-number">120</span>
num_dev = <span class="hljs-number">15</span>
num_test = <span class="hljs-number">15</span>
X, y = load_data(shuffle=<span class="hljs-literal">True</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"X shape: "</span>, X.shape, <span class="hljs-string">"y shape: "</span>, y.shape)
X_train, y_train = X[:num_train], y[:num_train]
X_dev, y_dev = X[num_train:num_train + num_dev], y[num_train:num_train + num_dev]
X_test, y_test = X[num_train + num_dev:], y[num_train + num_dev:]
<span class="hljs-comment"># 打印X_train和y_train的维度</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">"X_train shape: "</span>, X_train.shape, <span class="hljs-string">"y_train shape: "</span>, y_train.shape)
<span class="hljs-comment"># 打印前5个数据的标签</span>
<span class="hljs-built_in">print</span>(y_train[:<span class="hljs-number">5</span>])</code></pre> 
</div> 
<p>运行结果：</p> 
<h3 id="%E2%80%8B%C2%A03.3.2%20%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA"><img alt="" height="92" src="https://images2.imgbox.com/a4/7a/8RUarFBQ_o.png" width="722"><span title="点击并拖拽以改变尺寸">​</span> 3.3.2 模型构建</h3> 
<p> 使用Softmax回归模型进行鸢尾花分类实验，将模型的输入维度定义为4，输出维度定义为3。代码实现如下：</p> 
<div> 
 <pre><code class="language-python hljs"><span class="hljs-keyword">import</span> op,  metric, opitimizer, RunnerV2

<span class="hljs-comment"># 学习率</span>
lr = <span class="hljs-number">0.2</span>
<span class="hljs-comment"># 梯度下降法</span>
optimizer = opitimizer.SimpleBatchGD(init_lr=lr, model=model)
<span class="hljs-comment"># 交叉熵损失</span>
loss_fn = op.MultiCrossEntropyLoss()
<span class="hljs-comment"># 准确率</span>
metric2 = metric.accuracy
<span class="hljs-comment"># 实例化RunnerV2</span>
runner = RunnerV2.RunnerV2(model, optimizer, metric2, loss_fn)
<span class="hljs-comment"># 启动训练</span>
runner.train([X_train, y_train], [X_dev, y_dev], num_epochs=<span class="hljs-number">200</span>, log_epochs=<span class="hljs-number">10</span>, save_path=<span class="hljs-string">"best_model.pdparams"</span>)
</code></pre> 
</div> 
<p>运行结果：</p> 
<div> 
 <pre><code class="hljs language-python">best accuracy performence has been updated: 0.00000 --&gt; 0.46667
[Train] epoch: 0, loss: 1.09861159324646, score: 0.375
[Dev] epoch: 0, loss: 1.089357614517212, score: 0.46666666865348816
[Train] epoch: 10, loss: 0.9777260422706604, score: 0.699999988079071
[Dev] epoch: 10, loss: 1.023618221282959, score: 0.46666666865348816
[Train] epoch: 20, loss: 0.8894370794296265, score: 0.699999988079071
[Dev] epoch: 20, loss: 0.9739664793014526, score: 0.46666666865348816
[Train] epoch: 30, loss: 0.8196598887443542, score: 0.699999988079071
[Dev] epoch: 30, loss: 0.9317176938056946, score: 0.46666666865348816
[Train] epoch: 40, loss: 0.7635203003883362, score: 0.699999988079071
[Dev] epoch: 40, loss: 0.8957117199897766, score: 0.46666666865348816
[Train] epoch: 50, loss: 0.7176517248153687, score: 0.7250000238418579
[Dev] epoch: 50, loss: 0.8649960160255432, score: 0.46666666865348816
[Train] epoch: 60, loss: 0.679577648639679, score: 0.7416666746139526
[Dev] epoch: 60, loss: 0.8386644721031189, score: 0.46666666865348816
[Train] epoch: 70, loss: 0.6474865078926086, score: 0.7583333253860474
[Dev] epoch: 70, loss: 0.8159360289573669, score: 0.46666666865348816
[Train] epoch: 80, loss: 0.6200525760650635, score: 0.7666666507720947
[Dev] epoch: 80, loss: 0.7961668372154236, score: 0.46666666865348816
[Train] epoch: 90, loss: 0.5962967276573181, score: 0.7833333611488342
[Dev] epoch: 90, loss: 0.7788369655609131, score: 0.46666666865348816
[Train] epoch: 100, loss: 0.5754876732826233, score: 0.8166666626930237
[Dev] epoch: 100, loss: 0.7635290026664734, score: 0.46666666865348816
best accuracy performence has been updated: 0.46667 --&gt; 0.53333
[Train] epoch: 110, loss: 0.5570722818374634, score: 0.824999988079071
[Dev] epoch: 110, loss: 0.7499087452888489, score: 0.5333333611488342
best accuracy performence has been updated: 0.53333 --&gt; 0.60000
[Train] epoch: 120, loss: 0.5406263470649719, score: 0.824999988079071
[Dev] epoch: 120, loss: 0.7377070188522339, score: 0.6000000238418579
[Train] epoch: 130, loss: 0.525819718837738, score: 0.8500000238418579
[Dev] epoch: 130, loss: 0.726706862449646, score: 0.6000000238418579
[Train] epoch: 140, loss: 0.5123931169509888, score: 0.8583333492279053
[Dev] epoch: 140, loss: 0.7167317271232605, score: 0.6000000238418579
[Train] epoch: 150, loss: 0.5001395344734192, score: 0.875
[Dev] epoch: 150, loss: 0.7076371312141418, score: 0.6000000238418579
best accuracy performence has been updated: 0.60000 --&gt; 0.66667
[Train] epoch: 160, loss: 0.48889240622520447, score: 0.875
[Dev] epoch: 160, loss: 0.6993042826652527, score: 0.6666666865348816
[Train] epoch: 170, loss: 0.4785163998603821, score: 0.875
[Dev] epoch: 170, loss: 0.6916342973709106, score: 0.6666666865348816
[Train] epoch: 180, loss: 0.46889930963516235, score: 0.875
[Dev] epoch: 180, loss: 0.6845448017120361, score: 0.6000000238418579
[Train] epoch: 190, loss: 0.45994898676872253, score: 0.875
[Dev] epoch: 190, loss: 0.6779664158821106, score: 0.6000000238418579
</code></pre> 
</div> 
<p><img alt="" height="616" src="https://images2.imgbox.com/b8/df/DaYkq6Sa_o.png" width="1200"></p> 
<h3 id="3.3.4%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%C2%A0">3.3.4 模型评价 </h3> 
<p>使用测试数据对在训练过程中保存的最佳模型进行评价，观察模型在测试集上的准确率情况。代码实现如下：</p> 
<div> 
 <pre><code class="language-python hljs"><span class="hljs-comment"># 加载最优模型</span>
runner.load_model(<span class="hljs-string">'best_model.pdparams'</span>)
<span class="hljs-comment"># 模型评价</span>
score, loss = runner.evaluate([X_test, y_test])
<span class="hljs-built_in">print</span>(<span class="hljs-string">"[Test] score/loss: {:.4f}/{:.4f}"</span>.<span class="hljs-built_in">format</span>(score, loss))
</code></pre> 
</div> 
<p>运行结果：</p> 
<p><img alt="" height="45" src="https://images2.imgbox.com/3b/ad/vZVgk9oN_o.png" width="340"></p> 
<h3 id="3.3.5%20%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%C2%A0">3.3.5 模型预测 </h3> 
<p>使用保存好的模型，对测试集中的数据进行模型预测，并取出1条数据观察模型效果。代码实现如下：</p> 
<div> 
 <pre><code class="language-python hljs"><span class="hljs-comment"># 预测测试集数据</span>
logits = runner.predict(X_test)
<span class="hljs-comment"># 观察其中一条样本的预测结果</span>
pred = torch.argmax(logits[<span class="hljs-number">0</span>]).numpy()
<span class="hljs-built_in">print</span>(<span class="hljs-string">"pred:"</span>,pred)
<span class="hljs-comment"># 获取该样本概率最大的类别</span>
label = y_test[<span class="hljs-number">0</span>].numpy()
<span class="hljs-built_in">print</span>(<span class="hljs-string">"label:"</span>,label)
<span class="hljs-comment"># 输出真实类别与预测类别</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">"The true category is {0} and the predicted category is {1}"</span>.<span class="hljs-built_in">format</span>(label, pred))
</code></pre> 
</div> 
<p>运行结果：</p> 
<p><img alt="" height="45" src="https://images2.imgbox.com/6b/bd/jfc8TaJb_o.png" width="567"></p> 
<p> </p> 
<h2 id="3.4%20%E5%B0%8F%E7%BB%93%C2%A0">3.4 小结 </h2> 
<p>本节实现了Logistic回归和Softmax回归两种基本的线性分类模型。</p> 
<p></p> 
<h2 id="%C2%A03.5%20%E5%AE%9E%E9%AA%8C%E6%8B%93%E5%B1%95"> 3.5 实验拓展</h2> 
<p><strong><span style="color:#fe2c24;">习题1：</span></strong>尝试调整学习率和训练轮数等超参数，观察是否能够得到更高的精度；</p> 
<p><img alt="" height="51" src="https://images2.imgbox.com/8b/87/dXrr0fJb_o.png" width="333"></p> 
<p> 调整学习率和训练轮数等超参数，可以得到更高的精度</p> 
<p></p> 
<h2 id="%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99%C2%A0">参考资料 </h2> 
<p><a href="https://www.elecfans.com/d/1525848.html" rel="nofollow" title="详解十种激活函数的优缺点-电子发烧友网">详解十种激活函数的优缺点-电子发烧友网</a> </p> 
<p> <a href="http://zh.d2l.ai/chapter_linear-networks/linear-regression.html" rel="nofollow" title="3.1. 线性回归 — 动手学深度学习 2.0.0-beta1 documentation">3.1. 线性回归 — 动手学深度学习 2.0.0-beta1 documentation</a></p> 
<p> <a href="https://blog.csdn.net/asuro007/article/details/88812534" title="logistic回归的详细概述_asuro007的博客-CSDN博客_logistic回归描述">logistic回归的详细概述_asuro007的博客-CSDN博客_logistic回归描述</a></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b614498b8261ff3d8ce3ae190ff64c75/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">栈和队列相关操作及代码</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0a635b00e1e233906b7c25c2f0b88c9d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">RK3588通过IO命令操作寄存器的方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>