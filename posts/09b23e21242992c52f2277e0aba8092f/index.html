<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>浅谈卷积神经网络 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="浅谈卷积神经网络" />
<meta property="og:description" content="一、 卷积层、激活函数层、池化层、全连接层
1、 卷积层：
一般的，标准数码相机的图像将有三个通道——红、绿、蓝——我们可以把它们想象成三个相互叠加的2d矩阵(每种颜色一个)，每个像素值在0到255之间（颜色越亮数值越大）。卷积在卷积网络中的主要目的是从输入图像中提取特征。卷积通过使用输入数据的小方块学习图像特征来保持像素之间的空间关系。
下面仅以0，1的二维矩阵进行举例（实际过程中为0-255）。
三通道中的其中一个通道，其二维矩阵例举如上。
所用的卷积核（谨以此为例）。
一个卷积核在此通道上进行卷积的结果
这里有一个图片， 我们可以执行诸如边缘检测、锐化和模糊等操作，只需在卷积操作之前改变滤波器（卷积核）矩阵的数值——这意味着不同的滤波器可以从图像中检测出不同的特征，例如边缘、曲线等。不同滤波器的效果如下从上到下分别为（单位矩阵，边缘检测、锐化、标准化模糊、高斯滤波）：
下面的动画为卷积层的工作过程：
滤光片(带红色轮廓)滑过输入图像(卷积运算)以产生特征映射。另一个过滤器(绿色轮廓)的卷积，在相同的图像上给出一个不同的特征图，如图所示。卷积操作捕获了原始图像中的局部依赖性。还要注意这两个不同的过滤器是如何从相同的原始图像生成不同的特征映射的。，图像和上面的两个过滤器只是我们上面说明的的数字矩阵。
在运用过程中，CNN在训练过程中自己学习这些过滤器的值(虽然我们在训练过程之前还需要指定一些参数，比如过滤器的数量，过滤器的大小，网络的架构等)。我们拥有的过滤器越多，提取的图像特征就越多，我们的网络在识别不可见图像中的模式方面也就变得越好。
卷积层相关的参数：
1、 depth: depth对应于我们用于卷积操作的过滤器的数量。在下图所示的网络中，我们使用三个不同的过滤器对原始船图像进行卷积，从而产生三个不同的特征映射。这三个特征映射想象成堆叠的2d矩阵，所以，特征映射的“深度”应该是3。
2、 stride:stride是我们在输入矩阵上滑动过滤器矩阵的像素数。当stride为1时，我们每次移动滤镜一个像素。当步幅为2时，当我们滑动它们时，过滤器一次跳跃2个像素。更大的步幅会产生更小的特征图。相关的计算公式在前几天的caffe文献中有所说明。
3、 Zero-padding:零填充:有时候，用0填充输入矩阵是很方便的，这样我们可以对输入图像矩阵的边界元素应用过滤器。零填充的一个很好的特性是，它允许我们控制特征映射的大小。添加零填充也称为宽卷积，不使用零填充则为窄卷积。
综上：卷积层的主要作用就是对图像的特征增强以及提取，通过不同的卷积核会提取出不同的特征，用作后期学习的重要数据。同时通过调整卷积的步长也可以调整我们的感受野。
二、 ReLu数层
在每个卷积操作之后都会使用激活函数来额外操作。这里以ReLu举例，ReLU为整流线性单元，是一种非线性操作。其输出如下所示:
ReLU是一个基于元素的操作(应用于每个像素)，它将feature map中的所有负像素值替换为0。卷积是一个线性操作——对应位置上元素的矩阵乘法和加法操作,但是我们在学习时大多数的真实数据是非线性的，所以我们考虑通过引入一个非线性函数如ReLU，ReLU ReLU的目的是我们在学习之前引入非线性这一概念。
从下图可以清楚地理解ReLU操作。它显示了将ReLU操作应用下图获得的一个特性映射。输出特征映射在这里也被称为“矫正”特征映射。也可以使用其他非线性函数，如tanh或sigmoid，来代替ReLU，但是ReLU在大多数情况下性能更好。
三、 池化层
空间池化(也称为子采样或下采样)降低了每个特征图的维数，但保留了最重要的信息。空间池化可以有不同的类型:Max、Average、Sum等。
在最大池化的情况下，我们定义一个空间邻域(例如，一个2×2的窗口)，并从该窗口内的修正特征映射中取最大的元素。我们也可以取平均值(平均池)或该窗口中所有元素的总和，而不是取最大的元素。在实践中，最大池化已经被证明可以更好地工作。
下图是一个使用2×2窗口对经过卷积&#43; ReLU操作得到的修正后的Feature map进行最大池化操作的例子。
我们将2×2的窗口滑动2个单元格(也称为“步幅”)，并在每个区域中取最大值。如上图所示，这减少了我们的feature map的维数。
在下图所示的网络中，池操作分别应用于每个特性映射(注意，由于这个原因，我们从三个输入映射得到三个输出映射)。
下图显示了池化对我们在上图中的ReLU操作后接收到的修正后的特征映射的影响。
池化的作用是逐步减少输入表达的空间大小。具体如下：
使输入表达(特性维度)更小，更易于管理
减少了网络中参数和计算的数量，因此控制过拟合
使网络不受输入图像中小的变换、变形和平移的影响(输入中小的失真不会改变池的输出-因为我们取一个局部邻域的最大值/平均值)。
帮助我们得到图像的比例几乎不变的表达(准确的术语是“等变的”)。无论检测图像中的对象位于哪里我们都可以检测到。
四、 全连接层
全连接层是一个传统的多层感知器，在输出层使用softmax激活函数（其他分类器如SVM也可以使用）。“全连接”意味着上一层的每个神经元都与下一层的每个神经元相连。
卷积层和池化层的输出表示输入图像的高级特征。全连接层的目的是利用这些特征根据训练数据集将输入图像分类成不同的类。例如，我们开始执行的图像分类任务有四种可能的输出，如
卷积层和池化层的输出表示输入图像的高级特征。全连接层的目的是利用这些特征将训练数据集将输入图像分类成不同的类。例如，我们开始执行的图像分类任务有四种可能的输出，如下图。
除了分类之外，添加一个完全连接的层也是学习这些特征的非线性组合的一种廉价方法。来自卷积层和池化层的大多数特性可能对分类任务很好，但这些特性的组合可能更好。
全连通层输出概率之和为1。这是通过在全连接层的输出层使用Softmax作为激活函数来保证的。Softmax函数取一个由任意实值分数组成的向量，并将其压缩为一个由0到1之间的和为1的值组成的向量。
到目前为止，我们已经看到了卷积、激活函数层和池化是如何工作的。重要的是要理解这些层次是任何CNN的基本构建块。如下图所示，我们有两组卷积、ReLU和Pooling层-第2个卷积层使用6个过滤器对第1个Pooling层的输出进行卷积，总共产生6个feature map。然后将ReLU分别应用于所有这六个特性映射。然后，我们对这六个修正后的特征映射分别执行最大池操作。
这些层一起从图像中提取有用的特征，在我们的网络中引入非线性，降低特征维数，同时使特征在缩放和转换时具有一定的等变性。
如上所述，卷积&#43;池化层作为输入图像的特征提取器，而全连接层作为分类器。、
五、总结
卷积网络的整体训练过程总结如下:
Step1:用随机值初始化所有的过滤器和参数/权重
Step2:网络以训练图像为输入，进行前向传播步骤(卷积、ReLU、池化操作以及全连通层的前向传播)，求出每个类的输出概率。
假设上面的船图像的输出概率是[0.2,0.4,0.1,0.3]
由于第一个训练示例的权重是随机分配的，因此输出概率也是随机的。
Step3:计算输出层的总误差(所有4个类的总和)
总误差=∑½(目标概率-输出概率)²
Step4:使用Backpropagation计算误差相对于网络中所有权值的梯度，使用梯度下降更新所有滤波器值/权值和参数值，使输出误差最小化。权重按它们对总误差的贡献比例进行调整。
当再次输入相同的图像时，现在的输出概率可能是[0.1,0.1,0.7,0.1]，这更接近于目标向量[0,0,1,0]。
这意味着网络已经学会通过调整其权值/滤波器来正确分类这一特定图像，从而减少输出误差。
过滤器数量、过滤器大小、网络架构等参数在第1步之前都已经固定，并且在训练过程中不会改变，只更新过滤器矩阵的值和连接权值。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/09b23e21242992c52f2277e0aba8092f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-01-23T19:29:44+08:00" />
<meta property="article:modified_time" content="2021-01-23T19:29:44+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">浅谈卷积神经网络</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>一、 卷积层、激活函数层、池化层、全连接层<br> 1、 卷积层：<br> 一般的，标准数码相机的图像将有三个通道——红、绿、蓝——我们可以把它们想象成三个相互叠加的2d矩阵(每种颜色一个)，每个像素值在0到255之间（颜色越亮数值越大）。卷积在卷积网络中的主要目的是从输入图像中提取特征。卷积通过使用输入数据的小方块学习图像特征来保持像素之间的空间关系。<br> 下面仅以0，1的二维矩阵进行举例（实际过程中为0-255）。<br> <img src="https://images2.imgbox.com/8e/ad/AeJeBuBV_o.png" alt="在这里插入图片描述"></p> 
<p>三通道中的其中一个通道，其二维矩阵例举如上。<br> <img src="https://images2.imgbox.com/f2/f8/fTVG3bFO_o.png" alt="在这里插入图片描述"></p> 
<p>所用的卷积核（谨以此为例）。</p> 
<p>一个卷积核在此通道上进行卷积的结果<br> <img src="https://images2.imgbox.com/12/dd/MksRYvON_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/2d/61/EMccxDKD_o.png" alt="在这里插入图片描述"></p> 
<p>这里有一个图片， 我们可以执行诸如边缘检测、锐化和模糊等操作，只需在卷积操作之前改变滤波器（卷积核）矩阵的数值——这意味着不同的滤波器可以从图像中检测出不同的特征，例如边缘、曲线等。不同滤波器的效果如下从上到下分别为（单位矩阵，边缘检测、锐化、标准化模糊、高斯滤波）：<br> <img src="https://images2.imgbox.com/6d/78/OEK3iZah_o.png" alt="在这里插入图片描述"></p> 
<p>下面的动画为卷积层的工作过程：<img src="https://images2.imgbox.com/2d/27/SltV5olS_o.png" alt="在这里插入图片描述"></p> 
<p>滤光片(带红色轮廓)滑过输入图像(卷积运算)以产生特征映射。另一个过滤器(绿色轮廓)的卷积，在相同的图像上给出一个不同的特征图，如图所示。卷积操作捕获了原始图像中的局部依赖性。还要注意这两个不同的过滤器是如何从相同的原始图像生成不同的特征映射的。，图像和上面的两个过滤器只是我们上面说明的的数字矩阵。<br> 在运用过程中，CNN在训练过程中自己学习这些过滤器的值(虽然我们在训练过程之前还需要指定一些参数，比如过滤器的数量，过滤器的大小，网络的架构等)。我们拥有的过滤器越多，提取的图像特征就越多，我们的网络在识别不可见图像中的模式方面也就变得越好。<br> 卷积层相关的参数：<br> 1、 depth: depth对应于我们用于卷积操作的过滤器的数量。在下图所示的网络中，我们使用三个不同的过滤器对原始船图像进行卷积，从而产生三个不同的特征映射。这三个特征映射想象成堆叠的2d矩阵，所以，特征映射的“深度”应该是3。<br> <img src="https://images2.imgbox.com/10/96/ldL2tKyI_o.png" alt="在这里插入图片描述"></p> 
<p>2、 stride:stride是我们在输入矩阵上滑动过滤器矩阵的像素数。当stride为1时，我们每次移动滤镜一个像素。当步幅为2时，当我们滑动它们时，过滤器一次跳跃2个像素。更大的步幅会产生更小的特征图。相关的计算公式在前几天的caffe文献中有所说明。<br> 3、 Zero-padding:零填充:有时候，用0填充输入矩阵是很方便的，这样我们可以对输入图像矩阵的边界元素应用过滤器。零填充的一个很好的特性是，它允许我们控制特征映射的大小。添加零填充也称为宽卷积，不使用零填充则为窄卷积。<br> 综上：卷积层的主要作用就是对图像的特征增强以及提取，通过不同的卷积核会提取出不同的特征，用作后期学习的重要数据。同时通过调整卷积的步长也可以调整我们的感受野。<br> 二、 ReLu数层<br> 在每个卷积操作之后都会使用激活函数来额外操作。这里以ReLu举例，ReLU为整流线性单元，是一种非线性操作。其输出如下所示:<img src="https://images2.imgbox.com/4d/34/N5WxO3IG_o.png" alt="在这里插入图片描述"></p> 
<p>ReLU是一个基于元素的操作(应用于每个像素)，它将feature map中的所有负像素值替换为0。卷积是一个线性操作——对应位置上元素的矩阵乘法和加法操作,但是我们在学习时大多数的真实数据是非线性的，所以我们考虑通过引入一个非线性函数如ReLU，ReLU ReLU的目的是我们在学习之前引入非线性这一概念。<br> 从下图可以清楚地理解ReLU操作。它显示了将ReLU操作应用下图获得的一个特性映射。输出特征映射在这里也被称为“矫正”特征映射。也可以使用其他非线性函数，如tanh或sigmoid，来代替ReLU，但是ReLU在大多数情况下性能更好。<br> <img src="https://images2.imgbox.com/70/93/rGVk17fJ_o.png" alt="在这里插入图片描述"></p> 
<p>三、 池化层<br> 空间池化(也称为子采样或下采样)降低了每个特征图的维数，但保留了最重要的信息。空间池化可以有不同的类型:Max、Average、Sum等。<br> 在最大池化的情况下，我们定义一个空间邻域(例如，一个2×2的窗口)，并从该窗口内的修正特征映射中取最大的元素。我们也可以取平均值(平均池)或该窗口中所有元素的总和，而不是取最大的元素。在实践中，最大池化已经被证明可以更好地工作。<br> 下图是一个使用2×2窗口对经过卷积+ ReLU操作得到的修正后的Feature map进行最大池化操作的例子。<br> <img src="https://images2.imgbox.com/83/47/oSQ1I8e5_o.png" alt="在这里插入图片描述"></p> 
<p>我们将2×2的窗口滑动2个单元格(也称为“步幅”)，并在每个区域中取最大值。如上图所示，这减少了我们的feature map的维数。<br> 在下图所示的网络中，池操作分别应用于每个特性映射(注意，由于这个原因，我们从三个输入映射得到三个输出映射)。<br> <img src="https://images2.imgbox.com/34/64/j75MvYFh_o.png" alt="在这里插入图片描述"></p> 
<p>下图显示了池化对我们在上图中的ReLU操作后接收到的修正后的特征映射的影响。<br> <img src="https://images2.imgbox.com/9e/4b/iAaDdAfr_o.png" alt="在这里插入图片描述"></p> 
<p>池化的作用是逐步减少输入表达的空间大小。具体如下：<br> 使输入表达(特性维度)更小，更易于管理<br> 减少了网络中参数和计算的数量，因此控制过拟合<br> 使网络不受输入图像中小的变换、变形和平移的影响(输入中小的失真不会改变池的输出-因为我们取一个局部邻域的最大值/平均值)。<br> 帮助我们得到图像的比例几乎不变的表达(准确的术语是“等变的”)。无论检测图像中的对象位于哪里我们都可以检测到。<br> 四、 全连接层<br> 全连接层是一个传统的多层感知器，在输出层使用softmax激活函数（其他分类器如SVM也可以使用）。“全连接”意味着上一层的每个神经元都与下一层的每个神经元相连。<br> 卷积层和池化层的输出表示输入图像的高级特征。全连接层的目的是利用这些特征根据训练数据集将输入图像分类成不同的类。例如，我们开始执行的图像分类任务有四种可能的输出，如<br> 卷积层和池化层的输出表示输入图像的高级特征。全连接层的目的是利用这些特征将训练数据集将输入图像分类成不同的类。例如，我们开始执行的图像分类任务有四种可能的输出，如下图。<img src="https://images2.imgbox.com/ea/73/J6LcNGV7_o.png" alt="在这里插入图片描述"></p> 
<p>除了分类之外，添加一个完全连接的层也是学习这些特征的非线性组合的一种廉价方法。来自卷积层和池化层的大多数特性可能对分类任务很好，但这些特性的组合可能更好。<br> 全连通层输出概率之和为1。这是通过在全连接层的输出层使用Softmax作为激活函数来保证的。Softmax函数取一个由任意实值分数组成的向量，并将其压缩为一个由0到1之间的和为1的值组成的向量。<br> 到目前为止，我们已经看到了卷积、激活函数层和池化是如何工作的。重要的是要理解这些层次是任何CNN的基本构建块。如下图所示，我们有两组卷积、ReLU和Pooling层-第2个卷积层使用6个过滤器对第1个Pooling层的输出进行卷积，总共产生6个feature map。然后将ReLU分别应用于所有这六个特性映射。然后，我们对这六个修正后的特征映射分别执行最大池操作。<br> 这些层一起从图像中提取有用的特征，在我们的网络中引入非线性，降低特征维数，同时使特征在缩放和转换时具有一定的等变性。<br> <img src="https://images2.imgbox.com/a6/1f/X1ZPqyDn_o.png" alt="在这里插入图片描述"></p> 
<p>如上所述，卷积+池化层作为输入图像的特征提取器，而全连接层作为分类器。、<br> 五、总结<br> 卷积网络的整体训练过程总结如下:<br> Step1:用随机值初始化所有的过滤器和参数/权重<br> Step2:网络以训练图像为输入，进行前向传播步骤(卷积、ReLU、池化操作以及全连通层的前向传播)，求出每个类的输出概率。<br> 假设上面的船图像的输出概率是[0.2,0.4,0.1,0.3]<br> 由于第一个训练示例的权重是随机分配的，因此输出概率也是随机的。<br> Step3:计算输出层的总误差(所有4个类的总和)<br> 总误差=∑½(目标概率-输出概率)²<br> Step4:使用Backpropagation计算误差相对于网络中所有权值的梯度，使用梯度下降更新所有滤波器值/权值和参数值，使输出误差最小化。权重按它们对总误差的贡献比例进行调整。<br> 当再次输入相同的图像时，现在的输出概率可能是[0.1,0.1,0.7,0.1]，这更接近于目标向量[0,0,1,0]。<br> 这意味着网络已经学会通过调整其权值/滤波器来正确分类这一特定图像，从而减少输出误差。<br> 过滤器数量、过滤器大小、网络架构等参数在第1步之前都已经固定，并且在训练过程中不会改变，只更新过滤器矩阵的值和连接权值。<br> Step5:对训练集中的所有图像重复步骤2-4。<br> 以上步骤训练了ConvNet——这本质上意味着ConvNet的所有权值和参数现在都得到了优化，可以从训练集中正确地对图像进行分类。<br> 当一幅新的(不可见的)图像输入到ConvNet时，该网络将进行前向传播步骤，并为每个类输出一个概率(对于一幅新图像，输出概率使用已优化的权值计算，以正确分类所有以前的训练示例)。如果我们的训练集足够大，网络将(有希望)很好地泛化新图像并将它们分类到正确的类别中。<br> 在上面的例子中，我们使用了两组交替的卷积层和池层。但是，这些操作可以在单个ConvNet中重复任意次数。事实上，今天一些性能最好的ConvNets有几十个卷积和池化层，没有必要再每个卷积层之后都有一个池化层。如下图所示，在进行池化操作之前，我们可以连续进行多个卷积+ReLU操作。还要注意ConvNet的每一层在下面的图中是如何显示的。<img src="https://images2.imgbox.com/67/a0/MhlDPeEN_o.png" alt="在这里插入图片描述"></p> 
<p>再举两个例子：<br> 1、可视化卷积神经网络<br> 一般来说，我们有越多的卷积步骤，我们的网络就能学习识别越复杂的特征。例如，在图像分类中，ConvNet可以学习从第一层的原始像素中检测边缘，然后使用边缘来检测第二层的简单形状，然后使用这些形状来阻止更高层次的特征，例如较高层次中的面部形状。下图演示了这一点-这些特征是使用卷积深度置信网络学习的，这里包含的图只是为了演示思想(这只是一个例子:现实生活中的卷积滤波器可以检测到对人类没有意义的对象)。<img src="https://images2.imgbox.com/8a/bc/jfHOQMDH_o.png" alt="在这里插入图片描述"></p> 
<p>2、手写数字识别<br> 下面我们将看到网络对于输入“8”是如何工作的。注意，图18中的可视化并没有单独显示ReLU操作。<img src="https://images2.imgbox.com/50/bc/RP3JcQIr_o.png" alt="在这里插入图片描述"></p> 
<p>输入图像包含1024个像素(32 x 32图像)，第一个卷积层是由6个唯一的5×5滤波器与输入图像卷积而成。如图所示，使用6个不同的过滤器生成深度为6的特征图。!</p> 
<p>在Convolutional Layer 1之后是Pooling Layer 1，在Convolutional Layer 1的六个feature map上分别做2×2最大的Pooling(stride 2)。在2×2网格中拥有最大值(最亮的那个)的像素会进入池化层。<img src="https://images2.imgbox.com/ca/64/7KQgNyJp_o.png" alt="在这里插入图片描述"></p> 
<p>池化层1后面是16个执行卷积操作的5×5 (stride 1)卷积滤波器。接下来是池化层2，最大池化量为2×2(使用stride 2)。这两个层使用了与上面描述的相同的概念。<br> 然后我们有三个完全连接(FC)层。有:<br> 第一层FC层有120个神经元<br> 第二层FC层有100个神经元<br> 第三层的10个神经元对应10位数字-也称为输出层<br> 请注意在下图中，输出层中的10个节点中的每一个都连接到第2个完全连接层中的所有100个节点(因此称为完全连接)。<br> 另外，请注意输出层中唯一亮的节点是如何对应于“8”的——这意味着网络正确地分类了我们手写的数字(亮的节点表示它的输出更高，即在所有其他数字中，8的概率最高)。<br> <img src="https://images2.imgbox.com/02/7e/Fyzen1l4_o.png" alt="在这里插入图片描述"></p> 
<p>以上就是卷积层、激活函数层、池化层、以及全部连接层的全部说明。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/60d4ca08e5b4aec766ac5dd1a798512e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Java集合基础操作</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/32b107329eb9ef123dfb11a7168ee123/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Spring  Cloud各核心组件的功能及作用</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>