<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>（2019, StyleGAN）用于 GAN 的基于样式的生成器架构 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="（2019, StyleGAN）用于 GAN 的基于样式的生成器架构" />
<meta property="og:description" content="A Style-Based Generator Architecture for Generative Adversarial Networks
公众号：EDPJ
目录
0. 摘要
1. 简介
2. 基于风格的生成器
2.1 生成图像的质量
2.2 现有技术
3. 基于样式的生成器的属性
3.1 风格混合
3.2 随机变化
3.3 将全局效应与随机性分开
4. 解耦研究
4.1 感知路径长度
4.2 线性可分性
5. 结论
参考
S. 总结
S.1 核心思想
S.2 分析
S.3 其他贡献
0. 摘要 我们为生成对抗网络提出了一种替代生成器架构，借鉴了风格迁移。 新架构实现了高级属性（例如，在人脸上训练时的姿势和身份）和生成图像的随机变化（例如，雀斑、头发）的自动学习、无监督分离，并且可以直观地、特定规模地控制合成。 新的生成器在传统分布质量指标方面改进了最新技术，导致明显更好的插值特性，并且也更好地解耦了隐空间的变化因素。 为了量化插值质量和解耦，我们提出了两种适用于任何生成器架构的新的自动化方法。 最后，我们介绍了一个新的、高度多样化和高质量的人脸数据集。
1. 简介 受风格迁移的启发，我们重新设计了生成器架构，以展示控制图像合成过程的新方法。 我们的生成器从学习到的常量输入开始，根据隐编码调整每个卷积层的图像“风格”，从而直接控制不同尺度下图像特征的强度。 我们不以任何方式修改鉴别器或损失函数，因此我们的工作与正在进行的关于 GAN 损失函数、正则化和超参数的讨论是正交的。
我们的生成器将输入的隐编码嵌入到中间隐空间中，这对网络中变异因素的表示方式有着深远的影响。 输入隐空间必须遵循训练数据的概率密度，我们认为这会导致某种程度的不可避免的耦合。 我们的中间隐空间不受该限制，因此可以解耦。 由于以前估计隐空间解耦度的方法不能直接适用于我们的案例，我们提出了两个新的自动化指标——感知路径长度和线性可分性——来量化生成器的这些方面。 使用这些指标，我们表明，与传统的生成器架构相比，我们的生成器允许对不同的变化因素进行更线性、更少耦合的表示。
最后，我们展示了一个新的人脸数据集（Flickr-Faces-HQ，FFHQ），它提供了比现有的高分辨率数据集（附录 A）更高的质量和更广泛的变化。我们已公开提供此数据集以及我们的源代码和预训练网络。可以在同一链接下找到随附的视频。
2. 基于风格的生成器 传统上，隐编码通过输入层（即前馈网络的第一层）提供给生成器（图 1a）。 我们通过完全删除输入层并从学到的常量开始来偏离此设计（图 1b，右）。 给定输入隐空间 Z 中的隐编码 z，非线性映射网络 f : Z → W 首先生成 w ∈ W（图 1b，左）。 为简单起见，我们将两个空间的维数都设置为 512，映射 f 是使用 8 层 MLP 实现的，我们将在第 4." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/707b78dc057e2a4e563161237e598c17/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-23T17:21:07+08:00" />
<meta property="article:modified_time" content="2023-05-23T17:21:07+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">（2019, StyleGAN）用于 GAN 的基于样式的生成器架构</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p style="text-align:center;"><strong>A Style-Based Generator Architecture for Generative Adversarial Networks</strong></p> 
<p style="text-align:center;">公众号：EDPJ</p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="-toc" style="margin-left:0px;"><a href="#0.%20%E6%91%98%E8%A6%81" rel="nofollow">0. 摘要</a></p> 
<p id="1.%20%E7%AE%80%E4%BB%8B-toc" style="margin-left:0px;"><a href="#1.%20%E7%AE%80%E4%BB%8B" rel="nofollow">1. 简介</a></p> 
<p id="2.%20%E5%9F%BA%E4%BA%8E%E9%A3%8E%E6%A0%BC%E7%9A%84%E7%94%9F%E6%88%90%E5%99%A8-toc" style="margin-left:0px;"><a href="#2.%20%E5%9F%BA%E4%BA%8E%E9%A3%8E%E6%A0%BC%E7%9A%84%E7%94%9F%E6%88%90%E5%99%A8" rel="nofollow">2. 基于风格的生成器</a></p> 
<p id="2.1%20%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E7%9A%84%E8%B4%A8%E9%87%8F-toc" style="margin-left:40px;"><a href="#2.1%20%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E7%9A%84%E8%B4%A8%E9%87%8F" rel="nofollow">2.1 生成图像的质量</a></p> 
<p id="2.2%20%E7%8E%B0%E6%9C%89%E6%8A%80%E6%9C%AF-toc" style="margin-left:40px;"><a href="#2.2%20%E7%8E%B0%E6%9C%89%E6%8A%80%E6%9C%AF" rel="nofollow">2.2 现有技术</a></p> 
<p id="3.%20%E5%9F%BA%E4%BA%8E%E6%A0%B7%E5%BC%8F%E7%9A%84%E7%94%9F%E6%88%90%E5%99%A8%E7%9A%84%E5%B1%9E%E6%80%A7-toc" style="margin-left:0px;"><a href="#3.%20%E5%9F%BA%E4%BA%8E%E6%A0%B7%E5%BC%8F%E7%9A%84%E7%94%9F%E6%88%90%E5%99%A8%E7%9A%84%E5%B1%9E%E6%80%A7" rel="nofollow">3. 基于样式的生成器的属性</a></p> 
<p id="3.1%20%E9%A3%8E%E6%A0%BC%E6%B7%B7%E5%90%88-toc" style="margin-left:40px;"><a href="#3.1%20%E9%A3%8E%E6%A0%BC%E6%B7%B7%E5%90%88" rel="nofollow">3.1 风格混合</a></p> 
<p id="3.2%20%E9%9A%8F%E6%9C%BA%E5%8F%98%E5%8C%96-toc" style="margin-left:40px;"><a href="#3.2%20%E9%9A%8F%E6%9C%BA%E5%8F%98%E5%8C%96" rel="nofollow">3.2 随机变化</a></p> 
<p id="3.3%20%E5%B0%86%E5%85%A8%E5%B1%80%E6%95%88%E5%BA%94%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%80%A7%E5%88%86%E5%BC%80-toc" style="margin-left:40px;"><a href="#3.3%20%E5%B0%86%E5%85%A8%E5%B1%80%E6%95%88%E5%BA%94%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%80%A7%E5%88%86%E5%BC%80" rel="nofollow">3.3 将全局效应与随机性分开</a></p> 
<p id="4.%20%E8%A7%A3%E8%80%A6%E7%A0%94%E7%A9%B6-toc" style="margin-left:0px;"><a href="#4.%20%E8%A7%A3%E8%80%A6%E7%A0%94%E7%A9%B6" rel="nofollow">4. 解耦研究</a></p> 
<p id="4.1%20%E6%84%9F%E7%9F%A5%E8%B7%AF%E5%BE%84%E9%95%BF%E5%BA%A6-toc" style="margin-left:40px;"><a href="#4.1%20%E6%84%9F%E7%9F%A5%E8%B7%AF%E5%BE%84%E9%95%BF%E5%BA%A6" rel="nofollow">4.1 感知路径长度</a></p> 
<p id="4.2%20%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%80%A7-toc" style="margin-left:40px;"><a href="#4.2%20%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%80%A7" rel="nofollow">4.2 线性可分性</a></p> 
<p id="5.%20%E7%BB%93%E8%AE%BA-toc" style="margin-left:0px;"><a href="#5.%20%E7%BB%93%E8%AE%BA" rel="nofollow">5. 结论</a></p> 
<p id="%E5%8F%82%E8%80%83-toc" style="margin-left:0px;"><a href="#%E5%8F%82%E8%80%83" rel="nofollow">参考</a></p> 
<p id="S.%20%E6%80%BB%E7%BB%93-toc" style="margin-left:0px;"><a href="#S.%20%E6%80%BB%E7%BB%93" rel="nofollow">S. 总结</a></p> 
<p id="S.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-toc" style="margin-left:40px;"><a href="#S.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3" rel="nofollow">S.1 核心思想</a></p> 
<p id="S.2%20%E5%88%86%E6%9E%90-toc" style="margin-left:40px;"><a href="#S.2%20%E5%88%86%E6%9E%90" rel="nofollow">S.2 分析</a></p> 
<p id="S.3%20%E5%85%B6%E4%BB%96%E8%B4%A1%E7%8C%AE-toc" style="margin-left:40px;"><a href="#S.3%20%E5%85%B6%E4%BB%96%E8%B4%A1%E7%8C%AE" rel="nofollow">S.3 其他贡献</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="0.%20%E6%91%98%E8%A6%81">0. 摘要</h2> 
<p>我们为生成对抗网络提出了一种替代生成器架构，借鉴了风格迁移。 新架构实现了高级属性（例如，在人脸上训练时的姿势和身份）和生成图像的随机变化（例如，雀斑、头发）的自动学习、无监督分离，并且可以直观地、特定规模地控制合成。 新的生成器在传统分布质量指标方面改进了最新技术，导致明显更好的插值特性，并且也更好地解耦了隐空间的变化因素。 为了量化插值质量和解耦，我们提出了两种适用于任何生成器架构的新的自动化方法。 最后，我们介绍了一个新的、高度多样化和高质量的人脸数据集。</p> 
<h2 id="1.%20%E7%AE%80%E4%BB%8B">1. 简介</h2> 
<p>受风格迁移的启发，我们重新设计了生成器架构，以展示控制图像合成过程的新方法。 我们的生成器从学习到的常量输入开始，根据隐编码调整每个卷积层的图像“风格”，从而直接控制不同尺度下图像特征的强度。 我们不以任何方式修改鉴别器或损失函数，因此我们的工作与正在进行的关于 GAN 损失函数、正则化和超参数的讨论是正交的。</p> 
<p>我们的生成器将输入的隐编码嵌入到中间隐空间中，这对网络中变异因素的表示方式有着深远的影响。 输入隐空间必须遵循训练数据的概率密度，我们认为这会导致某种程度的不可避免的耦合。 我们的中间隐空间不受该限制，因此可以解耦。 由于以前估计隐空间解耦度的方法不能直接适用于我们的案例，我们提出了两个新的自动化指标——感知路径长度和线性可分性——来量化生成器的这些方面。 使用这些指标，我们表明，与传统的生成器架构相比，我们的生成器允许对不同的变化因素进行更线性、更少耦合的表示。</p> 
<p>最后，我们展示了一个新的人脸数据集（Flickr-Faces-HQ，FFHQ），它提供了比现有的高分辨率数据集（附录 A）更高的质量和更广泛的变化。我们已公开提供此数据集以及我们的源代码和预训练网络。可以在同一链接下找到随附的视频。</p> 
<h2 id="2.%20%E5%9F%BA%E4%BA%8E%E9%A3%8E%E6%A0%BC%E7%9A%84%E7%94%9F%E6%88%90%E5%99%A8">2. 基于风格的生成器</h2> 
<p class="img-center"><img alt="" height="811" src="https://images2.imgbox.com/56/d5/OXuFlTT6_o.png" width="552"></p> 
<p>传统上，隐编码通过输入层（即前馈网络的第一层）提供给生成器（图 1a）。 我们通过完全删除输入层并从学到的常量开始来偏离此设计（图 1b，右）。 给定输入隐空间 Z 中的隐编码 z，非线性映射网络 f : Z → W 首先生成 w ∈ W（图 1b，左）。 为简单起见，我们将两个空间的维数都设置为 512，映射 f 是使用 8 层 MLP 实现的，我们将在第 4.1 节中分析这一决定。然后，学到的仿射变换将 w 专门化为风格 y = (y_s, y_b)，在合成网络 g 的每个卷积层之后控制自适应实例归一化 (adaptive instance normalization, AdaIN) 操作。 AdaIN 操作定义为</p> 
<p class="img-center"><img alt="" height="51" src="https://images2.imgbox.com/d9/22/Jr1lg2AS_o.png" width="455"></p> 
<p>其中每个特征图 x_i 分别被归一化，然后使用来自样式 y 的相应标量分量进行缩放和偏置。 因此，y 的维数是该层上特征图数量的两倍。 </p> 
<p>比较我们的方法与风格迁移，我们从向量 w 而不是示例图像计算空间不变的风格 y。 我们选择对 y 重复使用“风格”一词，因为类似的网络架构已经用于前馈风格传输、无监督的图像到图像翻译和域混合。 与更一般的特征转换相比，AdaIN 由于其高效和紧凑的表示形式特别适合我们的目的。</p> 
<p>最后，我们为我们的生成器提供了一种直接方法，通过引入显式噪声输入来生成随机细节。 这些是由不相关的高斯噪声组成的单通道图像，我们将专用噪声图像提供给合成网络的每一层。 使用学习到的每个特征缩放因子将噪声图像广播到所有特征图，然后添加到相应卷积的输出中，如图 1b 所示。 添加噪声输入的影响在第 3.2 和 3.3 节中讨论。</p> 
<h3 id="2.1%20%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E7%9A%84%E8%B4%A8%E9%87%8F">2.1 生成图像的质量</h3> 
<p><img alt="" height="378" src="https://images2.imgbox.com/84/63/WOPDsAal_o.png" width="729"></p> 
<p>在研究我们的生成器的属性之前，我们通过实验证明重新设计不会影响图像质量，事实上，它大大提高了图像质量。表 1 给出了 CelebA-HQ 和我们新的 FFHQ 数据集（附录 A）中各种生成器架构的 Fréchet 初始距离（Fréchet inception distances，FID）。 其他数据集的结果在补充材料中给出。</p> 
<ul><li>我们的基线配置 (A) 是 Karras 等人的 Progressive GAN 设置，我们从中继承网络和所有超参数，除非另有说明。</li><li>我们首先通过使用 (B) 双线性上/下采样操作、更长的训练和调整的超参数，来切换到改进的基线。 补充材料中包含训练设置和超参数的详细描述。</li><li>然后，我们通过添加映射网络和 AdaIN 操作 (C) 进一步改进这个新基线，并令人惊讶地观察到网络不再受益于将隐编码馈送到第一个卷积层。</li><li>因此，我们通过移除传统输入层并从学习的 4×4×512 常量张量 (D) 开始图像合成来简化架构。 我们发现合成网络能够产生有意义的结果是非常了不起的，即使它仅通过控制 AdaIN 操作的样式，来接收输入。</li><li>最后，我们介绍了进一步改进结果的噪声输入 (E)，以及新的混合正则化 (F)，它可以消除相邻样式并实现对生成图像的更细粒度控制（第 3.1 节）。</li></ul> 
<p>我们使用两种不同的损失函数评估我们的方法：</p> 
<ul><li>对于 CelebA-HQ，我们依赖 WGAN-GP，</li><li>而 FFHQ， 对于配置 A 使用 WGAN-GP，对于配置  B~F，使用带有 R1 正则化的非饱和损失。 我们发现这些选择可以提供最佳结果。 我们的贡献不会修改损失函数。</li></ul> 
<p class="img-center"><img alt="" height="828" src="https://images2.imgbox.com/59/e8/zRyErZsH_o.png" width="547"></p> 
<p>我们观察到基于样式的生成器 (E) 比传统生成器 (B) 相当显着地提高了 FIDs ，几乎提高了 20%，证实了在并行工作中进行的大规模 ImageNet 测量。 图 2 显示了使用我们的生成器从 FFHQ 数据集生成的一组未经整理的（uncurated）新颖图像。 经FID确认，平均质量很高，连眼镜、帽子等配饰都合成成功。 对于此图，我们使用所谓的截断技巧（truncation trick）避免了从 W 的极端区域进行采样——附录 B 详细说明了如何在 W 而不是 Z 中执行该技巧。请注意，我们的生成器允许应用 仅选择性地截断低分辨率，以便不影响高分辨率细节。</p> 
<p>本文中的所有 FID 都是在没有截断技巧的情况下计算的，我们仅将其用于图 2 和视频中的说明目的。 所有图像均以 1024X1024 分辨率生成。</p> 
<h3 id="2.2%20%E7%8E%B0%E6%9C%89%E6%8A%80%E6%9C%AF">2.2 现有技术</h3> 
<p>GAN 架构的大部分工作都集中在改进鉴别器上，例如，使用多个鉴别器、多分辨率鉴别器或自注意力（self-attention）。 生成器端的工作主要集中在输入隐空间中的精确分布或通过高斯混合模型、聚类或鼓励凸性来塑造输入隐空间。</p> 
<p>最近的条件生成器通过一个单独的 embedding 网络将类标识符提供给生成器中的大量层，而隐编码仍然通过输入层提供。 一些作者考虑过将部分隐编码提供给多个生成器层。 在并行工作中，Chen 等人使用 AdaIN “自调制（self modulate）”生成器，类似于我们的工作，但不考虑中间隐空间或噪声输入。</p> 
<h2 id="3.%20%E5%9F%BA%E4%BA%8E%E6%A0%B7%E5%BC%8F%E7%9A%84%E7%94%9F%E6%88%90%E5%99%A8%E7%9A%84%E5%B1%9E%E6%80%A7">3. 基于样式的生成器的属性</h2> 
<p>我们的生成器架构可以通过对样式进行特定比例的修改来控制图像合成。 我们可以将映射网络和仿射变换视为从学到的分布中为每种风格抽取样本的一种方式，将合成网络视为一种基于风格集合生成新颖图像的方式。 每种风格的效果在网络中都是局部的，即，修改风格的特定子集预计只会影响图像的某些方面。</p> 
<p>要了解这种局部化的原因，让我们考虑一下 AdaIN 操作（公式 1）如何首先将每个通道归一化为零均值和单位方差，然后才根据样式应用尺度和偏差。 新的单通道统计数据，如风格所规定，修改了后续卷积操作的特征的相对重要性，但由于归一化，它们不依赖于原始统计数据。 因此，每种样式在被下一个 AdaIN 操作覆盖之前仅控制一个卷积。</p> 
<h3 id="3.1%20%E9%A3%8E%E6%A0%BC%E6%B7%B7%E5%90%88">3.1 风格混合</h3> 
<p>为了进一步鼓励样式局部化，我们采用混合正则化，其中给定百分比的图像是在训练期间使用两个随机隐编码而不是一个生成的。 在生成这样的图像时，我们只需在合成网络中随机选择的点从一个隐编码切换到另一个——我们称之为风格混合的操作。 具体来说，我们通过映射网络运行两个隐编码 z1、z2，并让相应的 w1、w2 控制样式，以便 w1 在交叉点之前应用，w2 在交叉点之后应用。 这种正则化技术可以防止网络假设相邻样式是相关的。</p> 
<p class="img-center"><img alt="" height="310" src="https://images2.imgbox.com/4f/c7/oacZArx7_o.png" width="543"></p> 
<p>表 2 显示了在训练期间启用混合正则化如何显着改善局部化，由在测试时混合多个隐编码的场景中改进的 FID 表明。</p> 
<p><img alt="" height="969" src="https://images2.imgbox.com/64/3a/bQwaduCM_o.png" width="762"></p> 
<p>图 3 展示了通过混合不同比例的两个隐编码合成的图像示例。 我们可以看到，每个样式子集都控制着图像的有意义的高级属性。</p> 
<ul><li>两组图像是从它们各自的隐编码（源 A 和 B）生成的； 其余图像是通过从源 B 复制指定样式子集并从源 A 获取其余样式而生成的。</li><li>复制与粗略（coarse）空间分辨率 (4X4 – 8X8) 相对应的样式会带来高级方面，例如来自源 B 的姿势、一般发型 、脸型和眼镜，而所有颜色（眼睛、头发、光线）和更精细的面部特征都与 A 相似。</li><li>如果我们从 B 复制中分辨率 (16X16 – 32X32) 的样式，我们将继承较小规模的面部特征 、发型、睁眼/闭眼来自 B，而姿势、一般脸型和来自 A 的眼镜被保留。</li><li>最后，从 B 复制精细样式 (64X64 – 1024X1024) 主要带来配色方案和微观结构。</li></ul> 
<h3 id="3.2%20%E9%9A%8F%E6%9C%BA%E5%8F%98%E5%8C%96">3.2 随机变化</h3> 
<p>人物肖像中有许多方面可以被视为随机的，例如头发、胡茬、雀斑或皮肤毛孔的准确位置。 只要它们遵循正确的分布，任何这些都可以随机化而不会影响我们对图像的感知。</p> 
<p>让我们考虑一下传统生成器如何实现随机变化。 鉴于网络的唯一输入是通过输入层，网络需要发明一种方法，以便在需要时从早期的激活中生成空间变化的伪随机数。 这会消耗网络容量，并且很难隐藏生成信号的周期性——而且并不总是成功，正如生成图像中常见的重复模式所证明的那样。 我们的架构通过在每次卷积后添加每像素噪声来完全回避这些问题。 </p> 
<p class="img-center"><img alt="" height="628" src="https://images2.imgbox.com/2a/a8/6eU5Mcdj_o.png" width="546"></p> 
<p>图 4 显示了同一底层图像的随机实现，这些图像是使用我们的生成器生成的，具有不同的噪声实现。 我们可以看到噪声只影响随机方面，而不会影响整体成分和身份等高级方面。</p> 
<ul><li>(a) 两个生成的图像。</li><li>(b) 放大输入噪声的不同实现。 虽然整体外观几乎相同，但各根毛发的位置却大不相同。</li><li>(c) 每个像素在 100 种不同实现中的标准偏差，突出显示图像的哪些部分受到噪声的影响。主要区域是头发、轮廓和部分背景，但眼睛反射也有有趣的随机变化。 身份和姿势等全局方面不受随机变化的影响。</li></ul> 
<p class="img-center"><img alt="" height="770" src="https://images2.imgbox.com/12/06/uUlMQdxW_o.png" width="550"></p> 
<p>图 5 进一步说明了将随机变化应用于层的不同子集的效果。</p> 
<ul><li>(a) 噪声应用于所有层。</li><li>(b) 没有噪音。</li><li>(c) 仅精细（fine）层中的噪声 (64X64 – 1024X1024)。</li><li>(d) 仅粗糙（coarse）层中的噪声 (4X4 – 32X32)。</li><li>我们可以看到人为地忽略噪声会导致毫无特色的“绘画”外观。 粗噪声会导致头发大面积卷曲和出现较大的背景特征，而细噪声会带来更细的头发卷曲、更精细的背景细节和皮肤毛孔。</li></ul> 
<p>由于这些效果最好在动画中看到，请参阅随附的视频，了解更改一层的噪声输入如何导致匹配比例下的随机变化的演示。</p> 
<p>我们发现有趣的是，噪声的影响似乎紧密地局限于网络中。 我们假设在生成器中的任何一点，都存在尽快引入新内容的压力，而我们的网络创建随机变化的最简单方法是依赖提供的噪声。 每层都有一组新的噪声，因此没有动机从早期的激活中产生随机效应，从而导致局部效应。</p> 
<h3 id="3.3%20%E5%B0%86%E5%85%A8%E5%B1%80%E6%95%88%E5%BA%94%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%80%A7%E5%88%86%E5%BC%80">3.3 将全局效应与随机性分开</h3> 
<p>前面的部分以及随附的视频表明，虽然风格的变化具有全局影响（改变姿势、身份等），但噪声仅影响无关紧要的随机变化（不同梳理的头发、胡须等）。 这一观察结果与风格迁移文献一致，其中已经确定空间不变统计（Gram 矩阵、通道均值、方差等）可靠地编码图像的风格，而空间变化特征编码一个特定的实例。 </p> 
<p>在我们基于样式的生成器中，样式会影响整个图像，因为完整的特征图是用相同的值缩放和偏置的。 因此，可以连贯地控制姿势、照明或背景风格等全局效果。 同时，噪声被独立地添加到每个像素，因此非常适合控制随机变化。 如果网络试图控制，例如，使用噪声摆姿势，这将导致空间不一致的判决，然后将被判别器惩罚。 因此，网络学会在没有明确指导的情况下适当地使用全局和局部通道。</p> 
<h2 id="4.%20%E8%A7%A3%E8%80%A6%E7%A0%94%E7%A9%B6">4. 解耦研究</h2> 
<p class="img-center"><img alt="" height="392" src="https://images2.imgbox.com/9d/6b/nlV4uHE8_o.png" width="547"></p> 
<p>解耦有多种定义，但一个共同的目标是一个由线性子空间组成的隐空间，每个子空间控制一个变化因素。 但是 Z 中各因子组合的采样概率需要与训练数据中相应的密度相匹配。 如图 6 所示，这排除了因子与典型数据集和输入隐分布完全分离的可能性。（少数为解耦研究设计的人工数据集将所有预定变化因素的所有组合制成表格，频率均匀，从而解决了问题）</p> 
<ul><li>(a) 缺少某些组合（例如，长发男性）的示例训练集。</li><li>(b) 这迫使从 Z 到图像特征的映射变得弯曲，以便禁止组合在 Z 中消失，从而防止采样无效的组合。</li><li>(c) 学到的从 Z 到 W 的映射能够“撤销”大部分扭曲。</li></ul> 
<p>我们的生成器架构的一个主要好处是中间隐空间 W 不必根据任何固定分布进行采样； 它的采样密度是由学到的分段连续映射 f(z) 引起的。 该映射可适用于“不扭曲的” W，以便变化因素变得更加线性。 我们假设生成器有这样做的压力，因为基于解耦的 representation.生成逼真的图像应该比基于耦合的 representation.更容易。 因此，我们期望训练在无监督环境中产生较少耦合的 W，即，事先不知道变化因素。</p> 
<h3 id="4.1%20%E6%84%9F%E7%9F%A5%E8%B7%AF%E5%BE%84%E9%95%BF%E5%BA%A6">4.1 感知路径长度</h3> 
<p>正如 Laine 所指出的，隐空间向量的插值可能会在图像中产生令人惊讶的非线性变化。 例如，任一端点中不存在的特征可能会出现在线性插值的路径中。 这是隐空间纠缠在一起并且变化因素没有正确分离的迹象。 为了量化这种影响，我们可以测量当我们在隐空间中执行插值时图像发生的剧烈变化。 直觉上，与高度弯曲的隐空间相比，较少弯曲的隐空间应该导致感知上更平滑的过渡。</p> 
<p>作为我们度量的基础，我们使用基于感知的成对图像距离，计算为两个 VGG16 embedding 之间的加权差异，其中权重是合适的，因此度量与人类感知相似性判断一致。 如果我们将隐空间插值路径细分为线性段，我们可以将此分段路径的总感知长度定义为每个段的感知差异之和，如图像距离度量所报告的那样。 感知路径长度的自然定义是无限精细细分下此总和的极限，但实际上我们使用小细分 ε = 10^(−4) 来近似它。 因此，在所有可能的端点上，隐空间 Z 中的平均感知路径长度是</p> 
<p class="img-center"><img alt="" height="91" src="https://images2.imgbox.com/55/63/lSMHSz6r_o.png" width="472"></p> 
<p>其中，z1, z2 ∼ P(z)，t∼ U(0, 1)，G 是生成器（即，g ◦ f 对于基于样式的网络），d(·,·) 评估图片之间的感知距离。 这里的 slerp 表示球形插值，这是在我们的归一化输入隐空间中最合适的插值方式。 为了专注于面部特征而不是背景，我们在评估成对图像度量之前裁剪生成的图像以仅包含面部。 由于度量 d 是二次的，我们除以 ε^2。 我们通过获取 100,000 个样本来计算期望值。</p> 
<p>以类似的方式计算 W 中的平均感知路径长度：</p> 
<p class="img-center"><img alt="" height="88" src="https://images2.imgbox.com/63/ff/L35ZCo16_o.png" width="491"></p> 
<p>唯一的区别是插值发生在 W 空间中。 因为 W 中的向量没有以任何方式归一化，所以我们使用线性插值 (lerp)</p> 
<p class="img-center"><img alt="" height="356" src="https://images2.imgbox.com/0b/36/8hrnG7lI_o.png" width="546"></p> 
<p>表 3 显示，对于我们带有噪声输入的基于样式的生成器，此全路径长度明显更短，表明 W 在感知上比 Z 更线性。然而，该测量实际上略微偏向于输入隐空间 Z。 如果 W 确实是 Z 的解耦和“扁平化”映射，它可能包含不在输入流形上的区域——因此生成器无法很好地重构——甚至不在从输入流形映射的点之间，而根据定义，输入隐空间 Z 没有这样的区域。 因此可以预期，如果我们将测量限制在路径端点，即 t ∈ {0, 1}，我们应该获得较小的 l_W 而 l_Z 不受影响。 这确实是我们在表 3 中观察到的。 </p> 
<p class="img-center"><img alt="" height="374" src="https://images2.imgbox.com/d8/0f/6KihG0Jx_o.png" width="546"></p> 
<p>表 4 显示了映射网络如何影响路径长度（方法后的数字表示映射网络的深度）。 我们看到传统生成器和基于样式的生成器都受益于映射网络，额外的深度通常会改善感知路径长度和 FID。 有趣的是，虽然 l_W 在传统生成器中有所改进，但 l_Z 变得相当糟糕，这说明了，在 GAN 中，输入隐空间确实可以任意纠缠。 </p> 
<h3 id="4.2%20%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%80%A7">4.2 线性可分性</h3> 
<p>如果隐空间充分解耦，则应该可以找到始终对应于各个变化因素的方向向量。 我们提出了另一种度量，通过测量隐空间点可以使用线性超平面分成两个不同的集合的程度来量化这种效果，以便每个集合对应于图像的特定二值属性。</p> 
<p>为了标记生成的图像，我们为许多二进制属性训练辅助分类网络，例如，区分男性和女性的面孔。 在我们的测试中，分类器与我们使用的鉴别器具有相同的架构，并使用 CelebA-HQ 数据集进行训练，该数据集保留了原始 CelebA 数据集中可用的 40 个属性。 为了衡量一个属性的可分离性，我们生成了 200,000 张 z ∼ P(z) 的图像，并使用辅助分类网络对它们进行分类。 然后，我们根据分类器置信度对样本进行排序，并移除置信度最低的一半，从而产生 100,000 个标记的隐空间向量。</p> 
<p>对于每个属性，我们拟合一个线性 SVM 来预测基于隐空间点的标签——z 代表常规，w 代表基于风格——并通过这个平面对点进行分类。 然后我们计算条件熵 H(Y |X)，其中 X 是由 SVM 预测的类别，Y 是由预训练分类器确定的类别。 这表明需要多少额外信息才能确定样本的真实类别，前提是我们知道它位于超平面的哪一侧。 低值表明对应的变异因子具有一致的隐空间方向。</p> 
<p>我们将最终的可分离性分数计算为 exp(Σ_i H(Y_i | X_i))，其中 i 列举了 40 个属性。 类似于初始分数，求幂将值从对数域带到线性域，以便它们更容易比较。</p> 
<p>表 3 和表 4 显示 W 始终比 Z 更可分离，表明纠缠较少的 representation。 此外，增加映射网络的深度可以提高 W 中的图像质量和可分离性，这符合合成网络天生倾向于解耦输入 representation 的假设。 有趣的是，在传统生成器前面添加一个映射网络会导致 Z 中的可分离性严重丧失，但会改善中间隐空间 W 中的情况，并且 FID 也会有所改善。 这表明，当我们引入一个不必遵循训练数据分布的中间隐空间时，即使是传统的生成器架构也能表现得更好。</p> 
<h2 id="5.%20%E7%BB%93%E8%AE%BA">5. 结论</h2> 
<p>基于我们的结果和 Chen 等人的平行工作，越来越明显的是，传统的 GAN 生成器架构在各个方面都不如基于样式的设计。 就已建立的质量指标而言，这是正确的，我们进一步相信，我们对高级属性和随机效应的分离以及中间隐空间的线性度的研究，将在提高对 GAN 合成的理解和可控性方面卓有成效。</p> 
<p>我们注意到，我们的平均路径长度度量可以很容易地在训练期间用作正则化器，并且线性可分性度量的某些变体可能也可以充当正则化器。 总的来说，我们希望在训练期间直接塑造中间隐空间的方法将为未来的工作提供有趣的途径。</p> 
<p></p> 
<h2 id="%E5%8F%82%E8%80%83">参考</h2> 
<p>Karras, Tero, Samuli Laine, and Timo Aila. "A style-based generator architecture for generative adversarial networks." <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 2019.</p> 
<p></p> 
<h2 id="S.%20%E6%80%BB%E7%BB%93">S. 总结</h2> 
<h3 id="S.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3">S.1 核心思想</h3> 
<p class="img-center"><img alt="" height="811" src="https://images2.imgbox.com/62/59/FJcMUBPt_o.png" width="552"></p> 
<p>本文基于风格迁移提出了一个新的生成器，如图 1 所示。该生成器可以在保留高级属性（例如，在人脸上训练时的姿势和身份）的同时，使生成图像产生随机变化（例如，雀斑、头发）。</p> 
<h3 id="S.2%20%E5%88%86%E6%9E%90">S.2 分析</h3> 
<p><strong>生成器的结构</strong>。该生成器移除了传统生成器的输入层，把采样于隐空间 Z 的隐编码 z 输入非线性映射网络（例如：MLP）获得中间隐空间 W 的隐编码 w。再把隐编码通过学到的仿射变换 A，获得风格 y = (y_s, y_b)。再用 y 输入（控制）生成网络每个卷积层的 AdaIN。</p> 
<p class="img-center"><img alt="" height="51" src="https://images2.imgbox.com/15/de/aaE3ZbZ3_o.png" width="455"></p> 
<p>风格 y 控制高级属性（例如，在人脸上训练时的姿势和身份）的生成。由于输入的噪声是随机采样的，所以会使生成图像产生随机变化 （例如，雀斑、头发），但不会对高级属性产生影响。</p> 
<h3 id="S.3%20%E5%85%B6%E4%BB%96%E8%B4%A1%E7%8C%AE">S.3 其他贡献</h3> 
<p><strong>W 空间的属性解耦</strong>。作者表明，W 空间的的各个属性是局部的（解耦的），即，改变一个属性并不会对其他属性产生影响。这是由于非线性映射器的作用，使 Z 空间中相互纠缠的属性在 W 空间中分离。</p> 
<p>解耦性使得基于 W 空间的插值和图像编辑更容易实现。</p> 
<p>当隐空间 W 充分解耦时，其中相邻的属性（区域）可以使用一个超平面分隔开。“<a class="link-info" href="https://blog.csdn.net/qq_44681809/article/details/129582405" title="Interpreting the Latent Space of GANs for Semantic Face Editing">Interpreting the Latent Space of GANs for Semantic Face Editing</a>” 一文就是对此的进一步研究。 </p> 
<p><strong>感知路径长度</strong>。基于感知的成对图像距离，计算为两个 VGG16 embedding 之间的加权差异，其中权重是合适的，因此度量与人类感知相似性判断一致。 如果将隐空间插值路径细分为线性段，可以将此分段路径的总感知长度定义为每个段的感知差异之和。</p> 
<p>隐空间越扭曲（特征的不同纬度相互耦合），例如 Z 空间，感知路径长度越大，反之亦然，例如 W 空间。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/84c7a3b837761badcd4f1c31a945fbcb/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【Vue基础】Vue路由，实现页面跳转</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1df7148b884a9afd79be4e186b930fbb/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">如何用Python进行屏幕录制？</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>