<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【论文解读】YOLOR: 2021年YOLO系列目标检测的最强王者 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【论文解读】YOLOR: 2021年YOLO系列目标检测的最强王者" />
<meta property="og:description" content="​
图表数据来源：
EfficientDet: https://arxiv.org/pdf/2011.08036.pdf
YOLOv3: https://arxiv.org/pdf/2011.08036.pdf
YOLOv4: https://github.com/AlexeyAB/darknet
YOLOv4-Scaled: https://github.com/WongKinYiu/ScaledYOLOv4
YOLO-PPv2: https://arxiv.org/pdf/2104.10419.pdf
YOLOv5: https://arxiv.org/pdf/2104.10419.pdf
YOLOX: https://github.com/Megvii-BaseDetection/YOLOX
YOLOR: https://github.com/WongKinYiu/yolor
YOLOF: https://arxiv.org/pdf/2103.09460.pdf
YOLOS: https://arxiv.org/pdf/2106.00666.pdf
YOLOP: https://arxiv.org/pdf/2108.11250.pdf
图表统计时间：2021年11月-12月
本篇文章是对目标检测YOLO系列的性能总结，主要介绍了2021年YOLO系列的最高精度YOLOR是怎样炼成的。
YOLOR出自论文You Only Learn One Representation: Unified Network for Multiple Tasks，受人类学习方式（使用五官，通过常规和潜意识学习，总结丰富的经验并编码存储，进而处理已知或未知的信息）的启发，本篇论文提出了一个统一的网络来同时编码显式知识和隐式知识，在网络中执行了kernel space alignment（核空间对齐）、prediction refinement（预测细化）和 multi-task learning（多任务学习），同时对多个任务形成统一的表示。结果表明神经网络中引入隐式知识有助于所有任务的性能提升，进一步的分析发现隐式表示之所以能带来性能提升，是因为其具备了捕获不同任务的物理意义的能力。
paper: https://arxiv.org/abs/2105.04206
code: https://github.com/WongKinYiu/yolor
论文作者 | Kin-Yiu Wong等
一、YOLOR思想动机
图1：人可以根据同一幅输入图像回答不同问题，本文也旨在训练一个单一的神经网络来服务于多个任务。 如图1所示，人可以从多个角度来分析同一个目标，然而通常训练CNN时只给予了一个角度，也就是说针对某一个任务得到的CNN特征很难适用于其他问题。作者认为造成上述问题的原因主要是模型只提取了神经元特征而丢弃了隐式知识的学习运用，然而就像人脑一样隐式知识对分析各种各样的任务是非常有用的。
人类对隐式知识的学习通常通过潜意识，然而并没有系统的定义怎样学习和获得隐式知识。对于神经网络而言，一般将浅层特征定义为显式知识，深层特征定义为隐式知识。本文将直接可观察的知识定义为显式知识，隐藏在神经网络中且无法观察的知识定义为隐式知识。
图2：多目的神经网络架构。（a）不同任务对应不同模型；（b）不同任务共享骨干网络，使用不同的输出头；（c）本文提出的统一网络：融合显式知识和隐式知识的一个表征服务多个任务。 如图2所示，本文提出了一个统一的网络来集成显式知识和隐式知识，通过学习统一的表达，使得各个子表示能够适用于不同任务。基于前人工作的理论基础，本文结合压缩感知和深度学习来构建统一网络。
本文主要贡献如下：
1. 提出了一个可同时完成多种任务的统一网络，它通过融合显式知识和隐式知识学习一个可以完成多个任务的统一表征，提出的网络可以有效的提升模型的表现，仅增加千分之一不到的计算成本；
2. 通过 kernel space alignment（核空间对齐）、prediction refinement（预测细化）和 multi-task learning（多任务学习）来完成隐式知识的学习，并验证了其有效性；
3. 分别讨论了隐式知识的建模方式，包括向量、神经网络、矩阵分解，并验证了这些方式的有效性；
4. 证实了所提出的内隐表征学习方法能够准确地对应于特定的物理特征，并以视觉的方式进行了呈现；还证实了如果算子符合目标的物理意义，它可以用来整合隐式知识和显式知识，并会产生乘数效应；" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/217cc47f80c789f6368594aaa0058a67/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-03-28T21:26:29+08:00" />
<meta property="article:modified_time" content="2022-03-28T21:26:29+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【论文解读】YOLOR: 2021年YOLO系列目标检测的最强王者</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>​</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/f4/c7/NQAzbS1L_o.png"></p> 
<p><strong>图表数据来源：</strong></p> 
<p>EfficientDet: https://arxiv.org/pdf/2011.08036.pdf</p> 
<p>YOLOv3: https://arxiv.org/pdf/2011.08036.pdf</p> 
<p>YOLOv4: https://github.com/AlexeyAB/darknet</p> 
<p>YOLOv4-Scaled: https://github.com/WongKinYiu/ScaledYOLOv4</p> 
<p>YOLO-PPv2: https://arxiv.org/pdf/2104.10419.pdf</p> 
<p>YOLOv5: https://arxiv.org/pdf/2104.10419.pdf</p> 
<p>YOLOX: https://github.com/Megvii-BaseDetection/YOLOX</p> 
<p>YOLOR: https://github.com/WongKinYiu/yolor</p> 
<p>YOLOF: https://arxiv.org/pdf/2103.09460.pdf</p> 
<p>YOLOS: https://arxiv.org/pdf/2106.00666.pdf</p> 
<p>YOLOP: https://arxiv.org/pdf/2108.11250.pdf</p> 
<p><strong>图表统计时间：</strong>2021年11月-12月</p> 
<hr> 
<p>本篇文章是对目标检测YOLO系列的性能总结，主要介绍了<strong>2021年YOLO系列的最高精度YOLOR</strong>是怎样炼成的。</p> 
<p>YOLOR出自论文<strong>You Only Learn One Representation: Unified Network for Multiple Tasks</strong>，受人类学习方式（使用五官，通过常规和潜意识学习，总结丰富的经验并编码存储，进而处理已知或未知的信息）的启发，本篇论文提出了一个统一的网络来同时编码显式知识和隐式知识，在网络中执行了kernel space alignment（核空间对齐）、prediction refinement（预测细化）和 multi-task learning（多任务学习），同时对多个任务形成统一的表示。结果表明神经网络中引入隐式知识有助于所有任务的性能提升，进一步的分析发现隐式表示之所以能带来性能提升，是因为其具备了捕获不同任务的物理意义的能力。</p> 
<p><strong>paper: </strong>https://arxiv.org/abs/2105.04206</p> 
<p><strong>code:</strong> https://github.com/WongKinYiu/yolor</p> 
<p><strong>论文作者 | </strong><strong>Kin-Yiu Wong等</strong></p> 
<hr> 
<p><strong>一、YOLOR思想动机</strong></p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="图1：人可以根据同一幅输入图像回答不同问题，本文也旨在训练一个单一的神经网络来服务于多个任务。" src="https://images2.imgbox.com/fe/af/dhqiXNNm_o.png"> 
  <figcaption>
    图1：人可以根据同一幅输入图像回答不同问题，本文也旨在训练一个单一的神经网络来服务于多个任务。 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<p>如图1所示，<strong>人可以从多个角度来分析同一个目标，然而通常训练CNN时只给予了一个角度，也就是说针对某一个任务得到的CNN特征很难适用于其他问题</strong>。作者认为造成上述问题的原因主要是<strong>模型只提取了神经元特征而丢弃了隐式知识的学习运用</strong>，然而就像人脑一样隐式知识对分析各种各样的任务是非常有用的。</p> 
<p></p> 
<p>人类对隐式知识的学习通常通过潜意识，然而并没有系统的定义怎样学习和获得隐式知识。对于神经网络而言，一般将浅层特征定义为显式知识，深层特征定义为隐式知识。<strong>本文将直接可观察的知识定义为显式知识，隐藏在神经网络中且无法观察的知识定义为隐式知识</strong>。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/9b/19/bHJIocDg_o.png"> 
  <figcaption>
    图2：多目的神经网络架构。（a）不同任务对应不同模型；（b）不同任务共享骨干网络，使用不同的输出头；（c）本文提出的统一网络：融合显式知识和隐式知识的一个表征服务多个任务。 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<p>如图2所示，本文提出了一个统一的网络来集成显式知识和隐式知识，通过学习统一的表达，使得各个子表示能够适用于不同任务。基于前人工作的理论基础，本文结合压缩感知和深度学习来构建统一网络。</p> 
<p></p> 
<p>本文主要贡献如下：</p> 
<p>1. <strong>提出了一个可同时完成多种任务的统一网络，它通过融合显式知识和隐式知识学习一个可以完成多个任务的统一表征</strong>，提出的网络可以有效的提升模型的表现，仅增加千分之一不到的计算成本；</p> 
<p></p> 
<p>2. <strong>通过 kernel space alignment（核空间对齐）、prediction refinement（预测细化）和 multi-task learning（多任务学习）来完成隐式知识的学习</strong>，并验证了其有效性；</p> 
<p></p> 
<p>3. 分别讨论了隐式知识的建模方式，包括向量、神经网络、矩阵分解，并验证了这些方式的有效性；</p> 
<p></p> 
<p>4. <strong>证实了所提出的内隐表征学习方法能够准确地对应于特定的物理特征，并以视觉的方式进行了呈现；还证实了如果算子符合目标的物理意义，它可以用来整合隐式知识和显式知识，并会产生乘数效应</strong>；</p> 
<p></p> 
<p>5. 与SOTA比较，YOLOR能够实现和目标检测Scaled-YOLOv4-P7一样的精度，但是推理速度快了88%。</p> 
<p></p> 
<p><strong>二、隐式知识学习</strong></p> 
<p><strong>2.1 隐式知识如何工作</strong></p> 
<p>隐式表征<img alt="\textbf{z}_{i}" class="mathcode" src="https://images2.imgbox.com/b6/fc/Cqg6GGP5_o.png">是与观察不相关的，它可以是一个常量tensor <img alt="Z = \left\{\textbf{z}_{1},\textbf{z}_{2},...,\textbf{z}_{k} \right\}" class="mathcode" src="https://images2.imgbox.com/e9/30/j9zlRMbj_o.png">。下面将介绍隐式知识是如何作为一个常量tensor在多个任务中作用的。</p> 
<p></p> 
<p><strong>流形空间约简</strong></p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/27/91/WqjzywlV_o.png"> 
  <figcaption>
    图3：流形空间约简 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<p>一个好的表征应该能够在它所属的流形空间中找到一个合适的投影，并有助于后续目标任务的顺利完成。如图3所示，如果在投影空间中利用超平面能成功地对目标类别进行分类，那将是最好的结果。在图3所示例子中，可以利用投影向量的内积和隐式表示来达到降低流形空间维度的目的，并有效地完成各种任务。</p> 
<p></p> 
<p><strong>核空间对齐</strong></p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/1e/17/cWzEDvVX_o.png"> 
  <figcaption>
    图4：核空间对齐 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<p>在多任务和多头神经网络中，核空间不对齐是经常发生的问题，图4（a）展示了一个多任务多头神经网络核空间不对齐的例子。为了解决这个问题，如图4（b）所示，可以对输出特征和隐式表征进行加法和乘法运算，这样就可以对核空间进行变换、旋转和缩放，以对齐神经网络的每个输出核空间。该方法广泛用于多个领域，比如说FPN中大目标与小目标的特征对齐、知识蒸馏中大模型与小模型的整合、处理zero-shot域迁移等问题。</p> 
<p></p> 
<p><strong>更多功能和处理方式</strong></p> 
<p></p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/ad/2b/6lehQJx3_o.png"> 
  <figcaption>
    图5：更多功能和处理方式 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<p>除了可以应用于不同任务的功能外，隐式知识还可以扩展为更多的功能。如图5所示，通过引入加法，可以使神经网络预测中心坐标的偏移；还可以引入乘法来自动搜索锚框的超参数集，这是基于锚框的目标检测器经常需要的；此外，可以分别使用点乘和concat来执行多任务特征选择并为后续计算设置前提条件。</p> 
<p></p> 
<p><strong>2.2 隐式知识统一网络建模</strong></p> 
<p>本节通过比较卷积网络和所提的统一网络，解释为什么在训练多任务网络中引入隐式知识是重要的，并详述实现细节。</p> 
<p></p> 
<p><strong>隐式知识的表示</strong></p> 
<p><strong>Conventional Networks:</strong></p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/4b/6f/SsNJpmA7_o.png"> 
  <figcaption>
    图6：建模误差项 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<p>传统神经网络可以由以下公式（1）表示，其中 <img alt="y" class="mathcode" src="https://images2.imgbox.com/22/83/RbyjNPKx_o.png"> 为目标，<img alt="x" class="mathcode" src="https://images2.imgbox.com/b5/92/dbLnZNwJ_o.png"> 为观测量，<img alt="f_{\theta}" class="mathcode" src="https://images2.imgbox.com/b2/74/2tosNUJI_o.png"> 表示神经网络操作，<img alt="\theta" class="mathcode" src="https://images2.imgbox.com/a2/ec/rfWXBznS_o.png"> 是神经网络学习的参数，<img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/eb/49/ItzDfWsP_o.png"> 是误差项： </p> 
<p><img alt="" height="156" src="https://images2.imgbox.com/92/ec/XF5D13PF_o.png" width="1134"></p> 
<p> 训练过程中最小化 <img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/62/a4/RBxLiLiT_o.png">，这表示我们期望同一目标的不同观测值是 <img alt="f_{\theta}" class="mathcode" src="https://images2.imgbox.com/56/14/5CgTMUVf_o.png">  所得到的子空间中的一个点，如图6（a）所示。换言之，我们期望得到的解空间只对当前任务 <img alt="t_{i}" class="mathcode" src="https://images2.imgbox.com/0d/51/t3sQFux1_o.png"> 有区别，对各种潜在任务中除 <img alt="t_{i}" class="mathcode" src="https://images2.imgbox.com/3e/f0/ppbpbM1z_o.png"> 以外的任务是不变的，其中 <img alt="T = \left\{ t_{1} , t_{2} ,..., t_{n} \right\}" class="mathcode" src="https://images2.imgbox.com/2c/79/huLyOygr_o.png"> 。（<strong>解释：以图6（a）为例，不同颜色的圆形解空间只对不同颜色的形状任务 <img alt="T = \left\{ t_{green-square}, t_{blue-triangle}, t_{yellow-diamond} \right\}" class="mathcode" src="https://images2.imgbox.com/ff/60/3vuGoDiv_o.png"> 有变化，比如绿色方形对应绿色圆形，而蓝色三角形对应结果就变化成了蓝色圆形，但是对绿色方形类内的不同观测值（潜在任务），如位置的变化，其对应的解空间是不变的。</strong>）  </p> 
<p></p> 
<p>对于更通用的神经网络，我们希望获得的表征可以服务于其他所有属于 <img alt="T" class="mathcode" src="https://images2.imgbox.com/7f/41/eUjBjVDd_o.png"> 的任务（既各种潜在任务），因此需要松弛 <img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/a3/e2/amI3stuf_o.png"> ，以便能够在流形空间上同时找到每个任务的解，如图6（b）所示。然而，上述要求使得我们不可能用简单的数学方法，如一个热独向量的最大值或欧氏距离的阈值来求解 <img alt="t_{i}" class="mathcode" src="https://images2.imgbox.com/f2/d8/Db3x8dS6_o.png"> 。为了解决这个问题，我们必须对错误项进行建模，以便为不同的任务找到解决方案，如图6（c）所示。</p> 
<p></p> 
<p><strong>Unified Networks:</strong></p> 
<p>为了训练所提出的统一网络，作者将显式知识和隐式知识结合起来对误差项进行建模，然后用它来指导多用途网络的训练过程，训练公式如下：   </p> 
<p><img alt="" height="158" src="https://images2.imgbox.com/11/89/yBFVip6P_o.png" width="1146"></p> 
<p>其中 <img alt="\epsilon_{ex}" class="mathcode" src="https://images2.imgbox.com/00/3e/kNLlMzPe_o.png"> 和 <img alt="\epsilon_{im}" class="mathcode" src="https://images2.imgbox.com/b9/af/72W5qSk0_o.png"> 分别建模来自观察量 <img alt="x" class="mathcode" src="https://images2.imgbox.com/bf/52/ohTwMrx4_o.png"> 的显式误差和来自隐编码 <img alt="z" class="mathcode" src="https://images2.imgbox.com/0d/2b/5SUIxMrV_o.png"> 的隐式误差，<img alt="g_{\phi}" class="mathcode" src="https://images2.imgbox.com/65/f7/D5rcK5P1_o.png">  这是一个特定于任务的操作，用于从显式知识和隐式知识中组合或选择信息。</p> 
<p></p> 
<p>有一些现有的方法来整合显性知识到 <img alt="f_{\theta}" class="mathcode" src="https://images2.imgbox.com/c0/3f/47JEkCAr_o.png"> ，所以将公式（2）重写为公式（3）：</p> 
<p><img alt="" height="110" src="https://images2.imgbox.com/84/ce/3KVwGbOk_o.png" width="1140"></p> 
<p> 其中 <img alt="\star" class="mathcode" src="https://images2.imgbox.com/ec/57/JFQq1F1g_o.png"> 表示一些可以融合 <img alt="f_{\theta}" class="mathcode" src="https://images2.imgbox.com/37/b9/0F5pE8kO_o.png"> 和 <img alt="g_{\phi}" class="mathcode" src="https://images2.imgbox.com/37/28/N02NXzbM_o.png"> 的操作，本文所使用的操作包括相加、相乘、concat。</p> 
<p></p> 
<p>如果把误差项的推导过程推广到处理多个任务，可以得到如下等式：</p> 
<p><img alt="" height="120" src="https://images2.imgbox.com/cb/53/BGM9JA4j_o.png" width="1140"></p> 
<p> 其中 <img alt="\textbf{Z} = \left\{\textbf{z}_{1}, \textbf{z}_{2}, ... , \textbf{z}_{T},\right\}" class="mathcode" src="https://images2.imgbox.com/3f/42/V6U7pG9Y_o.png"> 是一个用于不同任务 <img alt="T" class="mathcode" src="https://images2.imgbox.com/42/ca/gX5c74oE_o.png"> 的隐式潜编码集合， <img alt="\Phi" class="mathcode" src="https://images2.imgbox.com/c3/d7/0EenI0UX_o.png">  是用于从 <img alt="Z" class="mathcode" src="https://images2.imgbox.com/c0/bb/fjHzacuT_o.png"> 生成隐式表征的参数，<img alt="\Psi" class="mathcode" src="https://images2.imgbox.com/08/e0/DAmqOim8_o.png">  是用来从显示表征和隐式表征的不同组合计算最终的输出参数。</p> 
<p></p> 
<p>对于不同的任务，可以使用以下公式获得所有 <img alt="\textbf{z} \in \textbf{Z}" class="mathcode" src="https://images2.imgbox.com/7f/b8/e6SOGM2a_o.png"> 的预测： </p> 
<p><img alt="" height="120" src="https://images2.imgbox.com/73/1b/yPl6NGxK_o.png" width="1146"> </p> 
<p>对于所有任务，以一个共同统一的表征 <img alt="f_{\theta}\left(\textbf{x}\right)" class="mathcode" src="https://images2.imgbox.com/99/74/lYsNWwJ2_o.png"> 开始，然后进入基于任务而不同的隐式表征 <img alt="g_{\Phi}\left(\textbf{z}\right)" class="mathcode" src="https://images2.imgbox.com/2f/1a/fq1j1VjM_o.png"> ，最后用任务特定的判别器 <img alt="d_{\Psi}" class="mathcode" src="https://images2.imgbox.com/a0/29/66NMQuNj_o.png"> 完成不同的任务。</p> 
<p></p> 
<p><strong><strong>隐式知识的建模</strong></strong></p> 
<p>本文提出的隐式知识可用图7所示几种方式建模：</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/3c/be/JUmPk99x_o.png"> 
  <figcaption>
    图7：本文提出的3种建模隐式知识的方式，第一行显示了这三种不同建模方法的形成过程，第二行显示了它们对应的数学属性。（a）向量：单基，每个维度彼此独立；（b）神经网络：单基或多基，维度之间相互关联；（c）矩阵分解：多基，每个维数彼此独立。 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<p><strong>向量/矩阵/张量 </strong></p> 
<p><img alt="" height="122" src="https://images2.imgbox.com/81/64/d05j6X6M_o.png" width="1124"></p> 
<p>如图7（a）利用向量/矩阵/张量 <img alt="z" class="mathcode" src="https://images2.imgbox.com/45/94/dDPshwCL_o.png"> 直接作为隐式知识的先验，直接作为隐式表示。此时，必须假设每个维度彼此独立。</p> 
<p></p> 
<p><strong>神经网络 </strong></p> 
<p><img alt="" height="128" src="https://images2.imgbox.com/9d/5e/ABKKLE6k_o.png" width="1136"></p> 
<p>如图7（b）利用向量/矩阵/张量 <img alt="z" class="mathcode" src="https://images2.imgbox.com/4d/3b/v9pKKQEC_o.png"> 作为隐式知识的先验，然后利用权重矩阵 <img alt="W" class="mathcode" src="https://images2.imgbox.com/5b/07/qv0CwpYU_o.png"> 进行线性或非线性组合形成隐式表示。此时，必须假设每个维度相互依赖。也可以使用更复杂的神经网络来生成隐式表示。或者用马尔可夫链来模拟不同任务之间隐式表示的相关性。</p> 
<p></p> 
<p><strong>矩阵分解 </strong></p> 
<p><img alt="" height="134" src="https://images2.imgbox.com/a0/a6/8YtZ0Cjl_o.png" width="1160"></p> 
<p> 如图7（c）利用多个向量/矩阵/张量作为隐式知识的先验，这些隐式先验基 <img alt="Z" class="mathcode" src="https://images2.imgbox.com/6f/6b/AtTv2txa_o.png"> 和系数 <img alt="c" class="mathcode" src="https://images2.imgbox.com/49/e8/tA8ADRw7_o.png"> 构成隐式表示。还可以进一步对 <img alt="c" class="mathcode" src="https://images2.imgbox.com/98/0f/x3ilYgmO_o.png"> 进行稀疏约束，将其转化为稀疏表示形式。此外，还可以对 <img alt="Z" class="mathcode" src="https://images2.imgbox.com/5b/0e/0H2fkUbY_o.png"> 和 <img alt="c" class="mathcode" src="https://images2.imgbox.com/d5/0f/QYWfqV7S_o.png"> 施加非负约束，将它们转化为非负矩阵分解（ <img alt="NMF" class="mathcode" src="https://images2.imgbox.com/5d/ad/Z7rD1xq0_o.png"> ）形式。</p> 
<p></p> 
<p><strong>训练</strong></p> 
<p>假设模型没有任何的先验隐式知识，既隐式知识对显式表示 <img alt="f_{\theta}\left(\textbf{x}\right)" class="mathcode" src="https://images2.imgbox.com/f6/c4/vRF9WyXZ_o.png"> 没有任何影响。当融合操作为相加和concat时，初始化隐式先验 <img alt="\textbf{z} \sim N\left(0, \sigma \right)" class="mathcode" src="https://images2.imgbox.com/7e/1c/7kp2vBFE_o.png"> ，如果融合方式为相乘，则初始化为 <img alt="\textbf{z} \sim N\left(1, \sigma \right)" class="mathcode" src="https://images2.imgbox.com/da/6d/ihmtkAZC_o.png"> ，其中 <img alt="\sigma" class="mathcode" src="https://images2.imgbox.com/a1/4b/lHyO66AI_o.png"> 是一个接近0的很小的值，<img alt="z" class="mathcode" src="https://images2.imgbox.com/3c/c7/3GFb26GW_o.png">  和 <img alt="\Phi" class="mathcode" src="https://images2.imgbox.com/e9/c9/Yq5AHSbL_o.png"> 都通过反向传播算法进行训练更新。</p> 
<p></p> 
<p><strong>推理</strong></p> 
<p>由于隐式知识与观察量 <img alt="x" class="mathcode" src="https://images2.imgbox.com/82/15/jdXLSQLL_o.png"> 无关，无论内隐模型 <img alt="g_{\Phi}" class="mathcode" src="https://images2.imgbox.com/89/64/7kp8stmg_o.png"> 有多复杂，在执行推理阶段之前，它都可以被简化为一组常数张量，也就是说隐式信息不会影响算法的计算复杂度。当隐式操作是通过相乘进行，如果后续层是卷积层，本文使用公式（9）进行整合操作；当操作是相加，如果前面的层是卷积层且没有激活函数，则使用公式（10）进行整合。 </p> 
<p><img alt="" height="314" src="https://images2.imgbox.com/4b/f3/l84Ja4vZ_o.png" width="1200"></p> 
<p> </p> 
<p><strong>三、实验</strong></p> 
<p><strong>3.1 实验设置</strong></p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/07/b1/0BPV0TIs_o.png"> 
  <figcaption>
    图8：统一网络模型结构 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<p>本文通过FPN中的feature alignment（特征对齐）、目标检测中的prediction refinement（预测细化）、单模型中的multi-task learning（多任务学习）来应用implicit knowledge（隐式知识）（注：本文的多任务学习指特征嵌入、多标签图像分类和目标检测）。使用YOLOV4-CSP作为baseline model，隐式知识添加位置如图8所示，所有训练超参数与Scaled-YOLOv4一致。</p> 
<p></p> 
<p><strong><strong>3.2 FPN特征对齐</strong></strong></p> 
<p>使用简单的向量隐式表征和加法算子，在FPN的每一个特征映射层添加隐式知识进行特征对齐，各个指标获得到了有意义的提升，如表1所示。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/d8/4e/liuvKIvg_o.png"> 
  <figcaption>
    表1：特征对齐消融研究 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<p><strong><strong>3.3 目标检测预测细化</strong></strong></p> 
<p>使用简单的向量隐式表征和加法算子，在YOLO的每一个输出层添加隐式知识进行预测细化，大部分指标都获得到了一定的增益，如表2所示。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/18/52/qcgxdEZW_o.png"> 
  <figcaption>
    表2：预测细化消融研究 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<p>图9展示了隐式表征的引入如何影响检测结果（注：论文中对如何影响的检测结果并么有做进一步解释）。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/6e/52/kdfOBe5n_o.png"> 
  <figcaption>
    图9：预测细化学习到的隐式表征的值 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<p><strong><strong>3.4 多任务规范表征</strong></strong></p> 
<p>当需要同时训练一个被多个任务共享的模型时，由于损失函数的联合优化过程是必须执行的，因此在执行过程中往往会出现多方相互拉动的情况，这种情况将导致最终的整体性能比单独训练多个模型然后集成它们要差。为了解决这个问题，作者提出为训练多任务训练一个规范的表征，通过给每个任务分支引入隐式表征增强表征能力，表3展示了使用简单的向量隐式表征和加法算子进行不同联合训练方式的结果，  （检测和特征嵌入联合训练，引入加法隐式表征）取得了最好的对比结果。</p> 
<p></p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/12/28/Qn2242M0_o.png"> 
  <figcaption>
    表3：多任务联合学习消融研究 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<p><strong><strong>3.5 隐式知识建模不同算子比较</strong></strong></p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/75/da/a8Rep7D7_o.png"> 
  <figcaption>
    图10：隐式建模算子：（b）相加（c）相乘（d）串联 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<p>表4显示了图10中不同算子融合显式表征与隐式表征的结果。</p> 
<p></p> 
<p>在特征对齐实验中，相加与串联（concat）操作能够提升性能表现，相乘有所下降。特征对齐的实验结果完全符合其物理特性，因为它必须处理全局偏移和所有单个簇的缩放。</p> 
<p></p> 
<p>在预测细化实验中，由于concat会增加输出维度，所以只比较相加与相乘的效果，在这里相乘的效果更好。这是由于中心偏移在执行预测时使用加法解码，而锚框尺度使用乘法解码，而中心坐标是以网格为界的，影响较小，但人工设置的锚框具有较大的优化空间，因此改进更为显著。</p> 
<p></p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/4b/e3/dDVBWfQN_o.png"> 
  <figcaption>
    表4：不同算子消融研究 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<p>基于上面的分析，作者设计了另外两个实验。在第一个实验中，作者通过锚框聚类来划分特征空间，并执行相乘细化，第二个实验中，作者只在width和height上执行相乘细化。结果如表5所示，发现经过相应的修改，各项指标都得到了提高。实验表明，在设计显式知识与隐式知识的结合时，首先要考虑结合层的物理意义，以达到更好的效果。</p> 
<p></p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/96/84/fTBtzeBF_o.png"> 
  <figcaption>
    表5：不同算子消融研究 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<p><strong><strong>3.6 隐式知识建模不同方式比较</strong></strong></p> 
<p>本文尝试了向量、神经网络和矩阵分解三种建模隐式知识的方式，发现三种建模方式都带来了不同程度的性能提升，其中矩阵分解效果最好，不同建模隐式知识的潜力也值得进一步挖掘。</p> 
<p></p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/5f/c1/93VBTzH8_o.png"> 
  <figcaption>
    表6：不同建模方式消融研究 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<p><strong><strong>3.7 隐式知识模型分析</strong></strong></p> 
<p>如表7和图11所示，引入隐式知识，仅增加不到万分一的参数量和计算量，模型性能得到有意义的提升，同时收敛更快。</p> 
<p></p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/b2/33/MPlC51QY_o.png"> 
  <figcaption>
    表7：模型信息比较 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/2b/99/syoj6lmK_o.png"> 
  <figcaption>
    图11：学习曲线比较 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<p></p> 
<p><strong><strong>3.8 隐式知识提升<strong><strong>目标检测</strong></strong></strong></strong></p> 
<p>按照Scaled-YOLOv4训练过程，先从头训练 300 epochs，然后微调150 epochs，表8展示了目标检测中引入隐式知识的优势。表9与SOTA方法进行了比较，值得注意的是<strong>YOLOR并没有增加额外的数据和标注做训练，只通过引入隐式知识的统一网络，YOLOR不仅达到了足可以和SOTA方法比拟的结果，而且速度非常快</strong>。</p> 
<p></p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/73/b3/t3GSPvjb_o.png"> 
  <figcaption>
    表8：隐式知识增益 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<p></p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/9d/01/ir6zsPnZ_o.png"> 
  <figcaption>
    表9：SOTA比较 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<p><strong>四、总结</strong></p> 
<p>本文介绍了如何构造一个隐式知识与显示知识相结合的统一网络，并通过目标检测YOLOR证明了它在单模型结构下对多任务学习的有效性。在未来，作者计划将把训练扩展到多模型和多任务，如图12所示。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/bd/ca/4WiaoPeL_o.png"> 
  <figcaption>
    图12：多任务多模型统一网络 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<p></p> 
<p><strong>五、点评</strong></p> 
<p>1. 本文借鉴人学习知识的方式，提出了神经网络的显式知识学习和隐式知识学习，视角还是比较新颖的。</p> 
<p>2. 隐式知识学习的实现方式巧妙的使用了神经网络的一些常规操作，但其实现方式是否真正达到了隐式知识学习的构想，虽有一定的实验论证，但值得更进一步的挖掘探讨。</p> 
<p>3. 引入隐式知识的YOLOR取得了精度的提升，但论文中提升的精度还是比较有限的，但从其速度翻倍的角度应该也某种程度上体现了所提方法的有效性吧，另外下面这幅对比图中的精度top3来自github中的实现，还并未体现在论文中，期待作者后续的更多工作。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/e7/fc/zv9bMOY2_o.png"></p> 
<p>如果说2021年YOLO系列的最强王者是YOLOR，那么2022又会出现哪些挑战者，带来哪些精彩的工作，是否会有你的身影（致敬YOLO留给变体的命名机会已经不多了），让我们翘首以盼拭目以待！</p> 
<p></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/71c6a4a7c1a1f37ef63bd7a8e88209eb/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">STC8单片机PCA输出SPWM波形</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/15b0b52fc59b9f0117cdae1ff78bfa75/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">java 多线程入门</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>