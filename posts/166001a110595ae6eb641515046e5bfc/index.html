<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>å¤§è¯­è¨€æ¨¡å‹ä¹‹åä¸‰ LLama2ä¸­æ–‡æ¨ç† - ç¼–ç¨‹éšæƒ³</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="å¤§è¯­è¨€æ¨¡å‹ä¹‹åä¸‰ LLama2ä¸­æ–‡æ¨ç†" />
<meta property="og:description" content="åœ¨ã€Šå¤§è¯­è¨€æ¨¡å‹ä¹‹åäºŒ SentencePieceæ‰©å……LLama2ä¸­æ–‡è¯æ±‡ã€‹ä¸€æ–‡ä¸­å·²ç»æ‰©å……å¥½äº†ä¸­æ–‡è¯æ±‡è¡¨ï¼Œæ¥ä¸‹æ¥å°±æ˜¯ä½¿ç”¨æ•´ç†çš„ä¸­æ–‡è¯­æ–™å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒäº†ã€‚è¿™é‡Œå…ˆè·³è¿‡é¢„è®­ç»ƒç¯èŠ‚ã€‚å…ˆè¯•ç”¨å·²ç»è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œçœ‹çœ‹å¦‚ä½•æ¨ç†ã€‚
åˆå¹¶æ¨¡å‹ è¿™ä¸€æ­¥éª¤ä¼šåˆå¹¶LoRAæƒé‡ï¼Œç”Ÿæˆå…¨é‡æ¨¡å‹æƒé‡ã€‚æ­¤å¤„å¯ä»¥é€‰æ‹©è¾“å‡ºPyTorchç‰ˆæœ¬æƒé‡ï¼ˆ.pthæ–‡ä»¶ï¼‰æˆ–è€…è¾“å‡ºHuggingFaceç‰ˆæœ¬æƒé‡ï¼ˆ.binæ–‡ä»¶ï¼‰ã€‚æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
$ python scripts/merge_llama2_with_chinese_lora_low_mem.py \ --base_model path_to_original_llama2_hf_dir \ --lora_model path_to_chinese_llama2_or_alpaca2_lora \ --output_type huggingface \ --output_dir path_to_output_dir å‚æ•°è¯´æ˜ï¼š
â€“base_modelï¼šå­˜æ”¾HFæ ¼å¼çš„Llama-2æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶çš„ç›®å½•ï¼Œè¿™å¯ä»¥åœ¨ã€Šå¤§è¯­è¨€æ¨¡å‹ä¹‹åäºŒ SentencePieceæ‰©å……LLama2ä¸­æ–‡è¯æ±‡ã€‹çš„1.ä¸‹è½½åŸç‰ˆLLama-2æ¨¡å‹å°èŠ‚æ‰¾åˆ°å¦‚ä½•å°†åŸå§‹metaçš„LlaMA-2æ¨¡å‹è½¬ä¸ºHuggingfaceçš„æ ¼å¼ã€‚â€“lora_modelï¼šä¸­æ–‡LLaMA-2/Alpaca-2 LoRAè§£å‹åæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨ğŸ¤—Model Hubæ¨¡å‹è°ƒç”¨åç§°ï¼ˆä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰ï¼Œè¿™é‡Œä½¿ç”¨Chinese-LLaMA-Alpaca-2ç»™å‡ºçš„é¢„è®­ç»ƒå¥½çš„7Bæ¨¡å‹ã€‚â€“output_typeï¼šæŒ‡å®šè¾“å‡ºæ ¼å¼ï¼Œå¯ä¸ºpthæˆ–huggingfaceã€‚è‹¥ä¸æŒ‡å®šï¼Œé»˜è®¤ä¸ºhuggingfaceâ€“output_dirï¼šæŒ‡å®šä¿å­˜å…¨é‡æ¨¡å‹æƒé‡çš„ç›®å½•ï¼Œé»˜è®¤ä¸º./ï¼ˆå¯é€‰ï¼‰â€“verboseï¼šæ˜¾ç¤ºåˆå¹¶è¿‡ç¨‹ä¸­çš„è¯¦ç»†ä¿¡æ¯
è½¬æ¢å¥½æ ¼å¼ä¹‹åï¼Œå†…å®¹å¦‚ä¸‹ï¼ˆæ—¶é—´æˆ³ä¸º11ï¼š28çš„å³ä¸ºè½¬æ¢ç”Ÿæˆæ–‡ä»¶)ï¼š
å…¶ä¸­çš„ggmlå¼€å¤´çš„äº‹é‡åŒ–æ–‡ä»¶æ˜¯ç”¨äºæ¨¡å‹æ¨ç†ã€‚ æ¨ç† åœ¨attn_and_long_ctx_patches.pyå®ç°äº†åŸºäºNTKçš„è‡ªé€‚åº”ä¸Šä¸‹æ–‡é€‚é…æ–¹æ³•ï¼Œå…¶ä¸­åŸºäºtransformersçš„æ¨ç†è„šæœ¬ã€‚
å½“ä¸Šä¸‹æ–‡å°äº4Kæ—¶ï¼Œé»˜è®¤å…³é—­ï¼Œå› ä¸ºåŸç”Ÿçš„æ•ˆæœæ›´å¥½å¤§äº4Kæ—¶å¼€å¯NTKï¼ŒAUTO_COEFFé»˜è®¤ä¸º1.0
ä»¥ä¸‹æ˜¯ä¸åŒAUTO_COEFFä¸‹ï¼Œåœ¨ä¸åŒä¸Šä¸‹æ–‡é•¿åº¦ä¸Šçš„PPLå˜åŒ–ï¼ˆè¶Šä½è¶Šå¥½ï¼‰ï¼Œä¾›ä½¿ç”¨å‚è€ƒã€‚
å¯¹NTKæ–¹æ³•ç†Ÿæ‚‰çš„ç”¨æˆ·å¯ç›´æ¥ä¿®æ”¹ä»£ç ä¸­çš„ALPHAå–å€¼ã€‚12Kä»¥ä¸‹ï¼šå‡ ä¹å’ŒåŸç”Ÿ4Kçš„PPLæ²¡æœ‰æ˜¾è‘—å·®å¼‚12K-16Kï¼šå¼€å§‹å­˜åœ¨ä¸€å®šæŸå¤±ï¼Œå¤§çº¦æ˜¯3æ¯”ç‰¹é‡åŒ–çº§åˆ«çš„æ•ˆæœ18K&#43;ï¼šå­˜åœ¨è¾ƒå¤§æŸå¤±ï¼Œå¤§çº¦æ˜¯2æ¯”ç‰¹é‡åŒ–çº§åˆ«æ•ˆæœï¼Œ20K&#43;ä¸å¯ç”¨
ä»¥ä¸Šç»“æœä»…ä¾›å‚è€ƒï¼Œåº”åœ¨å®é™…åœºæ™¯ä¸­æµ‹è¯•è°ƒæ•´AUTO_COEFFæˆ–è€…ALPHAå–å€¼ã€‚ ä½¿ç”¨llama.cppæ¨ç† Step 1: å…‹éš†å’Œç¼–è¯‘llama.cpp
ï¼ˆå¯é€‰ï¼‰å¦‚æœå·²ä¸‹è½½æ—§ç‰ˆä»“åº“ï¼Œå»ºè®®git pullæ‹‰å–æœ€æ–°ä»£ç ï¼Œå¹¶æ‰§è¡Œmake cleanè¿›è¡Œæ¸…ç†æ‹‰å–æœ€æ–°ç‰ˆllama.cppä»“åº“ä»£ç  $ git clone https://github.com/ggerganov/llama.cpp å¯¹llama.cppé¡¹ç›®è¿›è¡Œç¼–è¯‘ï¼Œç”Ÿæˆ./mainï¼ˆç”¨äºæ¨ç†ï¼‰å’Œ./quantizeï¼ˆç”¨äºé‡åŒ–ï¼‰äºŒè¿›åˆ¶æ–‡ä»¶ã€‚ $ make Step 2: ç”Ÿæˆé‡åŒ–ç‰ˆæœ¬æ¨¡å‹
ç›®å‰llama.cppå·²æ”¯æŒ.pthæ–‡ä»¶ä»¥åŠhuggingfaceæ ¼å¼.binçš„è½¬æ¢ã€‚å°†å®Œæ•´æ¨¡å‹æƒé‡è½¬æ¢ä¸ºGGMLçš„FP16æ ¼å¼ï¼Œç”Ÿæˆæ–‡ä»¶è·¯å¾„ä¸ºzh-models/7B/ggml-model-f16.ggufã€‚è¿›ä¸€æ­¥å¯¹FP16æ¨¡å‹è¿›è¡Œ4-bité‡åŒ–ï¼Œç”Ÿæˆé‡åŒ–æ¨¡å‹æ–‡ä»¶è·¯å¾„ä¸ºzh-models/7B/ggml-model-q4_0.ggufã€‚ä¸åŒé‡åŒ–æ–¹æ³•çš„æ€§èƒ½å¯¹æ¯”è§æœ¬Wikiæœ€åéƒ¨åˆ†ã€‚
python3 convert.py ../merged_chinese_llama_7b $ ./quantize ../merged_chinese_llama_7b/ggml-model-f16.gguf ../merged_chinese_llama_7b/ggml-model-q4_0.gguf q4_0 Step 3: åŠ è½½å¹¶å¯åŠ¨æ¨¡å‹
llama.cpp git:(master) âœ— ./main -s 1 -m ../merged_chinese_llama_7b/ggml-model-q4_0.gguf -p &#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/166001a110595ae6eb641515046e5bfc/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-09-28T23:00:00+08:00" />
<meta property="article:modified_time" content="2023-09-28T23:00:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="ç¼–ç¨‹éšæƒ³" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">ç¼–ç¨‹éšæƒ³</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">å¤§è¯­è¨€æ¨¡å‹ä¹‹åä¸‰ LLama2ä¸­æ–‡æ¨ç†</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>åœ¨ã€Š<a href="https://blog.csdn.net/shichaog/article/details/133325234">å¤§è¯­è¨€æ¨¡å‹ä¹‹åäºŒ SentencePieceæ‰©å……LLama2ä¸­æ–‡è¯æ±‡</a>ã€‹ä¸€æ–‡ä¸­å·²ç»æ‰©å……å¥½äº†ä¸­æ–‡è¯æ±‡è¡¨ï¼Œæ¥ä¸‹æ¥å°±æ˜¯ä½¿ç”¨æ•´ç†çš„ä¸­æ–‡è¯­æ–™å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒäº†ã€‚è¿™é‡Œå…ˆè·³è¿‡é¢„è®­ç»ƒç¯èŠ‚ã€‚å…ˆè¯•ç”¨å·²ç»è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œçœ‹çœ‹å¦‚ä½•æ¨ç†ã€‚</p> 
<h3><a id="_1"></a>åˆå¹¶æ¨¡å‹</h3> 
<p>è¿™ä¸€æ­¥éª¤ä¼šåˆå¹¶LoRAæƒé‡ï¼Œç”Ÿæˆå…¨é‡æ¨¡å‹æƒé‡ã€‚æ­¤å¤„å¯ä»¥é€‰æ‹©è¾“å‡ºPyTorchç‰ˆæœ¬æƒé‡ï¼ˆ.pthæ–‡ä»¶ï¼‰æˆ–è€…è¾“å‡ºHuggingFaceç‰ˆæœ¬æƒé‡ï¼ˆ.binæ–‡ä»¶ï¼‰ã€‚æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š</p> 
<pre><code class="prism language-shell">$ python scripts/merge_llama2_with_chinese_lora_low_mem.py <span class="token punctuation">\</span>
    <span class="token parameter variable">--base_model</span> path_to_original_llama2_hf_dir <span class="token punctuation">\</span>
    <span class="token parameter variable">--lora_model</span> path_to_chinese_llama2_or_alpaca2_lora <span class="token punctuation">\</span>
    <span class="token parameter variable">--output_type</span> huggingface <span class="token punctuation">\</span>
    <span class="token parameter variable">--output_dir</span> path_to_output_dir 
</code></pre> 
<p>å‚æ•°è¯´æ˜ï¼š</p> 
<ul><li>â€“base_modelï¼šå­˜æ”¾HFæ ¼å¼çš„Llama-2æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶çš„ç›®å½•ï¼Œè¿™å¯ä»¥åœ¨ã€Šå¤§è¯­è¨€æ¨¡å‹ä¹‹åäºŒ SentencePieceæ‰©å……LLama2ä¸­æ–‡è¯æ±‡ã€‹çš„1.ä¸‹è½½åŸç‰ˆLLama-2æ¨¡å‹å°èŠ‚æ‰¾åˆ°å¦‚ä½•å°†åŸå§‹metaçš„LlaMA-2æ¨¡å‹è½¬ä¸ºHuggingfaceçš„æ ¼å¼ã€‚</li><li>â€“lora_modelï¼šä¸­æ–‡LLaMA-2/Alpaca-2 LoRAè§£å‹åæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨ğŸ¤—Model Hubæ¨¡å‹è°ƒç”¨åç§°ï¼ˆä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰ï¼Œè¿™é‡Œä½¿ç”¨<a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2">Chinese-LLaMA-Alpaca-2</a>ç»™å‡ºçš„é¢„è®­ç»ƒå¥½çš„7Bæ¨¡å‹ã€‚</li><li>â€“output_typeï¼šæŒ‡å®šè¾“å‡ºæ ¼å¼ï¼Œå¯ä¸ºpthæˆ–huggingfaceã€‚è‹¥ä¸æŒ‡å®šï¼Œé»˜è®¤ä¸ºhuggingface</li><li>â€“output_dirï¼šæŒ‡å®šä¿å­˜å…¨é‡æ¨¡å‹æƒé‡çš„ç›®å½•ï¼Œé»˜è®¤ä¸º./</li><li>ï¼ˆå¯é€‰ï¼‰â€“verboseï¼šæ˜¾ç¤ºåˆå¹¶è¿‡ç¨‹ä¸­çš„è¯¦ç»†ä¿¡æ¯<br> <img src="https://images2.imgbox.com/df/5c/YUOD4FIG_o.png" alt="è¯·æ·»åŠ å›¾ç‰‡æè¿°"><br> è½¬æ¢å¥½æ ¼å¼ä¹‹åï¼Œå†…å®¹å¦‚ä¸‹ï¼ˆæ—¶é—´æˆ³ä¸º11ï¼š28çš„å³ä¸ºè½¬æ¢ç”Ÿæˆæ–‡ä»¶)ï¼š<br> <img src="https://images2.imgbox.com/1e/fe/VeB9YCpf_o.png" alt="è¯·æ·»åŠ å›¾ç‰‡æè¿°"><br> å…¶ä¸­çš„ggmlå¼€å¤´çš„äº‹é‡åŒ–æ–‡ä»¶æ˜¯ç”¨äºæ¨¡å‹æ¨ç†ã€‚</li></ul> 
<h3><a id="_22"></a>æ¨ç†</h3> 
<p>åœ¨<a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/blob/main/scripts/attn_and_long_ctx_patches.py">attn_and_long_ctx_patches.py</a>å®ç°äº†åŸºäºNTKçš„è‡ªé€‚åº”ä¸Šä¸‹æ–‡é€‚é…æ–¹æ³•ï¼Œå…¶ä¸­åŸºäºtransformersçš„æ¨ç†è„šæœ¬ã€‚</p> 
<ul><li>å½“ä¸Šä¸‹æ–‡å°äº4Kæ—¶ï¼Œé»˜è®¤å…³é—­ï¼Œå› ä¸ºåŸç”Ÿçš„æ•ˆæœæ›´å¥½</li><li>å¤§äº4Kæ—¶å¼€å¯NTKï¼ŒAUTO_COEFFé»˜è®¤ä¸º1.0<br> ä»¥ä¸‹æ˜¯ä¸åŒAUTO_COEFFä¸‹ï¼Œåœ¨ä¸åŒä¸Šä¸‹æ–‡é•¿åº¦ä¸Šçš„PPLå˜åŒ–ï¼ˆè¶Šä½è¶Šå¥½ï¼‰ï¼Œä¾›ä½¿ç”¨å‚è€ƒã€‚<br> å¯¹NTKæ–¹æ³•ç†Ÿæ‚‰çš„ç”¨æˆ·å¯ç›´æ¥ä¿®æ”¹ä»£ç ä¸­çš„ALPHAå–å€¼ã€‚</li><li>12Kä»¥ä¸‹ï¼šå‡ ä¹å’ŒåŸç”Ÿ4Kçš„PPLæ²¡æœ‰æ˜¾è‘—å·®å¼‚</li><li>12K-16Kï¼šå¼€å§‹å­˜åœ¨ä¸€å®šæŸå¤±ï¼Œå¤§çº¦æ˜¯3æ¯”ç‰¹é‡åŒ–çº§åˆ«çš„æ•ˆæœ</li><li>18K+ï¼šå­˜åœ¨è¾ƒå¤§æŸå¤±ï¼Œå¤§çº¦æ˜¯2æ¯”ç‰¹é‡åŒ–çº§åˆ«æ•ˆæœï¼Œ20K+ä¸å¯ç”¨<br> ä»¥ä¸Šç»“æœä»…ä¾›å‚è€ƒï¼Œåº”åœ¨å®é™…åœºæ™¯ä¸­æµ‹è¯•è°ƒæ•´AUTO_COEFFæˆ–è€…ALPHAå–å€¼ã€‚</li></ul> 
<h4><a id="llamacpp_32"></a>ä½¿ç”¨llama.cppæ¨ç†</h4> 
<p>Step 1: å…‹éš†å’Œç¼–è¯‘llama.cpp</p> 
<ol><li>ï¼ˆå¯é€‰ï¼‰å¦‚æœå·²ä¸‹è½½æ—§ç‰ˆä»“åº“ï¼Œå»ºè®®git pullæ‹‰å–æœ€æ–°ä»£ç ï¼Œå¹¶æ‰§è¡Œmake cleanè¿›è¡Œæ¸…ç†</li><li>æ‹‰å–æœ€æ–°ç‰ˆllama.cppä»“åº“ä»£ç </li></ol> 
<pre><code class="prism language-shell">$ <span class="token function">git</span> clone https://github.com/ggerganov/llama.cpp
</code></pre> 
<ol start="3"><li>å¯¹llama.cppé¡¹ç›®è¿›è¡Œç¼–è¯‘ï¼Œç”Ÿæˆ./mainï¼ˆç”¨äºæ¨ç†ï¼‰å’Œ./quantizeï¼ˆç”¨äºé‡åŒ–ï¼‰äºŒè¿›åˆ¶æ–‡ä»¶ã€‚</li></ol> 
<pre><code class="prism language-shell">$ <span class="token function">make</span>
</code></pre> 
<p>Step 2: ç”Ÿæˆé‡åŒ–ç‰ˆæœ¬æ¨¡å‹<br> ç›®å‰llama.cppå·²æ”¯æŒ.pthæ–‡ä»¶ä»¥åŠhuggingfaceæ ¼å¼.binçš„è½¬æ¢ã€‚å°†å®Œæ•´æ¨¡å‹æƒé‡è½¬æ¢ä¸ºGGMLçš„FP16æ ¼å¼ï¼Œç”Ÿæˆæ–‡ä»¶è·¯å¾„ä¸ºzh-models/7B/ggml-model-f16.ggufã€‚è¿›ä¸€æ­¥å¯¹FP16æ¨¡å‹è¿›è¡Œ4-bité‡åŒ–ï¼Œç”Ÿæˆé‡åŒ–æ¨¡å‹æ–‡ä»¶è·¯å¾„ä¸ºzh-models/7B/ggml-model-q4_0.ggufã€‚ä¸åŒé‡åŒ–æ–¹æ³•çš„æ€§èƒ½å¯¹æ¯”è§æœ¬Wikiæœ€åéƒ¨åˆ†ã€‚</p> 
<pre><code class="prism language-shell">python3 convert.py <span class="token punctuation">..</span>/merged_chinese_llama_7b
$ ./quantize <span class="token punctuation">..</span>/merged_chinese_llama_7b/ggml-model-f16.gguf <span class="token punctuation">..</span>/merged_chinese_llama_7b/ggml-model-q4_0.gguf q4_0
</code></pre> 
<p>Step 3: åŠ è½½å¹¶å¯åŠ¨æ¨¡å‹</p> 
<pre><code>  llama.cpp git:(master) âœ— ./main -s 1 -m ../merged_chinese_llama_7b/ggml-model-q4_0.gguf -p "ä¸­å›½çš„é¦–éƒ½æ˜¯" --ignore-eos -c 64 -n 128 -t 3 -ngl 10
</code></pre> 
<ul><li> <p>GPUæ¨ç†ï¼šé€šè¿‡Metalç¼–è¯‘åˆ™åªéœ€åœ¨./mainä¸­æŒ‡å®š-ngl 1ï¼›cuBLASç¼–è¯‘éœ€è¦æŒ‡å®šoffloadå±‚æ•°ï¼Œä¾‹å¦‚-ngl 40è¡¨ç¤ºoffload 40å±‚æ¨¡å‹å‚æ•°åˆ°GPU</p> </li><li> <p>åŠ è½½é•¿ä¸Šä¸‹æ–‡æ¨¡å‹ï¼ˆ16Kï¼‰ï¼š</p> 
  <ul><li>å¯åŠ¨æ¨¡å‹ï¼ˆ./mainï¼‰ådebugä¿¡æ¯ä¸­æ˜¾ç¤ºllm_load_print_meta: freq_scale = 0.25ï¼Œåˆ™è¡¨ç¤ºæ¨¡å‹è½¬æ¢æ—¶å·²è½½å…¥ç›¸åº”è¶…å‚ï¼Œæ— éœ€å…¶ä»–ç‰¹æ®Šè®¾ç½®</li><li>å¦‚æœä¸Šè¿°debugä¿¡æ¯æ˜¾ç¤ºä¸ºllm_load_print_meta: freq_scale = 1.0ï¼Œåˆ™éœ€åœ¨./mainä¸­é¢å¤–æŒ‡å®šâ€“rope-scale 4</li></ul> </li><li> <p>é»˜è®¤çš„é‡åŒ–æ–¹æ³•ä¸ºq4_0ï¼Œè™½ç„¶é€Ÿåº¦æœ€å¿«ä½†æŸå¤±ä¹Ÿè¾ƒå¤§ï¼Œæ¨èä½¿ç”¨Q4_Kä½œä¸ºæ›¿ä»£</p> </li><li> <p>æœºå™¨èµ„æºå¤Ÿç”¨ä¸”å¯¹é€Ÿåº¦è¦æ±‚ä¸æ˜¯é‚£ä¹ˆè‹›åˆ»çš„æƒ…å†µä¸‹å¯ä»¥ä½¿ç”¨q8_0æˆ–Q6_Kï¼Œéå¸¸æ¥è¿‘F16æ¨¡å‹çš„æ•ˆæœ</p> </li></ul> 
<p>å¦‚æœä½¿ç”¨çš„æ˜¯Mac Intelå¯èƒ½æŠ¥å¦‚ä¸‹é”™ï¼š</p> 
<pre><code class="prism language-shell">ggml_metal_init: load pipeline error: Error <span class="token assign-left variable">Domain</span><span class="token operator">=</span>CompilerError <span class="token assign-left variable">Code</span><span class="token operator">=</span><span class="token number">2</span> <span class="token string">"SC compilation failure
There is a call to an undefined label"</span> <span class="token assign-left variable">UserInfo</span><span class="token operator">=</span><span class="token punctuation">{<!-- --></span>NSLocalizedDescription<span class="token operator">=</span>SC compilation failure
There is a call to an undefined label<span class="token punctuation">}</span>
llama_new_context_with_model: ggml_metal_init<span class="token punctuation">(</span><span class="token punctuation">)</span> failed
llama_init_from_gpt_params: error: failed to create context with model <span class="token string">'../merged_chinese_llama_7b/ggml-model-q4_0.gguf'</span>
main: error: unable to load model
</code></pre> 
<p>å¯ä»¥æŒ‰è¿™é‡Œçš„ä¿®æ”¹</p> 
<pre><code class="prism language-shell">$ <span class="token function">make</span> clean
$ brew update <span class="token operator">&amp;&amp;</span> brew <span class="token function">install</span> clblast
<span class="token comment">#disable metal and enable clblast </span>
$ <span class="token function">make</span> <span class="token assign-left variable">LLAMA_CLBLAST</span><span class="token operator">=</span><span class="token number">1</span> <span class="token assign-left variable">LLAMA_NO_METAL</span><span class="token operator">=</span><span class="token number">1</span>
<span class="token comment">#è¿™æ—¶å¯ä»¥ç”¨mainè¿›è¡Œæ¨ç†</span>
$./main <span class="token parameter variable">-s</span> <span class="token number">1</span> <span class="token parameter variable">-m</span> <span class="token punctuation">..</span>/merged_chinese_llama_7b/ggml-model-q4_0.gguf <span class="token parameter variable">-p</span> <span class="token string">"ä¸­å›½çš„é¦–éƒ½æ˜¯"</span> --ignore-eos <span class="token parameter variable">-c</span> <span class="token number">64</span> <span class="token parameter variable">-n</span> <span class="token number">128</span> <span class="token parameter variable">-t</span> <span class="token number">3</span> <span class="token parameter variable">-ngl</span> <span class="token number">10</span>
</code></pre> 
<p>å¯¹åº”çš„ç»ˆç«¯è¾“å‡ºä¸ºï¼š</p> 
<pre><code class="prism language-shell"><span class="token punctuation">(</span>venv<span class="token punctuation">)</span> âœ  llama.cpp git:<span class="token punctuation">(</span>master<span class="token punctuation">)</span> âœ— ./main <span class="token parameter variable">-s</span> <span class="token number">1</span> <span class="token parameter variable">-m</span> <span class="token punctuation">..</span>/merged_chinese_llama_7b/ggml-model-q4_0.gguf <span class="token parameter variable">-p</span> <span class="token string">"ä¸­å›½çš„é¦–éƒ½æ˜¯"</span> --ignore-eos <span class="token parameter variable">-c</span> <span class="token number">64</span> <span class="token parameter variable">-n</span> <span class="token number">128</span> <span class="token parameter variable">-t</span> <span class="token number">3</span> <span class="token parameter variable">-ngl</span> <span class="token number">10</span>
Log start
main: warning: changing RoPE frequency base to <span class="token number">0</span> <span class="token punctuation">(</span>default <span class="token number">10000.0</span><span class="token punctuation">)</span>
main: warning: scaling RoPE frequency by <span class="token number">0</span> <span class="token punctuation">(</span>default <span class="token number">1.0</span><span class="token punctuation">)</span>
main: build <span class="token operator">=</span> <span class="token number">1273</span> <span class="token punctuation">(</span>99115f3<span class="token punctuation">)</span>
main: built with Apple clang version <span class="token number">14.0</span>.3 <span class="token punctuation">(</span>clang-1403.0.22.14.1<span class="token punctuation">)</span> <span class="token keyword">for</span> x86_64-apple-darwin22.5.0
main: seed  <span class="token operator">=</span> <span class="token number">1</span>
ggml_opencl: selecting platform: <span class="token string">'Apple'</span>
ggml_opencl: selecting device: <span class="token string">'Intel(R) UHD Graphics 630'</span>
ggml_opencl: device FP16 support: <span class="token boolean">false</span>
llama_model_loader: loaded meta data with <span class="token number">19</span> key-value pairs and <span class="token number">291</span> tensors from <span class="token punctuation">..</span>/merged_chinese_llama_7b/ggml-model-q4_0.gguf <span class="token punctuation">(</span>version GGUF V2 <span class="token punctuation">(</span>latest<span class="token punctuation">))</span>
llama_model_loader: - tensor    <span class="token number">0</span>:                token_embd.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">55296</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor    <span class="token number">1</span>:              blk.0.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor    <span class="token number">2</span>:              blk.0.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor    <span class="token number">3</span>:              blk.0.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor    <span class="token number">4</span>:         blk.0.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor    <span class="token number">5</span>:            blk.0.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor    <span class="token number">6</span>:              blk.0.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor    <span class="token number">7</span>:            blk.0.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor    <span class="token number">8</span>:           blk.0.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor    <span class="token number">9</span>:            blk.0.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">10</span>:              blk.1.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">11</span>:              blk.1.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">12</span>:              blk.1.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">13</span>:         blk.1.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">14</span>:            blk.1.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">15</span>:              blk.1.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">16</span>:            blk.1.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">17</span>:           blk.1.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">18</span>:            blk.1.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">19</span>:              blk.2.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">20</span>:              blk.2.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">21</span>:              blk.2.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">22</span>:         blk.2.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">23</span>:            blk.2.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">24</span>:              blk.2.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">25</span>:            blk.2.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">26</span>:           blk.2.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">27</span>:            blk.2.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">28</span>:              blk.3.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">29</span>:              blk.3.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">30</span>:              blk.3.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">31</span>:         blk.3.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">32</span>:            blk.3.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">33</span>:              blk.3.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">34</span>:            blk.3.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">35</span>:           blk.3.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">36</span>:            blk.3.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">37</span>:              blk.4.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">38</span>:              blk.4.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">39</span>:              blk.4.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">40</span>:         blk.4.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">41</span>:            blk.4.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">42</span>:              blk.4.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">43</span>:            blk.4.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">44</span>:           blk.4.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">45</span>:            blk.4.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">46</span>:              blk.5.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">47</span>:              blk.5.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">48</span>:              blk.5.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">49</span>:         blk.5.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">50</span>:            blk.5.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">51</span>:              blk.5.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">52</span>:            blk.5.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">53</span>:           blk.5.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">54</span>:            blk.5.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">55</span>:              blk.6.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">56</span>:              blk.6.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">57</span>:              blk.6.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">58</span>:         blk.6.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">59</span>:            blk.6.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">60</span>:              blk.6.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">61</span>:            blk.6.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">62</span>:           blk.6.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">63</span>:            blk.6.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">64</span>:              blk.7.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">65</span>:              blk.7.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">66</span>:              blk.7.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">67</span>:         blk.7.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">68</span>:            blk.7.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">69</span>:              blk.7.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">70</span>:            blk.7.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">71</span>:           blk.7.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">72</span>:            blk.7.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">73</span>:              blk.8.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">74</span>:              blk.8.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">75</span>:              blk.8.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">76</span>:         blk.8.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">77</span>:            blk.8.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">78</span>:              blk.8.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">79</span>:            blk.8.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">80</span>:           blk.8.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">81</span>:            blk.8.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">82</span>:              blk.9.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">83</span>:              blk.9.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">84</span>:              blk.9.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">85</span>:         blk.9.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">86</span>:            blk.9.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">87</span>:              blk.9.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">88</span>:            blk.9.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">89</span>:           blk.9.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">90</span>:            blk.9.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">91</span>:             blk.10.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">92</span>:             blk.10.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">93</span>:             blk.10.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">94</span>:        blk.10.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">95</span>:           blk.10.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">96</span>:             blk.10.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">97</span>:           blk.10.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">98</span>:          blk.10.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor   <span class="token number">99</span>:           blk.10.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">100</span>:             blk.11.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">101</span>:             blk.11.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">102</span>:             blk.11.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">103</span>:        blk.11.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">104</span>:           blk.11.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">105</span>:             blk.11.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">106</span>:           blk.11.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">107</span>:          blk.11.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">108</span>:           blk.11.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">109</span>:             blk.12.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">110</span>:             blk.12.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">111</span>:             blk.12.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">112</span>:        blk.12.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">113</span>:           blk.12.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">114</span>:             blk.12.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">115</span>:           blk.12.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">116</span>:          blk.12.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">117</span>:           blk.12.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">118</span>:             blk.13.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">119</span>:             blk.13.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">120</span>:             blk.13.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">121</span>:        blk.13.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">122</span>:           blk.13.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">123</span>:             blk.13.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">124</span>:           blk.13.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">125</span>:          blk.13.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">126</span>:           blk.13.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">127</span>:             blk.14.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">128</span>:             blk.14.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">129</span>:             blk.14.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">130</span>:        blk.14.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">131</span>:           blk.14.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">132</span>:             blk.14.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">133</span>:           blk.14.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">134</span>:          blk.14.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">135</span>:           blk.14.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">136</span>:             blk.15.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">137</span>:             blk.15.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">138</span>:             blk.15.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">139</span>:        blk.15.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">140</span>:           blk.15.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">141</span>:             blk.15.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">142</span>:           blk.15.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">143</span>:          blk.15.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">144</span>:           blk.15.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">145</span>:             blk.16.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">146</span>:             blk.16.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">147</span>:             blk.16.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">148</span>:        blk.16.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">149</span>:           blk.16.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">150</span>:             blk.16.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">151</span>:           blk.16.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">152</span>:          blk.16.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">153</span>:           blk.16.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">154</span>:             blk.17.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">155</span>:             blk.17.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">156</span>:             blk.17.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">157</span>:        blk.17.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">158</span>:           blk.17.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">159</span>:             blk.17.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">160</span>:           blk.17.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">161</span>:          blk.17.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">162</span>:           blk.17.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">163</span>:             blk.18.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">164</span>:             blk.18.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">165</span>:             blk.18.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">166</span>:        blk.18.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">167</span>:           blk.18.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">168</span>:             blk.18.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">169</span>:           blk.18.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">170</span>:          blk.18.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">171</span>:           blk.18.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">172</span>:             blk.19.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">173</span>:             blk.19.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">174</span>:             blk.19.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">175</span>:        blk.19.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">176</span>:           blk.19.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">177</span>:             blk.19.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">178</span>:           blk.19.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">179</span>:          blk.19.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">180</span>:           blk.19.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">181</span>:             blk.20.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">182</span>:             blk.20.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">183</span>:             blk.20.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">184</span>:        blk.20.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">185</span>:           blk.20.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">186</span>:             blk.20.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">187</span>:           blk.20.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">188</span>:          blk.20.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">189</span>:           blk.20.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">190</span>:             blk.21.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">191</span>:             blk.21.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">192</span>:             blk.21.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">193</span>:        blk.21.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">194</span>:           blk.21.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">195</span>:             blk.21.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">196</span>:           blk.21.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">197</span>:          blk.21.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">198</span>:           blk.21.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">199</span>:             blk.22.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">200</span>:             blk.22.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">201</span>:             blk.22.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">202</span>:        blk.22.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">203</span>:           blk.22.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">204</span>:             blk.22.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">205</span>:           blk.22.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">206</span>:          blk.22.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">207</span>:           blk.22.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">208</span>:             blk.23.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">209</span>:             blk.23.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">210</span>:             blk.23.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">211</span>:        blk.23.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">212</span>:           blk.23.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">213</span>:             blk.23.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">214</span>:           blk.23.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">215</span>:          blk.23.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">216</span>:           blk.23.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">217</span>:             blk.24.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">218</span>:             blk.24.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">219</span>:             blk.24.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">220</span>:        blk.24.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">221</span>:           blk.24.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">222</span>:             blk.24.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">223</span>:           blk.24.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">224</span>:          blk.24.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">225</span>:           blk.24.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">226</span>:             blk.25.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">227</span>:             blk.25.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">228</span>:             blk.25.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">229</span>:        blk.25.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">230</span>:           blk.25.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">231</span>:             blk.25.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">232</span>:           blk.25.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">233</span>:          blk.25.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">234</span>:           blk.25.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">235</span>:             blk.26.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">236</span>:             blk.26.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">237</span>:             blk.26.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">238</span>:        blk.26.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">239</span>:           blk.26.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">240</span>:             blk.26.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">241</span>:           blk.26.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">242</span>:          blk.26.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">243</span>:           blk.26.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">244</span>:             blk.27.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">245</span>:             blk.27.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">246</span>:             blk.27.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">247</span>:        blk.27.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">248</span>:           blk.27.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">249</span>:             blk.27.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">250</span>:           blk.27.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">251</span>:          blk.27.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">252</span>:           blk.27.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">253</span>:             blk.28.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">254</span>:             blk.28.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">255</span>:             blk.28.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">256</span>:        blk.28.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">257</span>:           blk.28.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">258</span>:             blk.28.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">259</span>:           blk.28.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">260</span>:          blk.28.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">261</span>:           blk.28.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">262</span>:             blk.29.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">263</span>:             blk.29.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">264</span>:             blk.29.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">265</span>:        blk.29.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">266</span>:           blk.29.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">267</span>:             blk.29.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">268</span>:           blk.29.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">269</span>:          blk.29.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">270</span>:           blk.29.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">271</span>:             blk.30.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">272</span>:             blk.30.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">273</span>:             blk.30.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">274</span>:        blk.30.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">275</span>:           blk.30.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">276</span>:             blk.30.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">277</span>:           blk.30.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">278</span>:          blk.30.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">279</span>:           blk.30.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">280</span>:             blk.31.attn_q.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">281</span>:             blk.31.attn_k.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">282</span>:             blk.31.attn_v.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">283</span>:        blk.31.attn_output.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">284</span>:           blk.31.ffn_gate.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">285</span>:             blk.31.ffn_up.weight q4_0     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">11008</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">286</span>:           blk.31.ffn_down.weight q4_0     <span class="token punctuation">[</span> <span class="token number">11008</span>,  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">287</span>:          blk.31.attn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">288</span>:           blk.31.ffn_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">289</span>:               output_norm.weight f32      <span class="token punctuation">[</span>  <span class="token number">4096</span>,     <span class="token number">1</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - tensor  <span class="token number">290</span>:                    output.weight q6_K     <span class="token punctuation">[</span>  <span class="token number">4096</span>, <span class="token number">55296</span>,     <span class="token number">1</span>,     <span class="token number">1</span> <span class="token punctuation">]</span>
llama_model_loader: - kv   <span class="token number">0</span>:                       general.architecture str
llama_model_loader: - kv   <span class="token number">1</span>:                               general.name str
llama_model_loader: - kv   <span class="token number">2</span>:                       llama.context_length u32
llama_model_loader: - kv   <span class="token number">3</span>:                     llama.embedding_length u32
llama_model_loader: - kv   <span class="token number">4</span>:                          llama.block_count u32
llama_model_loader: - kv   <span class="token number">5</span>:                  llama.feed_forward_length u32
llama_model_loader: - kv   <span class="token number">6</span>:                 llama.rope.dimension_count u32
llama_model_loader: - kv   <span class="token number">7</span>:                 llama.attention.head_count u32
llama_model_loader: - kv   <span class="token number">8</span>:              llama.attention.head_count_kv u32
llama_model_loader: - kv   <span class="token number">9</span>:     llama.attention.layer_norm_rms_epsilon f32
llama_model_loader: - kv  <span class="token number">10</span>:                       llama.rope.freq_base f32
llama_model_loader: - kv  <span class="token number">11</span>:                          general.file_type u32
llama_model_loader: - kv  <span class="token number">12</span>:                       tokenizer.ggml.model str
llama_model_loader: - kv  <span class="token number">13</span>:                      tokenizer.ggml.tokens arr
llama_model_loader: - kv  <span class="token number">14</span>:                      tokenizer.ggml.scores arr
llama_model_loader: - kv  <span class="token number">15</span>:                  tokenizer.ggml.token_type arr
llama_model_loader: - kv  <span class="token number">16</span>:                tokenizer.ggml.bos_token_id u32
llama_model_loader: - kv  <span class="token number">17</span>:                tokenizer.ggml.eos_token_id u32
llama_model_loader: - kv  <span class="token number">18</span>:               general.quantization_version u32
llama_model_loader: - <span class="token builtin class-name">type</span>  f32:   <span class="token number">65</span> tensors
llama_model_loader: - <span class="token builtin class-name">type</span> q4_0:  <span class="token number">225</span> tensors
llama_model_loader: - <span class="token builtin class-name">type</span> q6_K:    <span class="token number">1</span> tensors
llm_load_print_meta: <span class="token function">format</span>         <span class="token operator">=</span> GGUF V2 <span class="token punctuation">(</span>latest<span class="token punctuation">)</span>
llm_load_print_meta: arch           <span class="token operator">=</span> llama
llm_load_print_meta: vocab <span class="token builtin class-name">type</span>     <span class="token operator">=</span> SPM
llm_load_print_meta: n_vocab        <span class="token operator">=</span> <span class="token number">55296</span>
llm_load_print_meta: n_merges       <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: n_ctx_train    <span class="token operator">=</span> <span class="token number">2048</span>
llm_load_print_meta: n_ctx          <span class="token operator">=</span> <span class="token number">64</span>
llm_load_print_meta: n_embd         <span class="token operator">=</span> <span class="token number">4096</span>
llm_load_print_meta: n_head         <span class="token operator">=</span> <span class="token number">32</span>
llm_load_print_meta: n_head_kv      <span class="token operator">=</span> <span class="token number">32</span>
llm_load_print_meta: n_layer        <span class="token operator">=</span> <span class="token number">32</span>
llm_load_print_meta: n_rot          <span class="token operator">=</span> <span class="token number">128</span>
llm_load_print_meta: n_gqa          <span class="token operator">=</span> <span class="token number">1</span>
llm_load_print_meta: f_norm_eps     <span class="token operator">=</span> <span class="token number">0</span>.0e+00
llm_load_print_meta: f_norm_rms_eps <span class="token operator">=</span> <span class="token number">1</span>.0e-05
llm_load_print_meta: n_ff           <span class="token operator">=</span> <span class="token number">11008</span>
llm_load_print_meta: freq_base      <span class="token operator">=</span> <span class="token number">10000.0</span>
llm_load_print_meta: freq_scale     <span class="token operator">=</span> <span class="token number">1</span>
llm_load_print_meta: model <span class="token builtin class-name">type</span>     <span class="token operator">=</span> 7B
llm_load_print_meta: model ftype    <span class="token operator">=</span> mostly Q4_0
llm_load_print_meta: model params   <span class="token operator">=</span> <span class="token number">6.93</span> B
llm_load_print_meta: model size     <span class="token operator">=</span> <span class="token number">3.69</span> GiB <span class="token punctuation">(</span><span class="token number">4.57</span> BPW<span class="token punctuation">)</span>
llm_load_print_meta: general.name   <span class="token operator">=</span> <span class="token punctuation">..</span>
llm_load_print_meta: BOS token <span class="token operator">=</span> <span class="token number">1</span> <span class="token string">'&lt;s&gt;'</span>
llm_load_print_meta: EOS token <span class="token operator">=</span> <span class="token number">2</span> <span class="token string">'&lt;/s&gt;'</span>
llm_load_print_meta: UNK token <span class="token operator">=</span> <span class="token number">0</span> <span class="token string">'&lt;unk&gt;'</span>
llm_load_print_meta: LF token  <span class="token operator">=</span> <span class="token number">13</span> <span class="token string">'&lt;0x0A&gt;'</span>
llm_load_tensors: ggml ctx size <span class="token operator">=</span>    <span class="token number">0.09</span> MB
llm_load_tensors: using OpenCL <span class="token keyword">for</span> GPU acceleration
llm_load_tensors: mem required  <span class="token operator">=</span> <span class="token number">2687.86</span> MB <span class="token punctuation">(</span>+   <span class="token number">32.00</span> MB per state<span class="token punctuation">)</span>
llm_load_tensors: offloading <span class="token number">10</span> repeating layers to GPU
llm_load_tensors: offloaded <span class="token number">10</span>/33 layers to GPU
llm_load_tensors: VRAM used: <span class="token number">1086</span> MB
<span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span>
llama_new_context_with_model: kv self size  <span class="token operator">=</span>   <span class="token number">32.00</span> MB
llama_new_context_with_model: compute buffer total size <span class="token operator">=</span>   <span class="token number">15.97</span> MB

system_info: n_threads <span class="token operator">=</span> <span class="token number">3</span> / <span class="token number">12</span> <span class="token operator">|</span> AVX <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> AVX2 <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> AVX512 <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> AVX512_VBMI <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> AVX512_VNNI <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> FMA <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> NEON <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> ARM_FMA <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> F16C <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> FP16_VA <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> WASM_SIMD <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> BLAS <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> SSE3 <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> SSSE3 <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> VSX <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span>
sampling: repeat_last_n <span class="token operator">=</span> <span class="token number">64</span>, repeat_penalty <span class="token operator">=</span> <span class="token number">1.100000</span>, presence_penalty <span class="token operator">=</span> <span class="token number">0.000000</span>, frequency_penalty <span class="token operator">=</span> <span class="token number">0.000000</span>, top_k <span class="token operator">=</span> <span class="token number">40</span>, tfs_z <span class="token operator">=</span> <span class="token number">1.000000</span>, top_p <span class="token operator">=</span> <span class="token number">0.950000</span>, typical_p <span class="token operator">=</span> <span class="token number">1.000000</span>, temp <span class="token operator">=</span> <span class="token number">0.800000</span>, mirostat <span class="token operator">=</span> <span class="token number">0</span>, mirostat_lr <span class="token operator">=</span> <span class="token number">0.100000</span>, mirostat_ent <span class="token operator">=</span> <span class="token number">5.000000</span>
generate: n_ctx <span class="token operator">=</span> <span class="token number">64</span>, n_batch <span class="token operator">=</span> <span class="token number">512</span>, n_predict <span class="token operator">=</span> <span class="token number">128</span>, n_keep <span class="token operator">=</span> <span class="token number">0</span>


 ä¸­å›½çš„é¦–éƒ½æ˜¯ä¸–ç•Œä¸Šæ”¿æ²»ã€å†›äº‹å’Œæ–‡åŒ–ä¸­å¿ƒã€‚é•¿å®‰å¤ç§°<span class="token string">"äº¬å¸ˆ"</span>,åä¸ºåŒ—äº¬ï¼›åŒ—å®‹æ—¶æœŸï¼Œä¸œäº¬å¼€å°åºœä¸€åº¦å‡æ ¼ä¸º<span class="token string">"ä¸­éƒ½"</span>æˆ–<span class="token string">"å¤§éƒ½"</span>ã€‚ã€Šé•¿å®‰å¿—ã€‹è®°è½½ï¼š"è‡ªå»ºéƒ½ä»¥æ¥,å› å¾—åæ›°<span class="token string">'é•¿å®‰'</span>è€…æœ‰â€¦
</code></pre> 
<h3><a id="_450"></a>ä¸€äº›è¯´æ˜</h3> 
<p>è¿™é‡Œå°†ä¸¤ä¸ªåŸºåº§æ¨¡å‹å’ŒLORA fine tuneæ¨¡å‹mergeçš„åŸå› åœ¨äºæ‰©å……è¯æ±‡è¡¨ä¹‹åï¼ŒEmbeddingä¹Ÿè¿›è¡Œäº†æ‰©å……ï¼Œè¯æ±‡è¡¨æ¯”åŸå§‹çš„LlaMA-2 32kå¤§ï¼Œå› è€Œè¦å°†Embeddingå±‚mergeï¼ˆå®é™…æ˜¯æ›¿æ¢)ï¼Œæ­¤å¤–Attentionï¼ˆq,k,v)ä»¥åŠMLPï¼ˆfeedforwardï¼Œw1,w2,w3)åŸºæœ¬éƒ½è¿›è¡Œäº†mergeæ“ä½œã€‚ç”±äºæ”¹åŠ¨å¦‚æ­¤ä¹‹å¤§ï¼Œä»¥è‡³äºã€Š<a href="https://blog.csdn.net/shichaog/article/details/132634620">å¤§è¯­è¨€æ¨¡å‹ä¹‹ä¸ƒ- Llama-2å•GPUå¾®è°ƒSFT</a>ã€‹åšå®¢é‡Œå¾®è°ƒæ–¹æ³•æ˜¯ä¸€æ ·çš„ï¼Œä½†æ˜¯æ”¹åŠ¨é‡å’Œè®­ç»ƒçš„èµ„æºéœ€æ±‚æ˜¯ä¸ä¸€æ ·çš„ï¼Œè¿™ä¹Ÿå¯¼è‡´äº†æ‰©å……ä¸­æ–‡çš„å¾®è°ƒè®­ç»ƒåœ¨colabå…è´¹çš„12G GPUå†…å­˜ä¸Šæ˜¯æ— æ³•å®Œæˆè®­ç»ƒçš„ã€‚</p> 
<p>PEFTæ˜¯ Hugging Faceæä¾›çš„æ¨¡å‹è®­ç»ƒçš„é«˜æ•ˆåº“ï¼ŒLORAæ˜¯å…¶æä¾›çš„æ–¹æ³•ä¹‹ä¸€ï¼ŒLORAæ–¹å¼æ˜¯2021å¹´è®ºæ–‡ <a href="https://arxiv.org/pdf/2106.09685.pdf" rel="nofollow">LoRA: Low-rank adaptation of Large Language Models.</a>é¦–å…ˆå¼•å…¥çš„æ–¹æ³•ã€‚<br> å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å¯ä»¥åœ¨ä»…è°ƒæ•´ä¸€å°éƒ¨åˆ†æƒé‡çš„åŒæ—¶å®ç°å‡ºè‰²çš„æ€§èƒ½ï¼Œè¿›è€Œæ— éœ€åœ¨å¤šå°æœºå™¨ä¸Šè°ƒæ•´æ•°åäº¿ä¸ªå‚æ•°ï¼Œä½¿æ•´ä¸ªå¾®è°ƒè¿‡ç¨‹æ›´åŠ å®ç”¨ä¸”ç»æµå¯è¡Œã€‚ä½¿ç”¨PEFTå’Œé‡åŒ–å…è®¸åœ¨å•ä¸ªGPUä¸Šå¾®è°ƒå…·æœ‰æ•°åäº¿ä¸ªå‚æ•°çš„å¤§å‹æ¨¡å‹ã€‚æ¯”å¦‚Embeddingæ˜¯è¯å‘é‡çš„ç¼–ç ï¼Œè™½ç„¶ä»»åŠ¡ä¸åŒï¼Œå¦‚é—®ç­”ã€æ‘˜è¦ã€åä½œç±»çš„å¤§æ¨¡å‹ï¼Œè™½ç„¶åº”ç”¨ä¸åŒï¼Œä½†æ˜¯è¯å‘é‡ç¼–ç æ˜¯å¯ä»¥å¤ç”¨çš„ï¼Œä¸éœ€è¦æ”¹ï¼Œå› è€Œåœ¨å¾®è°ƒçš„æ—¶å€™ï¼Œå°±ä¸æ”¹è¯å‘é‡äº†ï¼Œè¿™æ ·å°±èŠ‚çœå­˜å‚¨å’Œè¿ç®—èµ„æºã€‚</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/023ce330f0936b58746e28be50c90ecf/" rel="prev">
			<span class="pager__subtitle">Â«&thinsp;Previous</span>
			<p class="pager__title">Pythonåº“: keyword</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6657b74696a02a3d0ff0cfcba63c6660/" rel="next">
			<span class="pager__subtitle">Next&thinsp;Â»</span>
			<p class="pager__title">JavaScriptåŸºç¡€è¯­æ³•å’ŒçŸ¥è¯†</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 ç¼–ç¨‹éšæƒ³.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>