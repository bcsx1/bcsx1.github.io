<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【ElasticSearch】基于Docker 部署 ElasticSearch 和 Kibana，使用 Kibana 操作索引库，以及实现对文档的增删改查 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【ElasticSearch】基于Docker 部署 ElasticSearch 和 Kibana，使用 Kibana 操作索引库，以及实现对文档的增删改查" />
<meta property="og:description" content="文章目录 前言一、使用 Docker 部署 ElasticSearch 和 Kibana1.1 部署 ElasticSearch1.2 部署 Kibana1.3 利用 Kibana 演示 Elasticsearch 分词效果 二、解决中文分词的问题2.1 默认分词器对中文分词的问题2.2 引入 IK 分词器2.3 IK 分词器的两种分词模式2.4 IK 分词器存在的问题2.5 IK 分词器拓展词库和停用词条 三、使用 Kibana 操作索引库三、使用 Kibana 操作索引库3.1 Mapping 属性3.2 创建和获取索引3.3 修改索引库3.4 删除索引库 四、使用 Kibana 实现对文档的增删改查4.1 新增文档4.2 获取和删除文档4.3 修改文档：全量修改和增量修改4.4 文档的版本号4.5 动态 Mapping 映射 前言 Elasticsearch 和 Kibana 是强大的工具，用于构建实时搜索和数据可视化解决方案。Elasticsearch 是一个分布式、高性能的搜索引擎，可以用于存储和检索各种类型的数据，从文本文档到地理空间数据。Kibana 则是 Elasticsearch 的可视化工具，用于实时分析和可视化大规模数据集。
在本文中，将探讨如何使用 Docker 进行快速部署 Elasticsearch 和 Kibana，搭建一个强大的搜索和分析平台。我们将从基础开始，逐步介绍如何配置和管理这两个工具，以满足不同应用场景的需求。随后，将重点关注使用 Kibana 操作索引库的各种任务，包括创建、获取、修改和删除索引。这些任务对于数据管理和搜索引擎的构建至关重要，将帮助您更好地组织和利用数据。
通过阅读本文，希望能够帮助我们掌握 Elasticsearch 和 Kibana 的核心概念，具备构建强大搜索引擎和可视化工具的能力，以及解决中文分词和文档管理方面的专业知识。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/5afa395aee92f8d9c1e03e0416555228/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-08T18:15:14+08:00" />
<meta property="article:modified_time" content="2023-10-08T18:15:14+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【ElasticSearch】基于Docker 部署 ElasticSearch 和 Kibana，使用 Kibana 操作索引库，以及实现对文档的增删改查</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_7" rel="nofollow">前言</a></li><li><a href="#_Docker__ElasticSearch__Kibana_14" rel="nofollow">一、使用 Docker 部署 ElasticSearch 和 Kibana</a></li><li><ul><li><a href="#11__ElasticSearch_22" rel="nofollow">1.1 部署 ElasticSearch</a></li><li><a href="#12__Kibana_91" rel="nofollow">1.2 部署 Kibana</a></li><li><a href="#13__Kibana__Elasticsearch__147" rel="nofollow">1.3 利用 Kibana 演示 Elasticsearch 分词效果</a></li></ul> 
  </li><li><a href="#_188" rel="nofollow">二、解决中文分词的问题</a></li><li><ul><li><a href="#21__189" rel="nofollow">2.1 默认分词器对中文分词的问题</a></li><li><a href="#22__IK__206" rel="nofollow">2.2 引入 IK 分词器</a></li><li><a href="#23_IK__261" rel="nofollow">2.3 IK 分词器的两种分词模式</a></li><li><a href="#24_IK__301" rel="nofollow">2.4 IK 分词器存在的问题</a></li><li><a href="#25_IK__315" rel="nofollow">2.5 IK 分词器拓展词库和停用词条</a></li></ul> 
  </li><li><a href="#_Kibana__371" rel="nofollow">三、使用 Kibana 操作索引库</a></li><li><a href="#_Kibana__374" rel="nofollow">三、使用 Kibana 操作索引库</a></li><li><ul><li><a href="#31_Mapping__376" rel="nofollow">3.1 Mapping 属性</a></li><li><a href="#32__452" rel="nofollow">3.2 创建和获取索引</a></li><li><a href="#33__521" rel="nofollow">3.3 修改索引库</a></li><li><a href="#34__553" rel="nofollow">3.4 删除索引库</a></li></ul> 
  </li><li><a href="#_Kibana__578" rel="nofollow">四、使用 Kibana 实现对文档的增删改查</a></li><li><ul><li><a href="#41__580" rel="nofollow">4.1 新增文档</a></li><li><a href="#42__616" rel="nofollow">4.2 获取和删除文档</a></li><li><a href="#43__653" rel="nofollow">4.3 修改文档：全量修改和增量修改</a></li><li><a href="#44__722" rel="nofollow">4.4 文档的版本号</a></li><li><a href="#45__Mapping__756" rel="nofollow">4.5 动态 Mapping 映射</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<hr> 
<h2><a id="_7"></a>前言</h2> 
<p>Elasticsearch 和 Kibana 是强大的工具，用于构建实时搜索和数据可视化解决方案。Elasticsearch 是一个分布式、高性能的搜索引擎，可以用于存储和检索各种类型的数据，从文本文档到地理空间数据。Kibana 则是 Elasticsearch 的可视化工具，用于实时分析和可视化大规模数据集。</p> 
<p>在本文中，将探讨如何使用 Docker 进行快速部署 Elasticsearch 和 Kibana，搭建一个强大的搜索和分析平台。我们将从基础开始，逐步介绍如何配置和管理这两个工具，以满足不同应用场景的需求。随后，将重点关注使用 Kibana 操作索引库的各种任务，包括创建、获取、修改和删除索引。这些任务对于数据管理和搜索引擎的构建至关重要，将帮助您更好地组织和利用数据。</p> 
<p>通过阅读本文，希望能够帮助我们掌握 Elasticsearch 和 Kibana 的核心概念，具备构建强大搜索引擎和可视化工具的能力，以及解决中文分词和文档管理方面的专业知识。</p> 
<h2><a id="_Docker__ElasticSearch__Kibana_14"></a>一、使用 Docker 部署 ElasticSearch 和 Kibana</h2> 
<p>因为需要使用 Docker 部署 ElasticSearch 和 Kibana ，并且它们相互之间需要进行网络通信，所有首先创建一个虚拟网络，然后在运行容器的时候，加入这个网络即可。</p> 
<pre><code class="prism language-sh"><span class="token function">docker</span> network create es-net
</code></pre> 
<h3><a id="11__ElasticSearch_22"></a>1.1 部署 ElasticSearch</h3> 
<blockquote> 
 <ol><li><strong>获取 ElasticSearch 镜像：</strong></li></ol> 
</blockquote> 
<p>首先我们需要从 DockerHub 中拉取 ElasticSearch 镜像：</p> 
<p>DockerHup 地址：<a href="https://hub.docker.com/_/elasticsearch" rel="nofollow">https://hub.docker.com/_/elasticsearch</a>。</p> 
<pre><code class="prism language-sh"><span class="token function">docker</span> pull elasticsearch
</code></pre> 
<p>注意，ElasticSearch 的镜像体积比较大，如果能找到镜像的<code>tar</code>包的话，最好使用<code>tar</code>包加载。此时我使用的是 <code>7.12.1</code>版本的镜像：</p> 
<p><img src="https://images2.imgbox.com/99/9e/skPUCMgJ_o.png" alt=""></p> 
<p>使用命令加载这个 <code>tar</code> 包：</p> 
<pre><code class="prism language-sh"><span class="token function">docker</span> load <span class="token parameter variable">-i</span> es.tar
</code></pre> 
<blockquote> 
 <ol start="2"><li><strong>运行 ElasticSearch 容器</strong></li></ol> 
</blockquote> 
<p>运行 Docker 命令，部署单点的 ElasticSearch 服务：</p> 
<pre><code class="prism language-bash"><span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token punctuation">\</span>
	<span class="token parameter variable">--name</span> es <span class="token punctuation">\</span>
    <span class="token parameter variable">-e</span> <span class="token string">"ES_JAVA_OPTS=-Xms512m -Xmx512m"</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">-e</span> <span class="token string">"discovery.type=single-node"</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">-v</span> es-data:/usr/share/elasticsearch/data <span class="token punctuation">\</span>
    <span class="token parameter variable">-v</span> es-plugins:/usr/share/elasticsearch/plugins <span class="token punctuation">\</span>
    <span class="token parameter variable">--privileged</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--network</span> es-net <span class="token punctuation">\</span>
    <span class="token parameter variable">-p</span> <span class="token number">9200</span>:9200 <span class="token punctuation">\</span>
    <span class="token parameter variable">-p</span> <span class="token number">9300</span>:9300 <span class="token punctuation">\</span>
elasticsearch:7.12.1
</code></pre> 
<p><strong>上述 Docker 命令是为了运行 Elasticsearch 容器。下面是对命令的解释：</strong></p> 
<ol><li> <p><strong><code>docker run -d</code>：</strong> 这部分表示在后台运行容器。</p> </li><li> <p><strong><code>--name es</code>：</strong> 为容器指定一个名字，这里是 “es”。</p> </li><li> <p><strong><code>-e "ES_JAVA_OPTS=-Xms512m -Xmx512m"</code>：</strong> 设置 Java 虚拟机的参数，包括初始堆内存大小 (<code>-Xms</code>) 和最大堆内存大小 (<code>-Xmx</code>)，这里都设置为 512MB。</p> </li><li> <p><strong><code>-e "discovery.type=single-node"</code>：</strong> 设置 Elasticsearch 的节点发现机制为单节点，因为在这个配置中只有一个 Elasticsearch 实例。</p> </li><li> <p><strong><code>-v es-data:/usr/share/elasticsearch/data</code>：</strong> 将容器内 Elasticsearch 的数据目录挂载到宿主机的名为 “es-data” 的卷上，以便数据持久化。</p> </li><li> <p><strong><code>-v es-plugins:/usr/share/elasticsearch/plugins</code>：</strong> 类似上面，将容器内 Elasticsearch 的插件目录挂载到宿主机的名为 “es-plugins” 的卷上。</p> </li><li> <p><strong><code>--privileged</code>：</strong> 赋予容器一些特权，可能会有一些安全风险，需要慎用。</p> </li><li> <p><strong><code>--network es-net</code>：</strong> 将容器连接到名为 “es-net” 的网络上，目的是为了与其他容器进行通信。</p> </li><li> <p><strong><code>-p 9200:9200 -p 9300:9300</code>：</strong> 将容器内部的端口映射到宿主机上，这里分别是 Elasticsearch 的 HTTP REST API 端口（9200）和节点间通信的端口（9300）。</p> </li><li> <p><strong><code>elasticsearch:7.12.1</code>：</strong> 指定要运行的 Docker 镜像的名称和版本号，这里是 Elasticsearch 7.12.1 版本。</p> </li></ol> 
<p>这个命令配置了 ElasticSearch 的运行参数、数据卷、网络等，使其能够在后台运行，并且可以通过指定的端口访问 Elasticsearch 的 API。</p> 
<p><strong>当运行完这个命令之后，我们可以在浏览器中访问 <code>宿主机IP:9200</code>，即可看到 ElasticSearch 的响应结果：</strong></p> 
<p><img src="https://images2.imgbox.com/d9/0a/X9f7S7uY_o.png" alt=""></p> 
<h3><a id="12__Kibana_91"></a>1.2 部署 Kibana</h3> 
<blockquote> 
 <ol><li><strong>获取 Kibana 镜像</strong></li></ol> 
</blockquote> 
<p>Kibana 是 Elastic 官方提供的一个 ElasticSearch 的可视化界面，通过这个界面可以更好地对 ElasticSearch 进行操作。</p> 
<p>想要使用 Docker 部署 Kibana，首先同样需要从 DockerHub 中获取镜像：</p> 
<p>DockerHub：<a href="https://hub.docker.com/_/kibana" rel="nofollow">https://hub.docker.com/_/kibana</a>。</p> 
<pre><code class="prism language-bash"><span class="token function">docker</span> pull kibana
</code></pre> 
<p>同样的，Kibana 镜像的大小也超过了一个 G，因此如果能找到 <code>tar</code> 的话，尽量使用 <code>load</code> 获取镜像：</p> 
<p><img src="https://images2.imgbox.com/d0/ae/TAe3zr64_o.png" alt=""></p> 
<pre><code class="prism language-bash"><span class="token function">docker</span> load <span class="token parameter variable">-i</span> kibana
</code></pre> 
<blockquote> 
 <ol start="2"><li><strong>运行 Kibana 容器</strong></li></ol> 
</blockquote> 
<p>使用 Docker 命令运行 Kibana 容器：</p> 
<pre><code class="prism language-bash"><span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token punctuation">\</span>
<span class="token parameter variable">--name</span> kibana <span class="token punctuation">\</span>
<span class="token parameter variable">-e</span> <span class="token assign-left variable">ELASTICSEARCH_HOSTS</span><span class="token operator">=</span>http://es:9200 <span class="token punctuation">\</span>
<span class="token parameter variable">--network</span><span class="token operator">=</span>es-net <span class="token punctuation">\</span>
<span class="token parameter variable">-p</span> <span class="token number">5601</span>:5601  <span class="token punctuation">\</span>
kibana:7.12.1
</code></pre> 
<p>这 Docker 命令是为了运行 Kibana 容器，它与 Elasticsearch 一同组成了 ELK（Elasticsearch, Logstash, Kibana）技术栈的一部分。下面是对命令各个部分的解释：</p> 
<ol><li> <p><strong><code>docker run -d</code>：</strong> 在后台运行容器。</p> </li><li> <p><strong><code>--name kibana</code>：</strong> 为容器指定一个名字，这里是 “kibana”。</p> </li><li> <p><strong><code>-e ELASTICSEARCH_HOSTS=http://es:9200</code>：</strong> 设置 Kibana 运行时连接的 Elasticsearch 节点的地址，这里指定了 Elasticsearch 服务的地址为 <code>http://es:9200</code>，其中 “es” 是 Elasticsearch 服务的容器名，而不是具体的 IP 地址。这是因为在 <code>--network=es-net</code> 中指定了容器连接到 “es-net” 网络，容器名会被解析为相应的 IP 地址。</p> </li><li> <p><strong><code>--network=es-net</code>：</strong> 将容器连接到名为 “es-net” 的网络上，确保 Kibana 能够与 Elasticsearch 容器进行通信。</p> </li><li> <p><strong><code>-p 5601:5601</code>：</strong> 将容器内部的 5601 端口映射到宿主机上，允许通过宿主机的 5601 端口访问 Kibana 的 Web 界面。</p> </li><li> <p><strong><code>kibana:7.12.1</code>：</strong> 指定要运行的 Docker 镜像的名称和版本号，这里是 Kibana 7.12.1 版本。</p> </li></ol> 
<p>这个命令会在启动 Kibana 的同时连接到 Elasticsearch 服务，并映射 Kibana 的 Web 界面端口到宿主机，以便通过浏览器访问 Kibana 的用户界面进行 Elasticsearch 数据的可视化和管理。</p> 
<p><strong>当运行了这个命令之后，可以在浏览器中访问 <code>宿主机IP:5601</code>，然后就会进入 Kibana 的可视化界面：</strong></p> 
<p><img src="https://images2.imgbox.com/58/35/ElQzGcGE_o.png" alt=""></p> 
<h3><a id="13__Kibana__Elasticsearch__147"></a>1.3 利用 Kibana 演示 Elasticsearch 分词效果</h3> 
<p>在 Kibana 的可视化界面中，提供了一个 DevTools，在这个界面中我们就可以来编写 DSL 命令来操作 ElasticSearch，并且对 DSL 语句有提示和自动补全功能。</p> 
<p><strong>1. 打开 Kibana DevTools</strong></p> 
<p>首先，在 Kibana 的界面中找到左侧导航栏中的 “DevTools”，点击进入 DevTools 页面。</p> 
<p><img src="https://images2.imgbox.com/82/75/LW8Yaqk7_o.png" alt=""></p> 
<p><strong>2. 编写分词演示 DSL 命令</strong></p> 
<p>在 DevTools 中，我们可以直接编写 DSL（Domain Specific Language，领域特定语言）命令来与 Elasticsearch 进行交互。以下是一个简单的演示，我们将使用 analyze API 来查看文本在分词阶段的效果。</p> 
<blockquote> 
 <p><strong>什么是 DSL?</strong><br> 领域特定语言（Domain-Specific Language，DSL）是一种专注于解决特定问题领域的编程语言。与通用编程语言（如Java、Python）不同，DSL 被设计用来解决某一领域的具体问题，通常具有更高的表达能力和更简洁的语法。</p> 
</blockquote> 
<p>例如，现在使用了 <code>standard</code> 分析器，它是 Elasticsearch 默认的分析器之一。要分析的文本是 “Elasticsearch is a powerful search engine.”。</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
  <span class="token string-property property">"analyzer"</span><span class="token operator">:</span> <span class="token string">"standard"</span><span class="token punctuation">,</span>
  <span class="token string-property property">"text"</span><span class="token operator">:</span> <span class="token string">"Elasticsearch is a powerful search engine."</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>点击三角符号向 ElasticSearch 服务发送 HTTP 请求：<br> <img src="https://images2.imgbox.com/5e/58/7KAS07nQ_o.png" alt=""></p> 
<ol start="3"><li>查看分词效果</li></ol> 
<p><img src="https://images2.imgbox.com/75/53/5TqqPNsV_o.png" alt=""><br> 上述结果显示了文本被分解成了多个词条（tokens），每个词条都有其在文本中的起始和结束位置。这样的分析对于搜索引擎来说是关键的，因为它决定了搜索时如何匹配文本。</p> 
<p>另外，Elasticsearch 支持多种分析器，如：<code>simple</code>、<code>whitespace</code>、<code>english</code> 等，每个分析器有其自己的特性。选择不同的分词器，分词所得的结果也会有所不同。</p> 
<h2><a id="_188"></a>二、解决中文分词的问题</h2> 
<h3><a id="21__189"></a>2.1 默认分词器对中文分词的问题</h3> 
<p>在 Elasticsearch 中，默认的分词器在处理中文文本时可能会遇到一些问题，无法很好地将中文文本切分成有意义的词条。</p> 
<p><strong>例如下面的例子：</strong></p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
  <span class="token string-property property">"analyzer"</span><span class="token operator">:</span> <span class="token string">"standard"</span><span class="token punctuation">,</span>
  <span class="token string-property property">"text"</span><span class="token operator">:</span> <span class="token string">"Elasticsearch 是一个强大的搜索引擎。"</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>使用上面的 DSL 语句得到的分词结果为：</p> 
<p><img src="https://images2.imgbox.com/12/86/ErSQ4b02_o.png" alt=""><br> 此时发现，对于中文的分词是一个字就被分成了一个词条，要解决这个问题可以使用 IK 分词器。</p> 
<h3><a id="22__IK__206"></a>2.2 引入 IK 分词器</h3> 
<p>当需要处理中文分词使，一般都会使用 IK 分词器。</p> 
<p><strong>项目下载地址：</strong> <a href="https://github.com/medcl/elasticsearch-analysis-ik">https://github.com/medcl/elasticsearch-analysis-ik</a>。</p> 
<p>为 ElasticSearch 引入 IK 分词器有两种方法：</p> 
<p><strong>1. 在线按照 IK 插件</strong></p> 
<p>按照的步骤和命令如下：</p> 
<p>a）进入 ElasticSearch 容器内部</p> 
<pre><code class="prism language-bash"><span class="token function">docker</span> <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> elasticsearch /bin/bash
</code></pre> 
<p>b）在线下载并安装</p> 
<pre><code class="prism language-bash">./bin/elasticsearch-plugin  <span class="token function">install</span> https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.12.1/elasticsearch-analysis-ik-7.12.1.zip
</code></pre> 
<p>c）退出 ElasticSearch 容器</p> 
<pre><code class="prism language-bash"><span class="token builtin class-name">exit</span>
</code></pre> 
<p>d）重启 ElasticSearch 容器</p> 
<pre><code class="prism language-bash"><span class="token function">docker</span> restart elasticsearch
</code></pre> 
<p><strong>2. 离线按照 IK 插件</strong></p> 
<p>a）查看数据卷目录</p> 
<p>安装插件需要知道 ElasticSearch 的<code>plugins</code>目录位置，我们中运行 ES 容器的时候实现了数据卷的挂载，因此需要查看 ES 容器的数据卷目录，通过下面命令查看：</p> 
<pre><code class="prism language-bash"> <span class="token function">docker</span> volume inspect es-plugins
</code></pre> 
<p><img src="https://images2.imgbox.com/5b/7d/MTKnTUVL_o.png" alt=""></p> 
<p>b）进入挂载目录，然后之间将下载好的 IK 插件解压重命名后放到这个挂载目录下<br> <img src="https://images2.imgbox.com/df/5d/kTN7onUf_o.png" alt=""></p> 
<p><img src="https://images2.imgbox.com/cf/7d/hbGBrHUS_o.png" alt=""></p> 
<p>c）重启 ES 容器</p> 
<pre><code class="prism language-bash"><span class="token function">docker</span> restart es
</code></pre> 
<p>通过命令 <code>docker logs -f es</code> 查看 ES 容器的启动日志，就可以发现已经成功加载了 IK 分词器：<br> <img src="https://images2.imgbox.com/7e/97/bmTxq4X3_o.png" alt=""></p> 
<h3><a id="23_IK__261"></a>2.3 IK 分词器的两种分词模式</h3> 
<p>IK 分词器是一款专为中文文本设计的分词器，在 Elasticsearch 中包含两种主要的分词模式：<code>ik_smart</code> 和 <code>ik_max_word</code>。这两种模式在分词的细粒度上有所不同，适用于不同的应用场景。</p> 
<p><strong>下面演示两种模式的区别：</strong></p> 
<p>a）<code>ik_smart</code> 模式</p> 
<p><strong>DSL 语句：</strong></p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
  <span class="token string-property property">"analyzer"</span><span class="token operator">:</span> <span class="token string">"ik_smart"</span><span class="token punctuation">,</span>
  <span class="token string-property property">"text"</span><span class="token operator">:</span> <span class="token string">"Elasticsearch 是一个强大的搜索引擎。"</span>
<span class="token punctuation">}</span>
</code></pre> 
<p><strong>分词结果：</strong><br> <img src="https://images2.imgbox.com/26/9d/jOWLhmah_o.png" alt=""><br> 在 <code>ik_smart</code> 模式下，IK 分词器会进行最少切分，尽量保留专有名称和词组，以提高整体的可读性。</p> 
<p>b）<code>ik_max_word</code>模式</p> 
<p><strong>DSL 语句：</strong></p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
  <span class="token string-property property">"analyzer"</span><span class="token operator">:</span> <span class="token string">"ik_max_word"</span><span class="token punctuation">,</span>
  <span class="token string-property property">"text"</span><span class="token operator">:</span> <span class="token string">"Elasticsearch 是一个强大的搜索引擎。"</span>
<span class="token punctuation">}</span>
</code></pre> 
<p><strong>分词结果：</strong></p> 
<p><img src="https://images2.imgbox.com/50/4e/bc2Xrmqn_o.png" alt=""><br> 在 <code>ik_max_word</code> 模式下，IK 分词器会进行最细粒度的切分，尽量将文本拆分成单个词语，包括专有名称的切分。例如，对于 “搜索引擎”来说还进一步细分成了：“搜索”、“索引”、“引擎”。</p> 
<h3><a id="24_IK__301"></a>2.4 IK 分词器存在的问题</h3> 
<p>尽管 IK 分词器可以针对中文进行分词，但是随着互联网的不断发展，越来越多的网络用语和新的词汇在不断的产生，例如下面的例子：</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
  <span class="token string-property property">"analyzer"</span><span class="token operator">:</span> <span class="token string">"ik_smart"</span><span class="token punctuation">,</span>
  <span class="token string-property property">"text"</span><span class="token operator">:</span> <span class="token string">"鸡你太美，鸡哥，坤坤，奥利给，啦啦啦，么么么，哒哒哒"</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>此时对于上述词汇的分词结果为：<br> <img src="https://images2.imgbox.com/24/42/o3kvvICc_o.png" alt=""><br> 通过这个结果可以发现，IK 分词器并不能对新的网络用语进行分词，并且一些语气词也需要参与分词。</p> 
<h3><a id="25_IK__315"></a>2.5 IK 分词器拓展词库和停用词条</h3> 
<p>要解决上面使用 IK 分词器存在的问题可以对词典进行拓展和停用一些不必要或者敏感的词汇。</p> 
<p><strong>拓展词库：</strong></p> 
<p>要拓展 IK 分词器的词库，只需要修改 IK 分词器目录中的<code>config</code>目录下的<code>IkAnalyzer.cfg.xml</code>文件：</p> 
<pre><code class="prism language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span>
<span class="token doctype"><span class="token punctuation">&lt;!</span><span class="token doctype-tag">DOCTYPE</span> <span class="token name">properties</span> <span class="token name">SYSTEM</span> <span class="token string">"http://java.sun.com/dtd/properties.dtd"</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>properties</span><span class="token punctuation">&gt;</span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>comment</span><span class="token punctuation">&gt;</span></span>IK Analyzer 扩展配置<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>comment</span><span class="token punctuation">&gt;</span></span>
		<span class="token comment">&lt;!--用户可以在这里配置自己的扩展字典 --&gt;</span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>entry</span> <span class="token attr-name">key</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>ext_dict<span class="token punctuation">"</span></span><span class="token punctuation">&gt;</span></span>ext.dic<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>entry</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>properties</span><span class="token punctuation">&gt;</span></span>
</code></pre> 
<p>然后在名为<code>ext.dic</code>的文件中，添加想要拓展的词语即可：</p> 
<p><img src="https://images2.imgbox.com/86/17/rLWaTJSY_o.png" alt=""></p> 
<p><strong>停用词条：</strong></p> 
<p>要禁用某些不需要参与分词或者某些敏感词条，也是需要修改 IK 分词器目录中的<code>config</code>目录中的<code>IkAnalyzer.cfg.xml</code>文件：</p> 
<pre><code class="prism language-bash"><span class="token operator">&lt;</span>?xml <span class="token assign-left variable">version</span><span class="token operator">=</span><span class="token string">"1.0"</span> <span class="token assign-left variable">encoding</span><span class="token operator">=</span><span class="token string">"UTF-8"</span>?<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span><span class="token operator">!</span>DOCTYPE properties SYSTEM <span class="token string">"http://java.sun.com/dtd/properties.dtd"</span><span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>properties<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>comment<span class="token operator">&gt;</span>IK Analyzer 扩展配置<span class="token operator">&lt;</span>/comment<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span><span class="token operator">!</span>--用户可以在这里配置自己的扩展字典 --<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>entry <span class="token assign-left variable">key</span><span class="token operator">=</span><span class="token string">"ext_dict"</span><span class="token operator">&gt;</span>ext.dic<span class="token operator">&lt;</span>/entry<span class="token operator">&gt;</span>
	 <span class="token operator">&lt;</span><span class="token operator">!</span>--用户可以在这里配置自己的扩展停止词字典--<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>entry <span class="token assign-left variable">key</span><span class="token operator">=</span><span class="token string">"ext_stopwords"</span><span class="token operator">&gt;</span>stopword.dic<span class="token operator">&lt;</span>/entry<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>/properties<span class="token operator">&gt;</span>

</code></pre> 
<p><strong>然后在名为<code>stopword.dic</code>的文件中，添加想要停用的词语即可：</strong><br> <img src="https://images2.imgbox.com/34/4f/VdeBwN2w_o.png" alt=""></p> 
<p>配置完成，重启 ES 容器，再次运行刚才的 DSL 语句：</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
  <span class="token string-property property">"analyzer"</span><span class="token operator">:</span> <span class="token string">"ik_smart"</span><span class="token punctuation">,</span>
  <span class="token string-property property">"text"</span><span class="token operator">:</span> <span class="token string">"鸡你太美，鸡哥，坤坤，奥利给，啦啦啦，么么么，哒哒哒"</span>
<span class="token punctuation">}</span>
</code></pre> 
<p><strong>此时的运行结果：</strong></p> 
<p><img src="https://images2.imgbox.com/10/45/dvFUOV7V_o.png" alt=""><br> 至此，就成功为 IK 分词器拓展和停用了词条。</p> 
<h2><a id="_Kibana__371"></a>三、使用 Kibana 操作索引库</h2> 
<h2><a id="_Kibana__374"></a>三、使用 Kibana 操作索引库</h2> 
<h3><a id="31_Mapping__376"></a>3.1 Mapping 属性</h3> 
<p>在使用 Kibana 操作索引库之前，了解索引的 mapping 属性是非常重要的。Mapping 是对索引库中文档的结构和字段的约束，它定义了文档中每个字段的数据类型、分词器等属性。以下是一些常见的 mapping 属性：</p> 
<p><strong>1. <code>type</code> 字段数据类型</strong></p> 
<ul><li> <p><strong>字符串类型：</strong></p> 
  <ul><li><code>text</code>：用于存储可分词的文本，通常用于全文搜索。</li><li><code>keyword</code>：精确匹配的关键字，不分词，常用于过滤、聚合等场景。</li></ul> </li><li> <p><strong>数值类型：</strong></p> 
  <ul><li><code>long</code>, <code>integer</code>, <code>short</code>, <code>byte</code>, <code>double</code>, <code>float</code>：用于存储数值。</li></ul> </li><li> <p><strong>布尔类型：</strong></p> 
  <ul><li><code>boolean</code>：存储布尔值。</li></ul> </li><li> <p><strong>日期类型：</strong></p> 
  <ul><li><code>date</code>：存储日期和时间。</li></ul> </li><li> <p><strong>对象类型：</strong></p> 
  <ul><li><code>object</code>：存储复杂结构的对象。</li></ul> </li></ul> 
<p><strong>2. <code>index</code> 是否创建索引</strong></p> 
<ul><li>默认为 <code>true</code>，表示创建索引。可以设置为 <code>false</code>，表示不创建索引，该字段不可搜索。</li></ul> 
<p><strong>3. <code>analyzer</code> 使用的分词器</strong></p> 
<ul><li>定义字段使用的分词器，影响搜索和分词行为。</li></ul> 
<p><strong>4. <code>properties</code> 子字段</strong></p> 
<ul><li>如果字段是对象类型，可以定义子字段的 mapping。</li></ul> 
<p><strong>示例：</strong></p> 
<pre><code class="prism language-json"><span class="token constant">PUT</span> <span class="token operator">/</span>my_index
<span class="token punctuation">{<!-- --></span>
  <span class="token string-property property">"mappings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string-property property">"properties"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string-property property">"title"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string-property property">"type"</span><span class="token operator">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span>
        <span class="token string-property property">"analyzer"</span><span class="token operator">:</span> <span class="token string">"standard"</span>
      <span class="token punctuation">}</span><span class="token punctuation">,</span>
      <span class="token string-property property">"price"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string-property property">"type"</span><span class="token operator">:</span> <span class="token string">"double"</span>
      <span class="token punctuation">}</span><span class="token punctuation">,</span>
      <span class="token string-property property">"is_available"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string-property property">"type"</span><span class="token operator">:</span> <span class="token string">"boolean"</span>
      <span class="token punctuation">}</span><span class="token punctuation">,</span>
      <span class="token string-property property">"created_at"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string-property property">"type"</span><span class="token operator">:</span> <span class="token string">"date"</span>
      <span class="token punctuation">}</span><span class="token punctuation">,</span>
      <span class="token string-property property">"category"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string-property property">"type"</span><span class="token operator">:</span> <span class="token string">"keyword"</span>
      <span class="token punctuation">}</span><span class="token punctuation">,</span>
      <span class="token string-property property">"details"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string-property property">"type"</span><span class="token operator">:</span> <span class="token string">"object"</span><span class="token punctuation">,</span>
        <span class="token string-property property">"properties"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
          <span class="token string-property property">"description"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
            <span class="token string-property property">"type"</span><span class="token operator">:</span> <span class="token string">"text"</span>
          <span class="token punctuation">}</span><span class="token punctuation">,</span>
          <span class="token string-property property">"manufacturer"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
            <span class="token string-property property">"type"</span><span class="token operator">:</span> <span class="token string">"keyword"</span>
          <span class="token punctuation">}</span>
        <span class="token punctuation">}</span>
      <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>上述示例定义了一个索引的 mapping，包含了不同类型的字段以及它们的属性。理解 mapping 对于在 Kibana 中正确操作索引非常重要，它影响了索引中文档的结构和 Elasticsearch 的行为。</p> 
<h3><a id="32__452"></a>3.2 创建和获取索引</h3> 
<p>在 Elasticsearch 中，创建和获取索引是操作索引库的基本步骤之一。下面是如何在 Kibana 中执行 DSL 代码来创建和获取索引的详细说明：</p> 
<blockquote> 
 <ol><li>创建索引</li></ol> 
</blockquote> 
<p><strong>DSL 代码：</strong></p> 
<pre><code class="prism language-json"><span class="token constant">PUT</span> <span class="token operator">/</span>demo
<span class="token punctuation">{<!-- --></span>
  <span class="token string-property property">"mappings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string-property property">"properties"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string-property property">"info"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string-property property">"type"</span><span class="token operator">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span>
        <span class="token string-property property">"analyzer"</span><span class="token operator">:</span> <span class="token string">"ik_smart"</span>
      <span class="token punctuation">}</span><span class="token punctuation">,</span>
      <span class="token string-property property">"email"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
        <span class="token string-property property">"type"</span><span class="token operator">:</span> <span class="token string">"keyword"</span><span class="token punctuation">,</span>
        <span class="token string-property property">"index"</span><span class="token operator">:</span> <span class="token boolean">false</span>
      <span class="token punctuation">}</span><span class="token punctuation">,</span>
      <span class="token string-property property">"name"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
        <span class="token string-property property">"properties"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
          <span class="token string-property property">"firstName"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
            <span class="token string-property property">"type"</span><span class="token operator">:</span><span class="token string">"keyword"</span>
          <span class="token punctuation">}</span><span class="token punctuation">,</span>
          <span class="token string-property property">"lastName"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
            <span class="token string-property property">"type"</span><span class="token operator">:</span><span class="token string">"keyword"</span>
          <span class="token punctuation">}</span>
        <span class="token punctuation">}</span>
      <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p><strong>说明：</strong></p> 
<ul><li>使用 <code>PUT</code> 请求创建一个名为 <code>demo</code> 的索引库。</li><li>在 <code>mappings</code> 中定义了不同字段的属性，如 <code>info</code> 使用了中文分词器 <code>ik_smart</code>。</li><li><code>email</code> 字段设置了 <code>index</code> 为 <code>false</code>，表示不对该字段创建索引，该字段不可搜索。</li><li><code>name</code> 字段是一个对象类型，包含 <code>firstName</code> 和 <code>lastName</code> 两个子字段。</li></ul> 
<p><strong>执行结果：</strong></p> 
<p>在 Kibana 的 DevTools 中执行上述代码，会成功创建名为 <code>demo</code> 的索引库。</p> 
<p><img src="https://images2.imgbox.com/05/f7/FAOLclpH_o.png" alt="创建索引结果"></p> 
<blockquote> 
 <ol start="2"><li>获取索引</li></ol> 
</blockquote> 
<p><strong>DSL 代码：</strong></p> 
<pre><code class="prism language-bash">GET /索引名
</code></pre> 
<p><strong>说明：</strong></p> 
<ul><li>使用 <code>GET</code> 请求获取名为 <code>demo</code> 的索引库的信息。</li></ul> 
<p><strong>执行结果：</strong></p> 
<p>在 Kibana 的 DevTools 中执行上述代码，会获取刚刚创建的 <code>demo</code> 索引的信息。</p> 
<p><img src="https://images2.imgbox.com/b7/5d/9GQruqom_o.png" alt="获取索引结果"></p> 
<p>通过以上步骤，可以成功创建索引并获取索引的信息。在实际应用中，创建和管理索引是构建 Elasticsearch 数据结构的基础，对于数据的存储和检索具有重要作用。</p> 
<h3><a id="33__521"></a>3.3 修改索引库</h3> 
<p>在 Elasticsearch 中，一旦索引库和其对应的 mapping 创建后，mapping 一般无法直接修改。但是，我们可以通过添加新的字段的方式来间接修改索引库。下面是具体的操作步骤：</p> 
<p><strong>DSL 代码：</strong></p> 
<pre><code class="prism language-json"><span class="token constant">PUT</span> <span class="token operator">/</span>demo<span class="token operator">/</span>_mapping
<span class="token punctuation">{<!-- --></span>
  <span class="token string-property property">"properties"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string-property property">"age"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string-property property">"type"</span><span class="token operator">:</span> <span class="token string">"integer"</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p><strong>说明：</strong></p> 
<ul><li>使用 <code>PUT</code> 请求修改 <code>demo</code> 索引库的 mapping。</li><li>在 <code>properties</code> 中定义了一个新的字段 <code>age</code>，数据类型为 <code>integer</code>。</li></ul> 
<p><strong>执行结果：</strong></p> 
<p>在 Kibana 的 DevTools 中执行上述代码，成功为 <code>demo</code> 索引库添加了一个新的字段 <code>age</code>。</p> 
<p><img src="https://images2.imgbox.com/ac/31/R3ZaicK3_o.png" alt="修改索引库结果"></p> 
<p>通过这种方式，我们可以对索引库进行修改，添加新的字段，以满足数据结构的演进和变化需求。需要注意的是，对于已经存在的文档，新添加的字段默认是 <code>null</code> 或者空值，具体行为取决于字段的数据类型。</p> 
<h3><a id="34__553"></a>3.4 删除索引库</h3> 
<p>在 Elasticsearch 中，如果需要删除一个索引库，可以使用 <code>DELETE</code> 请求。以下是删除 <code>demo</code> 索引库的操作步骤：</p> 
<p><strong>DSL 代码：</strong></p> 
<pre><code class="prism language-json"><span class="token constant">DELETE</span> <span class="token operator">/</span>demo
</code></pre> 
<p><strong>执行结果：</strong></p> 
<p>在 Kibana 的 DevTools 中执行上述代码，成功删除了名为 <code>demo</code> 的索引库。</p> 
<p><img src="https://images2.imgbox.com/27/05/Z1SEN4o2_o.png" alt="删除索引库结果"></p> 
<p><strong>注意：</strong></p> 
<ul><li>删除索引库是一个慎重的操作，会删除索引库中的所有数据和 mapping。</li><li>如果尝试获取已删除的索引，将会得到 404 错误。</li></ul> 
<p><img src="https://images2.imgbox.com/69/81/tQOVuWo3_o.png" alt=""></p> 
<p>通过删除索引库，可以清空数据和结构，适用于重新创建索引或者不再需要的索引的情况。</p> 
<h2><a id="_Kibana__578"></a>四、使用 Kibana 实现对文档的增删改查</h2> 
<h3><a id="41__580"></a>4.1 新增文档</h3> 
<p>在 Elasticsearch 中，新增文档是将数据存储到索引库中的过程。以下是在 Kibana DevTools 中使用 DSL 语法新增文档的详细说明：</p> 
<p><strong>DSL 代码：</strong></p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> <span class="token operator">/</span>demo<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">1</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string-property property">"info"</span><span class="token operator">:</span> <span class="token string">"五虎上将之一"</span><span class="token punctuation">,</span>
  <span class="token string-property property">"email"</span><span class="token operator">:</span> <span class="token string">"zy@demo.cn"</span><span class="token punctuation">,</span>
  <span class="token string-property property">"name"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string-property property">"firstName"</span><span class="token operator">:</span> <span class="token string">"云"</span><span class="token punctuation">,</span>
    <span class="token string-property property">"lastName"</span><span class="token operator">:</span> <span class="token string">"赵"</span>
  <span class="token punctuation">}</span><span class="token punctuation">,</span>
  <span class="token string-property property">"age"</span><span class="token operator">:</span> <span class="token number">18</span>
<span class="token punctuation">}</span>
</code></pre> 
<p><strong>说明：</strong></p> 
<ul><li>使用 <code>POST</code> 请求将文档新增到 <code>demo</code> 索引库中。</li><li><code>_doc</code> 表示文档类型，<code>1</code> 是文档的唯一标识符。</li><li>文档内容包括字段 <code>info</code>、<code>email</code>、<code>name</code> 和 <code>age</code>。</li></ul> 
<p><strong>执行结果：</strong></p> 
<p>在 Kibana 的 DevTools 中执行上述代码，成功新增了一个文档到 <code>demo</code> 索引库中。</p> 
<p><img src="https://images2.imgbox.com/c0/1b/tiguK5AY_o.png" alt="新增文档结果"></p> 
<p>通过新增文档，可以将数据灵活地存储到 Elasticsearch 中，以便后续的检索和分析。</p> 
<h3><a id="42__616"></a>4.2 获取和删除文档</h3> 
<p><strong>1. 获取文档</strong></p> 
<p><strong>语法：</strong></p> 
<pre><code class="prism language-json"><span class="token constant">GET</span> <span class="token operator">/</span>索引库名<span class="token operator">/</span>_doc<span class="token operator">/</span>文档id 
</code></pre> 
<p><strong>例如，获取刚才新增的文档：</strong></p> 
<pre><code class="prism language-json"><span class="token constant">GET</span> <span class="token operator">/</span>demo<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">1</span>
</code></pre> 
<p><strong>执行结果：</strong></p> 
<p><img src="https://images2.imgbox.com/50/65/POssg7KP_o.png" alt="获取文档结果"></p> 
<p><strong>2. 删除文档</strong></p> 
<p>删除文档使用的方法是 <code>DELETE</code>，DSL 语法如下：</p> 
<pre><code class="prism language-json"><span class="token constant">DELETE</span> <span class="token operator">/</span>索引库名<span class="token operator">/</span>_doc<span class="token operator">/</span>文档id
</code></pre> 
<p><strong>例如，删除刚才新增的文档：</strong></p> 
<pre><code class="prism language-json"><span class="token constant">DELETE</span> <span class="token operator">/</span>demo<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">1</span>
</code></pre> 
<p>文档删除成功后，Elasticsearch 会返回一个包含删除信息的 JSON 响应。通过上述操作，我们可以灵活地对文档进行获取和删除，适用于不同的数据管理需求。</p> 
<h3><a id="43__653"></a>4.3 修改文档：全量修改和增量修改</h3> 
<p>在 Elasticsearch 中，对文档的修改主要分为全量修改和增量修改两种方式。</p> 
<p><strong>1. 全量修改</strong></p> 
<p>全量修改使用的方法是 <code>PUT</code>，它会删除旧文档并添加新文档。全量修改的语法如下：</p> 
<pre><code class="prism language-json"><span class="token constant">PUT</span> <span class="token operator">/</span>索引库名<span class="token operator">/</span>_doc<span class="token operator">/</span>文档id
<span class="token punctuation">{<!-- --></span>
    <span class="token string-property property">"字段1"</span><span class="token operator">:</span> <span class="token string">"值1"</span><span class="token punctuation">,</span>
    <span class="token string-property property">"字段2"</span><span class="token operator">:</span> <span class="token string">"值2"</span><span class="token punctuation">,</span>
    <span class="token comment">// ... 省略</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>例如，对刚才新增的文档进行全量修改：</p> 
<pre><code class="prism language-json"><span class="token constant">PUT</span> <span class="token operator">/</span>demo<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">1</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string-property property">"info"</span><span class="token operator">:</span> <span class="token string">"五虎上将之一"</span><span class="token punctuation">,</span>
  <span class="token string-property property">"email"</span><span class="token operator">:</span> <span class="token string">"ZhaoYun@demo.cn"</span><span class="token punctuation">,</span>
  <span class="token string-property property">"name"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string-property property">"firstName"</span><span class="token operator">:</span> <span class="token string">"云"</span><span class="token punctuation">,</span>
    <span class="token string-property property">"lastName"</span><span class="token operator">:</span> <span class="token string">"赵"</span>
  <span class="token punctuation">}</span><span class="token punctuation">,</span>
  <span class="token string-property property">"age"</span><span class="token operator">:</span> <span class="token number">18</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>执行结果如下：</p> 
<p><img src="https://images2.imgbox.com/40/f5/52UpWs4v_o.png" alt="全量修改结果"></p> 
<p>实际上，全量修改相当于删除原有的文档内容，然后进行新增操作。因此，此处 <code>PUT</code> 的作用也相当于新增。</p> 
<p>** 2. 增量修改**</p> 
<p>增量修改使用的方法是 <code>POST</code>，它可以修改指定字段的值。增量修改的语法如下：</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> <span class="token operator">/</span>索引库名<span class="token operator">/</span>_update<span class="token operator">/</span>文档id
<span class="token punctuation">{<!-- --></span>
    <span class="token string-property property">"doc"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
         <span class="token string-property property">"字段名"</span><span class="token operator">:</span> <span class="token string">"新的值"</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>例如，对刚才新增的文档进行增量修改：</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> <span class="token operator">/</span>demo<span class="token operator">/</span>_update<span class="token operator">/</span><span class="token number">1</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string-property property">"doc"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
    <span class="token string-property property">"email"</span><span class="token operator">:</span><span class="token string">"ZYun@demo.cn"</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>执行结果如下：</p> 
<p><img src="https://images2.imgbox.com/87/0a/NrDA0FLH_o.png" alt="增量修改结果"></p> 
<p>通过上述操作，我们可以实现对文档内容的全量修改和增量修改，具体选择哪种方式取决于业务需求和数据管理的策略。全量修改适用于需要更新整个文档内容的场景，而增量修改则更灵活，可以选择性地更新部分字段。</p> 
<h3><a id="44__722"></a>4.4 文档的版本号</h3> 
<blockquote> 
 <p><strong>什么是文档的版本号：</strong></p> 
</blockquote> 
<p>在 Elasticsearch 中，每个文档都有一个版本号（version），用于标识文档的变更历史。当对文档进行修改时，版本号会随之递增，每次修改都会生成一个新的版本。</p> 
<blockquote> 
 <p><strong>为什么需要文档的版本号：</strong></p> 
</blockquote> 
<p>文档的版本号在并发写入和更新的场景中起到了重要的作用。当多个客户端同时尝试修改同一文档时，通过版本号可以确保写入的顺序和一致性。在分布式系统中，版本号还用于控制数据的一致性和冲突解决。</p> 
<blockquote> 
 <p><strong>例如，对于 id 为 1 的文档：</strong></p> 
</blockquote> 
<p><strong>1. 新增时，<code>_version</code> 为 1</strong></p> 
<p><img src="https://images2.imgbox.com/b0/53/1knzrghl_o.png" alt=""></p> 
<p><strong>2. 第一次修改后，<code>_version</code> 变为 2</strong></p> 
<p><img src="https://images2.imgbox.com/d1/a7/pJl6XTPu_o.png" alt=""></p> 
<p><strong>3. 再次修改后，<code>_version</code> 变为 3</strong></p> 
<p><img src="https://images2.imgbox.com/2e/54/gdg7ltoJ_o.png" alt=""></p> 
<p>通过上面的操作，可以发现一个规律。那就是每修改一次文档，那么该文档的版本号就会加 1。</p> 
<blockquote> 
 <p><strong>版本号的作用：</strong></p> 
</blockquote> 
<p>版本号在处理并发写入、更新和冲突解决时非常重要。通过版本号，Elasticsearch 可以确保在并发修改的情况下，数据的一致性和正确性。当多个客户端尝试修改同一文档时，只有最新的版本会被接受，避免了数据的混乱和冲突。</p> 
<p>版本号的使用让 Elasticsearch 成为一个强大的分布式数据库，能够处理大规模的并发写入和更新操作。</p> 
<h3><a id="45__Mapping__756"></a>4.5 动态 Mapping 映射</h3> 
<p>在 Elasticsearch 中，我们有时会遇到需要向索引中添加新字段的情况。这就涉及到了动态 Mapping 映射的概念。动态 Mapping 是 Elasticsearch 在插入文档时自动识别字段类型并进行映射的机制。</p> 
<p>让我们通过一个具体的例子来说明动态 Mapping 的工作原理。假设我们向索引中插入以下文档：</p> 
<pre><code class="prism language-json"><span class="token constant">PUT</span> <span class="token operator">/</span>demo<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">2</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string-property property">"info"</span><span class="token operator">:</span> <span class="token string">"五虎上将之一"</span><span class="token punctuation">,</span>
  <span class="token string-property property">"email"</span><span class="token operator">:</span> <span class="token string">"zf@demo.cn"</span><span class="token punctuation">,</span>
  <span class="token string-property property">"name"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string-property property">"firstName"</span><span class="token operator">:</span> <span class="token string">"飞"</span><span class="token punctuation">,</span>
    <span class="token string-property property">"lastName"</span><span class="token operator">:</span> <span class="token string">"张"</span>
  <span class="token punctuation">}</span><span class="token punctuation">,</span>
  <span class="token string-property property">"age"</span><span class="token operator">:</span> <span class="token number">35</span><span class="token punctuation">,</span>
  <span class="token string-property property">"score"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token number">98.5</span><span class="token punctuation">,</span> <span class="token number">98.9</span><span class="token punctuation">,</span> <span class="token number">97.9</span><span class="token punctuation">,</span> <span class="token number">99.2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
  <span class="token string-property property">"isMarried"</span><span class="token operator">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>
  <span class="token string-property property">"birthday"</span><span class="token operator">:</span> <span class="token string">"1988-05-20"</span><span class="token punctuation">,</span>
  <span class="token string-property property">"city"</span><span class="token operator">:</span> <span class="token string">"上海"</span>
<span class="token punctuation">}</span>
</code></pre> 
<p><strong>执行结果：</strong></p> 
<p><img src="https://images2.imgbox.com/84/e7/Kz3ylX1R_o.png" alt=""></p> 
<p>这个文档中包含了索引中之前未见过的字段，例如 <code>score</code>、<code>isMarried</code>、<code>birthday</code> 和 <code>city</code>。然而，Elasticsearch 并没有报错，而是成功插入了文档。这是因为 Elasticsearch 会自动进行动态 Mapping 映射。</p> 
<p><strong>动态 Mapping 映射的规则如下：</strong></p> 
<table><thead><tr><th align="left">JSON类型</th><th align="left">Elasticsearch类型</th></tr></thead><tbody><tr><td align="left">字符串</td><td align="left">日期格式字符串：date 类型；普通字符串：text 类型，并添加 keyword 类型子字段</td></tr><tr><td align="left">布尔值</td><td align="left">boolean 类型</td></tr><tr><td align="left">浮点数</td><td align="left">float 类型</td></tr><tr><td align="left">整数</td><td align="left">long 类型</td></tr><tr><td align="left">对象嵌套</td><td align="left">object 类型，并添加 properties</td></tr><tr><td align="left">数组</td><td align="left">由数组中的第一个非空类型决定</td></tr><tr><td align="left">空值</td><td align="left">将被忽略</td></tr></tbody></table> 
<p>通过动态 Mapping，Elasticsearch 能够灵活地处理文档中新增的字段，为我们提供了方便和便利。这种功能使得 Elasticsearch 能够智能地适应不断变化的数据结构，为索引的管理提供了灵活性。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5504e9f4052be7ff94e0d09e4614e472/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">解决IDEA配置MAVEN时报错：Process terminated 显示进程异常退出Process finished with exit code -1073741819 (0xC0000005)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/afdebb602edc398ce990637a0196f71b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Python—Scrapy实践项目</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>