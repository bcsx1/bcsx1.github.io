<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>c/c&#43;&#43;实现简单线性回归问题(机器学习入门实例) - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="c/c&#43;&#43;实现简单线性回归问题(机器学习入门实例)" />
<meta property="og:description" content="线性回归问题简单分析 个人简单分析，有问题欢迎评论讨论
如果给出很多组数据（x,y），求拟合一条y=wx&#43;b直线模拟问题，高中数学也有提到过，就是求w,b两个参数，使得拟合的直线可以近似穿过目标数据区域的中间位置，尽可以使得更多的真实数据点落在拟合的直线上。这个问题常常出现在机器学习，深度学习的入门教程的前几个例子中，然而无奈网上教程都是用机器学习常用的python进行实现，作为一个习惯c/c&#43;&#43;的强类型语言的个人来说，看到python没有类型标识简直看出强迫症和云里雾里的感觉（写完感慨python真香，然而现在还不咋习惯）。
因此我打算自己动手用c&#43;&#43;来写一遍，希望能加深一下理解。对于给出的数据点集，通过迭代循环的手段进行优化w和b两个参数，迭代一次w，b的值更准确一次，而迭代n次运用的方法就是老生常谈的梯度下降法，经过n次迭代后得到的w和b就很准确了。而每一次迭代需要进行求导计算w(b)=w(b)-learningrate*▲w’(b’),具体过程可以求▲w’(b’)的平均值效果更好，learningrate学习率是用来控制下降的高度的，每次降太快容易越过极小值点。
那么，如何去衡量b直线的w和b准不准确呢，衡量的方法是累计（yi-yi’)^2的值， 定义成一个loss损失函数进行评估。
运行结果：具体结果和第一个图还是很接近的，数据点集和代码在下方
c&#43;&#43;重写实现
#include&lt;bits/stdc&#43;&#43;.h&gt; using namespace std; typedef long double ld; vector&lt;ld&gt; XArray; vector&lt;ld&gt; YArray; ld end_b,end_w; ld learning_rate = 0.0001; ld initial_b = 0; ld initial_w = 0; int num_iterations=5000; //处理csv文件得到数据点集 void getData(){ ifstream inFile(&#34;data.csv&#34;, ios::in); string lineStr; vector&lt;vector&lt;string&gt;&gt; strArray; while(getline(inFile,lineStr)){ stringstream ss(lineStr); string str; while(getline(ss,str,&#39;,&#39;)){ XArray.push_back(stod(str)); getline(ss,str,&#39;,&#39;); YArray.push_back(stod(str)); } } // for(int i=0;i&lt;XArray.size();i&#43;&#43;){ // cout&lt;&lt;XArray[i]&lt;&lt;&#34; &#34;&lt;&lt;YArray[i]&lt;&lt;endl; // } } //衡量精确度的方法：loss函数 ld compute_error_for_line_given_points(ld b,ld w){ ld totalError = 0; for(int i=0;i&lt;XArray." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/d3570b5e94e18b9271e2ade6a10b257b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-03-15T17:22:22+08:00" />
<meta property="article:modified_time" content="2020-03-15T17:22:22+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">c/c&#43;&#43;实现简单线性回归问题(机器学习入门实例)</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atelier-sulphurpool-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="_0"></a>线性回归问题简单分析</h3> 
<blockquote> 
 <p>个人简单分析，有问题欢迎评论讨论</p> 
</blockquote> 
<p>如果给出很多组数据（x,y），求拟合一条y=wx+b直线模拟问题，高中数学也有提到过，就是求w,b两个参数，使得拟合的直线可以近似穿过目标数据区域的中间位置，尽可以使得更多的真实数据点落在拟合的直线上。这个问题常常出现在机器学习，深度学习的入门教程的前几个例子中，然而无奈网上教程都是用机器学习常用的python进行实现，作为一个习惯c/c++的强类型语言的个人来说，看到python没有类型标识简直看出强迫症和云里雾里的感觉（写完感慨python真香，然而现在还不咋习惯）。</p> 
<p>因此我打算自己动手用c++来写一遍，希望能加深一下理解。对于给出的数据点集，通过迭代循环的手段进行优化w和b两个参数，迭代一次w，b的值更准确一次，而迭代n次运用的方法就是老生常谈的梯度下降法，经过n次迭代后得到的w和b就很准确了。而每一次迭代需要进行求导计算w(b)=w(b)-learningrate*▲w’(b’),具体过程可以求▲w’(b’)的平均值效果更好，learningrate学习率是用来控制下降的高度的，每次降太快容易越过极小值点。</p> 
<p>那么，如何去衡量b直线的w和b准不准确呢，衡量的方法是累计（yi-yi’)^2的值， 定义成一个loss损失函数进行评估。<br> <img src="https://images2.imgbox.com/8f/c9/rk75Ojy7_o.png" alt="在这里插入图片描述"><br> 运行结果：具体结果和第一个图还是很接近的，数据点集和代码在下方<br> <img src="https://images2.imgbox.com/fb/5d/bsDFGCp7_o.png" alt="在这里插入图片描述"></p> 
<p>c++重写实现</p> 
<pre><code class="prism language-cpp"><span class="token macro property">#<span class="token directive keyword">include</span><span class="token string">&lt;bits/stdc++.h&gt;</span></span>

<span class="token keyword">using</span> <span class="token keyword">namespace</span> std<span class="token punctuation">;</span>
<span class="token keyword">typedef</span> <span class="token keyword">long</span> <span class="token keyword">double</span> ld<span class="token punctuation">;</span>
vector<span class="token operator">&lt;</span>ld<span class="token operator">&gt;</span> XArray<span class="token punctuation">;</span>
vector<span class="token operator">&lt;</span>ld<span class="token operator">&gt;</span> YArray<span class="token punctuation">;</span>
ld end_b<span class="token punctuation">,</span>end_w<span class="token punctuation">;</span>
ld learning_rate <span class="token operator">=</span> <span class="token number">0.0001</span><span class="token punctuation">;</span>
ld initial_b <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
ld initial_w <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
<span class="token keyword">int</span> num_iterations<span class="token operator">=</span><span class="token number">5000</span><span class="token punctuation">;</span>

<span class="token comment">//处理csv文件得到数据点集</span>
<span class="token keyword">void</span> <span class="token function">getData</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
    ifstream <span class="token function">inFile</span><span class="token punctuation">(</span><span class="token string">"data.csv"</span><span class="token punctuation">,</span> ios<span class="token operator">::</span>in<span class="token punctuation">)</span><span class="token punctuation">;</span>
    string lineStr<span class="token punctuation">;</span>
    vector<span class="token operator">&lt;</span>vector<span class="token operator">&lt;</span>string<span class="token operator">&gt;&gt;</span> strArray<span class="token punctuation">;</span>
    <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token function">getline</span><span class="token punctuation">(</span>inFile<span class="token punctuation">,</span>lineStr<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
        stringstream <span class="token function">ss</span><span class="token punctuation">(</span>lineStr<span class="token punctuation">)</span><span class="token punctuation">;</span>
        string str<span class="token punctuation">;</span>
        <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token function">getline</span><span class="token punctuation">(</span>ss<span class="token punctuation">,</span>str<span class="token punctuation">,</span><span class="token string">','</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
            XArray<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span><span class="token function">stod</span><span class="token punctuation">(</span>str<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token function">getline</span><span class="token punctuation">(</span>ss<span class="token punctuation">,</span>str<span class="token punctuation">,</span><span class="token string">','</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            YArray<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span><span class="token function">stod</span><span class="token punctuation">(</span>str<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
     <span class="token punctuation">}</span>
    <span class="token comment">// for(int i=0;i&lt;XArray.size();i++){<!-- --></span>
    <span class="token comment">//     cout&lt;&lt;XArray[i]&lt;&lt;" "&lt;&lt;YArray[i]&lt;&lt;endl;</span>
    <span class="token comment">// }</span>
<span class="token punctuation">}</span>
<span class="token comment">//衡量精确度的方法：loss函数</span>
ld <span class="token function">compute_error_for_line_given_points</span><span class="token punctuation">(</span>ld b<span class="token punctuation">,</span>ld w<span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
    ld totalError <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>XArray<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
        ld x <span class="token operator">=</span>XArray<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
        ld y <span class="token operator">=</span>YArray<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
        totalError <span class="token operator">+</span><span class="token operator">=</span> <span class="token punctuation">(</span>y <span class="token operator">-</span> <span class="token punctuation">(</span>w <span class="token operator">*</span> x <span class="token operator">+</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token punctuation">(</span>y <span class="token operator">-</span> <span class="token punctuation">(</span>w <span class="token operator">*</span> x <span class="token operator">+</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token keyword">return</span> totalError <span class="token operator">/</span> <span class="token function">ld</span><span class="token punctuation">(</span>XArray<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
<span class="token comment">//每次下降的具体做法：求导变化</span>
vector<span class="token operator">&lt;</span>ld<span class="token operator">&gt;</span> <span class="token function">step_gradient</span><span class="token punctuation">(</span>ld b_current<span class="token punctuation">,</span>ld w_current<span class="token punctuation">,</span>ld learningRate<span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
    ld b_gradient <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
    ld w_gradient <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
    vector<span class="token operator">&lt;</span>ld<span class="token operator">&gt;</span> temp<span class="token punctuation">;</span>
    <span class="token keyword">int</span> NumOfData <span class="token operator">=</span> XArray<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>NumOfData<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
        ld x <span class="token operator">=</span> XArray<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
        ld y <span class="token operator">=</span> YArray<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
        b_gradient <span class="token operator">+</span><span class="token operator">=</span> <span class="token operator">-</span><span class="token punctuation">(</span><span class="token number">2.0</span><span class="token operator">/</span>NumOfData<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>y <span class="token operator">-</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>w_current <span class="token operator">*</span> x<span class="token punctuation">)</span> <span class="token operator">+</span> b_current<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        w_gradient <span class="token operator">+</span><span class="token operator">=</span> <span class="token operator">-</span><span class="token punctuation">(</span><span class="token number">2.0</span><span class="token operator">/</span>NumOfData<span class="token punctuation">)</span> <span class="token operator">*</span> x <span class="token operator">*</span> <span class="token punctuation">(</span>y <span class="token operator">-</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>w_current <span class="token operator">*</span> x<span class="token punctuation">)</span> <span class="token operator">+</span> b_current<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    temp<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span>b_current <span class="token operator">-</span> <span class="token punctuation">(</span>learningRate <span class="token operator">*</span> b_gradient<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    temp<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span>w_current <span class="token operator">-</span> <span class="token punctuation">(</span>learningRate <span class="token operator">*</span> w_gradient<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">return</span> temp<span class="token punctuation">;</span>
<span class="token punctuation">}</span>
    
<span class="token comment">//梯度下降法，迭代次数num_iterations</span>
<span class="token keyword">void</span> <span class="token function">gradient_descent_runner</span><span class="token punctuation">(</span>ld b<span class="token punctuation">,</span>ld w<span class="token punctuation">,</span><span class="token keyword">int</span> lr<span class="token punctuation">,</span><span class="token keyword">int</span> num_iterations<span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
    ld t_b <span class="token operator">=</span> b<span class="token punctuation">;</span>
    ld t_w <span class="token operator">=</span> w<span class="token punctuation">;</span>
    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>num_iterations<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
        vector<span class="token operator">&lt;</span>ld<span class="token operator">&gt;</span> v <span class="token operator">=</span> <span class="token function">step_gradient</span><span class="token punctuation">(</span>t_b<span class="token punctuation">,</span> t_w<span class="token punctuation">,</span>learning_rate<span class="token punctuation">)</span><span class="token punctuation">;</span>
        t_b <span class="token operator">=</span> v<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
        t_w <span class="token operator">=</span> v<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    end_b<span class="token operator">=</span>t_b<span class="token punctuation">;</span>
    end_w<span class="token operator">=</span>t_w<span class="token punctuation">;</span>
<span class="token punctuation">}</span>
<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
    <span class="token function">getData</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    cout<span class="token operator">&lt;&lt;</span><span class="token string">"Starting gradient descent at b = "</span><span class="token operator">&lt;&lt;</span>initial_b
        <span class="token operator">&lt;&lt;</span><span class="token string">" , w = "</span><span class="token operator">&lt;&lt;</span>initial_w<span class="token operator">&lt;&lt;</span><span class="token string">" , error = "</span>
        <span class="token operator">&lt;&lt;</span><span class="token function">compute_error_for_line_given_points</span><span class="token punctuation">(</span>initial_b<span class="token punctuation">,</span> initial_w<span class="token punctuation">)</span><span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span>
    cout<span class="token operator">&lt;&lt;</span><span class="token string">"Running..."</span><span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span>
    <span class="token function">gradient_descent_runner</span><span class="token punctuation">(</span>initial_b<span class="token punctuation">,</span> initial_w<span class="token punctuation">,</span> learning_rate<span class="token punctuation">,</span> num_iterations<span class="token punctuation">)</span><span class="token punctuation">;</span>
    cout<span class="token operator">&lt;&lt;</span><span class="token string">"After "</span><span class="token operator">&lt;&lt;</span>num_iterations<span class="token operator">&lt;&lt;</span><span class="token string">" iterations b = "</span><span class="token operator">&lt;&lt;</span>end_b
        <span class="token operator">&lt;&lt;</span><span class="token string">" , w = "</span><span class="token operator">&lt;&lt;</span>end_w<span class="token operator">&lt;&lt;</span><span class="token string">" , error = "</span>
        <span class="token operator">&lt;&lt;</span><span class="token function">compute_error_for_line_given_points</span><span class="token punctuation">(</span>end_b<span class="token punctuation">,</span> end_w<span class="token punctuation">)</span><span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span>
    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>原本的python实现</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token comment"># y = wx + b</span>
<span class="token keyword">def</span> <span class="token function">compute_error_for_line_given_points</span><span class="token punctuation">(</span>b<span class="token punctuation">,</span> w<span class="token punctuation">,</span> points<span class="token punctuation">)</span><span class="token punctuation">:</span>
    totalError <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>points<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> points<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
        y <span class="token operator">=</span> points<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>
        totalError <span class="token operator">+=</span> <span class="token punctuation">(</span>y <span class="token operator">-</span> <span class="token punctuation">(</span>w <span class="token operator">*</span> x <span class="token operator">+</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span>
    <span class="token keyword">return</span> totalError <span class="token operator">/</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>points<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">step_gradient</span><span class="token punctuation">(</span>b_current<span class="token punctuation">,</span> w_current<span class="token punctuation">,</span> points<span class="token punctuation">,</span> learningRate<span class="token punctuation">)</span><span class="token punctuation">:</span>
    b_gradient <span class="token operator">=</span> <span class="token number">0</span>
    w_gradient <span class="token operator">=</span> <span class="token number">0</span>
    N <span class="token operator">=</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>points<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>points<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> points<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
        y <span class="token operator">=</span> points<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>
        b_gradient <span class="token operator">+=</span> <span class="token operator">-</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">/</span>N<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>y <span class="token operator">-</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>w_current <span class="token operator">*</span> x<span class="token punctuation">)</span> <span class="token operator">+</span> b_current<span class="token punctuation">)</span><span class="token punctuation">)</span>
        w_gradient <span class="token operator">+=</span> <span class="token operator">-</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">/</span>N<span class="token punctuation">)</span> <span class="token operator">*</span> x <span class="token operator">*</span> <span class="token punctuation">(</span>y <span class="token operator">-</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>w_current <span class="token operator">*</span> x<span class="token punctuation">)</span> <span class="token operator">+</span> b_current<span class="token punctuation">)</span><span class="token punctuation">)</span>
    new_b <span class="token operator">=</span> b_current <span class="token operator">-</span> <span class="token punctuation">(</span>learningRate <span class="token operator">*</span> b_gradient<span class="token punctuation">)</span>
    new_w <span class="token operator">=</span> w_current <span class="token operator">-</span> <span class="token punctuation">(</span>learningRate <span class="token operator">*</span> w_gradient<span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span>new_b<span class="token punctuation">,</span> new_w<span class="token punctuation">]</span>

<span class="token keyword">def</span> <span class="token function">gradient_descent_runner</span><span class="token punctuation">(</span>points<span class="token punctuation">,</span> starting_b<span class="token punctuation">,</span> starting_m<span class="token punctuation">,</span> learning_rate<span class="token punctuation">,</span> num_iterations<span class="token punctuation">)</span><span class="token punctuation">:</span>
    b <span class="token operator">=</span> starting_b
    m <span class="token operator">=</span> starting_m
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_iterations<span class="token punctuation">)</span><span class="token punctuation">:</span>
        b<span class="token punctuation">,</span> m <span class="token operator">=</span> step_gradient<span class="token punctuation">(</span>b<span class="token punctuation">,</span> m<span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>points<span class="token punctuation">)</span><span class="token punctuation">,</span> learning_rate<span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span>b<span class="token punctuation">,</span> m<span class="token punctuation">]</span>

<span class="token keyword">def</span> <span class="token function">run</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    points <span class="token operator">=</span> np<span class="token punctuation">.</span>genfromtxt<span class="token punctuation">(</span><span class="token string">"data.csv"</span><span class="token punctuation">,</span> delimiter<span class="token operator">=</span><span class="token string">","</span><span class="token punctuation">)</span>
    learning_rate <span class="token operator">=</span> <span class="token number">0.0001</span>
    initial_b <span class="token operator">=</span> <span class="token number">0</span> <span class="token comment"># initial y-intercept guess</span>
    initial_w <span class="token operator">=</span> <span class="token number">0</span> <span class="token comment"># initial slope guess</span>
    num_iterations <span class="token operator">=</span> <span class="token number">5000</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Starting gradient descent at b = {0}, w = {1}, error = {2}"</span>
          <span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>initial_b<span class="token punctuation">,</span> initial_w<span class="token punctuation">,</span>
                  compute_error_for_line_given_points<span class="token punctuation">(</span>initial_b<span class="token punctuation">,</span> initial_w<span class="token punctuation">,</span> points<span class="token punctuation">)</span><span class="token punctuation">)</span>
          <span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Running..."</span><span class="token punctuation">)</span>
    <span class="token punctuation">[</span>b<span class="token punctuation">,</span> w<span class="token punctuation">]</span> <span class="token operator">=</span> gradient_descent_runner<span class="token punctuation">(</span>points<span class="token punctuation">,</span> initial_b<span class="token punctuation">,</span> initial_w<span class="token punctuation">,</span> learning_rate<span class="token punctuation">,</span> num_iterations<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"After {0} iterations b = {1}, w = {2}, error = {3}"</span><span class="token punctuation">.</span>
          <span class="token builtin">format</span><span class="token punctuation">(</span>num_iterations<span class="token punctuation">,</span> b<span class="token punctuation">,</span> w<span class="token punctuation">,</span>
                 compute_error_for_line_given_points<span class="token punctuation">(</span>b<span class="token punctuation">,</span> w<span class="token punctuation">,</span> points<span class="token punctuation">)</span><span class="token punctuation">)</span>
          <span class="token punctuation">)</span>

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    run<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>data.csv数据文件</p> 
<pre><code class="prism language-csv">32.502345269453031,31.70700584656992
53.426804033275019,68.77759598163891
61.530358025636438,62.562382297945803
47.475639634786098,71.546632233567777
59.813207869512318,87.230925133687393
55.142188413943821,78.211518270799232
52.211796692214001,79.64197304980874
39.299566694317065,59.171489321869508
48.10504169176825,75.331242297063056
52.550014442733818,71.300879886850353
45.419730144973755,55.165677145959123
54.351634881228918,82.478846757497919
44.164049496773352,62.008923245725825
58.16847071685779,75.392870425994957
56.727208057096611,81.43619215887864
48.955888566093719,60.723602440673965
44.687196231480904,82.892503731453715
60.297326851333466,97.379896862166078
45.618643772955828,48.847153317355072
38.816817537445637,56.877213186268506
66.189816606752601,83.878564664602763
65.41605174513407,118.59121730252249
47.48120860786787,57.251819462268969
41.57564261748702,51.391744079832307
51.84518690563943,75.380651665312357
59.370822011089523,74.765564032151374
57.31000343834809,95.455052922574737
63.615561251453308,95.229366017555307
46.737619407976972,79.052406169565586
50.556760148547767,83.432071421323712
52.223996085553047,63.358790317497878
35.567830047746632,41.412885303700563
42.436476944055642,76.617341280074044
58.16454011019286,96.769566426108199
57.504447615341789,74.084130116602523
45.440530725319981,66.588144414228594
61.89622268029126,77.768482417793024
33.093831736163963,50.719588912312084
36.436009511386871,62.124570818071781
37.675654860850742,60.810246649902211
44.555608383275356,52.682983366387781
43.318282631865721,58.569824717692867
50.073145632289034,82.905981485070512
43.870612645218372,61.424709804339123
62.997480747553091,115.24415280079529
32.669043763467187,45.570588823376085
40.166899008703702,54.084054796223612
53.575077531673656,87.994452758110413
33.864214971778239,52.725494375900425
64.707138666121296,93.576118692658241
38.119824026822805,80.166275447370964
44.502538064645101,65.101711570560326
40.599538384552318,65.562301260400375
41.720676356341293,65.280886920822823
51.088634678336796,73.434641546324301
55.078095904923202,71.13972785861894
41.377726534895203,79.102829683549857
62.494697427269791,86.520538440347153
49.203887540826003,84.742697807826218
41.102685187349664,59.358850248624933
41.182016105169822,61.684037524833627
50.186389494880601,69.847604158249183
52.378446219236217,86.098291205774103
50.135485486286122,59.108839267699643
33.644706006191782,69.89968164362763
39.557901222906828,44.862490711164398
56.130388816875467,85.498067778840223
57.362052133238237,95.536686846467219
60.269214393997906,70.251934419771587
35.678093889410732,52.721734964774988
31.588116998132829,50.392670135079896
53.66093226167304,63.642398775657753
46.682228649471917,72.247251068662365
43.107820219102464,57.812512976181402
70.34607561504933,104.25710158543822
44.492855880854073,86.642020318822006
57.50453330326841,91.486778000110135
36.930076609191808,55.231660886212836
55.805733357942742,79.550436678507609
38.954769073377065,44.847124242467601
56.901214702247074,80.207523139682763
56.868900661384046,83.14274979204346
34.33312470421609,55.723489260543914
59.04974121466681,77.634182511677864
57.788223993230673,99.051414841748269
54.282328705967409,79.120646274680027
51.088719898979143,69.588897851118475
50.282836348230731,69.510503311494389
44.211741752090113,73.687564318317285
38.005488008060688,61.366904537240131
32.940479942618296,67.170655768995118
53.691639571070056,85.668203145001542
68.76573426962166,114.85387123391394
46.230966498310252,90.123572069967423
68.319360818255362,97.919821035242848
50.030174340312143,81.536990783015028
49.239765342753763,72.111832469615663
50.039575939875988,85.232007342325673
48.149858891028863,66.224957888054632
25.128484647772304,53.454394214850524

</code></pre>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3f1748680bad9aa0897d616a9de794ed/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">为什么IEEE754标准中单精度浮点数的阶码取值范围是1~254（-126~127）？</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/31a647e05fe0b64f350ecdfdea9de901/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">新浪微博千万级消息推送如何实现？</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>