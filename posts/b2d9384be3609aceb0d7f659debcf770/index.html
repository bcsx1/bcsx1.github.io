<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Supervised learning/ Unsupervised learning监督学习/无监督学习 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Supervised learning/ Unsupervised learning监督学习/无监督学习" />
<meta property="og:description" content="[机器学习]两种方法——监督学习和无监督学习（通俗理解）
[机器学习] : 监督学习 (框架)
有监督学习与无监督学习的几大区别
目录
Supervised learning 监督学习 Unsupervised learning 无监督学习
Supervised learning 监督学习 VS Unsupervised learning 无监督学习
有标签 vs 无标签
分类 vs 聚类
同维 vs 降维
分类同时定性 vs 先聚类后定性
独立 vs 非独立
不透明 vs 可解释性
DataVisor无监督独有的拓展性
如何选择有监督和无监督
Supervised learning 监督学习 从给定的训练数据集中学习出一个函数（模型参数）；输入新的数据时，根据这个函数/模型可以得到预测结果。 监督学习的训练集要求包括输入输出，也可以说是特征和目标。训练集中的目标是由人标注的。
监督学习就是最常见的分类（注意和聚类区分）问题：
通过已有的训练样本去训练得到一个最优模型； 训练样本：即已知数据及其对应的输出；最优模型：这个模型属于某个函数的集合，最优表示某个评价准则下是最佳的；利用这个模型将所有的输入映射为相应的输出，对输出进行简单的判断从而实现分类的目的。也就具有了对未知数据分类的能力。监督学习的目标往往是让计算机去学习我们已经创建好的分类系统（模型）。 监督学习是训练神经网络和决策树的常见技术。
这两种技术高度依赖事先确定的分类系统给出的信息，对于神经网络，分类系统利用信息判断网络的错误，然后不断调整网络参数。对于决策树，分类系统用它来判断哪些属性提供了最多的信息。
常见的有监督学习算法：回归分析和统计分类。最典型的算法是KNN和SVM。
Unsupervised learning 无监督学习 输入：数据没有被标记，也没有确定的结果。样本数据：类别未知，需要根据样本间的相似性对样本集进行分类/聚类，试图使类内差距最小化，类间差距最大化。 通俗点，就是实际应用中，不少情况下无法预知样本的标签，也就是说没有训练样本对应的类别，因而只能从原先没有样本标签的样本集开始学习分类器设计。
非监督学习目标不是告诉计算机怎么做，而是让它（计算机）自己去学习怎样做事情。
无监督学习的方法分为两大类：
基于概率密度函数估计的直接方法：设法找到各类别在特征空间的分布参数，再进行分类。基于样本间相似性度量的简洁聚类方法：设法定出不同类别的核心或初始内核，然后依据样本与核心之间的相似性度量将样本聚集成不同的类别。 利用聚类结果，可以提取数据集中隐藏信息，对未来数据进行分类和预测。应用于数据挖掘，模式识别，图像处理等。
Supervised learning 监督学习 VS Unsupervised learning 无监督学习 有标签 vs 无标签 有监督机器学习又被称为“有老师的学习”，所谓的老师就是标签。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/b2d9384be3609aceb0d7f659debcf770/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-02T15:37:52+08:00" />
<meta property="article:modified_time" content="2022-04-02T15:37:52+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Supervised learning/ Unsupervised learning监督学习/无监督学习</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><a href="https://blog.csdn.net/zb1165048017/article/details/48579677?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1.pc_relevant_default&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1.pc_relevant_default&amp;utm_relevant_index=2" title="[机器学习]两种方法——监督学习和无监督学习（通俗理解）">[机器学习]两种方法——监督学习和无监督学习（通俗理解）</a></p> 
<p><a href="https://blog.csdn.net/qq_41709378/article/details/105374199" title="[机器学习] : 监督学习 (框架)">[机器学习] : 监督学习 (框架)</a></p> 
<p><a href="https://blog.csdn.net/u010299280/article/details/82981106?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E6%9C%89%E7%9B%91%E7%9D%A3%E6%97%A0%E7%9B%91%E7%9D%A3&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-82981106.142%5Ev5%5Epc_search_result_control_group,157%5Ev4%5Econtrol&amp;spm=1018.2226.3001.4187" title="有监督学习与无监督学习的几大区别">有监督学习与无监督学习的几大区别</a></p> 
<hr> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="Supervised%20learning%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%C2%A0-toc" style="margin-left:0px;"><a href="#Supervised%20learning%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%C2%A0" rel="nofollow">Supervised learning 监督学习 </a></p> 
<p id="Unsupervised%20learning%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-toc" style="margin-left:0px;"><a href="#Unsupervised%20learning%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0" rel="nofollow">Unsupervised learning 无监督学习</a></p> 
<p id="Supervised%20learning%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20VS%20Unsupervised%20learning%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-toc" style="margin-left:0px;"><a href="#Supervised%20learning%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20VS%20Unsupervised%20learning%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0" rel="nofollow">Supervised learning 监督学习 VS Unsupervised learning 无监督学习</a></p> 
<p id="%E6%9C%89%E6%A0%87%E7%AD%BE%20vs%20%E6%97%A0%E6%A0%87%E7%AD%BE-toc" style="margin-left:40px;"><a href="#%E6%9C%89%E6%A0%87%E7%AD%BE%20vs%20%E6%97%A0%E6%A0%87%E7%AD%BE" rel="nofollow">有标签 vs 无标签</a></p> 
<p id="%E5%88%86%E7%B1%BB%20vs%20%E8%81%9A%E7%B1%BB-toc" style="margin-left:40px;"><a href="#%E5%88%86%E7%B1%BB%20vs%20%E8%81%9A%E7%B1%BB" rel="nofollow">分类 vs 聚类</a></p> 
<p id="%E5%90%8C%E7%BB%B4%20vs%20%E9%99%8D%E7%BB%B4-toc" style="margin-left:40px;"><a href="#%E5%90%8C%E7%BB%B4%20vs%20%E9%99%8D%E7%BB%B4" rel="nofollow">同维 vs 降维</a></p> 
<p id="%E5%88%86%E7%B1%BB%E5%90%8C%E6%97%B6%E5%AE%9A%E6%80%A7%20vs%20%E5%85%88%E8%81%9A%E7%B1%BB%E5%90%8E%E5%AE%9A%E6%80%A7-toc" style="margin-left:40px;"><a href="#%E5%88%86%E7%B1%BB%E5%90%8C%E6%97%B6%E5%AE%9A%E6%80%A7%20vs%20%E5%85%88%E8%81%9A%E7%B1%BB%E5%90%8E%E5%AE%9A%E6%80%A7" rel="nofollow">分类同时定性 vs 先聚类后定性</a></p> 
<p id="%E7%8B%AC%E7%AB%8B%20vs%20%E9%9D%9E%E7%8B%AC%E7%AB%8B-toc" style="margin-left:40px;"><a href="#%E7%8B%AC%E7%AB%8B%20vs%20%E9%9D%9E%E7%8B%AC%E7%AB%8B" rel="nofollow">独立 vs 非独立</a></p> 
<p id="%E4%B8%8D%E9%80%8F%E6%98%8E%20vs%20%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7-toc" style="margin-left:40px;"><a href="#%E4%B8%8D%E9%80%8F%E6%98%8E%20vs%20%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7" rel="nofollow">不透明 vs 可解释性</a></p> 
<p id="DataVisor%E6%97%A0%E7%9B%91%E7%9D%A3%E7%8B%AC%E6%9C%89%E7%9A%84%E6%8B%93%E5%B1%95%E6%80%A7-toc" style="margin-left:40px;"><a href="#DataVisor%E6%97%A0%E7%9B%91%E7%9D%A3%E7%8B%AC%E6%9C%89%E7%9A%84%E6%8B%93%E5%B1%95%E6%80%A7" rel="nofollow">DataVisor无监督独有的拓展性</a></p> 
<p id="%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E6%9C%89%E7%9B%91%E7%9D%A3%E5%92%8C%E6%97%A0%E7%9B%91%E7%9D%A3-toc" style="margin-left:0px;"><a href="#%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E6%9C%89%E7%9B%91%E7%9D%A3%E5%92%8C%E6%97%A0%E7%9B%91%E7%9D%A3" rel="nofollow">如何选择有监督和无监督</a></p> 
<h2></h2> 
<hr> 
<h2 id="Supervised%20learning%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%C2%A0">Supervised learning 监督学习 </h2> 
<p><img alt="" height="710" src="https://images2.imgbox.com/bd/c1/LnFsXF2J_o.png" width="1108"></p> 
<p> <img alt="" height="723" src="https://images2.imgbox.com/c7/82/sVH6WjW6_o.png" width="1148"></p> 
<p> </p> 
<ul><li>从给定的训练数据集中学习出一个函数（模型参数）；</li><li>输入<span style="color:#ed7976;"><strong>新的数据</strong></span>时，根据这个函数/模型可以得到预测结果。</li></ul> 
<p>监督学习的训练集要求包括输入输出，也可以说是特征和目标。<span style="color:#ed7976;"><strong>训练集中的目标是由人标注的</strong></span><span style="color:#0d0016;">。</span></p> 
<p><span style="color:#511b78;"><strong>监督学习就是最常见的分类</strong></span>（注意和聚类区分）问题：</p> 
<ul><li>通过已有的<span style="color:#1c7331;"><strong>训练样本</strong></span>去训练得到一个<span style="color:#1c7331;"><strong>最优模型</strong></span>； 
  <ul><li>训练样本：即已知数据及其对应的输出；</li><li>最优模型：这个模型属于某个函数的集合，最优表示某个评价准则下是最佳的；</li></ul></li><li>利用这个模型将所有的输入映射为相应的输出，对输出进行简单的判断从而实现分类的目的。也就具有了对未知数据分类的能力。</li><li>监督学习的目标往往是让计算机去学习我们已经创建好的分类系统（模型）。</li></ul> 
<p><span style="color:#511b78;"><strong>监督学习是训练神经网络和决策树的常见技术。</strong></span></p> 
<p>这两种技术<span style="color:#1c7892;"><strong>高度依赖</strong></span>事先确定的分类系统给出的信息，对于神经网络，分类系统利用信息判断网络的错误，然后不断调整网络参数。对于决策树，分类系统用它来判断哪些属性提供了最多的信息。</p> 
<p>常见的有监督学习算法：回归分析和统计分类。最典型的算法是KNN和SVM。</p> 
<hr> 
<h2 id="Unsupervised%20learning%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0">Unsupervised learning 无监督学习</h2> 
<p><img alt="" height="504" src="https://images2.imgbox.com/55/c1/WmCBduj8_o.png" width="1040"></p> 
<ul><li><strong>输入：<span style="color:#ed7976;">数据没有被标记，也没有确定的结果。</span></strong></li><li><strong>样本数据：</strong>类别未知，需要根据样本间的相似性对样本集进行分类/聚类，试图使类内差距最小化，类间差距最大化。</li></ul> 
<p><strong>通俗点，</strong>就是实际应用中，不少情况下<strong>无法预知样本的标签</strong>，也就是说<span style="color:#ed7976;"><strong>没有训练样本对应的类别</strong></span>，因而只能<span style="color:#be191c;"><strong>从原先没有样本标签的样本集开始学习</strong></span>分类器设计。</p> 
<p>非监督学习目标不是告诉计算机怎么做，而是让它（计算机）自己去学习怎样做事情。</p> 
<p>无监督学习的方法分为两大类：</p> 
<ol><li><span style="color:#1c7331;"><strong>基于概率密度函数估计的直接方法</strong></span>：设法找到各类别在特征空间的分布参数，再进行分类。</li><li><span style="color:#1c7331;"><strong>基于样本间相似性度量的简洁聚类方法</strong></span>：设法定出不同类别的核心或初始内核，然后依据样本与核心之间的相似性度量将样本聚集成不同的类别。</li></ol> 
<p>利用聚类结果，可以提取数据集中隐藏信息，对未来数据进行分类和预测。应用于数据挖掘，模式识别，图像处理等。</p> 
<hr> 
<h2 id="Supervised%20learning%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20VS%20Unsupervised%20learning%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0">Supervised learning 监督学习 <span style="color:#ed7976;">VS</span> Unsupervised learning 无监督学习</h2> 
<h3 id="%E6%9C%89%E6%A0%87%E7%AD%BE%20vs%20%E6%97%A0%E6%A0%87%E7%AD%BE"><span style="color:#ed7976;"><strong>有标签 vs 无标签</strong></span></h3> 
<p><span style="color:#1c7331;"><strong>有监督机器学习又被称为“有老师的学习”，所谓的老师就是标签。</strong></span></p> 
<ul><li>有监督的过程为先通过已知的训练样本（如已知输入和对应的输出）来训练，从而得到一个最优模型，再将这个模型应用在新的数据上，映射为输出结果。再经过这样的过程后，模型就有了预知能力。</li></ul> 
<p><span style="color:#1c7331;"><strong>无监督机器学习被称为“没有老师的学习”。</strong></span></p> 
<ul><li>无监督相比于有监督，没有训练的过程，而是直接拿数据进行建模分析，意味着这些都是要通过机器学习自行学习探索。这听起来似乎有点不可思议，但是在我们自身认识世界的过程中也会用到无监督学习。<span style="color:#6eaad7;"><u>比如我们去参观一个画展，我们对艺术一无所知，但是欣赏完多幅作品之后，我们也能把它们分成不同的派别。比如哪些更朦胧一点，哪些更写实一些。即使我们不知道什么叫做朦胧派和写实派，但是至少我们能把他们分为两个类。</u></span></li></ul> 
<h3 id="%E5%88%86%E7%B1%BB%20vs%20%E8%81%9A%E7%B1%BB"><span style="color:#ed7976;"><strong>分类 vs 聚类</strong></span></h3> 
<p><span style="color:#1c7331;"><strong>有监督机器学习的核心是分类</strong></span>，有监督的工作是选择分类器和确定权值，</p> 
<p><span style="color:#1c7331;"><strong>无监督机器学习的核心是聚类</strong></span>（将数据集合分成由类似的对象组成的多个类）。无监督的工作是密度估计（寻找描述数据统计值）。<u><strong>这意味着无监督算法只要知道如何计算相似度就可以开始工作。</strong></u></p> 
<h3 id="%E5%90%8C%E7%BB%B4%20vs%20%E9%99%8D%E7%BB%B4"><span style="color:#ed7976;"><strong>同维 vs 降维</strong></span></h3> 
<p><span style="color:#1c7331;"><strong>有监督不具有降维的能力，</strong></span>如果输入是n维，特征即被认定为n维<span style="color:#1c7331;"><strong>。</strong></span></p> 
<p><span style="color:#1c7331;"><strong>无监督<strong>具</strong>有降维的能力，</strong></span>其参与深度学习，做特征提取，或者干脆采用层聚类或者项聚类，以减少数据特征的维度。</p> 
<h3 id="%E5%88%86%E7%B1%BB%E5%90%8C%E6%97%B6%E5%AE%9A%E6%80%A7%20vs%20%E5%85%88%E8%81%9A%E7%B1%BB%E5%90%8E%E5%AE%9A%E6%80%A7"><span style="color:#ed7976;"><strong>分类同时定性 vs 先聚类后定性</strong></span></h3> 
<p><span style="color:#1c7331;"><strong>有监督的输出结果，也就是分好类的结果会被直接贴上标签</strong></span>，是好还是坏。也即分类分好了，标签也同时贴好了。类似于中药铺的药匣，药剂师采购回来一批药材，需要做的只是把对应的每一颗药材放进贴着标签的药匣中。</p> 
<p>无监督的结果只是一群一群的聚类，就像被混在一起的多种中药，一个外行要处理这堆药材，能做的只有把看上去一样的药材挑出来聚成很多个小堆。如果要进一步识别这些小堆，就需要一个老中医（类比老师）的指导了。因此，<span style="color:#1c7331;"><strong>无监督属于先聚类后定性</strong></span>，有点类似于批处理。</p> 
<h3 id="%E7%8B%AC%E7%AB%8B%20vs%20%E9%9D%9E%E7%8B%AC%E7%AB%8B"><span style="color:#ed7976;"><strong>独立 vs 非独立</strong></span></h3> 
<p><span style="color:#1c7331;"><strong>手动对数据做标注</strong></span>作为训练样本，并把样本画在特征空间中，发现线性非常好，然而在分类面，总有一些<strong>混淆</strong>的数据样本。</p> 
<p>这种现象的一个解释是：</p> 
<ul><li>不管训练样本（有监督），还是待分类的数据（无监督），<strong>并不是所有数据都是相互独立分布的。</strong></li><li>数据和数据的分布之间存在联系。作为训练样本，大的偏移很可能会给分类器带来很大的噪声，而对于无监督，情况就会好很多。</li></ul> 
<p>可见，<span style="color:#1c7331;"><strong>独立分布数据更适合有监督，非独立数据更适合无监督。</strong></span></p> 
<h3 id="%E4%B8%8D%E9%80%8F%E6%98%8E%20vs%20%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7"><span style="color:#ed7976;"><strong>不透明 vs 可解释性</strong></span></h3> 
<p>由于<span style="color:#1c7331;"><strong>有监督算法最后输出的一个结果/标签，一定是会有一个倾向</strong></span>。但是，</p> 
<ul><li>探究为什么会这样？ 
  <ul><li>有监督会告诉你：因为我们给每个字段乘以了一个参数列[w1, w2, w3…wn]。</li></ul></li><li>为什么是这个参数列？为什么第一个字段乘以了0.01而不是0.02？ 
  <ul><li>有监督会告诉你：这是我自己学习计算的！然后，就拒绝再回答你的任何问题。</li><li>是的，有监督算法的分类原因是不具有可解释性的，或者说，是不透明的，因为这些规则都是通过人为建模得出，及其并不能自行产生规则。所以，对于像反洗钱这种需要明确规则的场景，就很难应用。</li></ul></li></ul> 
<p><span style="color:#1c7331;"><strong>无监督的聚类方式通常是有很好的解释性的，</strong></span></p> 
<ul><li>为什么把他们分成一类？ 
  <ul><li>无监督会告诉你，他们有多少特征有多少的一致性，所以才被聚成一组。你恍然大悟，原来如此！于是，进一步可以讲这个特征组总结成规则。如此这般分析，聚类原因便昭然若揭了。</li></ul></li></ul> 
<h3 id="DataVisor%E6%97%A0%E7%9B%91%E7%9D%A3%E7%8B%AC%E6%9C%89%E7%9A%84%E6%8B%93%E5%B1%95%E6%80%A7"><span style="color:#ed7976;"><strong>DataVisor无监督独有的拓展性</strong></span></h3> 
<p>试想这样一个n维模型，产出结果已经非常好，这时又增加了一维数据，变成了n+1维。那么，如果这是一个非常强的特征，足以将原来的分类或者聚类打散，一切可能需要从头再来，尤其是有监督，权重值几乎会全部改变。而DataVisor开发的无监督算法，具有极强的扩展性，无论多加的这一维数据的权重有多高，都不影响原来的结果输出，原来的成果仍然可以保留，只需要对多增加的这一维数据做一次处理即可。</p> 
<hr> 
<h2 id="%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E6%9C%89%E7%9B%91%E7%9D%A3%E5%92%8C%E6%97%A0%E7%9B%91%E7%9D%A3"><span style="color:#ed7976;"><strong>如何选择有监督和无监督</strong></span></h2> 
<p style="text-align:center;"><img alt="" height="335" src="https://images2.imgbox.com/84/47/r00Y3CaA_o.png" width="318"></p> 
<p>简单的方法就是从定义入手，<span style="color:#1c7331;"><strong>有训练样本则考虑采用监督学习方法；无训练样本，则一定不能用监督学习方法。</strong></span>但是，<span style="color:#511b78;"><strong>现实问题中，即使没有训练样本，从待分类的数据中，人工标注一些样本，并把它们作为训练样本，这样的话，可以把条件改善，用监督学习方法来做。</strong></span><span style="color:#1c7331;"><strong>对于不同的场景，正负样本的分布如果会存在偏移（可能大的偏移，可能比较小），这样的话，监督学习的效果可能就不如用非监督学习了。</strong></span><br>  </p> 
<p>写在后面的话，人工标注成本代价高，误差大，个人认为后期研究倾向于无监督学习，但是无监督学习的计算量也随之上升，又有待考量！</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/422c40ed2a63db81073654bae57c1526/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【Azure - App Service】如何使用PowerShell一键部署前端代码到微软Azure云的App Service上</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/842fa0d5932b034dc243f62839705548/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">C&#43;&#43;多线程：thread_local</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>