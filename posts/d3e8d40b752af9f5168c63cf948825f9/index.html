<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>CREStereo： Practical Stereo Matching via Cascaded Recurrent Networkwith Adaptive Correlation-论文阅读 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="CREStereo： Practical Stereo Matching via Cascaded Recurrent Networkwith Adaptive Correlation-论文阅读" />
<meta property="og:description" content="Paper
目录
摘要
介绍
相关工作
方法
自适应群相关层
级联的网络
叠加级联推理
损失函数
合成训练数据
实验
数据集和评估指标 实现细节 消融实验
与SOTA对比
实用性能
总结
摘要 随着卷积神经网络的出现，立体匹配算法最近取得了巨大的进展。但是，因为结构薄、非理想修正、相机模块不一致和各种硬情况场景等实际复杂因素，从智能手机等大众级设备拍摄的真实图像对中准确提取视差仍然是一个巨大的挑战。在本文中，作者提出了一套创新的设计来解决实际立体匹配的问题：
为了更好地恢复精细的深度细节，设计了一个循环细化的层次网络，以粗到细的方式更新视差，以及堆叠级联结构进行推理；提出了自适应群相关层来减轻错误修正的影响；采用了一个新的合成数据集，对困难的情形给予特别注意力，以便更好地推广到真实场景。 研究结果不仅在Middlebury和ETH3D基准中排名第一，显著超过现有的最先进的方法，而且在现实生活中显示了高质量的细节，这清楚地证明了贡献的有效性。 作者对来自Holopix50K数据集的图像的预测示例，展示了立体对的左侧图像及其相应的预测视差。 结果实现了较高的准确性，并显示了高质量的细节，为精细结构的对象。 介绍 立体匹配是计算机视觉的一个经典研究课题，给定一对修正图像，是计算两个对应像素之间的位移，即“视差”。它在自动驾驶、增强现实、模拟散景渲染等许多应用中都发挥着重要的作用。
近年来，在大型合成数据集的支持下，基于卷积神经网络(CNN)的立体匹配方法将视差估计的精度提高到了一个新的高度。然而，为了使该算法在日常摄影的场景中真正实用，我们仍然面临着三个主要的障碍。
首先，对于大多数现有的算法来说，精确恢复精细图像细节或网、线框等薄结构的视差仍然是一个复杂的问题。日常照片是高分辨率的，这更让问题复杂化。例如，在计算中，围绕细节的视差误差会导致渲染结果退化，不利于人类感知。其次，对于真实世界的立体图像对，很难获得完美的修正，因为它们通常是由具有不同特征的相机模块产生的。例如，目前大多数智能手机都用广角镜头和长焦镜头捕捉立体声对，它们具有焦距和失真参数等明显特征，不可避免地会导致不理想的校正。因此，现有的假设立体对被完全修正的方法在这种对抗性条件下很可能失败。此外，由不一致的摄像机模块产生的图像对可能在照明、白平衡、图像质量等方面发生变化，这使得估计任务更加困难。最后，虽然它已经表明，模型训练从足够大的合成数据集可以推广到真实场景，视差估计在典型的硬情况下，如非纹理或重复纹理区域，仍然很困难，这需要特别注意在覆盖相关场景的训练数据集。 【研究动机】 在本文中，作者提出了 CREStereo，交叉立体声匹配网络，即级联立体匹配网络，它包括一套新的设计，以解决实际的立体匹配的问题。为了更好地恢复复杂的图像细节，设计了一个分层网络，以粗到细的方式反复更新视差；此外，采用堆叠级联体系结构进行高分辨率推断。为了减轻校正误差的负面影响，设计了一个自适应群局部相关层来进行特征匹配。此外，我们引入了一个新的合成数据集，在光线、纹理和形状方面具有更丰富的变化，以便更好地推广到真实世界的场景。
目前为止，CREStereo在ETH3D立体双视图和 Middlebury基准测试中都排名第一，并在KITTI 2012/2015上取得了具有竞争力的性能。此外，作者的网络在任意真实场景中表现出了优越的性能，很好地证明了设计的有效性。 因此，作者主要贡献可以总结如下：
提出了一种用于实际立体匹配的级联递归网络和堆叠的高分辨率推理结构；设计了自适应群相关层来处理非理想校正；创建了一个新的合成数据集，以更好地推广到现实场景；方法在 Middlebury和ETH3D等公共基准上优于现有方法的显著优势，大大提高了真实立体图像恢复视差的准确性。 相关工作 传统算法：立体匹配是一个已经研究了很长时间，具有挑战性的问题。传统的算法分为局部算法和全局算法。局部方法使用以极线像素为中心的支持窗口来计算匹配代价。全局方法将立体匹配视为一个优化问题，其中显式代价函数由感知传播或图割算法制定和优化。在此基础上，提出了一种基于动态规划的半全局匹配方法，该方法利用互信息代替强度。
基于学习的算法：深度神经网络首次引入立体匹配任务，仅用于匹配成本计算。Zbontar和LeCun提出训练一个CNN来初始化补丁之间的匹配代价，通过交叉聚合和半叶优化，如SGM（由于代价计算步骤只考虑了局部的相关性，对噪声非常敏感，无法直接用来计算最优视差，所以SGM算法通过代价聚合步骤，使聚合后的代价值能够更准确的反应像素之间的相关性）。近年来，端到端网络已成为立体匹配领域的主流，其中一种网络只使用二维卷积。Mayer等人引入了第一个名为DispNet的端到端网络及其相关版本DispNetC用于视差估计。Pang等人提出了一个具有多尺度残差学习的两阶段框架。Guo等人提出了具有群体相关性的GwcNet来改进相似性度量。AANet介绍了一种新的利用稀疏点和多尺度相互作用的聚合方法。最近的一种方法RAFT-Stereo利用光流网络RAFT的迭代细化设计了一个适合于立体匹配的网络。另一条网络使用3D卷积来执行传统方法的成本量构建和聚合。GCNet和PSMNet提出构建一个具有三维沙漏聚合网络的四维成本量。对于高分辨率图像，Yang等人提出了一种从粗到细的分层网络来解决内存和速度问题。最近，神经结构搜索也被引入了深度立体网络。
实用的立体声匹配：面向真实世界图像的立体匹配是一个较少被探索的问题。Pang等人提出了一种自适应方法，将CNN推广到目标域，没有地面真实视差。Luo等人提出了一种小波合成网络，为智能手机上的散景应用产生更好的结果。Song等人引入了一个领域适应管道，以缩小合成和真实领域之间的差距。
合成数据集：足够的训练数据对于深度立体模型是必不可少的，但在现实世界中很难获得准确的视差。合成数据集提供了高精度和密集的地面真值。最近，He等人使用Blender建立了一个立体匹配的数据生成管道，使用来自公共数据集的真实图像的纹理。自动流引入了一种简单的方法来呈现随机多边形的运动光流训练。尽管这些数据集很有效，但它们的物体形状的变化仍然有限，视差/光流值的分布也很有限，这削弱了从合成到现实世界的推广能力。
方法 在本节中，将介绍提出的级联立体匹配网络(CREStereo)和新合成数据集
自适应群相关层 作者观察到，很难为现实世界的立体相机实现完美的校准。例如，两个相机可能不会严格放置在水平外极线上，导致在三维空间中轻微旋转；或者相机镜头的图像即使经过修正后也会有残余失真。因此，对于立体图像对，对应的点可能不位于同一扫描线上。因此，作者提出了一种自适应群相关层(AGCL)来减少这种情况下的匹配模糊性，在只计算局部相关的情况下，比全对匹配获得更好的性能。
局部特征注意力：作者不计算每对像素的全局相关性，而是只匹配一个局部窗口中的点，以避免大量的内存消耗和计算成本。针对稀疏特征匹配的LoFTR特征匹配，在级联第一阶段的相关计算之前添加了一个注意模块，以便将全局上下文信息聚合到单个或交叉特征图中。在之后，在主干输出中添加了位置编码，这增强了特征映射的位置依赖性。交替计算自注意和交叉注意，其中使用线性注意层来降低计算复杂度。
2D-1D转换局部搜索：不同于流量估计网络RAFT及其立体版本，其中全对相关性由两个C×H×W特征图的矩阵乘法计算，输出4DH×W×W×W或3DH×W×W成本量，只在一个局部搜索窗口中计算相关性，该窗口输出更小体积的H×W×D，以节省内存和计算成本。H和W表示特征图的高度和宽度，D是相关对的数量远小于W。作者的相关计算也不同于基于成本体积立体网络搜索范围与前景对象的最大位移。这个固定的范围比作者使用的局部相关对的数量要大得多，这导致了更多的噪声干扰。此外，当模型推广到具有不同基线的立体声对时，不需要预设范围。
给定两个重新采样和参与的特征图F1和F2，在位置(x，y)上的局部相关性可记为：
为d-th(d∈[0，D−1])相关对的匹配代价，C为特征通道数， f(d)和g(d)表示当前像素在水平和垂直方向上的固定偏移量。 传统上，在立体匹配中，两个校正图像之间的搜索方向只位于外极线上。为了处理非理想的立体整流情况，我们采用了2D-1D替代局部搜索策略来提高匹配精度。在一维搜索模式下，我们设置g(d)=0和f(d)∈[−r，r]，其中r=4。保留f(d)的正位移值，以便在每次迭代采样后调整不准确的结果。由等式计算的结果1被堆叠并连接在通道维度上，以获得最终的相关V。在二维搜索模式中，使用与扩张卷积相似的k×k网格进行相关计算。设置了k=√2r&#43;1来确保特征具有相同数量的通道，因此它们可以被输入到一个共享权重的更新块中。与迭代重采样合作，交替局部搜索也作为循环细化的传播模块，其中网络学习用其更准确的邻居替换对当前位置的有偏预测。
可变形的搜索窗口：立体匹配经常存在遮挡或无文本区域。在一个固定形状的局部搜索窗口中计算的相关性往往容易受到这些情况的影响。将可变形卷积扩展到相关计算中，使用内容自适应搜索窗口来生成相关对，这与AANet不同，后者仅在成本聚合中采用类似的策略。利用学习到的附加偏移量dx和dy，新的相关性可以计算为
偏移量如何改变传统搜索窗口的形式 自适应局部相关的说明。 顶部和底部分别是2D和1D情况，它们共享相同数量的搜索邻居，产生相同形状的相关图。 Group-wise相关性：受引入组级4D代价体积的启发，我们将特征图分成G组，分别计算局部相关性。最后，我们将G相关体积串联起来。在通道维度上的D × H × W，得到GD × H × W的输出量。过程如图。
级联的网络 对于非纹理或重复纹理区域，由于接受域大、语义信息充足，使用低分辨率和高级特征映射进行匹配更加鲁棒。 然而，在这种特征图中，精细结构的细节可能会丢失。为了保持鲁棒性，同时保留高分辨率输入中的细节，作者提出了级联迭代精化的相关计算和视差更新。 循环更新模块：我们基于GRU块和自适应组相关层(AGCL)构建了循环更新模块(RUM)。与PAFT不同的是，特征金字塔构建在单个相关层中，输出合并为一个卷，我们分别计算每个特征映射在不同级联级别的相关性，并单独细化几个迭代的视差。“sampler”以fn导出的坐标网格为输入，对分组特征的位置进行采样。{f1,…， fn}为初始化f0的n次迭代的中间预测。电流相关体积由学习到的偏移量o∈R2×(2r&#43;1)×h×w构造。GRU块更新当前预测并在下一次迭代时反馈给AGCL。 级联改进：除了级联的第一级(从输入分辨率的1/16开始，视差初始化为所有0)，其他级别将从前一级的预测的上采样版本作为初始化。虽然处理不同层次的细化，所有RUMs的重量相同。在最后一级细化后，进行凸上采样，得到输入分辨率下的最终预测结果。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/d3e8d40b752af9f5168c63cf948825f9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-06T15:37:56+08:00" />
<meta property="article:modified_time" content="2022-07-06T15:37:56+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">CREStereo： Practical Stereo Matching via Cascaded Recurrent Networkwith Adaptive Correlation-论文阅读</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><a class="link-info" href="https://github.com/megvii-research/CREStereo" title="Paper">Paper</a></p> 
<hr> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="%E6%91%98%E8%A6%81-toc" style="margin-left:0px;"><a href="#%E6%91%98%E8%A6%81" rel="nofollow">摘要</a></p> 
<p id="%E4%BB%8B%E7%BB%8D-toc" style="margin-left:0px;"><a href="#%E4%BB%8B%E7%BB%8D" rel="nofollow">介绍</a></p> 
<p id="%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C-toc" style="margin-left:0px;"><a href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C" rel="nofollow">相关工作</a></p> 
<p id="%E6%96%B9%E6%B3%95-toc" style="margin-left:0px;"><a href="#%E6%96%B9%E6%B3%95" rel="nofollow">方法</a></p> 
<p id="%E8%87%AA%E9%80%82%E5%BA%94%E7%BE%A4%E7%9B%B8%E5%85%B3%E5%B1%82-toc" style="margin-left:40px;"><a href="#%E8%87%AA%E9%80%82%E5%BA%94%E7%BE%A4%E7%9B%B8%E5%85%B3%E5%B1%82" rel="nofollow">自适应群相关层</a></p> 
<p id="%E7%BA%A7%E8%81%94%E7%9A%84%E7%BD%91%E7%BB%9C-toc" style="margin-left:40px;"><a href="#%E7%BA%A7%E8%81%94%E7%9A%84%E7%BD%91%E7%BB%9C" rel="nofollow">级联的网络</a></p> 
<p id="%E5%8F%A0%E5%8A%A0%E7%BA%A7%E8%81%94%E6%8E%A8%E7%90%86-toc" style="margin-left:40px;"><a href="#%E5%8F%A0%E5%8A%A0%E7%BA%A7%E8%81%94%E6%8E%A8%E7%90%86" rel="nofollow">叠加级联推理</a></p> 
<p id="%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-toc" style="margin-left:40px;"><a href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0" rel="nofollow">损失函数</a></p> 
<p id="%E5%90%88%E6%88%90%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE-toc" style="margin-left:40px;"><a href="#%E5%90%88%E6%88%90%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE" rel="nofollow">合成训练数据</a></p> 
<p id="%E5%AE%9E%E9%AA%8C-toc" style="margin-left:0px;"><a href="#%E5%AE%9E%E9%AA%8C" rel="nofollow">实验</a></p> 
<p id="%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%20%C2%A0%20%C2%A0%20%C2%A0-toc" style="margin-left:40px;"><a href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%20%C2%A0%20%C2%A0%20%C2%A0" rel="nofollow">数据集和评估指标      </a></p> 
<p id="%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0-toc" style="margin-left:40px;"><a href="#%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0" rel="nofollow">实现细节        </a></p> 
<p id="%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C-toc" style="margin-left:40px;"><a href="#%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C" rel="nofollow">消融实验</a></p> 
<p id="%E4%B8%8ESOTA%E5%AF%B9%E6%AF%94-toc" style="margin-left:40px;"><a href="#%E4%B8%8ESOTA%E5%AF%B9%E6%AF%94" rel="nofollow">与SOTA对比</a></p> 
<p id="%E5%AE%9E%E7%94%A8%E6%80%A7%E8%83%BD-toc" style="margin-left:40px;"><a href="#%E5%AE%9E%E7%94%A8%E6%80%A7%E8%83%BD" rel="nofollow">实用性能</a></p> 
<p id="%E6%80%BB%E7%BB%93-toc" style="margin-left:0px;"><a href="#%E6%80%BB%E7%BB%93" rel="nofollow">总结</a></p> 
<hr> 
<h2>摘要</h2> 
<p>       随着卷积神经网络的出现，立体匹配算法最近取得了巨大的进展。但是，<span style="color:#be191c;"><strong>因为结构薄、非理想修正、相机模块不一致和各种硬情况场景等实际复杂因素，从智能手机等大众级设备拍摄的真实图像对中准确提取视差仍然是一个巨大的挑战。</strong></span>在本文中，作者提出了一套创新的设计来解决实际立体匹配的问题：</p> 
<ol><li>为了更好地恢复精细的深度细节，设计了一个循环细化的层次网络，以粗到细的方式更新视差，以及堆叠级联结构进行推理；</li><li>提出了自适应群相关层来减轻错误修正的影响；</li><li>采用了一个新的合成数据集，对困难的情形给予特别注意力，以便更好地推广到真实场景。 研究结果不仅在Middlebury和ETH3D基准中排名第一，显著超过现有的最先进的方法，而且在现实生活中显示了高质量的细节，这清楚地证明了贡献的有效性。</li></ol> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" height="351" src="https://images2.imgbox.com/32/12/AFwLmk4e_o.png" width="1200"> 
  <figcaption>
    作者对来自Holopix50K数据集的图像的预测示例，展示了立体对的左侧图像及其相应的预测视差。 
   <br> 结果实现了较高的准确性，并显示了高质量的细节，为精细结构的对象。 
  </figcaption> 
 </figure> 
</div> 
<h2 id="%E4%BB%8B%E7%BB%8D">介绍</h2> 
<p>       立体匹配是计算机视觉的一个经典研究课题，给定一对修正图像，是计算两个对应像素之间的位移，即“视差”。它在自动驾驶、增强现实、模拟散景渲染等许多应用中都发挥着重要的作用。</p> 
<p class="img-center"><img alt="" height="167" src="https://images2.imgbox.com/0b/58/dyic2SNd_o.png" width="277"></p> 
<p>         近年来，在大型合成数据集的支持下，基于卷积神经网络(CNN)的立体匹配方法将视差估计的精度提高到了一个新的高度。<span style="color:#be191c;"><strong>然而，为了使该算法在日常摄影的场景中真正实用，我们仍然面临着三个主要的障碍。</strong></span></p> 
<p>       首先，对于大多数现有的算法来说，精确恢复精细图像细节或网、线框等薄结构的视差仍然是一个复杂的问题。日常照片是高分辨率的，这更让问题复杂化。例如，在计算中，围绕细节的视差误差会导致渲染结果退化，不利于人类感知。其次，对于真实世界的立体图像对，很难获得完美的修正，因为它们通常是由具有不同特征的相机模块产生的。例如，目前大多数智能手机都用广角镜头和长焦镜头捕捉立体声对，它们<span style="color:#be191c;"><strong>具有焦距和失真参数等明显特征</strong></span>，不可避免地会导致不理想的校正。因此，现有的假设立体对被完全修正的方法在这种对抗性条件下很可能失败。此外，由不一致的摄像机模块产生的图像对可能在<span style="color:#be191c;"><strong>照明、白平衡、图像质量</strong></span>等方面发生变化，这使得估计任务更加困难。最后，虽然它已经表明，模型训练从足够大的合成数据集可以推广到真实场景，视差估计在典型的硬情况下，如<span style="color:#be191c;"><strong>非纹理或重复纹理区域</strong></span>，仍然很困难，这需要特别注意在覆盖相关场景的训练数据集。      </p> 
<p>     <span style="color:#ff9900;"><strong>【研究动机】</strong></span><u><strong><span style="color:#ff9900;"><strong> </strong></span> 在本文中，作者提出了 CREStereo，交叉立体声匹配网络，即级联立体匹配网络，它包括一套新的设计，以解决实际的立体匹配的问题。为了更好地恢复复杂的图像细节，设计了一个分层网络，以粗到细的方式反复更新视差；此外，采用<span style="color:#be191c;">堆叠级联体系结构</span>进行高分辨率推断。为了减轻校正误差的负面影响，设计了一个<span style="color:#be191c;">自适应群局部相关层</span>来进行特征匹配。此外，我们引入了一个新的合成数据集，在光线、纹理和形状方面具有更丰富的变化，以便更好地推广到真实世界的场景。</strong></u></p> 
<p>        目前为止，CREStereo在ETH3D立体双视图和 Middlebury基准测试中都排名第一，并在KITTI 2012/2015上取得了具有竞争力的性能。此外，作者的网络在任意真实场景中表现出了优越的性能，很好地证明了设计的有效性。        </p> 
<p>因此，作者主要贡献可以总结如下：</p> 
<ol><li>提出了一种用于实际立体匹配的级联递归网络和堆叠的高分辨率推理结构；</li><li>设计了自适应群相关层来处理非理想校正；</li><li>创建了一个新的合成数据集，以更好地推广到现实场景；</li><li>方法在 Middlebury和ETH3D等公共基准上优于现有方法的显著优势，大大提高了真实立体图像恢复视差的准确性。</li></ol> 
<p><img alt="" height="339" src="https://images2.imgbox.com/3f/8a/5G6LDTEE_o.png" width="1200"></p> 
<h2 id="%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C">相关工作</h2> 
<p><span style="color:#a2e043;"><strong>传统算法：</strong></span>立体匹配是一个已经研究了很长时间，具有挑战性的问题。传统的算法分为局部算法和全局算法。局部方法使用以极线像素为中心的支持窗口来计算匹配代价。全局方法将立体匹配视为一个优化问题，其中显式代价函数由感知传播或图割算法制定和优化。在此基础上，提出了一种基于动态规划的半全局匹配方法，该方法利用互信息代替强度。</p> 
<p><span style="color:#a2e043;"><strong>基于学习的算法：</strong></span>深度神经网络首次引入立体匹配任务，仅用于匹配成本计算。Zbontar和LeCun提出训练一个CNN来初始化补丁之间的匹配代价，通过交叉聚合和半叶优化，如SGM（<u>由于代价计算步骤只考虑了局部的相关性，对噪声非常敏感，无法直接用来计算最优视差，所以SGM算法通过代价聚合步骤，使聚合后的代价值能够更准确的反应像素之间的相关性</u>）。近年来，端到端网络已成为立体匹配领域的主流，其中一种网络只使用二维卷积。Mayer等人引入了第一个名为DispNet的端到端网络及其相关版本DispNetC用于视差估计。Pang等人提出了一个具有多尺度残差学习的两阶段框架。Guo等人提出了具有群体相关性的GwcNet来改进相似性度量。AANet介绍了一种新的利用稀疏点和多尺度相互作用的聚合方法。最近的一种方法RAFT-Stereo利用光流网络RAFT的迭代细化设计了一个适合于立体匹配的网络。另一条网络使用3D卷积来执行传统方法的成本量构建和聚合。GCNet和PSMNet提出构建一个具有三维沙漏聚合网络的四维成本量。对于高分辨率图像，<u>Yang等人提出了一种从粗到细的分层网络来解决内存和速度问题</u>。最近，神经结构搜索也被引入了深度立体网络。</p> 
<p><span style="color:#a2e043;"><strong>实用的立体声匹配：</strong></span>面向真实世界图像的立体匹配是一个较少被探索的问题。Pang等人提出了一种自适应方法，将CNN推广到目标域，没有地面真实视差。Luo等人提出了一种小波合成网络，为智能手机上的散景应用产生更好的结果。Song等人引入了一个领域适应管道，以缩小合成和真实领域之间的差距。</p> 
<p><span style="color:#a2e043;"><strong>合成数据集：</strong></span>足够的训练数据对于深度立体模型是必不可少的，但在现实世界中很难获得准确的视差。合成数据集提供了高精度和密集的地面真值。最近，He等人使用Blender建立了一个立体匹配的数据生成管道，使用来自公共数据集的真实图像的纹理。自动流引入了一种简单的方法来呈现随机多边形的运动光流训练。尽管这些数据集很有效，但它们的物体形状的变化仍然有限，视差/光流值的分布也很有限，这削弱了从合成到现实世界的推广能力。</p> 
<h2 id="%E6%96%B9%E6%B3%95">方法</h2> 
<p>在本节中，将介绍提出的级联立体匹配网络(CREStereo)和新合成数据集</p> 
<h3 id="%E8%87%AA%E9%80%82%E5%BA%94%E7%BE%A4%E7%9B%B8%E5%85%B3%E5%B1%82">自适应群相关层</h3> 
<p>       作者观察到，很难为现实世界的立体相机实现完美的校准。例如，两个相机可能不会严格放置在水平外极线上，导致在三维空间中轻微旋转；或者相机镜头的图像即使经过修正后也会有残余失真。因此<span style="color:#be191c;"><strong>，对于立体图像对，对应的点可能不位于同一扫描线上。</strong></span>因此，作者提出了一种自适应群相关层(AGCL)来减少这种情况下的匹配模糊性，在只计算局部相关的情况下，比全对匹配获得更好的性能。</p> 
<p><span style="color:#a2e043;"><strong>局部特征注意力：</strong></span>作者不计算每对像素的全局相关性，而是只匹配一个局部窗口中的点，以避免大量的内存消耗和计算成本。针对稀疏特征匹配的LoFTR特征匹配，在级联第一阶段的相关计算之前添加了一个注意模块，以便将全局上下文信息聚合到单个或交叉特征图中。在之后，在主干输出中添加了位置编码，这增强了特征映射的位置依赖性。交替计算自注意和交叉注意，其中使用线性注意层来降低计算复杂度。</p> 
<p><span style="color:#a2e043;"><strong>2D-1D转换局部搜索：</strong></span>不同于流量估计网络RAFT及其立体版本，其中全对相关性由两个C×H×W特征图的矩阵乘法计算，输出4DH×W×W×W或3DH×W×W成本量，<span style="color:#be191c;"><strong>只在一个局部搜索窗口中计算相关性，该窗口输出更小体积的H×W×D，以节省内存和计算成本。</strong></span>H和W表示特征图的高度和宽度，D是相关对的数量远小于W。作者的相关计算也不同于基于成本体积立体网络搜索范围与前景对象的最大位移。这个固定的范围比作者使用的局部相关对的数量要大得多，这导致了更多的噪声干扰。此外，当模型推广到具有不同基线的立体声对时，不需要预设范围。</p> 
<p>给定两个重新采样和参与的特征图F1和F2，在位置(x，y)上的局部相关性可记为：</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" height="55" src="https://images2.imgbox.com/ba/5f/37PPiTRP_o.png" width="279"> 
  <figcaption>
    为d-th(d∈[0，D−1])相关对的匹配代价，C为特征通道数， 
   <br> f(d)和g(d)表示当前像素在水平和垂直方向上的固定偏移量。 
  </figcaption> 
 </figure> 
</div> 
<p style="text-align:justify;">       传统上，在立体匹配中，两个校正图像之间的搜索方向只位于外极线上。为了处理非理想的立体整流情况，我们采用了2D-1D替代局部搜索策略来提高匹配精度。在一维搜索模式下，我们设置g(d)=0和f(d)∈[−r，r]，其中r=4。<u>保留f(d)的正位移值，以便在每次迭代采样后调整不准确的结果。</u>由等式计算的结果1被堆叠并连接在通道维度上，以获得最终的相关V。在二维搜索模式中，使用与扩张卷积相似的k×k网格进行相关计算。设置了k=√2r+1来确保特征具有相同数量的通道，因此它们可以被输入到一个共享权重的更新块中。与迭代重采样合作，交替局部搜索也作为循环细化的传播模块，其中网络学习用其更准确的邻居替换对当前位置的有偏预测。</p> 
<p><span style="color:#a2e043;"><strong>可变形的搜索窗口：</strong></span>立体匹配经常存在遮挡或无文本区域。在一个固定形状的局部搜索窗口中计算的相关性往往容易受到这些情况的影响。将可变形卷积扩展到相关计算中，使用内容自适应搜索窗口来生成相关对，这与AANet不同，后者仅在成本聚合中采用类似的策略。利用学习到的附加偏移量dx和dy，新的相关性可以计算为</p> 
<p class="img-center"><img alt="" height="65" src="https://images2.imgbox.com/9b/63/YZmtZ1A2_o.png" width="331"></p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" height="231" src="https://images2.imgbox.com/f2/45/TVZzxPBc_o.png" width="560"> 
  <figcaption>
    偏移量如何改变传统搜索窗口的形式 
   <br> 自适应局部相关的说明。 
   <br> 顶部和底部分别是2D和1D情况，它们共享相同数量的搜索邻居，产生相同形状的相关图。 
  </figcaption> 
 </figure> 
</div> 
<p><span style="color:#a2e043;"><strong>Group-wise相关性：</strong></span>受引入组级4D代价体积的启发，我们将特征图分成G组，分别计算局部相关性。最后，我们将G相关体积串联起来。在通道维度上的D × H × W，得到GD × H × W的输出量。过程如图。</p> 
<p><img alt="" height="324" src="https://images2.imgbox.com/cb/f4/92NWfzOo_o.png" width="1200"></p> 
<h3 id="%E7%BA%A7%E8%81%94%E7%9A%84%E7%BD%91%E7%BB%9C">级联的网络</h3> 
<p style="text-align:justify;">       对于非纹理或重复纹理区域，由于接受域大、语义信息充足，使用低分辨率和高级特征映射进行匹配更加鲁棒。 然而，在这种特征图中，精细结构的细节可能会丢失。为了保持鲁棒性，同时保留高分辨率输入中的细节，作者提出了<span style="color:#be191c;"><strong>级联迭代精化</strong></span>的相关计算和视差更新。 循环更新模块：我们基于GRU块和自适应组相关层(AGCL)构建了循环更新模块(RUM)。与PAFT不同的是，特征金字塔构建在单个相关层中，输出合并为一个卷，我们分别计算每个特征映射在不同级联级别的相关性，并单独细化几个迭代的视差。“sampler”以fn导出的坐标网格为输入，对分组特征的位置进行采样。{f1,…， fn}为初始化f0的n次迭代的中间预测。电流相关体积由学习到的偏移量o∈R2×(2r+1)×h×w构造。GRU块更新当前预测并在下一次迭代时反馈给AGCL。 级联改进：除了级联的第一级(从输入分辨率的1/16开始，视差初始化为所有0)，其他级别将从前一级的预测的上采样版本作为初始化。虽然处理不同层次的细化，所有RUMs的重量相同。在最后一级细化后，进行凸上采样，得到输入分辨率下的最终预测结果。</p> 
<h3 id="%E5%8F%A0%E5%8A%A0%E7%BA%A7%E8%81%94%E6%8E%A8%E7%90%86" style="text-align:justify;">叠加级联推理</h3> 
<p style="text-align:justify;">正如前几节所讨论的，在训练过程中，作者使用固定分辨率的三层特征金字塔进行层次细化。然而，对于分辨率较高的图像作为输入，需要进行更多的降采样，以扩大接收域，进行特征提取和相关计算。但对于高分辨率图像中位移较大的小目标，直接下采样可能会使这些区域的特征退化。为了解决这一问题，作者设计了一种具有推理快捷方式的堆叠级联架构。特别的，作者预先对图像对进行下采样，构建一个图像金字塔，并将它们输入到同一个训练好的特征提取网络中，以利用多层次的上下文。图下图右侧显示了堆叠级联架构的概览，为了简洁起见，没有显示同一阶段的跳跃连接。对于堆叠级联的某一特定阶段，该阶段的所有RUM将与更高分辨率阶段的最后一个RUM一起使用。叠层梯级的所有阶段在训练中都有相同的重量，所以没有精细的调整。</p> 
<p style="text-align:justify;"><img alt="" height="266" src="https://images2.imgbox.com/c7/eb/aGfhQdOl_o.png" width="1072"></p> 
<h3 id="%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0" style="text-align:justify;">损失函数</h3> 
<p style="text-align:justify;">对于每个阶段s∈{116, 18, 41}的特征金字塔，作者用上采样算子µs将输出{fis，···，f sn}的序列调整到完全预测分辨率，并使用类似RAFT的指数加权l1距离为损失函数(γ设为0.9)。给定ground truth视差dgt，总损失定义为:</p> 
<p class="img-center"><img alt="" height="46" src="https://images2.imgbox.com/9c/51/9Abz1NAx_o.png" width="279"></p> 
<h3 id="%E5%90%88%E6%88%90%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE">合成训练数据</h3> 
<p>与以前的合成数据集相比，作者的数据生成管道将额外的注意力放在现实场景中具有挑战性的案例上，并具有各种增强功能。作者利用Blender生成我们的合成训练数据。每个场景由左右图像对和对应像素精确的密集视差图组成，由双虚拟相机和习惯位置的物体捕获。作者的主要设计考虑如下所示。</p> 
<p><span style="color:#a2e043;"><strong>形状：</strong></span>作者用多种来源使模型的形状多样化，作为主要的场景内容:</p> 
<ol><li>ShapeNet数据集，超过40000个不同形状的常见物体的3D模型，形成基本内容来源。</li><li>Blender的树苗生成插件，提供精细而杂乱的视差图。</li><li>使用搅拌机的内部基本形状结合线框修改器来生成具有挑战性的场景的模型，包括孔洞和开放式结构。</li></ol> 
<p><span style="color:#a2e043;"><strong>灯光和纹理：</strong></span>作者将颜色和亮度随机的不同类型的灯光放置在场景内部的随机位置，造成了一个复杂的照明环境。现实世界的图像被用作物体和场景背景的纹理，特别是包含重复模式或缺乏可见特征的硬场景。此外，利用了Blender的Cycles渲染器的光追踪能力，并随机将对象设置为透明或带有金属反射，以便用类似的属性覆盖真实世界的场景。</p> 
<p><span style="color:#a2e043;"><strong>视差分布：</strong></span>为了覆盖不同的基线设置，作者努力保证生成的数据的视差在大范围内平滑分布。我们将物体放置在由摄像机的视野和最大距离形成的截锥体形状的空间中。从概率分布中随机选择每个物体的确切位置，然后根据其距离进行缩放，以防止遮挡视线。这种做法导致随机但可控的视差分布。</p> 
<h2 id="%E5%AE%9E%E9%AA%8C">实验</h2> 
<h3 id="%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%20%C2%A0%20%C2%A0%20%C2%A0"><strong>数据集和评估指标   </strong>   </h3> 
<p>       根据三个流行的公共基准来评估本文的方法。Middlebury2014提供了不同光照环境下的23对高分辨率图像。用大基线立体相机拍摄，Middlebury的最大视差可以超过600像素。ETH3D由27幅单色立体图像对组成，通过激光扫描仪进行视差采样，覆盖室内和室外场景。KITTI 2012/2015由200幅广角立体街景图像对组成，使用激光雷达采样稀疏视差地面真实度。        </p> 
<p>        除了作者渲染的数据集，收集主要的公共数据集用于训练，包括Sceneflow ， Sintel和Falling Things。scenflow包含39k多个合成场景设置的训练对。坠落物包含了大量来自家居物体模型场景的图像。Sintel提供来自各种合成序列的1.2k立体声对。作者利用的其他数据源有InStereo2K ， Carla和AirSim。        </p> 
<p>        为了进行评估，作者遵循流行的度量标准，包括AvgErr(平均误差)、Bad2.0(视差误差大于2像素的像素百分比)、D1-all(左图视差异常像素百分比)等。</p> 
<h3 id="%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0">实现细节        </h3> 
<p><span style="color:#a2e043;"><strong>训练：</strong></span>本文的网络是用Pytorch实现的框架。模型在8 NVIDIA GTX上训练2080Ti gpu，批量大小为16。整个训练过程设置为30万次迭代。使用标准学习率为0.0004的Adam优化器。在训练开始时进行6000次迭代的热身过程，学习率从标准值的5%线性增加到100%。在180000次迭代后，在训练结束时，学习率线性下降到标准值的5%。训练模型的输入尺寸为384 × 512。所有训练样本在输入模型之前都要经过一组增广运算。        </p> 
<p><span style="color:#a2e043;"><strong>增强：</strong></span>为了模拟摄像机模块的不一致性和非理想校正，我们采用多种数据增强技术进行训练。首先，我们对两个输入分别应用非对称色差增强，包括亮度、对比度和伽马的偏移。为了进一步增强现实图像中校正误差的鲁棒性，我们只对正确的图像进行了空间增强:在很小的范围内略微进行随机单应变换和垂直位移(&lt; 2像素)。为了避免不适定区域的不匹配，我们使用高度和宽度在50到100像素之间的随机矩形遮挡块。最后，为了将来自不同来源的输入数据拟合到网络的训练输入大小中，对这组立体图像和视差进行随机调整和裁剪操作。</p> 
<h3 id="%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C">消融实验</h3> 
<p>在本节中，作者将在不同的设置下评估我们的模型，以证明网络组件的有效性。除对层叠级联进行烧蚀研究外，所有评价分辨率均为768 × 1024。</p> 
<p><strong>相关类型：</strong>为了比较不同类型的相关性的影响，作者将他们的相关性层替换为其他形式。如表1所示，2D和 与它们的局部形式相比，[45]和[23]中使用的1D全对相关导致准确性大幅下降。当作者将交替局部关联替换为单一的二维或一维关联会降低最终精度，当网络包含1级联以上时，由于校正误差随着分辨率的增加而增加，这一点更加明显。</p> 
<p><strong>组件AGCL：</strong>如表1的下半部分所示，使用固定的相关窗口而不学习偏移降低了精度，这证明了自适应机制的有效性。用单一形式替换组相关性和去除局部特征注意模块都降低了算法的精度。</p> 
<p><strong>级联的RUMs：</strong>比较了不同数量级联的性能。如表1所示，使用没有级联的单一RUM会导致精度的大幅下降。当改变级联数时，在保持关联类型不变的情况下，级联数越多，预测误差明显减小。这说明了级联架构的重要性。     </p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" height="250" src="https://images2.imgbox.com/20/b3/PtHfqj1I_o.png" width="298"> 
  <figcaption>
    RUMs的消融研究： 上半部分是对不同形式的相关层和不同级别级联的比较，训练在除Middlebury和ETH3D之外的公共数据集上。 下半部分是在完整数据集上训练的AGCL关键组件的评估。 
  </figcaption> 
 </figure> 
</div> 
<p><strong>级联：</strong>在推理过程中，使用不同层次的图像金字塔作为输入输入级联，同时共享相同的训练参数。比较了不同分辨率下不同级联的性能。如表2所示，仅使用单个级联时，预测误差随输入大小的增加而增大。多层输入大大降低了误差，表明我们的叠级联方案对视差精度有很大的提高。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" height="156" src="https://images2.imgbox.com/1c/34/q3hGMa4B_o.png" width="311"> 
  <figcaption>
    叠级联结构推理过程的消融研究 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<p><strong>新的合成数据：</strong>为了分析作者提出的合成数据的有效性，从训练数据集中采样了35,000对图像，并与类似大小的图像进行比较Sceneflow。这两个数据集用于训练我们的模型，在50,000次迭代中使用相同的增强。如图6所示，作者的合成数据降低了训练损失和在ETH3D和Middlebury验证数据中都有更好的性能。这表明作者的数据集在领域泛化方面更有优势。 </p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" height="133" src="https://images2.imgbox.com/87/89/prOPB3LR_o.png" width="474"> 
  <figcaption>
    使用scenflow和作者合成数据集训练的模型的训练损失和ETH3D / Middlebury验证误差 
  </figcaption> 
 </figure> 
</div> 
<h3 id="%E4%B8%8ESOTA%E5%AF%B9%E6%AF%94">与SOTA对比</h3> 
<p><strong>Middlebury：</strong>用来自的23对图像(包括另外13对带有ground truth的图像)来训练本文的网络Middlebury 2014数据集与作者的完整训练集在没有微调。将Middlebury训练集的比例提高到整个训练集的2%。使用调整大小的全分辨率图像，采用两阶段推断，在1536 × 2048的测试集进行评估，并将结果提交到在线排行榜。在120多种其他方法中，作者在大多数指标上都取得了第一名，超过了公布的最先进的bad的2.0指标占21.73%，A95指标占31.00%。与其他方法的定量比较结果如表3所示。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" height="226" src="https://images2.imgbox.com/7e/ee/BU3ELxyD_o.png" width="361"> 
  <figcaption>
    量化结果在Middlebury基准 
  </figcaption> 
 </figure> 
</div> 
<p><strong>ETH3D：</strong>作者在整个训练集上训练我们的网络，其中2%的增强训练数据来自ETH3D低分辨率双视图立体数据集。在不进行微调的情况下，评估测试集的大小为768 × 1024，其中采用两阶段推断。在撰写本文时，作者在所有指标的在线基准测试中实现了最先进的发布方法。在糟糕的1.0度量上，本文的方法比发布的最先进的方法高出59.84%。定量比较如表4所示。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" height="216" src="https://images2.imgbox.com/49/e4/SJpBBhO9_o.png" width="350"> 
  <figcaption>
    ETH3D基准的定量结果 
  </figcaption> 
 </figure> 
</div> 
<p><strong>KITTI：</strong>与Middlebury和ETH3D的训练过程不同，作者在完整训练集上对预训练的模型进行微调，在KITTI上再进行50K次迭代2012年和2015年训练集。初始学习率设置为0.0001。作者增强KITTI数据集的75%，其余部分从整个训练集中随机抽样。在评估过程中，作者将输入填充到384×1248后再馈入网络，采用单级推理。作者在两个数据集上都实现了具有竞争力的性能，在2像素错误阈值下的outnoc上超过了KITTI 2012中的LEAStereo[8] 9.47%。我们在图8中展示了KITTI 2015的视觉对比。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" height="194" src="https://images2.imgbox.com/46/3a/XxxeOhSZ_o.png" width="458"> 
  <figcaption>
    视觉上与其他方法比较的一例 KITTI 2015测试集。作者的方法保留了更多的细节 
  </figcaption> 
 </figure> 
</div> 
<h3 id="%E5%AE%9E%E7%94%A8%E6%80%A7%E8%83%BD">实用性能</h3> 
<p>与来自标准立体数据集的真实图像相比，从消费级设备上获取的图像对立体匹配提出了更大的挑战。为了公平的比较，用作者发布的代码和推荐的设置在我们的完整训练集上训练所有其他立体声网络。</p> 
<p><strong>Holopix50K：</strong>图9显示了作者的网络在不同场景下与Holopix50K[16]数据集上发表的几次立体匹配的定性比较结果。进行预认证以消除可能的负视差。可视化结果表明，该方法在猫须和金属丝网等细物体上具有明显优势。本文呢也在无纹理的区域实现了更好的性能，如墙壁和窗户。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" height="397" src="https://images2.imgbox.com/fe/60/sNXvg2IL_o.png" width="1200"> 
  <figcaption>
    Holopix50K[16]数据集上不同方法的结果比较。放大以获得最佳视野 
  </figcaption> 
 </figure> 
</div> 
<p>扰乱的ETH3D：作者在ETH3D数据集上模拟了实际场景中常见的干扰，以测试所提方法的鲁棒性，并给出了定量结果。图10所示。这些干扰包括图像模糊、颜色变换、色差噪声、图像透视变换、垂直位移和空间畸变。结果表明，本文的方法不容易受到这些干扰。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" height="239" src="https://images2.imgbox.com/d0/6a/X3as7WU9_o.png" width="465"> 
  <figcaption>
    不同干扰类型下ETH3D训练数据集的方法比较 
  </figcaption> 
 </figure> 
</div> 
<p><strong>智能手机的照片：</strong>由于真实场景中ground truth的视差难以获取，一种经验方法是手动标记前景掩模Mf来评估视差质量。IoU(交集/联合)是分割任务中常用的度量方法。对于视差图，可以设置一个阈值t，得到前景掩模Mt，其中前景的视差值大于t。“mxIoU”表示通过改变t, Mf和Mt之间的最大IoU。“mxIoUbd”意味着从Mf边界处的带状区域内的mxIoU(设置p = 4)个像素。定量和定性比较结果分别如表5和图11。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" height="276" src="https://images2.imgbox.com/fc/a3/AbQp47BE_o.png" width="1040"> 
  <figcaption>
    对400个智能手机捕捉到的场景进行定量分析。 作者为每种方法选择性能最好的分辨率。 
   <br> 使用RAFTStereo比较智能手机照片中重复纹理和非纹理情况的预测差异。mxIoU得分在图中标出。 
  </figcaption> 
 </figure> 
</div> 
<h2 id="%E6%80%BB%E7%BB%93">总结</h2> 
<p>尽管深度立体视觉网络取得了前所未有的成功，但在真实场景中精确恢复差异仍然存在障碍。</p> 
<p>本文，作者提出CREStereo，一个新颖的立体匹配网络，在公共基准和现实场景中都能获得最先进的结果。作者在这里的关键信息是，<span style="color:#be191c;"><strong>为了让算法真正在现实世界中工作，网络架构和训练数据都值得缜密的思考。</strong></span></p> 
<p>通过自适应相关的级联递归网络，能够比现有方法更好地恢复精细的深度细节; 通过精心设计的合成数据集，能够更好地处理非纹理或重复纹理区域等硬情况场景。本文的方法的一个限制是，该模型还不够有效，不能在当前的移动应用程序中运行。</p> 
<p>未来的改进可以使我们的网络适应各种便携式设备，最好是实时的。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/54d9ec5ec2a488809647737a729bbb1a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Linux基础知识笔记</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/45a7caefa80a684d6da3bd948b054835/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Mac利用iTMSTransporter命令行上传 ipa 到 App Store</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>