<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>hive相关汇总 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="hive相关汇总" />
<meta property="og:description" content="hive 1. hive 有哪些方式保存元数据，各有哪些特点？2. hive内部表和外部表的区别3.生产环境中为什么建议使用外部表？什么时候使用内部表？什么时候使用外部表？4.你们数据库怎么导入hive 的,有没有出现问题5.简述Hive中的虚拟列作用是什么，使用它的注意事项扩展 6.hive partition分区7. hive partition什么时候使用手动分区8.hive partition怎么手动分区9.hive partition什么时候使用自动分区10.hive partition怎么自动分区11.如何查看分区12.分桶结构表clustered13.分桶表怎么构建14.分区表和分桶表的区别分区表分桶表 15.insert into 和 insert overwrite区别？16.假如一个分区的数据主部错误怎么通过hivesql删除hdfs17.Hive 表关联查询，如何解决数据倾斜的问题？1）倾斜原因：2）解决⽅案 18.请谈⼀下 Hive 的特点，Hive 和 RDBMS 有什么异同？（重新整理）19.请说明 hive 中 Sort By，Order By，Cluster By，Distrbute By 各代表什么意思？20.简要描述数据库中的 null，说出 null 在 hive 底层如何存储，并 解释 select a.* from t1 a left outer join t2 b on a.id=b.id where b.id is null; 语句的含义？21.写出 hive 中 split、coalesce 及 collect_list 函数的⽤法（可举 例）？22.Hive 有哪些⽅式保存元数据，各有哪些特点？23.Hive 内部表和外部表的区别？24.Hive 的 HSQL 转换为 MapReduce 的过程？（重新整理）25." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/84b242bea58da186718f4d62ae2f9855/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-05T19:17:29+08:00" />
<meta property="article:modified_time" content="2023-12-05T19:17:29+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">hive相关汇总</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>hive</h4> 
 <ul><li><a href="#1_hive__1" rel="nofollow">1. hive 有哪些方式保存元数据，各有哪些特点？</a></li><li><a href="#2_hive_5" rel="nofollow">2. hive内部表和外部表的区别</a></li><li><a href="#3_15" rel="nofollow">3.生产环境中为什么建议使用外部表？什么时候使用内部表？什么时候使用外部表？</a></li><li><a href="#4hive__22" rel="nofollow">4.你们数据库怎么导入hive 的,有没有出现问题</a></li><li><a href="#5Hive_25" rel="nofollow">5.简述Hive中的虚拟列作用是什么，使用它的注意事项</a></li><li><ul><li><a href="#_47" rel="nofollow">扩展</a></li></ul> 
  </li><li><a href="#6hive_partition_63" rel="nofollow">6.hive partition分区</a></li><li><a href="#7_hive_partition_73" rel="nofollow">7. hive partition什么时候使用手动分区</a></li><li><a href="#8hive_partition_77" rel="nofollow">8.hive partition怎么手动分区</a></li><li><a href="#9hive_partition_103" rel="nofollow">9.hive partition什么时候使用自动分区</a></li><li><a href="#10hive_partition_105" rel="nofollow">10.hive partition怎么自动分区</a></li><li><a href="#11_158" rel="nofollow">11.如何查看分区</a></li><li><a href="#12clustered_164" rel="nofollow">12.分桶结构表clustered</a></li><li><a href="#13_175" rel="nofollow">13.分桶表怎么构建</a></li><li><a href="#14_207" rel="nofollow">14.分区表和分桶表的区别</a></li><li><ul><li><a href="#_214" rel="nofollow">分区表</a></li><li><a href="#_221" rel="nofollow">分桶表</a></li></ul> 
  </li><li><a href="#15insert_into__insert_overwrite_228" rel="nofollow">15.insert into 和 insert overwrite区别？</a></li><li><a href="#16hivesqlhdfs_235" rel="nofollow">16.假如一个分区的数据主部错误怎么通过hivesql删除hdfs</a></li><li><a href="#17Hive__243" rel="nofollow">17.Hive 表关联查询，如何解决数据倾斜的问题？</a></li><li><ul><li><a href="#1_245" rel="nofollow">1）倾斜原因：</a></li><li><a href="#2_252" rel="nofollow">2）解决⽅案</a></li></ul> 
  </li><li><a href="#18_Hive_Hive__RDBMS__262" rel="nofollow">18.请谈⼀下 Hive 的特点，Hive 和 RDBMS 有什么异同？（重新整理）</a></li><li><a href="#19_hive__Sort_ByOrder_ByCluster_ByDistrbute_By__267" rel="nofollow">19.请说明 hive 中 Sort By，Order By，Cluster By，Distrbute By 各代表什么意思？</a></li><li><a href="#20_null_null__hive___select_a_from_t1_a_left_outer_join_t2_b_on_aidbid_where_bid_is_null__272" rel="nofollow">20.简要描述数据库中的 null，说出 null 在 hive 底层如何存储，并 解释 select a.* from t1 a left outer join t2 b on a.id=b.id where b.id is null; 语句的含义？</a></li><li><a href="#21_hive__splitcoalesce__collect_list___276" rel="nofollow">21.写出 hive 中 split、coalesce 及 collect_list 函数的⽤法（可举 例）？</a></li><li><a href="#22Hive__280" rel="nofollow">22.Hive 有哪些⽅式保存元数据，各有哪些特点？</a></li><li><a href="#23Hive__285" rel="nofollow">23.Hive 内部表和外部表的区别？</a></li><li><a href="#24Hive__HSQL__MapReduce__293" rel="nofollow">24.Hive 的 HSQL 转换为 MapReduce 的过程？（重新整理）</a></li><li><a href="#25Hive__303" rel="nofollow">25.Hive 底层与数据库交互原理？</a></li><li><a href="#26_Hive__307" rel="nofollow">26.请把下⾯语句⽤ Hive 实现</a></li><li><a href="#27_texttxt__hive__test_20161010__test__l_date_314" rel="nofollow">27.写出将 text.txt ⽂件放⼊ hive 中 test 表‘2016-10-10’ 分区 的语句，test 的分区字段是 l_date</a></li><li><a href="#28Hive__320" rel="nofollow">28.Hive 如何进⾏权限控制？</a></li><li><a href="#29_hive_udf__372" rel="nofollow">29.对于 hive，你写过哪些 udf 函数，作⽤是什么？</a></li><li><ul><li><a href="#_374" rel="nofollow">开发过程</a></li></ul> 
  </li><li><a href="#30Hive__TextFileSequenceFileRCfile_ORCfile__393" rel="nofollow">30.Hive 中的压缩格式 TextFile、SequenceFile、RCfile 、ORCfile 各有什么区别？</a></li><li><a href="#31Hive_join__445" rel="nofollow">31.Hive join 过程中⼤表⼩表的放置顺序？</a></li><li><a href="#32Hive__MapReduce__450" rel="nofollow">32.Hive 的两张表关联，使⽤ MapReduce 怎么实现？</a></li><li><a href="#33Hive__in__454" rel="nofollow">33.Hive 中使⽤什么代替 in 查询？</a></li><li><a href="#34_Hive__MapReduce__458" rel="nofollow">34.所有的 Hive 任务都会有 MapReduce 的执⾏吗？</a></li><li><a href="#35Hive_UDFUDAFUDTF__462" rel="nofollow">35.Hive 的函数：UDF、UDAF、UDTF 的区别？</a></li><li><a href="#36_Hive__467" rel="nofollow">36.说说对 Hive 桶表的理解？</a></li><li><a href="#37Hive__UDF__472" rel="nofollow">37.Hive ⾃定义 UDF 函数的流程?</a></li><li><a href="#38Hive__483" rel="nofollow">38.Hive 可以像关系型数据库那样建⽴多个库吗？</a></li><li><a href="#39Hive__485" rel="nofollow">39.Hive 实现统计的查询语句是什么？</a></li><li><a href="#40Hive__487" rel="nofollow">40.Hive 优化措施</a></li><li><ul><li><a href="#1Fetch__489" rel="nofollow">1.Fetch 抓取</a></li><li><a href="#2_493" rel="nofollow">2.本地模式</a></li><li><a href="#3_517" rel="nofollow">3.表的优化</a></li><li><a href="#4_541" rel="nofollow">优化措施4.数据倾斜</a></li><li><a href="#5_546" rel="nofollow">优化措施5.`并⾏执⾏`</a></li><li><a href="#6_547" rel="nofollow">优化措施6.`严格模式`</a></li><li><a href="#7JVM__548" rel="nofollow">优化措施7.`JVM 重⽤`</a></li><li><a href="#8_549" rel="nofollow">化措施8.`推测执⾏`</a></li><li><a href="#9_550" rel="nofollow">优化措施9.`压缩`</a></li><li><a href="#10EXPLAIN_551" rel="nofollow">优化措施10.EXPLAIN（执⾏计划）</a></li></ul> 
  </li><li><a href="#41Hive__552" rel="nofollow">41.Hive 数据分析⾯试题</a></li><li><ul><li><a href="#1__TOPN_568" rel="nofollow">1 情景题：分组 TOPN</a></li><li><a href="#2_where__having_593" rel="nofollow">2 情景题：where 与 having</a></li><li><a href="#3__609" rel="nofollow">3 情景题：数据倾斜</a></li><li><a href="#4__624" rel="nofollow">4 情景题：分区表</a></li></ul> 
  </li><li><a href="#42hive_645" rel="nofollow">42.hive导入导出</a></li><li><ul><li><a href="#_646" rel="nofollow">导入</a></li><li><a href="#_653" rel="nofollow">导出</a></li></ul> 
  </li><li><a href="#43Hive_663" rel="nofollow">43.Hive的存储格式及压缩算法</a></li><li><ul><li><a href="#_664" rel="nofollow">存储格式</a></li><li><a href="#_680" rel="nofollow">压缩算法</a></li></ul> 
  </li><li><a href="#44Hive_698" rel="nofollow">44.Hive中小文件问题</a></li><li><ul><li><a href="#_699" rel="nofollow">产生:</a></li><li><a href="#_703" rel="nofollow">影响</a></li><li><a href="#_706" rel="nofollow">解决</a></li><li><a href="#mapreduce_717" rel="nofollow">map/reduce端的相关参数的设置</a></li><li><a href="#Hive_738" rel="nofollow">配置Hive结果合并</a></li></ul> 
  </li><li><a href="#45HiveJoin_759" rel="nofollow">45.Hive中Join的类型和用法</a></li><li><ul><li><a href="#_760" rel="nofollow">概览:</a></li><li><a href="#left_semi_joinleft_join_777" rel="nofollow">left semi join和left join区别</a></li><li><a href="#_782" rel="nofollow">实例</a></li></ul> 
  </li><li><a href="#46Hive_950" rel="nofollow">46.Hive严格模式</a></li><li><a href="#47sparkSQLhivesparkSQLhive_1025" rel="nofollow">47.sparkSQL一定比hive快吗，能否想出一种场景，sparkSQL比hive慢(重点)</a></li><li><ul><li><a href="#sparkSQL_1026" rel="nofollow">sparkSQL之所以快</a></li></ul> 
  </li><li><a href="#48Hive_1042" rel="nofollow">48.Hive表为什么不支持删除字段？</a></li><li><a href="#49HivePrestoSpark_1056" rel="nofollow">49.Hive、Presto、Spark引擎该用哪个？</a></li></ul> 
</div> 
<p></p> 
<h2><a id="1_hive__1"></a>1. hive 有哪些方式保存元数据，各有哪些特点？</h2> 
<ul><li>解答： 
  <ul><li>1、默认的存储位置:<code>内存数据库derby，安装小，但是数据存在内存，不稳定</code></li><li>2、<code>mysql数据库</code>，数据存储模式可以自己设置，<code>持久化好</code>，查看方便。</li></ul> </li></ul> 
<h2><a id="2_hive_5"></a>2. hive内部表和外部表的区别</h2> 
<ul><li>解答：</li><li><code>主要体现在删除时</code></li><li>内部表：加载数据到hive所在的hdfs目录 
  <ul><li>元数据中表的信息会被删除，<mark>HDFS中表的目录以及数据也会被删除</mark></li><li>适用于中间表、结果表（数据恢复起来比较快的）</li></ul> </li><li>外部表(external关键字)：不加载数据到hive所在的hdfs目录 
  <ul><li>元数据中表的信息会被删除，<mark>但是HDFS中表的目录以及数据依旧存在</mark></li><li>只是删除了HDFS与Hive的表的关联</li><li><code>适合应用于多张表共用一份数据的情况下</code>，即共享源数据的时候</li></ul> </li></ul> 
<h2><a id="3_15"></a>3.生产环境中为什么建议使用外部表？什么时候使用内部表？什么时候使用外部表？</h2> 
<ul><li>解答： 
  <ul><li>1、因为<code>外部表不会加载数据到hive</code>，<code>减少数据传输、数据还能共享</code>。</li><li>2、hive不会修改数据，所以无需担心数据的损坏</li><li>3、删除表时，只删除表结构、不删除数据。</li><li>4、ods层的埋点日志和业务抽取数据可以使用<code>外部表</code>，因为被误删除恢复会比较麻烦（实时采集）</li><li>5、抽取过来处理后的数据恢复起来很快，建议使用<code>内部表</code>（中间表&amp;结果表）</li></ul> </li></ul> 
<h2><a id="4hive__22"></a>4.你们数据库怎么导入hive 的,有没有出现问题</h2> 
<ul><li>解答： 
  <ul><li>在导入hive的时候，如果数据库中有blob或者text字段，会报错。有个参数limit</li></ul> </li></ul> 
<h2><a id="5Hive_25"></a>5.简述Hive中的虚拟列作用是什么，使用它的注意事项</h2> 
<ul><li>解答：</li><li>解释:<code>此列在表中并未真正存在</code></li><li>Hive提供了三个虚拟列： 
  <ul><li>INPUT__FILE__NAME 
    <ul><li>每一个map任务对应 输入文件的目录及文件名</li></ul> </li><li>BLOCK__OFFSET__INSIDE__FILE 
    <ul><li>当前map任务处理的数据所对应的偏移量,文件中的<code>块内偏移量</code></li></ul> </li><li>ROW__OFFSET__INSIDE__BLOCK 
    <ul><li>默认不开启,需要设置参数,<code>文件的行偏移量</code></li></ul> </li></ul> </li><li>但ROW__OFFSET__INSIDE__BLOCK默认是不可用的，需要设置hive.exec.rowoffset为true才可以。- 可以用来排查有问题的输入数据。 
  <ul><li>INPUT__FILE__NAME, <code>mapper任务的输出文件名</code>。</li><li>BLOCK__OFFSET__INSIDE__FILE, <code>当前全局文件的偏移量</code>。对于块压缩文件，就是当前块的文件偏移量，即当前块的第一个字节在文件中的偏移量。</li></ul> </li></ul> 
<pre><code class="prism language-java">hive<span class="token operator">&gt;</span> <span class="token class-name">SELECT</span> <span class="token constant">INPUT__FILE__NAME</span><span class="token punctuation">,</span> <span class="token constant">BLOCK__OFFSET__INSIDE__FILE</span><span class="token punctuation">,</span> line
<span class="token operator">&gt;</span> <span class="token constant">FROM</span> hive_text <span class="token constant">WHERE</span> line <span class="token constant">LIKE</span> <span class="token char">'%hive%'</span> <span class="token class-name">LIMIT</span> <span class="token number">2</span><span class="token punctuation">;</span>
har<span class="token operator">:</span><span class="token operator">/</span><span class="token operator">/</span>file<span class="token operator">/</span>user<span class="token operator">/</span>hive<span class="token operator">/</span>warehouse<span class="token operator">/</span>hive_text<span class="token operator">/</span>folder<span class="token operator">=</span>docs<span class="token operator">/</span>
data<span class="token punctuation">.</span>har<span class="token operator">/</span>user<span class="token operator">/</span>hive<span class="token operator">/</span>warehouse<span class="token operator">/</span>hive_text<span class="token operator">/</span>folder<span class="token operator">=</span>docs<span class="token operator">/</span><span class="token constant">README</span><span class="token punctuation">.</span>txt  <span class="token number">2243</span>
har<span class="token operator">:</span><span class="token operator">/</span><span class="token operator">/</span>file<span class="token operator">/</span>user<span class="token operator">/</span>hive<span class="token operator">/</span>warehouse<span class="token operator">/</span>hive_text<span class="token operator">/</span>folder<span class="token operator">=</span>docs<span class="token operator">/</span>
data<span class="token punctuation">.</span>har<span class="token operator">/</span>user<span class="token operator">/</span>hive<span class="token operator">/</span>warehouse<span class="token operator">/</span>hive_text<span class="token operator">/</span>folder<span class="token operator">=</span>docs<span class="token operator">/</span><span class="token constant">README</span><span class="token punctuation">.</span>txt  <span class="token number">3646</span>
</code></pre> 
<h3><a id="_47"></a>扩展</h3> 
<ul><li>文件的行偏移量平时作用不大,但这三个参数在找错误方面很有用</li><li>已知clickcube_mid表中有一个字段 regioncode , regioncode 描述了 一个ip对应的region信息，这个regioncode 目前使用的是原始值，为日志中直接获取。</li><li>某一天，由于regioncode 异常，导致spark 进程中断，查找得知是 regioncode 不合理导致，此时我们需要找到错误的regioncode, 可以进行如下的查询：</li></ul> 
<pre><code class="prism language-sql"><span class="token keyword">SELECT</span> 
    INPUT__FILE__NAME<span class="token punctuation">,</span>
    BLOCK__OFFSET__INSIDE__FILE<span class="token punctuation">,</span> 
    ROW__OFFSET__INSIDE__BLOCK<span class="token punctuation">,</span>
     substr<span class="token punctuation">(</span>regioncode<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">)</span> 
<span class="token keyword">FROM</span> clickcube_mid 
<span class="token keyword">WHERE</span> length<span class="token punctuation">(</span>regioncode<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">100</span><span class="token punctuation">;</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/3e/36/fZU51xbM_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="6hive_partition_63"></a>6.hive partition分区</h2> 
<ul><li>功能：将数据按照数据本身分区的规则来存储</li><li>本质:实现了底层程序的输入的优化 
  <ul><li>通过分区，先对分区过滤，然后再作为输入</li><li>比先读取所有数据，再过滤，性能要高的多</li></ul> </li><li>不做分区表：表的最后一级目录就是表的目录</li><li>做了分区表：表的最后一级目录是分区的目录</li><li>分区的方式： 
  <ul><li>手动分区</li><li>自动分区</li></ul> </li></ul> 
<h2><a id="7_hive_partition_73"></a>7. hive partition什么时候使用手动分区</h2> 
<ul><li>如果<code>数据文件是已经按照分区分好的每个文件，就用手动分区</code> 
  <ul><li>每天一个文件</li><li>每个部门一个文件</li></ul> </li></ul> 
<h2><a id="8hive_partition_77"></a>8.hive partition怎么手动分区</h2> 
<ul><li>step1:创建分区表</li></ul> 
<pre><code class="prism language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> tb_emp_part1<span class="token punctuation">(</span>
empno string<span class="token punctuation">,</span>
ename string<span class="token punctuation">,</span>
job string<span class="token punctuation">,</span>
managerno string<span class="token punctuation">,</span>
hiredate string<span class="token punctuation">,</span>
salary <span class="token keyword">double</span><span class="token punctuation">,</span>
jiangjin <span class="token keyword">double</span><span class="token punctuation">,</span>
deptno string
<span class="token punctuation">)</span> 
partitioned <span class="token keyword">by</span> <span class="token punctuation">(</span>department <span class="token keyword">int</span><span class="token punctuation">)</span>
<span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated</span> <span class="token keyword">by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span>
</code></pre> 
<ul><li>setp2:加载数据</li></ul> 
<pre><code class="prism language-sql"><span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/export/datas/emp10.txt'</span> <span class="token keyword">into</span> <span class="token keyword">table</span> tb_emp_part1 <span class="token keyword">partition</span><span class="token punctuation">(</span>department <span class="token operator">=</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/export/datas/emp20.txt'</span> <span class="token keyword">into</span> <span class="token keyword">table</span> tb_emp_part1 <span class="token keyword">partition</span><span class="token punctuation">(</span>department <span class="token operator">=</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/export/datas/emp30.txt'</span> <span class="token keyword">into</span> <span class="token keyword">table</span> tb_emp_part1 <span class="token keyword">partition</span><span class="token punctuation">(</span>department <span class="token operator">=</span> <span class="token number">30</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<h2><a id="9hive_partition_103"></a>9.hive partition什么时候使用自动分区</h2> 
<ul><li>如果数据是一个整体，没有按照分区规则变成多个文件</li></ul> 
<h2><a id="10hive_partition_105"></a>10.hive partition怎么自动分区</h2> 
<ul><li>step1:先创建一个张原始数据表，放所有数据：tb_emp</li><li>step2:将所有数据通过程序来实现分区，写入新的分区表</li><li>step3:开启自动分区 
  <ul><li>set hive.exec.dynamic.partition.mode=nonstrict;</li></ul> </li><li>step4:创建分区表</li></ul> 
<pre><code class="prism language-java">create table <span class="token function">tb_emp_part2</span><span class="token punctuation">(</span>
empno string<span class="token punctuation">,</span>
ename string<span class="token punctuation">,</span>
job string<span class="token punctuation">,</span>
managerno string<span class="token punctuation">,</span>
hiredate string<span class="token punctuation">,</span>
salary <span class="token keyword">double</span><span class="token punctuation">,</span>
jiangjin <span class="token keyword">double</span>
<span class="token punctuation">)</span> 
partitioned by <span class="token punctuation">(</span>dept string<span class="token punctuation">)</span>
row format delimited fields terminated by <span class="token char">'\t'</span><span class="token punctuation">;</span>
</code></pre> 
<ul><li>step5:从原始数据表加载到分区表</li></ul> 
<pre><code class="prism language-java">insert into table tb_emp_part2 <span class="token function">partition</span><span class="token punctuation">(</span>dept<span class="token punctuation">)</span>
select <span class="token operator">*</span> from tb_emp<span class="token punctuation">;</span>
</code></pre> 
<ul><li>实现：判断tb_emp中的部门编号：deptno有哪几种值，第一种值作为第一个分区，第二种值作为第二个分区，将对应的数据放入对应的分区 
  <ul><li>如何知道我是按照tb_emp中的deptno做的分区？</li><li>按照查询语句的最后一个字段做分区，写入新表</li><li>如果分区的字段不是最后一个字段怎么办？</li><li>例如：<code>我想按照job进行分区，那么select的时候将其放到最后</code></li></ul> </li></ul> 
<pre><code class="prism language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> tb_emp_part2<span class="token punctuation">(</span>
empno string<span class="token punctuation">,</span>
ename string<span class="token punctuation">,</span>
managerno string<span class="token punctuation">,</span>
hiredate string<span class="token punctuation">,</span>
salary <span class="token keyword">double</span><span class="token punctuation">,</span>
jiangjin <span class="token keyword">double</span>，
deptno string
<span class="token punctuation">)</span> 
partitioned <span class="token keyword">by</span> <span class="token punctuation">(</span>job string<span class="token punctuation">)</span>
<span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated</span> <span class="token keyword">by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span>

<span class="token keyword">insert</span> <span class="token keyword">into</span> <span class="token keyword">table</span> tb_emp_part2 <span class="token keyword">partition</span><span class="token punctuation">(</span>job<span class="token punctuation">)</span>
<span class="token keyword">select</span> 
empno<span class="token punctuation">,</span>ename<span class="token punctuation">,</span>managerno<span class="token punctuation">,</span>hiredate
<span class="token punctuation">,</span>salary<span class="token punctuation">,</span>jiangjin<span class="token punctuation">,</span>deptno
<span class="token punctuation">,</span>job  
<span class="token keyword">from</span> tb_emp<span class="token punctuation">;</span>
</code></pre> 
<h2><a id="11_158"></a>11.如何查看分区</h2> 
<pre><code class="prism language-java">show partitions 表名<span class="token punctuation">;</span>
</code></pre> 
<h2><a id="12clustered_164"></a>12.分桶结构表clustered</h2> 
<ul><li>本质:<code>就是底层MapReduce的分区，分桶的规则按照分桶字段的HASH取余</code></li><li>应用场景:</li><li>SMB Join：适合于<code>大表 join 大表</code>的应用场景，是一种优化以后 join 
  <ul><li>sort merge bulket join</li><li>如果两张表要进行SMB Join，要求两张表必须都为桶表</li></ul> </li><li>Reduce join：适合于`大表join大表，但是比较慢 
  <ul><li>A表：1亿条</li><li>B表：1亿条</li><li>reducejoin时：A表的每一条都需要与B表的每一条进行比较，然后关联 
    <ul><li><code>比较过程类似于笛卡尔积</code></li></ul> </li></ul> </li></ul> 
<h2><a id="13_175"></a>13.分桶表怎么构建</h2> 
<ul><li>注意:<code>分桶的数据是不能直接load进去的，必须通过MapReduce进行分区处理才能分桶</code></li><li>step1:开启分桶</li></ul> 
<pre><code class="prism language-java">set hive<span class="token punctuation">.</span>enforce<span class="token punctuation">.</span>bucketing<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
</code></pre> 
<ul><li>step2:创建分桶表</li></ul> 
<pre><code class="prism language-java">create table <span class="token function">tb_emp_bucket</span><span class="token punctuation">(</span>
empno string<span class="token punctuation">,</span>
ename string<span class="token punctuation">,</span>
job string<span class="token punctuation">,</span>
managerno string<span class="token punctuation">,</span>
hiredate string<span class="token punctuation">,</span>
salary <span class="token keyword">double</span><span class="token punctuation">,</span>
jiangjin <span class="token keyword">double</span><span class="token punctuation">,</span>
deptno string
<span class="token punctuation">)</span> 
clustered by <span class="token punctuation">(</span>deptno<span class="token punctuation">)</span> into <span class="token number">3</span> <span class="token constant">BUCKETS</span>
row format delimited fields terminated by <span class="token char">'\t'</span><span class="token punctuation">;</span>
</code></pre> 
<ul><li>step3:从原始数据表加载到分桶表</li></ul> 
<pre><code class="prism language-java">insert overwrite table tb_emp_bucket
select <span class="token operator">*</span> from tb_emp cluster by <span class="token punctuation">(</span>deptno<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<h2><a id="14_207"></a>14.分区表和分桶表的区别</h2> 
<ul><li>共同点：大数据的处理思想：<code>大而化小，分而治之</code></li><li>字段：分区的字段是假的，分桶的字段是真的</li><li>目录：<code>分区是目录级别，分桶是文件级别</code></li><li>目的： 
  <ul><li><code>分区</code>是为了<code>加快输入</code>的性能，会构建元数据，表的最后一级目录是分区的目录</li><li><code>分桶</code>是为了<code>SMB Join</code>而构建的，表到最后一级目录还是表的目录</li></ul> </li></ul> 
<h3><a id="_214"></a>分区表</h3> 
<ul><li>本质减少mr的工作量，比如我们要查询某一字段的全部数据，可以只对目录下的分区文件进行一个mr作业就可以了</li><li>对数据进行水平切分，每个分区即为一个物理文件夹。</li><li>按照某列或某些列分为多个分区,容易数据倾斜</li><li>物理表现为：目录到分区,对应不同文件夹</li><li>分区字段为<code>假</code></li><li>应用场景：比如国家，时间</li></ul> 
<h3><a id="_221"></a>分桶表</h3> 
<ul><li>本质减少shuffle花费的时间，表文件会被物理分桶为不同的文件，被分桶的数据被统一放到了一个或几个切片，这样可以让相同分桶的字段join，只需要把这两个分桶shuffer到一个reducer中就可以了</li><li>对数据进行垂直切分，每个分桶即为一个文件。</li><li>按照某列hash值%桶个数分割</li><li>物理表现为：目录到表,对应不同文件,细粒度</li><li>分区字段为真</li><li>应用场景：比如班级，某些有固定规定的字段</li></ul> 
<h2><a id="15insert_into__insert_overwrite_228"></a>15.insert into 和 insert overwrite区别？</h2> 
<ul><li>解答： 
  <ul><li>insert into： 
    <ul><li>将某一张表中的数据写到另一张表中</li></ul> </li><li>insert overwrite： 
    <ul><li>覆盖之前的内容。</li><li>如果有分区,只会重写当前分区数据</li></ul> </li></ul> </li></ul> 
<h2><a id="16hivesqlhdfs_235"></a>16.假如一个分区的数据主部错误怎么通过hivesql删除hdfs</h2> 
<ul><li>解答：</li></ul> 
<pre><code class="prism language-java">alter table ptable drop partition <span class="token punctuation">(</span>daytime<span class="token operator">=</span>'<span class="token number">20140911</span><span class="token char">',city='</span>bj'<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<ul><li>元数据，数据文件都删除，但目录daytime= 20140911还在</li></ul> 
<h2><a id="17Hive__243"></a>17.Hive 表关联查询，如何解决数据倾斜的问题？</h2> 
<ul><li>可以参考 <a href="https://blog.csdn.net/qq_46893497/article/details/110183438">https://blog.csdn.net/qq_46893497/article/details/110183438</a></li></ul> 
<h3><a id="1_245"></a>1）倾斜原因：</h3> 
<ul><li>map 输出数据按 key Hash 的分配到 reduce 中，由于 key 分布不均匀、业务数据本身的特、 建表时考虑不周、等原因<code>造成的 reduce 上的数据量差异过⼤</code>。 
  <ul><li>（1）key 分布不均匀;</li><li>（2）业务数据本身的特性;</li><li>（3）建表时考虑不周;</li><li>（4）某些 SQL 语句本身就有数据倾斜;</li><li>如何避免：对于 key 为空产⽣的数据倾斜，可以对其赋予⼀个随机值</li></ul> </li></ul> 
<h3><a id="2_252"></a>2）解决⽅案</h3> 
<ul><li>（1）参数调节： 
  <ul><li>hive.map.aggr = true</li><li>hive.groupby.skewindata=true</li></ul> </li><li>有数据倾斜的时候进⾏负载均衡，当选项设定位 true,⽣成的查询计划会有两个 <code>MR Job</code>。 第⼀个 MR Job 中，Map 的输出结果集合会<code>随机分布</code>到 Reduce 中，每个 Reduce 做部分聚合 操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从⽽达到负载均衡的⽬的；第⼆个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同⼀个 Reduce 中）， 最后完成最终的<code>聚合操作</code>。</li><li>（2）SQL 语句调节： 
  <ul><li>① 选⽤ join key 分布最均匀的表作为驱动表。做好列裁剪和 filter 操作，以达到两表做 join 的时候，数据量相对变⼩的效果。</li><li>② ⼤⼩表 Join： 使⽤ map join 让⼩的维度表（1000 条以下的记录条数）先进内存。在 map 端完成 reduce.</li><li>③⼤表 Join ⼤表： <code>把空值的 key 变成⼀个字符串加上随机数，把倾斜的数据分到不同的 reduce 上，由于 null 值关联不上，处理后并不影响最终结果</code>。</li><li>④ count distinct ⼤量相同特殊值: count distinct 时，将值为空的情况单独处理，如果是计算 count distinct，可以不⽤处理， 直接过滤，在最后结果中加 1。如果还有其他计算，需要进⾏ group by，可以先将值为空的 记录单独处理，再和其他计算结果进⾏ union。</li></ul> </li></ul> 
<h2><a id="18_Hive_Hive__RDBMS__262"></a>18.请谈⼀下 Hive 的特点，Hive 和 RDBMS 有什么异同？（重新整理）</h2> 
<ul><li>hive 是基于 Hadoop 的⼀个<code>数据仓库</code>⼯具，可以<code>将结构化的数据⽂件映射为⼀张数据库 表，并提供完整的 sql 查询功能，可以将 sql 语句转换为 MapReduce 任务进⾏运⾏</code>。 
  <ul><li>其优点 是学习成本低，<code>可以通过类 SQL 语句快速实现简单的 MapReduce 统计</code>，不必开发专⻔的 MapReduce 应⽤，⼗分适合数据仓库的统计分析，但是 Hive 不⽀持实时查询。</li></ul> </li><li>Hive 与关系型数据库的区别：<br> <img src="https://images2.imgbox.com/7f/38/V80U8ChA_o.png" alt="在这里插入图片描述"></li></ul> 
<h2><a id="19_hive__Sort_ByOrder_ByCluster_ByDistrbute_By__267"></a>19.请说明 hive 中 Sort By，Order By，Cluster By，Distrbute By 各代表什么意思？</h2> 
<ul><li>order by：会对输⼊做<code>全局排序</code>，因此只有⼀个 reduce（r 多个 reducer ⽆法保证全局有序）。 只有⼀个 reducer，会导致当输⼊规模较⼤时，需要较⻓的计算时间。</li><li>sort by：分区内排序，<code>不是全局排序，其在数据进⼊ reducer 前完成排序</code>，每个reduce内有序。</li><li>distribute by：<code>按照指定的字段对数据进⾏划分输出到不同的 reduce 中，一般结合sort by</code>，类似MR中Partition，按照指定的key进行分区。</li><li>cluster by：<code>除了具有 distribute by 的功能外还兼具 sort by 的功能</code>，指定的列<code>只能是降序</code>，不能指定asc和desc。</li></ul> 
<h2><a id="20_null_null__hive___select_a_from_t1_a_left_outer_join_t2_b_on_aidbid_where_bid_is_null__272"></a>20.简要描述数据库中的 null，说出 null 在 hive 底层如何存储，并 解释 select a.* from t1 a left outer join t2 b on a.id=b.id where b.id is null; 语句的含义？</h2> 
<ul><li><code>null 与任何值运算的结果都是 null</code>, 可以使⽤ is null、is not null 函数指定在其值为 null 情况下的取值。</li><li>null 在 hive 底 层 默 认 是 ⽤<code>'\N'</code>来 存 储 的 ， 可 以 通 过 alter table test SET SERDEPROPERTIES(‘serialization.null.format’ = ‘a’);来修改。</li><li><code> 查询出 ID在t1表存在在t2不存在的数据</code></li></ul> 
<h2><a id="21_hive__splitcoalesce__collect_list___276"></a>21.写出 hive 中 split、coalesce 及 collect_list 函数的⽤法（可举 例）？</h2> 
<ul><li>split 将字符串转化为数组，即：split(‘a,b,c,d’ , ‘,’) ==&gt; [“a”,“b”,“c”,“d”]。</li><li>coalesce(T v1, T v2, …) 返回参数中的第⼀个⾮空值；如果所有值都为 NULL，那么返 回 NULL。</li><li><code>collect_list 列出该字段所有的值，不去重</code> select collect_list(id) from table。</li></ul> 
<h2><a id="22Hive__280"></a>22.Hive 有哪些⽅式保存元数据，各有哪些特点？</h2> 
<ul><li>Hive ⽀持三种不同的元存储服务器，分别为：<code>内嵌式元存储服务器、本地元存储服务器、 远程元存储服务器</code>，每种存储⽅式使⽤不同的配置参数。</li><li>内嵌式元存储主要⽤于单元测试，在该模式下每次只有⼀个进程可以连接到元存储， Derby 是内嵌式元存储的默认数据库。</li><li>在本地模式下，每个 Hive 客户端都会打开到数据存储的连接并在该连接上请求 SQL 查 询。</li><li>在远程模式下，所有的 Hive 客户端都将打开⼀个到元数据服务器的连接，该服务器依 次查询元数据，元数据服务器和客户端之间使⽤ Thrift 协议通信。</li></ul> 
<h2><a id="23Hive__285"></a>23.Hive 内部表和外部表的区别？</h2> 
<ul><li>创建表时： 
  <ul><li>创建<code>内部表</code>时，<code>会将数据移动到数据仓库指向的路径</code>；</li><li>若创建<code>外部表</code>，<code>仅记 录数据所在的路径， 不对数据的位置做任何改变</code>。</li></ul> </li><li>删除表时： 
  <ul><li>在删除表的时候，<code>内部表的元数据和数据会被⼀起删除</code>，</li><li>⽽<code>外部表只删除元 数据，不删除数据</code>。</li><li>这样<code>外部表相对来说更加安全些，数据组织也更加灵活，⽅便共享源数 据</code>。</li></ul> </li></ul> 
<h2><a id="24Hive__HSQL__MapReduce__293"></a>24.Hive 的 HSQL 转换为 MapReduce 的过程？（重新整理）</h2> 
<ul><li><code>HiveSQL -&gt;AST(抽象语法树) -&gt; QB(查询块) -&gt;OperatorTree（操作树）-&gt;优化后的操作 树-&gt;mapreduce 任务树-&gt;优化后的 mapreduce 任务树</code><br> <img src="https://images2.imgbox.com/db/77/oMY8PqSJ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ac/eb/q5h7DIaV_o.png" alt="在这里插入图片描述"></li><li>过程描述如下： 
  <ul><li>SQL Parser：Antlr 定义 SQL 的语法规则，完成 SQL 词法，语法解析，将 SQL 转化为抽 象 语法树 AST Tree；</li><li>Semantic Analyzer：遍历 AST Tree，抽象出查询的基本组成单元 QueryBlock；</li><li>Logical plan：遍历 QueryBlock，翻译为执⾏操作树 OperatorTree； Logical plan optimizer: 逻 辑 层 优 化 器 进 ⾏ OperatorTree 变 换 ，合 并 不 必 要 的 ReduceSinkOperator，减少 shuffle 数据量；</li><li>Physical plan：遍历 OperatorTree，翻译为 MapReduce 任务；</li><li>Logical plan optimizer：物理层优化器进⾏ MapReduce 任务的变换，⽣成最终的执⾏计 划；</li></ul> </li></ul> 
<h2><a id="25Hive__303"></a>25.Hive 底层与数据库交互原理？</h2> 
<ul><li>由于 Hive 的元数据可能要⾯临不断地更新、修改和读取操作，所以它显然不适合使⽤ Hadoop ⽂件系统进⾏存储。⽬前 <code>Hive 将元数据存储在 RDBMS</code> 中，⽐如存储在 MySQL、 Derby 中。元数据信息包括：<code>存在的表、表的列、权限和更多的其他信息</code>。<br> <img src="https://images2.imgbox.com/05/29/WzCYLpvp_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/25/45/zoHQ4ivb_o.png" alt="在这里插入图片描述"></li></ul> 
<h2><a id="26_Hive__307"></a>26.请把下⾯语句⽤ Hive 实现</h2> 
<p><img src="https://images2.imgbox.com/06/4f/zgM5W1bl_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-sql"><span class="token keyword">select</span> a<span class="token punctuation">.</span><span class="token keyword">key</span><span class="token punctuation">,</span>a<span class="token punctuation">.</span><span class="token keyword">value</span> 
<span class="token keyword">from</span> a <span class="token keyword">where</span> a<span class="token punctuation">.</span><span class="token keyword">key</span> <span class="token operator">not</span> <span class="token keyword">exists</span> <span class="token punctuation">(</span><span class="token keyword">select</span> b<span class="token punctuation">.</span><span class="token keyword">key</span> <span class="token keyword">from</span> b<span class="token punctuation">)</span>
</code></pre> 
<h2><a id="27_texttxt__hive__test_20161010__test__l_date_314"></a>27.写出将 text.txt ⽂件放⼊ hive 中 test 表‘2016-10-10’ 分区 的语句，test 的分区字段是 l_date</h2> 
<pre><code class="prism language-sql"><span class="token keyword">LOAD</span> <span class="token keyword">DATA</span> <span class="token keyword">LOCAL</span> INPATH <span class="token string">'/your/path/test.txt'</span> 
OVERWRITE <span class="token keyword">INTO</span> <span class="token keyword">TABLE</span> test <span class="token keyword">PARTITION</span> <span class="token punctuation">(</span>l_date<span class="token operator">=</span><span class="token string">'2016-10-10'</span><span class="token punctuation">)</span>
</code></pre> 
<h2><a id="28Hive__320"></a>28.Hive 如何进⾏权限控制？</h2> 
<ul><li>⽬前 hive ⽀持简单的权限管理，默认情况下是不开启，这样所有的⽤户都具有相同的权 限，同时也是超级管理员，也就对 hive 中的所有表都有查看和改动的权利，这样是不符合 ⼀般数据仓库的安全原则的。Hive 可以是基于元数据的权限管理，也可以基于⽂件存储级 别的权限管理。</li><li>为了使⽤ Hive 的授权机制，有两个参数必须在 hive-site.xml 中设置：</li></ul> 
<pre><code class="prism language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>hive.security.authorization.enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">&gt;</span></span>enable or disable the hive client authorization<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>hive.security.authorization.createtable.owner.grants<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>ALL<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">&gt;</span></span>the privileges automatically granted to the owner whenever a table gets created.
An example like "select,drop" will grant select and drop privilege to the owner of the
table<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> 
<ul><li>Hive ⽀持以下权限：<br> <img src="https://images2.imgbox.com/79/19/2AXg9Wf6_o.png" alt="在这里插入图片描述"></li><li>Hive 授权的核⼼就是⽤户（user）、组（group）、⻆⾊（role）。</li><li>Hive 中的⻆⾊和平常我们认知的⻆⾊是有区别的，Hive 中的⻆⾊可以理解为⼀部分有 ⼀些相同“属性”的⽤户或组或⻆⾊的集合。这⾥有个递归的概念，就是⼀个⻆⾊可以是⼀些⻆⾊的集合。</li><li>下⾯举例进⾏说明：</li></ul> 
<pre><code class="prism language-java">⽤户 组
张三 <span class="token class-name">G_db1</span>
李四 <span class="token class-name">G_db2</span>
王五 <span class="token class-name">G_bothdb</span>
</code></pre> 
<ul><li>如上有三个⽤户分别属于 G_db1、G_db2、G_alldb。G_db1、G_db2、G_ bothdb 分别表示该组⽤户可以访问数据库 1、数据库 2和可以访问 1、2两个数据库。现在可以创建 role_db1和 role_db2，分别并授予访问数据库 1和数据库 2的权限。这样只要将 role_eb1赋给 G_db1（或者该组的所有⽤户），将 role_eb2赋给 G_db2，就可以是实现指定⽤户访问指定数据库。最后创建role_bothdb 指向 role_db1、role_db2（role_bothdb 不需要指定访问那个数据库），然后 role_bothdb 授予 G_bothdb，则 G_bothdb 中的⽤户可以访问两个数据库。</li><li>Hive 的⽤户和组使⽤的是 Linux 机器上的⽤户和组，⽽⻆⾊必须⾃⼰创建。</li></ul> 
<pre><code class="prism language-sql">⻆⾊管理：
<span class="token comment">--创建和删除⻆⾊</span>
<span class="token keyword">create</span> role role_name<span class="token punctuation">;</span>
<span class="token keyword">drop</span> role role_name<span class="token punctuation">;</span>
<span class="token comment">--展示所有 roles</span>
<span class="token keyword">show</span> roles
<span class="token comment">--赋予⻆⾊权限</span>
<span class="token keyword">grant</span> <span class="token keyword">select</span> <span class="token keyword">on</span> <span class="token keyword">database</span> db_name <span class="token keyword">to</span> role role_name<span class="token punctuation">;</span>
<span class="token keyword">grant</span> <span class="token keyword">select</span> <span class="token keyword">on</span> <span class="token punctuation">[</span><span class="token keyword">table</span><span class="token punctuation">]</span> t_name <span class="token keyword">to</span> role role_name<span class="token punctuation">;</span>
<span class="token comment">--查看⻆⾊权限</span>
<span class="token keyword">show</span> <span class="token keyword">grant</span> role role_name <span class="token keyword">on</span> <span class="token keyword">database</span> db_name<span class="token punctuation">;</span>
<span class="token keyword">show</span> <span class="token keyword">grant</span> role role_name <span class="token keyword">on</span> <span class="token punctuation">[</span><span class="token keyword">table</span><span class="token punctuation">]</span> t_name<span class="token punctuation">;</span>
<span class="token comment">--⻆⾊赋予⽤户</span>
<span class="token keyword">grant</span> role role_name <span class="token keyword">to</span> <span class="token keyword">user</span> user_name
<span class="token comment">--回收⻆⾊权限</span>
<span class="token keyword">revoke</span> <span class="token keyword">select</span> <span class="token keyword">on</span> <span class="token keyword">database</span> db_name <span class="token keyword">from</span> role role_name<span class="token punctuation">;</span>
<span class="token keyword">revoke</span> <span class="token keyword">select</span> <span class="token keyword">on</span> <span class="token punctuation">[</span><span class="token keyword">table</span><span class="token punctuation">]</span> t_name <span class="token keyword">from</span> role role_name<span class="token punctuation">;</span>
<span class="token comment">--查看某个⽤户所有⻆⾊</span>
<span class="token keyword">show</span> role <span class="token keyword">grant</span> <span class="token keyword">user</span> user_name<span class="token punctuation">;</span>
</code></pre> 
<h2><a id="29_hive_udf__372"></a>29.对于 hive，你写过哪些 udf 函数，作⽤是什么？</h2> 
<ul><li>⽇期处理 UDF 函数。</li></ul> 
<h3><a id="_374"></a>开发过程</h3> 
<ul><li>1-开发udf程序：<code>继承UDF类，实现一个或者多个evaluate方法</code></li><li>2-打成jar包</li><li>3-上传jar包到集群中，并添加到hive的环境变量中，在hive中执行</li></ul> 
<pre><code class="prism language-sql"> <span class="token keyword">add</span> jar <span class="token operator">/</span>export<span class="token operator">/</span>datas<span class="token operator">/</span>udf<span class="token punctuation">.</span>jar<span class="token punctuation">;</span>
</code></pre> 
<ul><li>4-创建临时函数：</li></ul> 
<pre><code class="prism language-sql"><span class="token keyword">create</span> <span class="token keyword">temporary</span> <span class="token keyword">function</span>  transDate <span class="token keyword">as</span> <span class="token string">'bigdata.hanjiaxiaozhi.cn.hive.udf.UserUDF'</span><span class="token punctuation">;</span>
</code></pre> 
<ul><li>5-测试函数：</li></ul> 
<pre><code class="prism language-sql"><span class="token keyword">select</span> transDate<span class="token punctuation">(</span><span class="token string">"18/Aug/2019:12:30:05"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<ul><li>6-删除临时函数</li></ul> 
<pre><code class="prism language-sql">   <span class="token keyword">DROP</span> <span class="token keyword">TEMPORARY</span> <span class="token keyword">FUNCTION</span> transDate<span class="token punctuation">;</span>
</code></pre> 
<h2><a id="30Hive__TextFileSequenceFileRCfile_ORCfile__393"></a>30.Hive 中的压缩格式 TextFile、SequenceFile、RCfile 、ORCfile 各有什么区别？</h2> 
<ul><li>TextFile 
  <ul><li>默认格式，存储⽅式为⾏存储，数据不做压缩，磁盘开销⼤，数据解析开销⼤。 可结合Gzip、Bzip2使⽤(系统⾃动检查，执⾏查询时⾃动解压)，但使⽤这种⽅式，压缩后的⽂件不⽀持 split，Hive 不会对数据进⾏切分，从⽽⽆法对数据进⾏并⾏操作。并且在反序列化过程中，必须逐个字符判断是不是分隔符和⾏结束符，因此反序列化开销会⽐ SequenceFile ⾼⼏⼗倍。</li></ul> </li><li>SequenceFile 
  <ul><li>SequenceFile 是 Hadoop API 提供的⼀种⼆进制⽂件⽀持，，存储⽅式为⾏存储，其具有使⽤⽅便、可分割、可压缩的特点。</li><li>SequenceFile ⽀持三种压缩选择：NONE，RECORD，BLOCK。Record 压缩率低，⼀般建议使⽤ BLOCK 压缩。</li><li>优势是⽂件和 hadoop api 中的 MapFile 是相互兼容的</li></ul> </li><li>RCFile 
  <ul><li>存储⽅式：数据按⾏分块，每块按列存储。结合了⾏存储和列存储的优点： 
    <ul><li>⾸先，RCFile 保证同⼀⾏的数据位于同⼀节点，因此元组重构的开销很低；</li><li>其次，像列存储⼀样，RCFile 能够利⽤列维度的数据压缩，并且能跳过不必要的列读取；</li></ul> </li><li>RCFile 的⼀个⾏组包括三个部分： 
    <ul><li>第⼀部分是⾏组头部的【同步标识】，主要⽤于分隔 hdfs 块中的两个连续⾏组</li><li>第⼆部分是⾏组的【元数据头部】，⽤于存储⾏组单元的信息，包括⾏组中的记录数、每个列的字节数、列中每个域的字节数</li><li>第三部分是【表格数据段】，即实际的列存储数据。在该部分中，同⼀列的所有域顺序存储。</li><li>从图可以看出，⾸先存储了列 A 的所有域，然后存储列 B 的所有域等。</li><li>数据追加：RCFile 不⽀持任意⽅式的数据写操作，仅提供⼀种追加接⼝，这是因为底层的 HDFS 当前仅仅⽀持数据追加写⽂件尾部。</li><li>⾏组⼤⼩：⾏组变⼤有助于提⾼数据压缩的效率，但是可能会损害数据的读取性能，因为这样增加了 Lazy 解压性能的消耗。⽽且⾏组变⼤会占⽤更多的内存，这会影响并发执⾏的其他 MR 作业。考虑到存储空间和查询效率两个⽅⾯，Facebook 选择 4MB 作为默认的⾏组⼤⼩，当然也允许⽤户⾃⾏选择参数进⾏配置。</li></ul> </li></ul> </li><li>ORCFile 
  <ul><li>存储⽅式：数据按⾏分块 每块按照列存储。</li><li>压缩快 快速列存取。</li><li>效率⽐ rcfile ⾼,是 rcfile 的改良版本。</li><li>以下为 RCFile、TextFile、SequenceFile 三种⽂件的存储情况：</li></ul> </li></ul> 
<pre><code class="prism language-java"><span class="token punctuation">[</span>hadoop<span class="token annotation punctuation">@master</span> <span class="token operator">~</span><span class="token punctuation">]</span>$ hadoop dfs <span class="token operator">-</span>dus <span class="token operator">/</span>user<span class="token operator">/</span><span class="token class-name">Hive</span><span class="token operator">/</span>warehouse<span class="token comment">/*
hdfs://master :9000/user/Hive/warehouse/hbase_table_1 0
hdfs://master :9000/user/Hive/warehouse/hbase_table_2 0
hdfs://master :9000/user/Hive/warehouse/orcfile_table 0
hdfs://master :9000/user/Hive/warehouse/rcfile_table 102638073
hdfs://master :9000/user/Hive/warehouse/seqfile_table 112497695
hdfs://master :9000/user/Hive/warehouse/testfile_table 536799616
hdfs://master :9000/user/Hive/warehouse/textfile_table 107308067
[hadoop@singlehadoop ~]$ hadoop dfs -ls /user/Hive/warehouse/*/</span>
<span class="token operator">-</span>rw<span class="token operator">-</span>r<span class="token operator">--</span>r<span class="token operator">--</span> <span class="token number">2</span> hadoop supergroup <span class="token number">51328177</span> <span class="token number">2014</span><span class="token operator">-</span><span class="token number">03</span><span class="token operator">-</span><span class="token number">20</span> <span class="token number">00</span><span class="token operator">:</span><span class="token number">42</span>
<span class="token operator">/</span>user<span class="token operator">/</span><span class="token class-name">Hive</span><span class="token operator">/</span>warehouse<span class="token operator">/</span>rcfile_table<span class="token operator">/</span><span class="token number">000000_0</span>
<span class="token operator">-</span>rw<span class="token operator">-</span>r<span class="token operator">--</span>r<span class="token operator">--</span> <span class="token number">2</span> hadoop supergroup <span class="token number">51309896</span> <span class="token number">2014</span><span class="token operator">-</span><span class="token number">03</span><span class="token operator">-</span><span class="token number">20</span> <span class="token number">00</span><span class="token operator">:</span><span class="token number">43</span>
<span class="token operator">/</span>user<span class="token operator">/</span><span class="token class-name">Hive</span><span class="token operator">/</span>warehouse<span class="token operator">/</span>rcfile_table<span class="token operator">/</span><span class="token number">000001_0</span>
<span class="token operator">-</span>rw<span class="token operator">-</span>r<span class="token operator">--</span>r<span class="token operator">--</span> <span class="token number">2</span> hadoop supergroup <span class="token number">56263711</span> <span class="token number">2014</span><span class="token operator">-</span><span class="token number">03</span><span class="token operator">-</span><span class="token number">20</span> <span class="token number">01</span><span class="token operator">:</span><span class="token number">20</span>
<span class="token operator">/</span>user<span class="token operator">/</span><span class="token class-name">Hive</span><span class="token operator">/</span>warehouse<span class="token operator">/</span>seqfile_table<span class="token operator">/</span><span class="token number">000000_0</span>
<span class="token operator">-</span>rw<span class="token operator">-</span>r<span class="token operator">--</span>r<span class="token operator">--</span> <span class="token number">2</span> hadoop supergroup <span class="token number">56233984</span> <span class="token number">2014</span><span class="token operator">-</span><span class="token number">03</span><span class="token operator">-</span><span class="token number">20</span> <span class="token number">01</span><span class="token operator">:</span><span class="token number">21</span>
<span class="token operator">/</span>user<span class="token operator">/</span><span class="token class-name">Hive</span><span class="token operator">/</span>warehouse<span class="token operator">/</span>seqfile_table<span class="token operator">/</span><span class="token number">000001_0</span>
<span class="token operator">-</span>rw<span class="token operator">-</span>r<span class="token operator">--</span>r<span class="token operator">--</span> <span class="token number">2</span> hadoop supergroup <span class="token number">536799616</span> <span class="token number">2014</span><span class="token operator">-</span><span class="token number">03</span><span class="token operator">-</span><span class="token number">19</span> <span class="token number">23</span><span class="token operator">:</span><span class="token number">15</span>
<span class="token operator">/</span>user<span class="token operator">/</span><span class="token class-name">Hive</span><span class="token operator">/</span>warehouse<span class="token operator">/</span>testfile_table<span class="token operator">/</span>weibo<span class="token punctuation">.</span>txt
<span class="token operator">-</span>rw<span class="token operator">-</span>r<span class="token operator">--</span>r<span class="token operator">--</span> <span class="token number">2</span> hadoop supergroup <span class="token number">53659758</span> <span class="token number">2014</span><span class="token operator">-</span><span class="token number">03</span><span class="token operator">-</span><span class="token number">19</span> <span class="token number">23</span><span class="token operator">:</span><span class="token number">24</span>
<span class="token operator">/</span>user<span class="token operator">/</span><span class="token class-name">Hive</span><span class="token operator">/</span>warehouse<span class="token operator">/</span>textfile_table<span class="token operator">/</span><span class="token number">000000_0.</span>gz
<span class="token operator">-</span>rw<span class="token operator">-</span>r<span class="token operator">--</span>r<span class="token operator">--</span> <span class="token number">2</span> hadoop supergroup <span class="token number">53648309</span> <span class="token number">2014</span><span class="token operator">-</span><span class="token number">03</span><span class="token operator">-</span><span class="token number">19</span> <span class="token number">23</span><span class="token operator">:</span><span class="token number">26</span>
<span class="token operator">/</span>user<span class="token operator">/</span><span class="token class-name">Hive</span><span class="token operator">/</span>warehouse<span class="token operator">/</span>textfile_table<span class="token operator">/</span><span class="token number">000001_1.</span>gz
</code></pre> 
<ul><li>总结:(重要) 
  <ul><li>相⽐ TEXTFILE 和 SEQUENCEFILE，RCFILE 由于列式存储⽅式，数据加载时性能消耗较⼤，但是具有较好的压缩⽐和查询响应。</li><li>数据仓库的特点是⼀次写⼊、多次读取，因此，整体来看，RCFILE 相⽐其余两种格式具有较明显的优势。</li></ul> </li></ul> 
<h2><a id="31Hive_join__445"></a>31.Hive join 过程中⼤表⼩表的放置顺序？</h2> 
<ul><li>解答: 
  <ul><li>个人感觉这个解释不太满意，不过小表放前，养成习惯也好</li><li>1.将最⼤的表放置在 JOIN 语句的最右边，或者直接使⽤/*+ streamtable(table_name) */指出。</li><li>2.在编写带有 join 操作的代码语句时，应该<code>将条⽬少的表/⼦查询放在 Join 操作符的左边</code>。因为在 Reduce 阶段，位于 Join 操作符左边的表的内容会被加载进内存，载⼊条⽬较少的表可以有效减少 OOM（out of memory）即内存溢出。所以对于同⼀个 key 来说，对应的 value 值⼩的放前，⼤的放后，这便是“⼩表放前”原则。若⼀条语句中有多个 Join，依据Join 的条件相同与否，有不同的处理⽅法。</li></ul> </li></ul> 
<h2><a id="32Hive__MapReduce__450"></a>32.Hive 的两张表关联，使⽤ MapReduce 怎么实现？</h2> 
<ul><li>解答: 
  <ul><li>如果其中有⼀张表为⼩表，直接使⽤ map 端 join 的⽅式（map 端加载⼩表）进⾏聚合。</li><li>如果两张都是⼤表，那么采⽤联合 key，联合 key 的第⼀个组成部分是 join on 中的公共字段，第⼆部分是⼀个 flag，0 代表表 A，1 代表表 B，由此让 Reduce 区分客户信息和订单信息；在 Mapper 中同时处理两张表的信息，将 join on 公共字段相同的数据划分到同⼀个分区中，进⽽传递到⼀个 Reduce 中，然后在 Reduce 中实现聚合。</li></ul> </li></ul> 
<h2><a id="33Hive__in__454"></a>33.Hive 中使⽤什么代替 in 查询？</h2> 
<ul><li>解答: 
  <ul><li>在 Hive 0.13 版本之前，通过 left outer join 实现 SQL 中的 in 查询</li><li>0.13 版本之后，Hive已经⽀持 in 查询。</li></ul> </li></ul> 
<h2><a id="34_Hive__MapReduce__458"></a>34.所有的 Hive 任务都会有 MapReduce 的执⾏吗？</h2> 
<ul><li>解答:</li><li>不是，从 Hive0.10.0 版本开始</li><li>对于简单的不需要聚合的类似 <code>SELECT &lt;col&gt; from &lt;table&gt;LIMIT n</code>语句，不需要起 MapReduce job，直接通过 <code>Fetch task </code>获取数据。</li></ul> 
<h2><a id="35Hive_UDFUDAFUDTF__462"></a>35.Hive 的函数：UDF、UDAF、UDTF 的区别？</h2> 
<ul><li>解答:</li><li><code>UDF</code>: 单⾏进⼊，单⾏输出 →<code>一对一</code></li><li><code>UDAF</code>: 多⾏进⼊，单⾏输出 →<code>多对一</code></li><li><code>UDTF</code>: 单⾏输⼊，多⾏输出 →<code>一对多</code></li></ul> 
<h2><a id="36_Hive__467"></a>36.说说对 Hive 桶表的理解？</h2> 
<ul><li>解答: 
  <ul><li>桶表是<code>对数据进⾏哈希取值，然后放到不同⽂件中存储</code>。</li><li>数据加载到桶表时，会<code>对字段取 hash 值，然后与桶的数量取模</code> 。把数据放到对应的⽂件中。物理上，每个桶就是表(或分区⽬录⾥的⼀个⽂件，⼀个作业产⽣的桶(输出⽂件)和reduce 任务个数相同。</li><li>桶表专⻔⽤于抽样查询，是很专业性的，不是⽇常⽤来存储数据的表，需要抽样查询时，才创建和使⽤桶表。</li></ul> </li></ul> 
<h2><a id="37Hive__UDF__472"></a>37.Hive ⾃定义 UDF 函数的流程?</h2> 
<ul><li>解答: 
  <ul><li>1）<code>写⼀个类继承（org.apache.hadoop.hive.ql.）UDF 类</code>；</li><li>2）<code>覆盖⽅法 evaluate()</code>；</li><li>3）<code>打 JAR 包</code>；</li><li>4）通过 hive 命令将 JAR <code>添加</code>到 Hive 的类路径： 
    <ul><li>hive&gt; add jar /home/ubuntu/ToDate.jar;</li></ul> </li><li>5）<code>注册函数</code>： 
    <ul><li>hive&gt; create temporary function xxx as ‘XXX’;</li></ul> </li><li>6）<code>使⽤函数</code>；</li><li>7）[可选] drop 临时函数；</li></ul> </li></ul> 
<h2><a id="38Hive__483"></a>38.Hive 可以像关系型数据库那样建⽴多个库吗？</h2> 
<ul><li>可以建⽴多个库</li></ul> 
<h2><a id="39Hive__485"></a>39.Hive 实现统计的查询语句是什么？</h2> 
<ul><li>count 等语句。</li></ul> 
<h2><a id="40Hive__487"></a>40.Hive 优化措施</h2> 
<ul><li>可以参考 <a href="https://blog.csdn.net/qq_46893497/article/details/114047447">https://blog.csdn.net/qq_46893497/article/details/114047447</a></li></ul> 
<h3><a id="1Fetch__489"></a>1.Fetch 抓取</h3> 
<ul><li>Hive 中对某些情况的查询可以不必使⽤ MapReduce 计算。 
  <ul><li>hive-default.xml.template ⽂件中 hive.fetch.task.conversion 默认是 more,老版本hive是minimal（修改为more）</li><li>more时全局查找、字段查找、limit 查找等都不⾛ mapreduce。</li></ul> </li></ul> 
<h3><a id="2_493"></a>2.本地模式</h3> 
<ul><li>单台机器上处理所有的任务,适合于小数据集,执行时间会大大缩短</li><li>Hive 的输⼊数据量⾮常⼩的情况下</li><li>set hive.exec.mode.local.auto=true; （开启本地模式）</li><li>set hive.exec.mode.local.auto.inputbytes.max=134217728;（小于128mb采用本地模式，默认134217728）</li><li>set hive.exec.mode.local.auto.input.files.max=10;（输入文件个数小于10个采用本地模式，默认为 4）</li></ul> 
<pre><code class="prism language-sql">场景:
我想要验证一个函数的执行结果
我先建一张表xxx<span class="token punctuation">,</span>
往表里插入一行数据
<span class="token keyword">select</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">+</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token keyword">from</span> xxx
结果需要好久???
name我可以开启本地模式<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span><span class="token keyword">mode</span><span class="token punctuation">.</span><span class="token keyword">local</span><span class="token punctuation">.</span>auto<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span><span class="token punctuation">(</span>默认为<span class="token boolean">false</span><span class="token punctuation">)</span>
但是你会发现job确实是以本地模式运行了（看job名字就能看出来<span class="token punctuation">,</span>中间有<span class="token keyword">local</span>字样）<span class="token punctuation">,</span>但是还是会报错，各种找不到jar包。
这里还要运行一个语句：<span class="token keyword">set</span> fs<span class="token punctuation">.</span>defaultFS<span class="token operator">=</span><span class="token keyword">file</span>:<span class="token comment">///</span>
在执行<span class="token keyword">sql</span>发现速度大大提高了<span class="token operator">!</span>
当一个job满足下面条件的时候才能真正使用本地模式
输入数据小于参数：hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span><span class="token keyword">mode</span><span class="token punctuation">.</span><span class="token keyword">local</span><span class="token punctuation">.</span>auto<span class="token punctuation">.</span>inputbytes<span class="token punctuation">.</span><span class="token function">max</span><span class="token punctuation">(</span>默认<span class="token number">128</span>MB<span class="token punctuation">)</span>
map数小于参数：hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span><span class="token keyword">mode</span><span class="token punctuation">.</span><span class="token keyword">local</span><span class="token punctuation">.</span>auto<span class="token punctuation">.</span>tasks<span class="token punctuation">.</span><span class="token function">max</span><span class="token punctuation">(</span>默认<span class="token number">4</span><span class="token punctuation">)</span>
reduce为<span class="token number">0</span>或<span class="token number">1</span>
</code></pre> 
<h3><a id="3_517"></a>3.表的优化</h3> 
<ul><li>表的优化1.⼩表、⼤表 Join（新版的 hive 已经对⼩表 JOIN ⼤表和⼤表 JOIN ⼩表进⾏了优化。⼩表 放在左边和右边已经没有明显区别。） 
  <ul><li>将 key 相对分散，并且数据量⼩的表放在 join 的左边</li><li>优点： 
    <ul><li>1.有效减少内存溢出错误发⽣的⼏率</li><li>2.可以使⽤ Group 让⼩的维度表（1000条以下的记录条数）先进内存</li><li>3.在 map 端完成 reduce。</li></ul> </li></ul> </li><li>表的优化2.⼤表 Join ⼤表 
  <ul><li><code>空 KEY 过滤</code>（is not null） 
    <ul><li>有时 join 超时是因为某些 key 对应的数据太多，⽽相同 key 对应的数据都会发送到相同的 reducer 上，从⽽导致内存不够。此时我们应该仔细分析这些异常的 key，很多情况下，这些 key 对应的数据是异常数据，我们需要在 SQL 语句中进⾏过滤。例如 key 对应的字段为空</li></ul> </li><li><code>空 key 转换</code>（concat（‘含空列’,rand()）） 
    <ul><li>有时虽然某个 key 为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join 的结果中，此时我们可以表 a 中 key 为空的字段赋⼀个随机的值，使得数据随机均匀地分不到不同的 reducer 上。</li></ul> </li></ul> </li><li>表的优化3.<code>MapJoin</code></li><li>表的优化4.<code>Group By</code></li><li>表的优化5.<code>Count(Distinct) 去重统计</code></li><li>表的优化6.<code>避免笛卡尔积</code> 
  <ul><li>尽量避免笛卡尔积（join 的时候不加 on 条件，或者⽆效的 on 条件）</li></ul> </li><li>表的优化7.<code>⾏列过滤</code> 
  <ul><li>列处理：在 SELECT 中，<code>只拿需要的列</code>，如果有，尽量使⽤<code>分区过滤</code>，少⽤ SELECT *。</li><li>⾏处理：在分区剪裁中，当使⽤外关联时，如果将副表的过滤条件写在 Where 后⾯，那么就会先全表关联，之后再过滤</li></ul> </li><li>表的优化8.<code>动态分区</code>调整 
  <ul><li>关系型数据库中，对分区表 Insert 数据时候，数据库⾃动会<code>根据分区字段的值，将数据插⼊到相应的分区中</code>，Hive 中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使⽤ Hive 的动态分区，需要进⾏相应的配置。</li></ul> </li><li>表的优化9.<code>分桶</code></li><li>表的优化10.<code>分区</code></li></ul> 
<h3><a id="4_541"></a>优化措施4.数据倾斜</h3> 
<ul><li>Map 数</li><li>⼩⽂件进⾏合并</li><li>复杂⽂件增加 Map 数</li><li>Reduce 数</li></ul> 
<h3><a id="5_546"></a>优化措施5.<code>并⾏执⾏</code></h3> 
<h3><a id="6_547"></a>优化措施6.<code>严格模式</code></h3> 
<h3><a id="7JVM__548"></a>优化措施7.<code>JVM 重⽤</code></h3> 
<h3><a id="8_549"></a>化措施8.<code>推测执⾏</code></h3> 
<h3><a id="9_550"></a>优化措施9.<code>压缩</code></h3> 
<h3><a id="10EXPLAIN_551"></a>优化措施10.EXPLAIN（执⾏计划）</h3> 
<h2><a id="41Hive__552"></a>41.Hive 数据分析⾯试题</h2> 
<ul><li>场景举例.北京市学⽣成绩分析.</li><li>成绩的数据格式:时间,学校,年纪,姓名,科⽬,成绩</li><li>样例数据如下:</li></ul> 
<pre><code class="prism language-sql"><span class="token number">2013</span><span class="token punctuation">,</span>北⼤<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span>裘容絮<span class="token punctuation">,</span>语⽂<span class="token punctuation">,</span><span class="token number">97</span>
<span class="token number">2013</span><span class="token punctuation">,</span>北⼤<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span>庆眠拔<span class="token punctuation">,</span>语⽂<span class="token punctuation">,</span><span class="token number">52</span>
<span class="token number">2013</span><span class="token punctuation">,</span>北⼤<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span>乌洒筹<span class="token punctuation">,</span>语⽂<span class="token punctuation">,</span><span class="token number">85</span>
<span class="token number">2012</span><span class="token punctuation">,</span>清华<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span>钦尧<span class="token punctuation">,</span>英语<span class="token punctuation">,</span><span class="token number">61</span>
<span class="token number">2015</span><span class="token punctuation">,</span>北理⼯<span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>冼殿<span class="token punctuation">,</span>物理<span class="token punctuation">,</span><span class="token number">81</span>
<span class="token number">2016</span><span class="token punctuation">,</span>北科<span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span>况飘索<span class="token punctuation">,</span>化学<span class="token punctuation">,</span><span class="token number">92</span>
<span class="token number">2014</span><span class="token punctuation">,</span>北航<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span>孔须<span class="token punctuation">,</span>数学<span class="token punctuation">,</span><span class="token number">70</span>
<span class="token number">2012</span><span class="token punctuation">,</span>清华<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span>王脊<span class="token punctuation">,</span>英语<span class="token punctuation">,</span><span class="token number">59</span>
<span class="token number">2014</span><span class="token punctuation">,</span>北航<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span>⽅部盾<span class="token punctuation">,</span>数学<span class="token punctuation">,</span><span class="token number">49</span>
<span class="token number">2014</span><span class="token punctuation">,</span>北航<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span>东⻔雹<span class="token punctuation">,</span>数学<span class="token punctuation">,</span><span class="token number">77</span>
</code></pre> 
<h3><a id="1__TOPN_568"></a>1 情景题：分组 TOPN</h3> 
<pre><code class="prism language-sql"><span class="token comment">-- 1.分组 TOPN 选出 今年每个学校,每个年级,分数前三的科⽬.</span>
hive <span class="token operator">-</span>e<span class="token string">"set mapreduce.job.queuename=low;
select t.*
from(select
		school,
		class,
		subjects,
		score,
		row_number() over (partition by school,class,subjects order by score desc) rank_code
	from spark_test_wx
	where partition_id = "</span><span class="token number">2017</span><span class="token string">"
) t
where t.rank_code &lt;= 3;"</span>
<span class="token comment">-- 今年,北航,每个班级,每科的分数,及分数上下浮动 2分的总和</span>

<span class="token keyword">select</span> school<span class="token punctuation">,</span>class<span class="token punctuation">,</span>subjects<span class="token punctuation">,</span>score<span class="token punctuation">,</span>
<span class="token function">sum</span><span class="token punctuation">(</span>score<span class="token punctuation">)</span> <span class="token keyword">over</span><span class="token punctuation">(</span><span class="token keyword">order</span> <span class="token keyword">by</span> score range <span class="token operator">between</span> <span class="token number">2</span> <span class="token keyword">preceding</span> <span class="token operator">and</span> <span class="token number">2</span> <span class="token keyword">following</span><span class="token punctuation">)</span> sscore
<span class="token keyword">from</span> spark_test_wx
<span class="token keyword">where</span> partition_id <span class="token operator">=</span> <span class="token string">"2017"</span> <span class="token operator">and</span> school<span class="token operator">=</span><span class="token string">"北航"</span>
提问<span class="token punctuation">,</span>上述 <span class="token keyword">sql</span> 有没有可优化的点<span class="token punctuation">.</span>
<span class="token comment">-- row_number() over (distribute by school,class,subjects sort by score desc) rank_code </span>
    
</code></pre> 
<h3><a id="2_where__having_593"></a>2 情景题：where 与 having</h3> 
<pre><code class="prism language-sql"><span class="token comment">-- 今年 清华 1 年级 总成绩⼤于 200 分的学⽣ 以及学⽣数</span>
hive <span class="token operator">-</span>e <span class="token string">"
set mapreduce.job.queuename=low;
select school,class,name,sum(score) as total_score,
count(1) over (partition by school,class) nct
from spark_test_wx
where partition_id = "</span><span class="token number">2017</span><span class="token string">" and school="</span>清华<span class="token string">" and class = 1
group by school,class,name
having total_score&gt;200;
"</span>
<span class="token keyword">having</span> 是分组（<span class="token keyword">group</span> <span class="token keyword">by</span>）后的筛选条件，分组后的数据组内再筛选，也就是说 <span class="token keyword">HAVING</span>⼦句可以让我们筛选成组后的各组数据。
<span class="token keyword">where</span>则是在分组<span class="token punctuation">,</span>聚合前先筛选记录。也就是说作⽤在<span class="token keyword">GROUP</span> <span class="token keyword">BY</span>⼦句和<span class="token keyword">HAVING</span>⼦句前。
</code></pre> 
<h3><a id="3__609"></a>3 情景题：数据倾斜</h3> 
<pre><code class="prism language-sql">今年加⼊进来了 <span class="token number">10</span> 个学校<span class="token punctuation">,</span>学校数据差异很⼤计算每个学校的平均分。
该题主要是考察数据倾斜的处理⽅式。
<span class="token keyword">Group</span> <span class="token keyword">by</span> ⽅式很容易产⽣数据倾斜。需要注意⼀下⼏点
<span class="token number">1</span>）Map 端部分聚合
	hive<span class="token punctuation">.</span>map<span class="token punctuation">.</span>aggr<span class="token operator">=</span><span class="token boolean">true</span>（⽤于设定是否在 map 端进⾏聚合，默认值为真，相当于combine）
	hive<span class="token punctuation">.</span>groupby<span class="token punctuation">.</span>mapaggr<span class="token punctuation">.</span>checkinterval<span class="token operator">=</span><span class="token number">100000</span>（⽤于设定 map 端进⾏聚合操作的条数）
<span class="token number">2</span>）有数据倾斜时进⾏负载均衡
	设定 hive<span class="token punctuation">.</span>groupby<span class="token punctuation">.</span>skewindata，当选项设定为 <span class="token boolean">true</span> 是，⽣成的查询计划有两个MapReduce 任务。
	在第⼀个MapReduce 中，map 的输出结果集合会随机分布到reduce 中，每个reduce做部分聚合操作，并输出结果。这样处理的结果是，相同的 <span class="token keyword">Group</span> <span class="token keyword">By</span> <span class="token keyword">Key</span> 有可能分发到不同的 reduce 中，从⽽达到负载均衡的⽬的；
	第⼆个 MapReduce 任务再根据预处理的数据结果按照 <span class="token keyword">Group</span> <span class="token keyword">By</span> <span class="token keyword">Key</span> 分布到 reduce中（这个过程可以保证相同的 <span class="token keyword">Group</span> <span class="token keyword">By</span> <span class="token keyword">Key</span> 分布到同⼀个 reduce 中），最后完成最终的聚合操作。
    
</code></pre> 
<h3><a id="4__624"></a>4 情景题：分区表</h3> 
<pre><code class="prism language-sql">假设我创建了⼀张表，其中包含了 <span class="token number">2016</span> 年客户完成的所有交易的详细信息：<span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> transaction_details <span class="token punctuation">(</span>cust_id <span class="token keyword">INT</span><span class="token punctuation">,</span> amount <span class="token keyword">FLOAT</span><span class="token punctuation">,</span> <span class="token keyword">month</span> STRING<span class="token punctuation">,</span> country STRING<span class="token punctuation">)</span>
<span class="token keyword">ROW</span> FORMAT DELIMITED <span class="token keyword">FIELDS</span> <span class="token keyword">TERMINATED</span> <span class="token keyword">BY</span> ‘<span class="token punctuation">,</span>’<span class="token punctuation">;</span>
	现在我插⼊了 <span class="token number">100</span> 万条数据，我想知道每个⽉的总收⼊。
	问：如何⾼效的统计出结果。写出步骤即可。
解答:
<span class="token number">1</span>）⾸先分析这个需求，其实并不难，但是由于题⽬说了，要⾼效。⽽且数据量也不⼩<span class="token punctuation">,</span>直接写 <span class="token keyword">sql</span> 查询估计肯定会挂。
<span class="token number">2</span>）分析：
（<span class="token number">1</span>）我们可以通过根据每个⽉对表进⾏分区来解决查询慢的问题。 因此，对于每个⽉我们将只扫描分区的数据，⽽不是整个数据集。
（<span class="token number">2</span>）但是我们不能直接对现有的⾮分区表进⾏分区。 所以我们会采取以下步骤来解决这个问题：
（<span class="token number">3</span>）创建⼀个分区表，partitioned_transaction：
	<span class="token keyword">create</span> <span class="token keyword">table</span> partitioned_transaction <span class="token punctuation">(</span>cust_id <span class="token keyword">int</span><span class="token punctuation">,</span> amount <span class="token keyword">float</span><span class="token punctuation">,</span> country string<span class="token punctuation">)</span> partitioned <span class="token keyword">by</span><span class="token punctuation">(</span><span class="token keyword">month</span> string<span class="token punctuation">)</span> <span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated</span> <span class="token keyword">by</span> ‘<span class="token punctuation">,</span>’<span class="token punctuation">;</span>
（<span class="token number">4</span>）在 Hive 中启⽤动态分区：
	<span class="token keyword">SET</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span><span class="token keyword">partition</span> <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>
	<span class="token keyword">SET</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span><span class="token keyword">partition</span><span class="token punctuation">.</span><span class="token keyword">mode</span> <span class="token operator">=</span> nonstrict<span class="token punctuation">;</span>
（<span class="token number">5</span>）将数据从⾮分区表导⼊到新创建的分区表中：
	<span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> partitioned_transaction <span class="token keyword">partition</span> <span class="token punctuation">(</span><span class="token keyword">month</span><span class="token punctuation">)</span> <span class="token keyword">select</span> cust_id<span class="token punctuation">,</span> amount<span class="token punctuation">,</span>country<span class="token punctuation">,</span> <span class="token keyword">month</span> <span class="token keyword">from</span> transaction_details<span class="token punctuation">;</span>
（<span class="token number">6</span>）使⽤新建的分区表实现需求。
</code></pre> 
<h2><a id="42hive_645"></a>42.hive导入导出</h2> 
<h3><a id="_646"></a>导入</h3> 
<ul><li><code>本地文件</code>导入到Hive表–<code>load data local inpath '路径' into/overwrite table 表名</code></li><li><code>HDFS文件</code>导入到Hive表–<code>load data inpath 'hdfs路径' into/overwrite table 表名</code></li><li><code>Hive表</code>导入到Hive表(select 或者insert ) 
  <ul><li>创建表的过程中从其他表导入–<code>create table ... as select xxx from 其他表</code></li><li>从其他表导入–<code>insert into/overwrite table 表名 select xxx from 其他表</code></li></ul> </li><li>指定数据库目录在HDFS上的地址–<code>location 'hdfs路径'</code></li></ul> 
<h3><a id="_653"></a>导出</h3> 
<ul><li>导出到本地文件系统； 
  <ul><li>insert overwrite local directory ‘/home/wyp/wyp’ select * from wyp；</li><li>和导入数据到Hive不一样，不能用insert into来将数据导出</li><li>还可以用hive的-e和-f参数来导出数据。其中-e 表示后面直接接带双引号的sql语句；而-f是接一个文件，文件的内容为一个sql语句 
    <ul><li>hive -e “select * from wyp” &gt;&gt; local/wyp.txt</li></ul> </li></ul> </li><li>导出到HDFS中； 
  <ul><li>insert overwrite directory ‘/home/wyp/wyp’ select * from wyp；</li></ul> </li><li>导出到Hive的另一个表中。 
  <ul><li>insert into/overwrite table 表名 select xxx from 其他表</li></ul> </li></ul> 
<h2><a id="43Hive_663"></a>43.Hive的存储格式及压缩算法</h2> 
<h3><a id="_664"></a>存储格式</h3> 
<ul><li><mark>TextFile</mark> 
  <ul><li><code>行式存储</code>:查询一整行的时候,查询快</li><li>默认格式</li><li>数据不做压缩,磁盘开销大,数据解析开销大</li><li>可使用Gzip、Bzip2，但压缩后的文件不支持split</li><li>在反序列化的时候，必须逐个字段判断是不是分隔符和行结束符，因此反序列化开销高</li></ul> </li><li>SequenceFile 
  <ul><li>行式存储</li></ul> </li><li><mark>ORC</mark> 
  <ul><li><code>列式存储</code>:查询只有<code>少数几个字段</code>的时候,<code>大大减少读取的数据量</code></li><li>每个ORC文件由3部分组成：Index Data（1w行做一个索引）、Row Data（具体数据）、Stripe Footer（类型，长度）</li></ul> </li><li><mark>Parquent</mark> 
  <ul><li><code>列式存储</code></li><li><code>二进制存储</code></li><li>会按照Block大小设置行组的大小</li></ul> </li></ul> 
<h3><a id="_680"></a>压缩算法</h3> 
<ul><li>Bzip2 
  <ul><li>可切分</li><li>压缩率高,高于Gzip,压缩速度很慢</li><li>适合:对速度要求不高,但需要较大压缩率</li></ul> </li><li>Gzip 
  <ul><li>不可切分</li></ul> </li><li>LZO 
  <ul><li>不可切分</li><li>压缩率低于Gzip</li><li>需要建索引,并指定输入格式</li></ul> </li><li>LZ4 
  <ul><li>不可切分</li></ul> </li><li><mark>Snappy</mark> 
  <ul><li><code>不可切分</code></li><li><code>高效的压缩速度和合理的压缩率</code></li><li>压缩率低于Gzip<br> <img src="https://images2.imgbox.com/a7/a1/1sI6tHT0_o.png" alt="在这里插入图片描述"></li></ul> </li></ul> 
<h2><a id="44Hive_698"></a>44.Hive中小文件问题</h2> 
<h3><a id="_699"></a>产生:</h3> 
<ul><li><code>动态分区插入</code></li><li><code>reduce数量过多</code></li><li><code>数据源本身包含大量小文件</code></li></ul> 
<h3><a id="_703"></a>影响</h3> 
<ul><li><code>会开很多map,一个map开一个jvm执行,这些任务的初始化和启动和回收会占用大量资源 会占用大量元数据内存(每个小文件对象150byte左右)</code></li></ul> 
<h3><a id="_706"></a>解决</h3> 
<ul><li>不使用textfile</li><li>减少reduce数量</li><li>少用动态分区,用的话加上distribute by分区</li><li>对于已存在的数据 
  <ul><li>hadoop archive命令把小文件进行归档</li><li>重建表,建表时减少reduce数量</li><li>参数进行调节,设置map/reduce相关参数 
    <ul><li>map端/reduce端输出进行合并</li><li>设置合并文件大小</li><li>当输出文件的平均大小小于某值时,启动一个独立的MR任务进行文件merge</li></ul> </li></ul> </li></ul> 
<h3><a id="mapreduce_717"></a>map/reduce端的相关参数的设置</h3> 
<ul><li>设置map输入合并小文件的相关参数：</li></ul> 
<pre><code class="prism language-sql"><span class="token comment">//每个Map最大输入大小(这个值决定了合并后文件的数量)</span>

<span class="token keyword">set</span> mapred<span class="token punctuation">.</span>max<span class="token punctuation">.</span>split<span class="token punctuation">.</span>size<span class="token operator">=</span><span class="token number">256000000</span><span class="token punctuation">;</span>

<span class="token comment">//一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并)</span>

<span class="token keyword">set</span> mapred<span class="token punctuation">.</span>min<span class="token punctuation">.</span>split<span class="token punctuation">.</span>size<span class="token punctuation">.</span>per<span class="token punctuation">.</span>node<span class="token operator">=</span><span class="token number">100000000</span><span class="token punctuation">;</span>

<span class="token comment">//一个交换机下split的至少的大小(这个值决定了多个交换机上的文件是否需要合并)</span>

<span class="token keyword">set</span> mapred<span class="token punctuation">.</span>min<span class="token punctuation">.</span>split<span class="token punctuation">.</span>size<span class="token punctuation">.</span>per<span class="token punctuation">.</span>rack<span class="token operator">=</span><span class="token number">100000000</span><span class="token punctuation">;</span>

<span class="token comment">//执行Map前进行小文件合并</span>

<span class="token keyword">set</span> hive<span class="token punctuation">.</span>input<span class="token punctuation">.</span>format<span class="token operator">=</span>org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>io<span class="token punctuation">.</span>CombineHiveInputFormat<span class="token punctuation">;</span>
</code></pre> 
<h3><a id="Hive_738"></a>配置Hive结果合并</h3> 
<ul><li>我们可以通过一些配置项来使Hive在执行结束后对结果文件进行合并：</li></ul> 
<pre><code class="prism language-sql"><span class="token comment">//设置map端输出进行合并，默认为true</span>

<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">merge</span><span class="token punctuation">.</span>mapfiles <span class="token operator">=</span> <span class="token boolean">true</span>

<span class="token comment">//设置reduce端输出进行合并，默认为false</span>

<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">merge</span><span class="token punctuation">.</span>mapredfiles <span class="token operator">=</span> <span class="token boolean">true</span>

<span class="token comment">//设置合并文件的大小</span>

<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">merge</span><span class="token punctuation">.</span>size<span class="token punctuation">.</span>per<span class="token punctuation">.</span>task <span class="token operator">=</span> <span class="token number">256</span><span class="token operator">*</span><span class="token number">1000</span><span class="token operator">*</span><span class="token number">1000</span>

<span class="token comment">//当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件merge。</span>

<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">merge</span><span class="token punctuation">.</span>smallfiles<span class="token punctuation">.</span>avgsize<span class="token operator">=</span><span class="token number">16000000</span>
</code></pre> 
<ul><li>注意：对于输出结果为压缩文件形式存储的情况，要解决小文件问题，如果在Map输入前合并，对输出的文件存储格式并没有限制。但是如果使用输出合并，则必须配合SequenceFile来存储，否则无法进行合并。</li></ul> 
<h2><a id="45HiveJoin_759"></a>45.Hive中Join的类型和用法</h2> 
<h3><a id="_760"></a>概览:</h3> 
<ul><li><code>内关联（JOIN）</code> 
  <ul><li>只返回能关联上的结果。</li></ul> </li><li><code>左外关联（LEFT [OUTER] JOIN）</code> 
  <ul><li>以LEFT [OUTER] JOIN关键字前面的表作为主表，和其他表进行关联，返回记录和主表的记录数一致，关联不上的字段置为NULL。</li><li>是否指定OUTER关键字，貌似对查询结果无影响。</li></ul> </li><li><code>右外关联（RIGHT [OUTER] JOIN）</code> 
  <ul><li>和左外关联相反，以RIGTH [OUTER] JOIN关键词后面的表作为主表，和前面的表做关联，返回记录数和主表一致，关联不上的字段为NULL。</li><li>是否指定OUTER关键字，貌似对查询结果无影响。</li></ul> </li><li><code>全外关联（FULL [OUTER] JOIN）</code> 
  <ul><li>以两个表的记录为基准，返回两个表的记录去重之和，关联不上的字段为NULL。</li><li>是否指定OUTER关键字，貌似对查询结果无影响。</li><li>注意：FULL JOIN时候，Hive不会使用MapJoin来优化。</li></ul> </li><li><code>LEFT SEMI JOIN</code> 
  <ul><li>以LEFT SEMI JOIN关键字前面的表为主表，返回主表的KEY 也在副表中的记录。</li></ul> </li><li><code>笛卡尔积关联（CROSS JOIN）</code> 
  <ul><li>返回两个表的笛卡尔积结果，不需要指定关联键。</li></ul> </li></ul> 
<h3><a id="left_semi_joinleft_join_777"></a>left semi join和left join区别</h3> 
<ul><li>LEFT SEMI JOIN 是 IN/EXISTS 子查询的一种更高效的实现。</li><li>LEFT SEMI JOIN 的限制是， JOIN 子句中右边的表只能在 ON 子句中设置过滤条件，在 WHERE 子句、SELECT 子句或其他地方都不行。</li><li>因为 left semi join 是 in(keySet) 的关系，遇到右表重复记录，左表会跳过，而 join 则会一直遍历。这就导致右表有重复值得情况下 left semi join 只产生一条，join 会产生多条，也会导致 left semi join 的性能更高。</li><li>left semi join 是只传递表的 join key 给 map 阶段，因此left semi join 中最后 select 的结果只许出现左表。因为右表只有 join key 参与关联计算了，而left join on 默认是整个关系模型都参与计算了</li></ul> 
<h3><a id="_782"></a>实例</h3> 
<ul><li>Hive中除了支持和传统数据库中一样的内关联、左关联、右关联、全关联，还支持LEFT SEMI JOIN和CROSS JOIN，但这两种JOIN类型也可以用前面的代替。</li><li>注意：Hive中Join的关联键必须在ON ()中指定，不能在Where中指定，否则就会先做笛卡尔积，再过滤。</li><li>数据准备</li></ul> 
<pre><code class="prism language-sql">hive<span class="token operator">&gt;</span> <span class="token keyword">desc</span> lxw1234_a<span class="token punctuation">;</span>
OK
id                      string                                      
name                    string                                      
<span class="token keyword">Time</span> taken: <span class="token number">0.094</span> seconds<span class="token punctuation">,</span> Fetched: <span class="token number">2</span> <span class="token keyword">row</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span>
hive<span class="token operator">&gt;</span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> lxw1234_a<span class="token punctuation">;</span>
OK
<span class="token number">1</span>       zhangsan
<span class="token number">2</span>       lisi
<span class="token number">3</span>       wangwu
<span class="token keyword">Time</span> taken: <span class="token number">0.116</span> seconds<span class="token punctuation">,</span> Fetched: <span class="token number">3</span> <span class="token keyword">row</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span>
hive<span class="token operator">&gt;</span> <span class="token keyword">desc</span> lxw1234_b<span class="token punctuation">;</span>
OK
id                      string                                      
age                     <span class="token keyword">int</span>                                         
<span class="token keyword">Time</span> taken: <span class="token number">0.159</span> seconds<span class="token punctuation">,</span> Fetched: <span class="token number">2</span> <span class="token keyword">row</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span>
hive<span class="token operator">&gt;</span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> lxw1234_b<span class="token punctuation">;</span>
OK
<span class="token number">1</span>       <span class="token number">30</span>
<span class="token number">2</span>       <span class="token number">29</span>
<span class="token number">4</span>       <span class="token number">21</span>
<span class="token keyword">Time</span> taken: <span class="token number">0.09</span> seconds<span class="token punctuation">,</span> Fetched: <span class="token number">3</span> <span class="token keyword">row</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span>
</code></pre> 
<ul><li>内关联（JOIN） 
  <ul><li>只返回能关联上的结果。</li></ul> </li></ul> 
<pre><code class="prism language-sql"><span class="token keyword">SELECT</span> a<span class="token punctuation">.</span>id<span class="token punctuation">,</span>
a<span class="token punctuation">.</span>name<span class="token punctuation">,</span>
b<span class="token punctuation">.</span>age 
<span class="token keyword">FROM</span> lxw1234_a a 
<span class="token keyword">join</span> lxw1234_b b 
<span class="token keyword">ON</span> <span class="token punctuation">(</span>a<span class="token punctuation">.</span>id <span class="token operator">=</span> b<span class="token punctuation">.</span>id<span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment">--执行结果</span>

<span class="token number">1</span>       zhangsan    <span class="token number">30</span>
<span class="token number">2</span>       lisi        <span class="token number">29</span>
</code></pre> 
<ul><li>左外关联（LEFT [OUTER] JOIN） 
  <ul><li>以LEFT [OUTER] JOIN关键字前面的表作为主表，和其他表进行关联，返回记录和主表的记录数一致，关联不上的字段置为NULL。</li><li>是否指定OUTER关键字，貌似对查询结果无影响。</li></ul> </li></ul> 
<pre><code class="prism language-sql"><span class="token keyword">SELECT</span> a<span class="token punctuation">.</span>id<span class="token punctuation">,</span>
a<span class="token punctuation">.</span>name<span class="token punctuation">,</span>
b<span class="token punctuation">.</span>age 
<span class="token keyword">FROM</span> lxw1234_a a 
<span class="token keyword">left</span> <span class="token keyword">join</span> lxw1234_b b 
<span class="token keyword">ON</span> <span class="token punctuation">(</span>a<span class="token punctuation">.</span>id <span class="token operator">=</span> b<span class="token punctuation">.</span>id<span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment">--执行结果：</span>

<span class="token number">1</span>   zhangsan    <span class="token number">30</span>
<span class="token number">2</span>   lisi        <span class="token number">29</span>
<span class="token number">3</span>   wangwu      <span class="token boolean">NULL</span>
</code></pre> 
<ul><li>右外关联（RIGHT [OUTER] JOIN） 
  <ul><li>和左外关联相反，以RIGTH [OUTER] JOIN关键词后面的表作为主表，和前面的表做关联，返回记录数和主表一致，关联不上的字段为NULL。</li><li>是否指定OUTER关键字，貌似对查询结果无影响。</li></ul> </li></ul> 
<pre><code class="prism language-sql"><span class="token keyword">SELECT</span> a<span class="token punctuation">.</span>id<span class="token punctuation">,</span>
a<span class="token punctuation">.</span>name<span class="token punctuation">,</span>
b<span class="token punctuation">.</span>age 
<span class="token keyword">FROM</span> lxw1234_a a 
<span class="token keyword">RIGHT</span> <span class="token keyword">OUTER</span> <span class="token keyword">JOIN</span> lxw1234_b b 
<span class="token keyword">ON</span> <span class="token punctuation">(</span>a<span class="token punctuation">.</span>id <span class="token operator">=</span> b<span class="token punctuation">.</span>id<span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment">--执行结果：</span>

<span class="token number">1</span>          zhangsan    <span class="token number">30</span>
<span class="token number">2</span>          lisi        <span class="token number">29</span>
<span class="token boolean">NULL</span>       <span class="token boolean">NULL</span>        <span class="token number">21</span>
</code></pre> 
<ul><li>全外关联（FULL [OUTER] JOIN） 
  <ul><li>以两个表的记录为基准，返回两个表的记录去重之和，关联不上的字段为NULL。<br> 是否指定OUTER关键字，貌似对查询结果无影响。</li><li>注意：FULL JOIN时候，Hive不会使用MapJoin来优化。</li></ul> </li></ul> 
<pre><code class="prism language-sql"><span class="token keyword">SELECT</span> a<span class="token punctuation">.</span>id<span class="token punctuation">,</span>
a<span class="token punctuation">.</span>name<span class="token punctuation">,</span>
b<span class="token punctuation">.</span>age 
<span class="token keyword">FROM</span> lxw1234_a a 
<span class="token keyword">FULL</span> <span class="token keyword">OUTER</span> <span class="token keyword">JOIN</span> lxw1234_b b 
<span class="token keyword">ON</span> <span class="token punctuation">(</span>a<span class="token punctuation">.</span>id <span class="token operator">=</span> b<span class="token punctuation">.</span>id<span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment">--执行结果：</span>

<span class="token number">1</span>       zhangsan        <span class="token number">30</span>
<span class="token number">2</span>       lisi            <span class="token number">29</span>
<span class="token number">3</span>       wangwu          <span class="token boolean">NULL</span>
<span class="token boolean">NULL</span>    <span class="token boolean">NULL</span>            <span class="token number">21</span>
</code></pre> 
<ul><li>LEFT SEMI JOIN 
  <ul><li>以LEFT SEMI JOIN关键字前面的表为主表，返回主表的KEY也在副表中的记录。</li></ul> </li></ul> 
<pre><code class="prism language-sql"><span class="token keyword">SELECT</span> a<span class="token punctuation">.</span>id<span class="token punctuation">,</span>
a<span class="token punctuation">.</span>name 
<span class="token keyword">FROM</span> lxw1234_a a 
<span class="token keyword">LEFT</span> SEMI <span class="token keyword">JOIN</span> lxw1234_b b 
<span class="token keyword">ON</span> <span class="token punctuation">(</span>a<span class="token punctuation">.</span>id <span class="token operator">=</span> b<span class="token punctuation">.</span>id<span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment">--执行结果：</span>

<span class="token number">1</span>       zhangsan
<span class="token number">2</span>       lisi

<span class="token comment">--等价于：</span>

<span class="token keyword">SELECT</span> a<span class="token punctuation">.</span>id<span class="token punctuation">,</span>
a<span class="token punctuation">.</span>name 
<span class="token keyword">FROM</span> lxw1234_a a 
<span class="token keyword">WHERE</span> a<span class="token punctuation">.</span>id <span class="token operator">IN</span> <span class="token punctuation">(</span><span class="token keyword">SELECT</span> id <span class="token keyword">FROM</span> lxw1234_b<span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment">--也等价于：</span>

<span class="token keyword">SELECT</span> a<span class="token punctuation">.</span>id<span class="token punctuation">,</span>
a<span class="token punctuation">.</span>name 
<span class="token keyword">FROM</span> lxw1234_a a 
<span class="token keyword">join</span> lxw1234_b b 
<span class="token keyword">ON</span> <span class="token punctuation">(</span>a<span class="token punctuation">.</span>id <span class="token operator">=</span> b<span class="token punctuation">.</span>id<span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment">--也等价于：</span>

<span class="token keyword">SELECT</span> a<span class="token punctuation">.</span>id<span class="token punctuation">,</span>
a<span class="token punctuation">.</span>name 
<span class="token keyword">FROM</span> lxw1234_a a 
<span class="token keyword">WHERE</span> <span class="token keyword">EXISTS</span> <span class="token punctuation">(</span><span class="token keyword">SELECT</span> <span class="token number">1</span> <span class="token keyword">FROM</span> lxw1234_b b <span class="token keyword">WHERE</span> a<span class="token punctuation">.</span>id <span class="token operator">=</span> b<span class="token punctuation">.</span>id<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<ul><li>笛卡尔积关联（CROSS JOIN） 
  <ul><li>返回两个表的笛卡尔积结果，不需要指定关联键。</li></ul> </li></ul> 
<pre><code class="prism language-sql"><span class="token keyword">SELECT</span> a<span class="token punctuation">.</span>id<span class="token punctuation">,</span>
a<span class="token punctuation">.</span>name<span class="token punctuation">,</span>
b<span class="token punctuation">.</span>age 
<span class="token keyword">FROM</span> lxw1234_a a 
<span class="token keyword">CROSS</span> <span class="token keyword">JOIN</span> lxw1234_b b<span class="token punctuation">;</span>

<span class="token comment">--执行结果：</span>

<span class="token number">1</span>       zhangsan        <span class="token number">30</span>
<span class="token number">1</span>       zhangsan        <span class="token number">29</span>
<span class="token number">1</span>       zhangsan        <span class="token number">21</span>
<span class="token number">2</span>       lisi            <span class="token number">30</span>
<span class="token number">2</span>       lisi            <span class="token number">29</span>
<span class="token number">2</span>       lisi            <span class="token number">21</span>
<span class="token number">3</span>       wangwu          <span class="token number">30</span>
<span class="token number">3</span>       wangwu          <span class="token number">29</span>
<span class="token number">3</span>       wangwu          <span class="token number">21</span>
</code></pre> 
<ul><li>除非特殊需求，并且数据量不是特别大的情况下，才可以慎用CROSS JOIN，否则，很难跑出正确的结果，或者JOB压根不能执行完。</li><li>Hive中只要是涉及到两个表关联，首先得了解一下数据，看是否存在多对多的关联。</li></ul> 
<h2><a id="46Hive_950"></a>46.Hive严格模式</h2> 
<ul><li>开启后禁止3种类型查询 
  <ul><li>分区表没有指定分区字段为过滤条件的查询</li><li>order by不加limit的查询</li><li>笛卡尔积的查询</li></ul> </li><li>Hive严格模式 
  <ul><li>Hive提供了一个严格模式，可以防止用户执行那些可能产生意向不到的不好的效果的查询。说通俗一点就是这种模式可以阻止某些查询的执行。通过如下语句设置严格模式：</li></ul> </li></ul> 
<pre><code class="prism language-sql">hive<span class="token operator">&gt;</span> <span class="token keyword">set</span> hive<span class="token punctuation">.</span>mapred<span class="token punctuation">.</span><span class="token keyword">mode</span><span class="token operator">=</span>strict<span class="token punctuation">;</span>
</code></pre> 
<ul><li>设置为严格模式后，可以禁止3种类型的查询：</li><li>(1)：带有分区的表的查询 
  <ul><li>如果在一个分区表执行hive，<code>除非where语句中包含分区字段过滤条件来显示数据范围</code>，否则不允许执行。换句话说就是在严格模式下不允许用户扫描所有的分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。如果不进行分区限制的查询会消耗巨大的资源来处理，如下不带分区的查询语句：</li></ul> </li></ul> 
<pre><code class="prism language-sql">hive<span class="token operator">&gt;</span> <span class="token keyword">SELECT</span> <span class="token keyword">DISTINCT</span><span class="token punctuation">(</span>planner_id<span class="token punctuation">)</span> <span class="token keyword">FROM</span> fracture_ins <span class="token keyword">WHERE</span> planner_id<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">;</span>
</code></pre> 
<ul><li>执行后会出现如下错误：</li></ul> 
<pre><code class="prism language-sql">FAILED: Error <span class="token operator">in</span> semantic analysis: <span class="token keyword">No</span> <span class="token keyword">Partition</span> Predicate Found <span class="token keyword">for</span> Alias <span class="token string">"fracture_ins"</span> <span class="token keyword">Table</span> "fracture_ins
</code></pre> 
<ul><li>解决方案是在where中增加分区条件：</li></ul> 
<pre><code class="prism language-sql">hive<span class="token operator">&gt;</span> <span class="token keyword">SELECT</span> <span class="token keyword">DISTINCT</span><span class="token punctuation">(</span>planner_id<span class="token punctuation">)</span> <span class="token keyword">FROM</span> fracture_ins
       <span class="token operator">&gt;</span> <span class="token keyword">WHERE</span> planner_id<span class="token operator">=</span><span class="token number">5</span> <span class="token operator">AND</span> hit_date<span class="token operator">=</span><span class="token number">20120101</span><span class="token punctuation">;</span>
</code></pre> 
<ul><li>(2)：带有order by的查询</li><li>对于使用了order by的查询，要求必须有limit语句。因为order by为了执行排序过程会将所有的结果分发到同一个reduce中进行处理，强制要求用户增加这个limit语句可以防止reduce额外消耗资源，如下是不带limit关键字的查询语句：</li></ul> 
<pre><code class="prism language-sql">hive<span class="token operator">&gt;</span> <span class="token keyword">SELECT</span> <span class="token operator">*</span> <span class="token keyword">FROM</span> fracture_ins <span class="token keyword">WHERE</span> hit_date<span class="token operator">&gt;</span><span class="token number">2012</span> <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> planner_id<span class="token punctuation">;</span>
</code></pre> 
<ul><li>出现如下错误：</li></ul> 
<pre><code class="prism language-sql">FAILED: Error <span class="token operator">in</span> semantic analysis: line <span class="token number">1</span>:<span class="token number">56</span> <span class="token operator">In</span> strict <span class="token keyword">mode</span><span class="token punctuation">,</span>
<span class="token keyword">limit</span> must be specified <span class="token keyword">if</span> <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> <span class="token operator">is</span> present planner_id
</code></pre> 
<ul><li>解决方案就是增加一个limit关键字：</li></ul> 
<pre><code class="prism language-sql">hive<span class="token operator">&gt;</span> <span class="token keyword">SELECT</span> <span class="token operator">*</span> <span class="token keyword">FROM</span> fracture_ins <span class="token keyword">WHERE</span> hit_date<span class="token operator">&gt;</span><span class="token number">2012</span> <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> planner_id
        <span class="token operator">&gt;</span> <span class="token keyword">LIMIT</span> <span class="token number">100000</span><span class="token punctuation">;</span>
</code></pre> 
<ul><li>(3)：限制笛卡尔积的查询</li><li>对关系型数据库非常了解的用户可能期望在执行join查询的时候不使用on语句而是使用where语句，这样关系型数据库的执行优化器就可以高效的将where语句转换成那个on语句了。不幸的是，Hive并不支持这样的优化，因为如果表非常大的话，就会出现不可控的情况，如下是不带on的语句：</li></ul> 
<pre><code class="prism language-sql">hive<span class="token operator">&gt;</span> <span class="token keyword">SELECT</span> <span class="token operator">*</span> <span class="token keyword">FROM</span> fracture_act <span class="token keyword">JOIN</span> fracture_ads
<span class="token operator">&gt;</span> <span class="token keyword">WHERE</span> fracture_act<span class="token punctuation">.</span>planner_id <span class="token operator">=</span> fracture_ads<span class="token punctuation">.</span>planner_id<span class="token punctuation">;</span>
</code></pre> 
<ul><li>出现如下错误：</li></ul> 
<pre><code class="prism language-sql">FAILED: Error <span class="token operator">in</span> semantic analysis: <span class="token operator">In</span> strict <span class="token keyword">mode</span><span class="token punctuation">,</span> cartesian product
<span class="token operator">is</span> <span class="token operator">not</span> allowed<span class="token punctuation">.</span> <span class="token keyword">If</span> you really want <span class="token keyword">to</span> perform the operation<span class="token punctuation">,</span>
<span class="token operator">+</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span>mapred<span class="token punctuation">.</span><span class="token keyword">mode</span><span class="token operator">=</span>nonstrict<span class="token operator">+</span>
</code></pre> 
<ul><li>解决方案就是加上on语句：</li></ul> 
<pre><code class="prism language-sql">hive<span class="token operator">&gt;</span> <span class="token keyword">SELECT</span> <span class="token operator">*</span> <span class="token keyword">FROM</span> fracture_act <span class="token keyword">JOIN</span> fracture_ads
        <span class="token operator">&gt;</span> <span class="token keyword">ON</span> <span class="token punctuation">(</span>fracture_act<span class="token punctuation">.</span>planner_id <span class="token operator">=</span> fracture_ads<span class="token punctuation">.</span>planner_id<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<h2><a id="47sparkSQLhivesparkSQLhive_1025"></a>47.sparkSQL一定比hive快吗，能否想出一种场景，sparkSQL比hive慢(重点)</h2> 
<h3><a id="sparkSQL_1026"></a>sparkSQL之所以快</h3> 
<ul><li><mark>中间阶段不落盘</mark> 
  <ul><li><code>Hadoop 每次 shuffle 操作后，必须写到磁盘，而 Spark 在 shuffle 后不一定落盘，可以 cache 到内存中，以便迭代时使用。</code>如果操作复杂，很多的 shufle 操作，那么 Hadoop 的读写 IO 时间会大大增加，也是 Hive 更慢的主要原因了。</li></ul> </li><li>消除了冗余的 MapReduce 阶段 
  <ul><li>Hadoop 的 shuffle 操作一定连着完整的 MapReduce 操作，冗余繁琐。而 Spark 基于 <code>RDD</code> 提供了丰富的算子操作，且 reduce 操作产生 shuffle 数据，可以缓存在内存中。</li></ul> </li><li>JVM 的优化 
  <ul><li>Hadoop 每次 <code>MapReduce</code> 操作，启动一个 Task 便会启动一次 JVM，基于<code>进程</code>的操作。而 <code>Spark 每次 MapReduce 操作是基于线程的，只在启动 Executor 是启动一次 JVM，内存的 Task 操作是在线程复用</code>的。每次启动 JVM 的时间可能就需要几秒甚至十几秒，那么当 Task 多了，这个时间 Hadoop 不知道比 Spark 慢了多少。</li></ul> </li><li>极端查询</li><li>这个查询<code>只有一次 shuffle 操作</code>，此时，也许 Hive HQL 的运行时间也许比 Spark 还快，反正 shuffle 完了都会落一次盘，或者都不落盘。</li></ul> 
<pre><code class="prism language-sql"><span class="token keyword">Select</span> month_id
	<span class="token punctuation">,</span> <span class="token function">sum</span><span class="token punctuation">(</span>sales<span class="token punctuation">)</span> 
<span class="token keyword">from</span> T 
<span class="token keyword">group</span> <span class="token keyword">by</span> month_id<span class="token punctuation">;</span>
</code></pre> 
<h2><a id="48Hive_1042"></a>48.Hive表为什么不支持删除字段？</h2> 
<ul><li>Hive是一个基于Hadoop的数据仓库工具，用于处理和分析大规模的结构化数据。虽然Hive提供了很多SQL-like的功能，但是在设计上它与传统的关系型数据库有很大的不同。</li><li><code>Hive的主要目标是高效地读取和分析数据，而不是数据的实时修改</code>。因此，Hive在数据更新和删除方面的功能有限。</li><li>在Hive中，<code>表的结构信息存储在元数据中，而数据实际存储在HDFS（Hadoop Distributed File System）或其他分布式文件系统中</code>。</li><li>Hive表不支持删除字段的原因如下： 
  <ul><li>1.<code>数据存储不连续</code>：Hive中的数据<code>以文件形式存储，这些文件中的记录并不一定按照某种特定顺序排列</code>。如果需要删除一个字段，就需要对整个数据集进行重写，这在大规模数据场景下会导致巨大的计算和I/O成本。</li><li>2.<code>数据不可变性</code>：Hive基于Hadoop，而Hadoop的文件系统（HDFS）是基于数据不可变性原则设计的。这意味着一旦数据写入HDFS，就不能进行修改。<code>删除字段意味着需要修改已经存储的数据</code>，这与HDFS的设计原则相悖。</li><li>3.<code>兼容性问题</code>：如果支持删除字段，那么需要处理与现有数据的兼容性问题。<code>删除字段后，已经存在的数据中的这个字段应该如何处理？</code>这会导致数据不一致，增加处理复杂性。</li></ul> </li><li>尽管Hive不支持删除字段，但你可以通过以下方法变更表结构： 
  <ul><li>1.创建一个新表，包含原表中除了要删除的字段之外的所有字段。</li><li>2.使用INSERT INTO或INSERT OVERWRITE从原表中选择所需的字段并将其插入新表。</li><li>3.在确认新表数据正确无误后，删除原表，并将新表重命名为原表名。</li><li>这种方法虽然存在一定的数据迁移成本，但可以确保数据的完整性和一致性。</li></ul> </li></ul> 
<h2><a id="49HivePrestoSpark_1056"></a>49.Hive、Presto、Spark引擎该用哪个？</h2> 
<ul><li>Hive适用于离线批处理数据仓库场景。</li><li>Presto适用于实时或交互式查询大数据集的场景。</li><li>Spark适用于多种大数据工作负载，包括批处理、实时流处理、机器学习和图形计算等，具有更广泛的应用领域。</li><li>以下是详细对比：</li><li>1、Hive： 
  <ul><li>适用场景：Hive是一个数据仓库工具，专注于批量数据处理。它通常用于处理大规模的离线数据，如数据仓库、数据分析和报表生成等场景。</li><li>查询语言：Hive使用HiveQL，这是类似SQL的查询语言，使非技术人员能够轻松查询和分析数据。</li><li>存储：Hive通常将数据存储在Hadoop分布式文件系统（HDFS）中，支持多种数据格式，如Parquet、ORC等。</li><li>执行引擎：Hive使用MapReduce或Tez等底层执行引擎来处理查询。</li></ul> </li><li>2、Presto： 
  <ul><li>适用场景：Presto是一个分布式SQL查询引擎，旨在实时或交互式地查询大数据集。它适用于需要快速查询和分析数据的场景，如数据探索、仪表板、实时分析等。</li><li>查询语言：Presto使用标准SQL，支持复杂的查询和连接操作，具有较低的查询延迟。</li><li>存储：Presto支持多种数据存储系统，包括Hive、HDFS、Amazon S3、MySQL等，因此可以查询不同数据源中的数据。</li><li>执行引擎：Presto使用自己的执行引擎，它将查询分解为任务，并并行执行这些任务。</li></ul> </li><li>3、Spark： 
  <ul><li>适用场景：Spark是一个通用的大数据计算引擎，适用于多种场景，包括批处理、实时流处理、机器学习和图形计算等。它可以在多种工作负载下运行。</li><li>编程模型：Spark提供了丰富的API，包括Spark SQL、Spark Streaming、MLlib（机器学习库）和GraphX（图形计算库），使开发人员可以编写复杂的数据处理应用程序。</li><li>存储：Spark可以与多种数据存储系统集成，包括HDFS、Amazon S3、Cassandra等。</li><li>执行引擎：Spark使用内存计算来加速数据处理，它的执行引擎可以在内存中保留中间数据，减少了磁盘IO，从而提高了性能。</li></ul> </li></ul>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5a4c1bfd87f7a08a3206a7090e8fffbd/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Linux修改时区失败，手动修改localtime无效</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/82b71105c8df8ccc9b18e507dd9aaeda/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【ASP】读取数据库并显示字段的值例子</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>