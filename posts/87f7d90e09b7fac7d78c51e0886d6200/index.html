<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>机器学习之集成学习 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="机器学习之集成学习" />
<meta property="og:description" content="一、集成学习
集成学习（Ensemble learning）是使用一系列学习器进行学习，并使用某种规则把各个学习结果进行整合，从而获得比单个学习器显著优越的泛化性能。它不是一种单独的机器学习算法，而更像是一种优化策略。
集成学习的一般结构是，先产生一组个体学习器，再用某种结合策略将它们结合起来。
集成学习优势在于：
1）个体学习器之间存在一定的差异性，这会导致分类边界不同，也就是说可能存在错误。那么将多个个体学习器合并后，就可以得到更加合理的边界，减少整体的错误率，实现更好的效果；
2）对于数据集过大或过小的情况，可以分别进行划分和有放回的操作，产生不同的数据子集，然后使用数据子集训练不同的学习器，最终再合并成为一个强学习器；
3）如果数据的划分边界过于复杂，使用线性模型很难描述情况，那么可以训练多个模型，然后再进行模型的融合；
4）对于多个异构的特征集的时候，很难直接融合，那么可以考虑使用每个数据集构建一个分类模型，然后将多个模型融合。
个体学习器分类
同质：所有个体学习器都是一个种类的——基学习器
异质：所有个体学习器不全是一个种类的——组件学习器
基学习器有时也被称为弱学习器，指的是准确率略微好于随机猜测的学习器。
集成算法的两个主要问题：如何选择若干个体学习器，以及选择何种策略将这些个体学习器集成为一个强学习器。
集成算法的成功在于保证个体学习器的多样性（好而不同），且集成不稳定的算法也能够得到一个比较明显的性能提升。
常见的集成学习有：
用于减少方差的Bagging
用于减少偏差的Boosting
用于提升预测结果的Stacking
二、Bagging
Bagging自举汇聚法（Bootstrap Aggregating），思想是：为了使基学习器尽可能的具有较大的差异（好而不同），对训练样本进行采样以产生若干个不同的子集，对每一个子集训练一个基学习器，然后结合策略进行集成的方法。为了不让每个基学习器效果太差，这些子集不能完全不同，因此使用子集之间相互有交叠的采样方法，即bootstrap方法。
基本流程：给定包含m个样本的数据集，随机取出一个样本放入采样集中，再把它放回到原始数据集中，重复m次，得到含m个样本的采样集，可知原始数据集中有63.2%的样本出现在采样集中，有36.8%的样本不会出现在采样集中，这些不在采样集中的样本可以作为验证集对泛化性能进行包外估计（out-of-bag estimate）。进行同样的操作进行T次得到T个每个含m个样本的采样集，基于每个采样集训练一个基学习器，再将基学习器进行组合，一般使用多数投票或求均值的方式来统计最终的分类结果。
Bagging方法的基学习器可以是基本的算法模型，如：Linear、Ridge、Lasso、Logistic、Softmax、ID3、C4.5、CART、SVM、kNN等。
Bagging计算复杂度与基学习器复杂度同阶，效率高；Bagging可以不经修改地用于多分类和回归问题。
Bagging主要关注降低方差，因此在不剪枝决策树、神经网络等易受样本扰动的学习器上效果更加明显。
Bagging方法的训练过程
Bagging方法的预测过程
三、Boosting
Boosting是一族可将弱学习器提升为强学习器的算法，先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器分错的样本获得较大的权重，然后基于调整后的样本分布来训练下一个基学习器；如此反复进行，直至基学习器数量达到指定值，最终将这些学习器进行加权结合，正确率越高的基学习器的获得的权重越大。Boosting可以用于分类和回归问题。
提升技术的意义：如果一个问题存在弱预测模型（偏差较大），那么可以通过依据损失函数的梯度方式进行提升（Gradient Boosting），从而得到一个强预测模型（偏差减小）。
对样本分布进行调整主要有两种方法：
（1）重新赋权法（re-weighting）：在每一轮学习中，根据样本分布为每个训练样本重新赋予一个权重值。
（2）重采样法（re-sampling）：对于无法接受带权样本的基学习算法，在每一轮学习中，根据样本分布对训练集重新进行采样，再用重采样得到的样本集对基学习器进行训练。
Boosting算法在训练的每一轮都要检查当前生成的基学习器是否满足基本条件（例如在AdaBoost算法中橙线部分），一旦不满足该条件，则废弃当前基学习器，使用重采样法可以在重新采样后继续训练过程。
常见的Boosting模型有：AdaBoost、GBDT、XGBoost
Bagging和Boosting的区别
（1）样本选择：Bagging算法是有放回的随机采样；Boosting算法是每一轮训练集不变，只是训练集中的每个样例的权重发生变化，权重值根据上一轮的分类结果进行调整。
（2）样例权重：Bagging使用随机抽样，样例的权重相等；Boosting根据错误率不断的调整样例的权重值，错误率越大则权重越大。
（3）基学习器权重：Bagging所有基学习器的权重相等；Boosting对于误差小的基学习器赋予更大的权重。
（4）并行计算：Bagging可以并行生成各个基学习器；Boosting理论上只能顺序/串行生成，因为后一个模型需要前一个模型的结果。
（5）Bagging中每个基学习器都是强学习器，主要关注的问题是降低方差；Boosting中每个基学习器都是弱学习器，主要关注的问题是降低偏差。
四、学习法Stacking
当训练数据很多时，一种更为强大的结合策略是使用学习法，即通过训练一个模型用于组合其他模型的技术。Stacking是学习法的典型代表，方法是首先从原始数据集中训练出初级学习器（异质），将初级学习器的预测结果作为训练次级学习器的输入，再次训练得到最终结果，一般使用单层的Logistic回归作为组合模型。
在训练阶段，次级训练集是初级学习器产生的，若直接用初级学习器的训练集来产生次级训练集，则过拟合风险会比较大；因此通过交叉验证或留一法的方式，用训练初级学习器未使用的样本来产生次级学习器的训练样本。
算法流程：
（1）使用留出法划分原始数据集，一部分为训练集，一部分为测试集。
（2）对于训练集，进行k-fold交叉验证，上图中k=5，从而将训练集划分为5份，其中四份作为新的训练集，剩下一份作为新的测试集。
（3）选择k个合适的初级学习器，基于新的训练集训练模型，使用新的测试集产生预测值。
（4）将所有预测值合并称为一个新的数据集，作为次级学习器的训练数据（即New Feature数据集）。
（5）利用k次产生的模型，和老的测试集特征，产生一系列预测值，对这些预测值求平均，并将这些平均数据作为下一轮的测试集。
（6）下一轮的目标值Y保持不变，还为原始数据集的Y值。
五、结合策略
学习器结合可能会从三个方面带来好处：
（1）统计的原因
由于学习任务的假设空间往往很大，可能有多个假设在训练集上达到同等的性能，此时若是有单个学习器会导致泛化性能不佳，结合多个学习器会减小这一风险。
（2）计算的原因
单个学习算法往往会陷入局部最优解，多个学习器结合可降低陷入糟糕局部最优解的风险。
（3）表示的原因
某些任务的真实假设可能不在当前学习算法所考虑的假设空间中，此时若使用单个学习器拟合效果一定不佳，而通过结合多个学习器，由于相应的假设空间有所扩大，则有可能学得更好的近似。
1、平均法
用于hi为连续型数据。
（1）简单平均值法：
（2）加权平均值：，
2、投票法
用于hi为离散型数据，类别集合为{c1,c2,...cN}，hi在样本x上的预测输出表示为一个N维向量，hij(x)是hi在类别标记cj上的输出。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/87f7d90e09b7fac7d78c51e0886d6200/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-06-06T17:56:00+08:00" />
<meta property="article:modified_time" content="2018-06-06T17:56:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习之集成学习</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><strong><span style="font-family:'微软雅黑';">一、集成学习</span></strong></p> 
<p><span style="font-family:'微软雅黑';">        集成学习</span><span style="font-family:'微软雅黑';">（</span>Ensemble learning）<span style="font-family:'微软雅黑';">是使用一系列学习器进行学习，并使用某种规则把各个学习结果进行整合，从而获得比单个学习器显著优越的泛化性能。它不是一种单独的机器学习算法，而更像是一种优化策略。</span></p> 
<p><span style="font-family:'微软雅黑';">        集成学习的一般结构是，先产生一组个体学习器，再用某种结合策略将它们结合起来。</span></p> 
<p>         <img src="https://images2.imgbox.com/a1/dc/h9NicYWI_o.png" alt=""></p> 
<p> </p> 
<p>         <img src="https://images2.imgbox.com/3c/3c/PBhPGHeS_o.png" alt=""></p> 
<p> </p> 
<p><span style="font-family:'微软雅黑';">        集成学习优势在于：</span></p> 
<p>        1）<span style="font-family:'微软雅黑';">个体学习器之间存在一定的差异性，这会导致分类边界不同，也就是说可能存在错误。那么将多个个体学习器合并后，就可以得到更加合理的边界，减少整体的错误率，实现更好的效果；</span></p> 
<p>        2）<span style="font-family:'微软雅黑';">对于数据集过大或过小的情况，可以分别进行划分和有放回的操作，产生不同的数据子集，然后使用数据子集训练不同的学习器，最终再合并成为一个强学习器；</span></p> 
<p>        3）<span style="font-family:'微软雅黑';">如果数据的划分边界过于复杂，使用线性模型很难描述情况，那么可以训练多个模型，然后再进行模型的融合；</span></p> 
<p>        4）<span style="font-family:'微软雅黑';">对于多个异构的特征集的时候，很难直接融合，那么可以考虑使用每个数据集构建一个分类模型，然后将多个模型融合。</span></p> 
<p> </p> 
<p><span style="font-family:'微软雅黑';">        个体学习器分类</span></p> 
<p><span style="font-family:'微软雅黑';">            同质：所有个体学习器都是一个种类的</span>——基学习器</p> 
<p><span style="font-family:'微软雅黑';">            异质：所有个体学习器不全是一个种类的</span>——组件学习器</p> 
<p><span style="font-family:'微软雅黑';">            基学习器有时也被称为弱学习器，指的是准确率略微好于随机猜测的学习器。</span></p> 
<p><span style="font-family:'微软雅黑';">        <br></span></p> 
<p><span style="font-family:'微软雅黑';">        集成算法的两个主要问题：如何选择若干个体学习器，以及选择何种策略将这些个体学习器集成为一个强学习器。</span></p> 
<p><span style="font-family:'微软雅黑';">        集成算法的成功在于保证个体学习器的多样性（好而不同），且集成不稳定的算法也能够得到一个比较明显的性能提升。</span></p> 
<p><span style="font-family:'微软雅黑';">        常见的集成学习有：</span></p> 
<p><span style="font-family:'微软雅黑';">            用于减少方差的</span>Bagging</p> 
<p><span style="font-family:'微软雅黑';">            用于减少偏差的</span>Boosting</p> 
<p><span style="font-family:'微软雅黑';">            用于提升预测结果的</span>Stacking</p> 
<p> </p> 
<p><strong><span style="font-family:'微软雅黑';">二、</span>Bagging</strong></p> 
<p>        Bagging自举汇聚法（<strong><u>B</u></strong>ootstrap<strong> <u>Agg</u></strong>regat<strong><u>ing</u></strong><span style="font-family:'微软雅黑';">），思想是：为了使基学习器尽可能的具有较大的差异（好而不同），对训练样本进行采样以产生若干个不同的子集，对每一个子集训练一个基学习器，然后结合策略进行集成的方法。为了不让每个基学习器效果太差，这些子集不能完全不同，因此使用子集之间相互有交叠的采样方法，即</span>bootstrap方法。</p> 
<p><span style="font-family:'微软雅黑';">        基本流程：给定包含</span>m个样本的数据集，随机取出一个样本放入采样集中，再把它放回到原始数据集中，重复m次，得到含m个样本的采样集，可知原始数据集中有63.2%的样本出现在采样集中，有36.8%的样本不会出现在采样集中，这些不在采样集中的样本可以作为验证集对泛化性能进行包外估计（out-of-bag estimate）。进行同样的操作进行T次得到T个每个含m个样本的采样集，基于每个采样集训练一个基学习器，再将基学习器进行组合，一般使用多数投票或求均值的方式来统计最终的分类结果。</p> 
<p>        Bagging方法的基学习器可以是基本的算法模型，如：Linear、Ridge、Lasso、Logistic、Softmax、ID3、C4.5、CART、SVM、kNN等。</p> 
<p>        Bagging计算复杂度与基学习器复杂度同阶，效率高；Bagging可以不经修改地用于多分类和回归问题。</p> 
<p>        Bagging主要关注降低方差，因此在不剪枝决策树、神经网络等易受样本扰动的学习器上效果更加明显。</p> 
<p>        <br></p> 
<p>        Bagging方法的训练过程</p> 
<p>         <img src="https://images2.imgbox.com/b7/6c/Ia5G9x3h_o.png" alt=""></p> 
<p> </p> 
<p>        Bagging方法的预测过程</p> 
<p>         <img src="https://images2.imgbox.com/f3/75/vMqBM0UX_o.png" alt=""></p> 
<p>        <img src="https://images2.imgbox.com/b1/d6/SYCwJ96M_o.png" alt=""><br></p> 
<p><strong>  <span style="font-family:'微软雅黑';">三、</span>Boosting</strong></p> 
<p>        Boosting是一族可将弱学习器提升为强学习器的算法，先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器<u><span style="font-family:'微软雅黑';">分错的样本获得较大的权重</span></u><span style="font-family:'微软雅黑';">，然后基于调整后的样本分布来训练下一个基学习器；如此反复进行，直至基学习器数量达到指定值，最终将这些学习器进行加权结合，</span><u><span style="font-family:'微软雅黑';">正确率越高的基学习器的获得的权重越大</span></u><span style="font-family:'微软雅黑';">。</span>Boosting可以用于分类和回归问题。</p> 
<p><span style="font-family:'微软雅黑';">        提升技术的意义：如果一个问题存在弱预测模型（偏差较大），那么可以通过依据损失函数的梯度方式进行提升（</span>Gradient Boosting），从而得到一个强预测模型（偏差减小）。</p> 
<p>         <img src="https://images2.imgbox.com/3d/1d/AFL9dCGM_o.png" alt=""></p> 
<p> </p> 
<p><span style="font-family:'微软雅黑';">        对样本分布进行调整主要有两种方法：</span></p> 
<p><span style="font-family:'微软雅黑';">            （</span>1）重新赋权法（re-weighting）：在每一轮学习中，根据样本分布为每个训练样本重新赋予一个权重值。</p> 
<p>             <img src="https://images2.imgbox.com/73/30/PM7BY2sQ_o.png" alt=""></p> 
<p><span style="font-family:'微软雅黑';">            （</span>2）重采样法（re-sampling）：对于无法接受带权样本的基学习算法，在每一轮学习中，根据样本分布对训练集重新进行采样，再用重采样得到的样本集对基学习器进行训练。</p> 
<p>        Boosting算法在训练的每一轮都要检查当前生成的基学习器是否满足基本条件（例如在AdaBoost算法中橙线部分），一旦不满足该条件，则废弃当前基学习器，使用重采样法可以在重新采样后继续训练过程。</p> 
<p>         <img src="https://images2.imgbox.com/b2/a0/WpVEyBvb_o.png" alt=""></p> 
<p> </p> 
<p><span style="font-family:'微软雅黑';">        常见的</span>Boosting模型有：AdaBoost、GBDT、XGBoost</p> 
<p> </p> 
<p> </p> 
<p>        Bagging和Boosting的区别</p> 
<p>            （1）<span style="font-family:'微软雅黑';">样本选择：</span>Bagging算法是有放回的随机采样；Boosting算法是每一轮训练集不变，只是训练集中的每个样例的权重发生变化，权重值根据上一轮的分类结果进行调整。</p> 
<p>            （2）<span style="font-family:'微软雅黑';">样例权重：</span>Bagging使用随机抽样，样例的权重相等；Boosting根据错误率不断的调整样例的权重值，错误率越大则权重越大。</p> 
<p>            （3）<span style="font-family:'微软雅黑';">基学习器权重：</span>Bagging所有基学习器的权重相等；Boosting对于误差小的基学习器赋予更大的权重。</p> 
<p>            （4）<span style="font-family:'微软雅黑';">并行计算：</span>Bagging可以并行生成各个基学习器；Boosting理论上只能顺序/串行生成，因为后一个模型需要前一个模型的结果。</p> 
<p>            （5）Bagging中每个基学习器都是强学习器，主要关注的问题是降低方差；Boosting中每个基学习器都是弱学习器，主要关注的问题是降低偏差。</p> 
<p> </p> 
<p> </p> 
<p><strong><span style="font-family:'微软雅黑';">四、学习法</span>Stacking</strong></p> 
<p><span style="font-family:'微软雅黑';">        当训练数据很多时，一种更为强大的结合策略是使用学习法，即通过训练一个模型用于组合其他模型的技术。</span>Stacking是学习法的典型代表，方法是首先从原始数据集中训练出初级学习器（异质），将初级学习器的预测结果作为训练次级学习器的输入，再次训练得到最终结果，一般使用单层的Logistic回归作为组合模型。</p> 
<p><span style="font-family:'微软雅黑';">        在训练阶段，次级训练集是初级学习器产生的，若直接用初级学习器的训练集来产生次级训练集，则过拟合风险会比较大；因此通过交叉验证或留一法的方式，用训练初级学习器未使用的样本来产生次级学习器的训练样本。</span></p> 
<p><span style="font-family:'微软雅黑';">        <img src="https://images2.imgbox.com/cb/46/Z7b45yqS_o.png" alt=""><br></span></p> 
<p>         <img src="https://images2.imgbox.com/d3/6d/CIfJezMZ_o.png" alt=""></p> 
<p>        <img src="https://images2.imgbox.com/2e/32/xQOkHx5J_o.png" alt=""><br></p> 
<br> 
<p><span style="font-family:'微软雅黑';">        算法流程：</span></p> 
<p>            （1）<span style="font-family:'微软雅黑';">使用留出法划分原始数据集，一部分为训练集，一部分为测试集。</span></p> 
<p>            （2）<span style="font-family:'微软雅黑';">对于训练集，进行</span>k-fold交叉验证，上图中k=5，从而将训练集划分为5份，其中四份作为新的<span style="color:#3333FF;"><span style="font-family:'微软雅黑';">训练集</span></span><span style="font-family:'微软雅黑';">，剩下一份作为新的</span><span style="color:#ed7d31;"><span style="font-family:'微软雅黑';">测试集</span></span><span style="font-family:'微软雅黑';">。</span></p> 
<p>            （3）<span style="font-family:'微软雅黑';">选择</span>k个合适的初级学习器，基于新的<span style="color:#3333FF;"><span style="font-family:'微软雅黑';">训练集</span></span><span style="font-family:'微软雅黑';">训练模型，使用新的</span><span style="color:#ed7d31;"><span style="font-family:'微软雅黑';">测试集</span></span><span style="font-family:'微软雅黑';">产生预测值。</span></p> 
<p>            （4）<span style="font-family:'微软雅黑';">将所有预测值合并称为一个新的数据集，作为次级学习器的训练数据（即</span>New Feature数据集）。</p> 
<p>            （5）<span style="font-family:'微软雅黑';">利用</span>k次产生的模型，和老的测试集特征，产生一系列<span style="color:#009900;"><span style="font-family:'微软雅黑';">预测值</span></span><span style="font-family:'微软雅黑';">，对这些预测值求平均，并将这些平均数据作为下一轮的测试集。</span></p> 
<p>            （6）<span style="font-family:'微软雅黑';">下一轮的目标值</span>Y保持不变，还为原始数据集的Y值。</p> 
<p>           <br></p> 
<p><strong><span style="font-family:'微软雅黑';">五、结合策略</span></strong></p> 
<p><span style="font-family:'微软雅黑';">        学习器结合可能会从三个方面带来好处：</span></p> 
<p><span style="font-family:'微软雅黑';">        （</span>1）统计的原因</p> 
<p><span style="font-family:'微软雅黑';">            由于学习任务的假设空间往往很大，可能有多个假设在训练集上达到同等的性能，此时若是有单个学习器会导致泛化性能不佳，结合多个学习器会减小这一风险。</span></p> 
<p><span style="font-family:'微软雅黑';">        （</span>2）计算的原因</p> 
<p><span style="font-family:'微软雅黑';">            单个学习算法往往会陷入局部最优解，多个学习器结合可降低陷入糟糕局部最优解的风险。</span></p> 
<p><span style="font-family:'微软雅黑';">        （</span>3）表示的原因</p> 
<p><span style="font-family:'微软雅黑';">            某些任务的真实假设可能不在当前学习算法所考虑的假设空间中，此时若使用单个学习器拟合效果一定不佳，而通过结合多个学习器，由于相应的假设空间有所扩大，则有可能学得更好的近似。</span></p> 
<p>             <img src="https://images2.imgbox.com/16/7c/twtJlPBd_o.png" alt=""></p> 
<p> </p> 
<p>        1、<span style="font-family:'微软雅黑';">平均法</span></p> 
<p><span style="font-family:'微软雅黑';">            用于</span>h<sub>i</sub><span style="font-family:'微软雅黑';">为连续型数据。</span></p> 
<p>            （1）<span style="font-family:'微软雅黑';">简单平均值法：<img src="https://images2.imgbox.com/1d/c8/bwdptuvm_o.png" alt=""></span></p> 
<p>            （2）<span style="font-family:'微软雅黑';">加权平均值：<img src="https://images2.imgbox.com/d3/c9/6tmYdcRP_o.png" alt="">，<img src="https://images2.imgbox.com/d2/b2/aRvyBxY8_o.png" alt=""></span></p> 
<p> </p> 
<p>        2、投票法</p> 
<p><span style="font-family:'微软雅黑';">            用于</span>h<sub>i</sub><span style="font-family:'微软雅黑';">为离散型数据，类别集合为</span>{c<sub>1</sub>,c<sub>2</sub>,...c<sub>N</sub>}，h<sub>i</sub><span style="font-family:'微软雅黑';">在样本</span>x上的预测输出表示为一个N维向量，h<sub>i</sub><sup>j</sup>(x)是h<sub>i</sub><span style="font-family:'微软雅黑';">在类别标记</span>c<sub>j</sub><span style="font-family:'微软雅黑';">上的输出。</span></p> 
<p>            （1）<span style="font-family:'微软雅黑';">绝对多数投票法：某标记得票超过半数，则预测为该标记；否则拒绝预测。</span></p> 
<p><span style="font-family:'微软雅黑';">                    <img src="https://images2.imgbox.com/9b/27/Ks3gemfJ_o.png" alt=""><br></span></p> 
<p>            （2）<span style="font-family:'微软雅黑';">相对多数投票法：预测为得票最多的标记，若同时有多个标记获最高票，则从中随机选取一个。</span></p> 
<p>                    <img src="https://images2.imgbox.com/e0/50/XPoKbjf4_o.png" alt=""></p> 
<p>            （3）<span style="font-family:'微软雅黑';">加权投票法：</span></p> 
<p>                    <img src="https://images2.imgbox.com/c4/fa/kQDHBCqm_o.png" alt="">    <img src="https://images2.imgbox.com/cb/b0/jRF7ZK9N_o.png" alt=""><br></p> 
<p> </p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1ef4d31d2323b3a6c9d87af34508fa13/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">时间片轮转法：平均周转时间</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0930c9889fdc92ae9b96e4b380e65557/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">基于Android平台的个人时间管理系统的设计与实现</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>