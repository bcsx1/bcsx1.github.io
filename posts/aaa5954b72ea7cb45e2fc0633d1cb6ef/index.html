<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>YOLOv5实现佩戴安全帽检测和识别(含佩戴安全帽数据集&#43;训练代码) - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="YOLOv5实现佩戴安全帽检测和识别(含佩戴安全帽数据集&#43;训练代码)" />
<meta property="og:description" content="YOLOv5实现佩戴安全帽检测和识别(含佩戴安全帽数据集&#43;训练代码) 目录
YOLOv5实现佩戴安全帽检测和识别(含佩戴安全帽数据集&#43;训练代码)
1. 前言
2. 佩戴安全帽检测和识别的方法
（1）基于目标检测的佩戴安全帽识别方法
（2）基于头部检测&#43;佩戴安全帽分类识别方法
3. 佩戴安全帽数据集说明
（1）佩戴安全帽数据集
（2）自定义数据集
4. 基于YOLOv5的佩戴安全帽模型训练
（1）YOLOv5安装
（2）准备Train和Test数据
（3）配置数据文件
（4）配置模型文件
（5）重新聚类Anchor（可选）
（6）开始训练
（7）可视化训练过程
（8）常见的错误
5. Python版本佩戴安全帽检测效果
6. Android版本佩戴安全帽检测效果（即将发布）
7.项目源码下载
1. 前言 安全帽是作业场所作业时头部防护所用的头部防护用品，它对使用者的头部在受坠落物或小型飞溅物体等其他因素引起的伤害起到防护作用。近年来，因不佩戴安全帽、不规范佩戴安全帽等原因导致的安全生产事故屡禁不止，事故发生背后的影响是巨大的，不仅为家人带来巨大的伤痛，也为企业的利益带来巨大的损失。而如何使员工规范佩戴安全帽、保障员工和企业双方利益，成为了一直以来各方坚持不懈想要实现的目标。因此，研究佩戴安全帽的监测算法，具有重大的意义和广泛的应用价值。
本篇博客，将手把手教你搭建一个基于YOLOv5的佩戴安全帽目标检测识别方法。目前，基于YOLOv5s的目标检测的佩戴安全帽识别方法的平均精度平均值mAP_0.5=0.93，mAP_0.5:0.95=0.63，基本满足业务的性能需求。
另外，为了能部署在手机Android平台上，本人对YOLOv5s进行了模型轻量化，开发了一个轻量级的版本yolov5s05_416和yolov5s05_320，在普通Android手机上可以达到实时的检测和识别效果，CPU(4线程)约30ms左右，GPU约25ms左右 ，基本满足业务的性能需求。
先展示一下Python版本佩戴安全帽检测和识别Demo效果（head表示(头部)未佩戴安全帽，helmet表示佩戴了安全帽）：
【源码下载】 YOLOv5实现佩戴安全帽检测和识别(含佩戴安全帽数据集&#43;训练代码)
【尊重原创，转载请注明出处】https://blog.csdn.net/guyuealian/article/details/127250780
还有Android版本的佩戴安全帽检测效果：
2. 佩戴安全帽检测和识别的方法 （1）基于目标检测的佩戴安全帽识别方法 基于目标检测的佩戴安全帽识别方法，一步到位，把佩戴安全帽类别直接当成多个目标检测的类别进行训练。
该方案采用one-stage的方法，直接端到端训练，任务简单，速度较快；新增类别或者数据，需要人工拉框标注佩戴安全帽的框，成本较大部署简单 （2）基于头部检测&#43;佩戴安全帽分类识别方法 该方法，先训练一个通用的头部检测模型(不区分是否佩戴安全帽)，然后外扩检测框并裁剪头部区域，再训练一个佩戴安全帽分类器，完成对佩戴安全帽的分类识别。当然一个更加简单的方法是，可以将人脸检测器当做头部检测器，即将人脸检测框外扩2倍左右，作为头部检测框的区域。只是这样做，若人背向，人脸检测器一般效果比较差，无法识别是否佩戴了安全帽。
该方案采用two-stage方法，可针对性分别提高检测模型和分类模型的性能头部检测模型不区分是否佩戴安全帽，只检测头部检测框，检测精度较高，佩戴安全帽分类模型可以做到很轻量由于采用two-stage方法进行检测-识别，因此速度相对较慢 考虑到佩戴安全帽检测和识别的任务比较简单，因此本博客将采用“基于目标检测的佩戴安全帽识别方法”。目标检测的的方法较多，比如Faster-RCNN，YOLO系列，SSD等均可以采用，本博客将采用YOLOv5进行佩戴安全帽目标检测模型训练。
3. 佩戴安全帽数据集说明 （1）佩戴安全帽数据集 目前收集了2W&#43;的佩戴安全帽数据集：
Helmet-Asian亚洲人佩戴安全帽数据集：总共有19000&#43;图片，VOC的XML数据格式，可直接用于目标检测模型训练。Helmet-Europe欧洲人佩戴安全帽数据集：总共有3000&#43;图片，VOC的XML数据格式，可直接用于目标检测模型训练。 关于佩戴安全帽数据集说明，详见另一篇博客说明：佩戴安全帽数据集使用说明和下载_PKing666666的博客-CSDN博客
（2）自定义数据集 如果需要增/删类别数据进行训练，或者需要自定数据集进行训练，可参考如下步骤：
采集图片，建议不少于200张图片使用Labelme等标注工具，对目标进行拉框标注：labelme工具：GitHub - wkentaro/labelme: Image Polygonal Annotation with Python (polygon, rectangle, circle, line, point and image-level flag annotation)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/aaa5954b72ea7cb45e2fc0633d1cb6ef/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-23T09:00:44+08:00" />
<meta property="article:modified_time" content="2023-04-23T09:00:44+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">YOLOv5实现佩戴安全帽检测和识别(含佩戴安全帽数据集&#43;训练代码)</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2 style="text-align:center;">YOLOv5实现佩戴安全帽检测和识别(含佩戴安全帽数据集+训练代码)</h2> 
<p></p> 
<p><strong>目录</strong></p> 
<p style="margin-left:0px;"><a href="#Fruit-Dataset%E6%B0%B4%E6%9E%9C%E6%95%B0%E6%8D%AE%E9%9B%86%2B%E6%B0%B4%E6%9E%9C%E5%88%86%E7%B1%BB%E8%AF%86%E5%88%AB%E8%AE%AD%E7%BB%83%E4%BB%A3%E7%A0%81%28%E6%94%AF%E6%8C%81googlenet%2C%20resnet%2C%20inception_v3%2C%20mobilenet_v2%29" rel="nofollow">YOLOv5实现佩戴安全帽检测和识别(含佩戴安全帽数据集+训练代码)</a></p> 
<p style="margin-left:40px;"><a href="#1.%20%E5%89%8D%E8%A8%80" rel="nofollow">1. 前言</a></p> 
<p style="margin-left:40px;"><a href="#1.%E6%88%B4%E5%8F%A3%E7%BD%A9%E8%AF%86%E5%88%AB%E7%9A%84%E6%96%B9%E6%B3%95" rel="nofollow">2. 佩戴安全帽检测和识别的方法</a></p> 
<p style="margin-left:80px;"><a href="#%EF%BC%881%EF%BC%89%E5%9F%BA%E4%BA%8E%E5%A4%9A%E7%B1%BB%E5%88%AB%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%9A%84%E6%88%B4%E5%8F%A3%E7%BD%A9%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95" rel="nofollow">（1）基于目标检测的佩戴安全帽识别方法</a></p> 
<p style="margin-left:80px;"><a href="#%EF%BC%882%EF%BC%89%E5%9F%BA%E4%BA%8E%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%2B%E6%88%B4%E5%8F%A3%E7%BD%A9%E5%88%86%E7%B1%BB%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95" rel="nofollow">（2）基于头部检测+佩戴安全帽分类识别方法</a></p> 
<p style="margin-left:40px;"><a href="#2.%20Fruit-Dataset%E6%B0%B4%E6%9E%9C%E6%95%B0%E6%8D%AE%E9%9B%86" rel="nofollow">3. 佩戴安全帽数据集说明</a></p> 
<p style="margin-left:80px;"><a href="#%EF%BC%881%EF%BC%89%20Fruit-Dataset" rel="nofollow">（1）佩戴安全帽数据集</a></p> 
<p style="margin-left:80px;"><a href="#%EF%BC%883%EF%BC%89%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86" rel="nofollow">（2）自定义数据集</a></p> 
<p style="margin-left:40px;"><a href="#4.%20%E5%9F%BA%E4%BA%8EYOLOv5%E7%9A%84%E6%89%8B%E5%8A%BF%E8%AF%86%E5%88%AB%E8%AE%AD%E7%BB%83" rel="nofollow">4. 基于YOLOv5的佩戴安全帽模型训练</a></p> 
<p style="margin-left:80px;"><a href="#%EF%BC%881%EF%BC%89%E9%A1%B9%E7%9B%AE%E6%A1%86%E6%9E%B6%E8%AF%B4%E6%98%8E" rel="nofollow">（1）YOLOv5安装</a></p> 
<p style="margin-left:80px;"><a href="#%EF%BC%881%EF%BC%89%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE" rel="nofollow">（2）准备Train和Test数据</a></p> 
<p style="margin-left:80px;"><a href="#%EF%BC%883%EF%BC%89%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%EF%BC%9Aconfig.yaml" rel="nofollow">（3）配置数据文件</a></p> 
<p style="margin-left:80px;"><a href="#%C2%A0%EF%BC%884%EF%BC%89%E6%A8%A1%E5%9E%8B%E9%85%8D%E7%BD%AE" rel="nofollow">（4）配置模型文件</a></p> 
<p style="margin-left:80px;"><a href="#%EF%BC%885%EF%BC%89%E9%87%8D%E6%96%B0%E8%81%9A%E7%B1%BBAnchor%EF%BC%88%E5%8F%AF%E9%80%89%EF%BC%89" rel="nofollow">（5）重新聚类Anchor（可选）</a></p> 
<p style="margin-left:80px;"><a href="#%EF%BC%886%EF%BC%89%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83" rel="nofollow">（6）开始训练</a></p> 
<p style="margin-left:80px;"><a href="#%EF%BC%883%EF%BC%89%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B" rel="nofollow">（7）可视化训练过程</a></p> 
<p style="margin-left:80px;"><a href="#%C2%A0%EF%BC%888%EF%BC%89%E5%B8%B8%E8%A7%81%E7%9A%84%E9%94%99%E8%AF%AF" rel="nofollow">（8）常见的错误</a></p> 
<p style="margin-left:40px;"><a href="#4.%20%E5%9E%83%E5%9C%BE%E5%88%86%E7%B1%BB%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%E6%B5%8B%E8%AF%95%E6%95%88%E6%9E%9C" rel="nofollow">5. Python版本佩戴安全帽检测效果</a></p> 
<p style="margin-left:40px;"><a href="#5.Android%E6%89%8B%E5%8A%BF%E8%AF%86%E5%88%AB" rel="nofollow">6. Android版本佩戴安全帽检测效果（即将发布）</a></p> 
<p style="margin-left:40px;"><a href="#5.%E9%A1%B9%E7%9B%AE%E6%BA%90%E7%A0%81%E4%B8%8B%E8%BD%BD" rel="nofollow">7.项目源码下载</a></p> 
<hr> 
<h3>1. 前言</h3> 
<p>安全帽是作业场所作业时头部防护所用的头部防护用品，它对使用者的头部在受坠落物或小型飞溅物体等其他因素引起的伤害起到防护作用。近年来，因不佩戴安全帽、不规范佩戴安全帽等原因导致的安全生产事故屡禁不止，事故发生背后的影响是巨大的，不仅为家人带来巨大的伤痛，也为企业的利益带来巨大的损失。而如何使员工规范佩戴安全帽、保障员工和企业双方利益，成为了一直以来各方坚持不懈想要实现的目标。因此，研究佩戴安全帽的监测算法，具有重大的意义和广泛的应用价值。</p> 
<p>本篇博客，将手把手教你搭建一个基于YOLOv5的佩戴安全帽目标检测识别方法。目前，基于YOLOv5s的目标检测的佩戴安全帽识别方法的平均精度平均值mAP_0.5=0.93，mAP_0.5:0.95=0.63，基本满足业务的性能需求。</p> 
<blockquote> 
 <p>另外，为了能部署在手机Android平台上，本人对YOLOv5s进行了模型轻量化，开发了一个轻量级的版本yolov5s05_416和yolov5s05_320，在普通Android手机上可以达到实时的检测和识别效果，CPU(4线程)约30ms左右，GPU约25ms左右 ，基本满足业务的性能需求。</p> 
</blockquote> 
<p>先展示一下Python版本佩戴安全帽检测和识别Demo效果（head表示(头部)未佩戴安全帽，helmet表示佩戴了安全帽）：</p> 
<table border="1" cellpadding="1" cellspacing="1"><tbody><tr><td><img alt="2a3893ba2e6643c6892e2f1997121e53.gif" src="https://images2.imgbox.com/3f/be/2ra5tZjO_o.gif"></td><td><img alt="86dda151e01f4d529a2d14dfa096cf7c.jpeg" src="https://images2.imgbox.com/05/a5/L953YN1d_o.jpg"></td></tr></tbody></table> 
<p>【源码下载】 <a href="https://mp.weixin.qq.com/s/7bKD5DFu92Ba_CcH99nHvg" rel="nofollow" title="YOLOv5实现佩戴安全帽检测和识别(含佩戴安全帽数据集+训练代码)">YOLOv5实现佩戴安全帽检测和识别(含佩戴安全帽数据集+训练代码)</a></p> 
<p>【尊重原创，转载请注明出处】<a href="https://blog.csdn.net/guyuealian/article/details/127250780" title="https://blog.csdn.net/guyuealian/article/details/127250780">https://blog.csdn.net/guyuealian/article/details/127250780</a></p> 
<p>还有Android版本的佩戴安全帽检测效果：</p> 
<p><img alt="88154a19d1f640ceb914458a8a9dcb37.gif" src="https://images2.imgbox.com/ce/a4/O8Bd4GB5_o.gif"></p> 
<hr> 
<p></p> 
<h3>2. 佩戴安全帽检测和识别的方法</h3> 
<h4>（1）基于目标检测的佩戴安全帽识别方法</h4> 
<p>基于目标检测的佩戴安全帽识别方法，一步到位，把佩戴安全帽类别直接当成多个目标检测的类别进行训练。</p> 
<blockquote> 
 <ol><li>该方案采用one-stage的方法，直接端到端训练，任务简单，速度较快；</li><li>新增类别或者数据，需要人工拉框标注佩戴安全帽的框，成本较大</li><li>部署简单</li></ol> 
</blockquote> 
<h4>（2）基于头部检测+佩戴安全帽分类识别方法</h4> 
<p>该方法，先训练一个通用的头部检测模型(不区分是否佩戴安全帽)，然后外扩检测框并裁剪头部区域，再训练一个佩戴安全帽分类器，完成对佩戴安全帽的分类识别。当然一个更加简单的方法是，可以将人脸检测器当做头部检测器，即将人脸检测框外扩2倍左右，作为头部检测框的区域。只是这样做，若人背向，人脸检测器一般效果比较差，无法识别是否佩戴了安全帽。</p> 
<blockquote> 
 <ol><li>该方案采用two-stage方法，可针对性分别提高检测模型和分类模型的性能</li><li>头部检测模型不区分是否佩戴安全帽，只检测头部检测框，检测精度较高，</li><li>佩戴安全帽分类模型可以做到很轻量</li><li>由于采用two-stage方法进行检测-识别，因此速度相对较慢</li></ol> 
</blockquote> 
<p>考虑到佩戴安全帽检测和识别的任务比较简单，因此本博客将采用“<strong>基于目标检测的佩戴安全帽识别方法</strong>”。目标检测的的方法较多，比如Faster-RCNN，YOLO系列，SSD等均可以采用，本博客将采用YOLOv5进行佩戴安全帽目标检测模型训练。</p> 
<hr> 
<h3>3. 佩戴安全帽数据集说明</h3> 
<h4>（1）佩戴安全帽数据集</h4> 
<p>目前收集了2W+的佩戴安全帽数据集：</p> 
<blockquote> 
 <ul><li>Helmet-Asian亚洲人佩戴安全帽数据集：总共有19000+图片，VOC的XML数据格式，可直接用于目标检测模型训练。</li><li>Helmet-Europe欧洲人佩戴安全帽数据集：总共有3000+图片，VOC的XML数据格式，可直接用于目标检测模型训练。</li></ul> 
</blockquote> 
<p>关于佩戴安全帽数据集说明，详见另一篇博客说明：<a href="https://panjinquan.blog.csdn.net/article/details/127331580" rel="nofollow" title="佩戴安全帽数据集使用说明和下载_PKing666666的博客-CSDN博客">佩戴安全帽数据集使用说明和下载_PKing666666的博客-CSDN博客</a></p> 
<h4>（2）自定义数据集</h4> 
<p>如果需要增/删类别数据进行训练，或者需要自定数据集进行训练，可参考如下步骤：</p> 
<blockquote> 
 <ol><li>采集图片，建议不少于200张图片</li><li>使用Labelme等标注工具，对目标进行拉框标注：labelme工具：<a href="https://github.com/wkentaro/labelme" title="GitHub - wkentaro/labelme: Image Polygonal Annotation with Python (polygon, rectangle, circle, line, point and image-level flag annotation).">GitHub - wkentaro/labelme: Image Polygonal Annotation with Python (polygon, rectangle, circle, line, point and image-level flag annotation).</a></li><li>将标注格式转换为VOC数据格式，参考工具：<a href="https://github.com/wkentaro/labelme/blob/main/examples/bbox_detection/labelme2voc.py" title="labelme/labelme2voc.py at main · wkentaro/labelme · GitHub">labelme/labelme2voc.py at main · wkentaro/labelme · GitHub</a></li><li>生成训练集train.txt和验证集val.txt文件列表</li><li>修改engine/configs/voc_local.yaml的train和val的数据路径</li><li>重新开始训练</li></ol> 
</blockquote> 
<p><img alt="da93bc711ce94c209968df81fbcc5ff4.png" src="https://images2.imgbox.com/d0/08/zJUYOdSe_o.png"></p> 
<hr> 
<h3>4. 基于YOLOv5的佩戴安全帽模型训练</h3> 
<h4>（1）YOLOv5安装</h4> 
<p>训练Pipeline采用YOLOv5: <a href="https://github.com/ultralytics/yolov5" title="https://github.com/ultralytics/yolov5">https://github.com/ultralytics/yolov5</a> , 原始代码训练需要转换为YOLO的格式，不支持VOC的数据格式。为了适配VOC数据，本人新增了LoadVOCImagesAndLabels用于解析VOC数据集进行训练。另外，为了方便测试，还增加demo.py文件，可支持对图片,视频和摄像头的测试。</p> 
<p>Python依赖环境，使用pip安装即可：</p> 
<pre><code class="language-python">
matplotlib&gt;=3.2.2
numpy&gt;=1.18.5
opencv-python&gt;=4.1.2
Pillow
PyYAML&gt;=5.3.1
scipy&gt;=1.4.1
torch&gt;=1.7.0
torchvision&gt;=0.8.1
tqdm&gt;=4.41.0
tensorboard&gt;=2.4.1
seaborn&gt;=0.11.0
pandas
thop  # FLOPs computation
pybaseutils
</code></pre> 
<p> 项目安装教程请参考（<strong>初学者入门，麻烦先看完下面教程，配置好开发环境</strong>）：</p> 
<blockquote> 
 <ul><li><a href="https://blog.csdn.net/guyuealian/article/details/129163343" title="项目开发使用教程和常见问题和解决方法">项目开发使用教程和常见问题和解决方法</a></li><li><strong>视频教程：</strong><a href="https://www.bilibili.com/video/BV1sY411c7JS/?spm_id_from=333.999.0.0" rel="nofollow" title="1 手把手教你安装CUDA和cuDNN(1)">1 手把手教你安装CUDA和cuDNN(1)</a></li><li><strong>视频教程：</strong><a href="https://www.bilibili.com/video/BV1q5411d7GD/?spm_id_from=333.999.0.0" rel="nofollow" title="2 手把手教你安装CUDA和cuDNN(2)">2 手把手教你安装CUDA和cuDNN(2)</a></li><li><strong>视频教程：</strong><a href="https://www.bilibili.com/video/BV18Y411c7Qt/?spm_id_from=333.999.0.0" rel="nofollow" title="3 如何用Anaconda创建pycharm环境">3 如何用Anaconda创建pycharm环境</a></li><li><strong>视频教程：</strong><a href="https://www.bilibili.com/video/BV1SY4y1z7eS/?spm_id_from=333.999.0.0" rel="nofollow" title="4 如何在pycharm中使用Anaconda创建的python环境">4 如何在pycharm中使用Anaconda创建的python环境</a></li></ul> 
</blockquote> 
<h4>（2）准备Train和Test数据</h4> 
<p>下载Helmet-Asian和Helmet-Europe佩戴安全帽数据集，总共约2W+的图片</p> 
<table border="1" cellpadding="1" cellspacing="1"><tbody><tr><td style="text-align:center;"><strong>Helmet-Asian</strong></td><td style="text-align:center;"><strong>Helmet-Europe</strong></td></tr><tr><td><img alt="a362d7f4bf2c4b47baf68cbbeeeed1e3.png" src="https://images2.imgbox.com/91/5a/VKeTe1A4_o.png"></td><td><img alt="b7295bb426ec42e6b1a4e0280c40a231.png" src="https://images2.imgbox.com/4d/73/eGYNMgLp_o.png"></td></tr></tbody></table> 
<p></p> 
<h4>（3）配置数据文件</h4> 
<ul><li>修改训练和测试数据的路径：engine/configs/voc_local.yaml</li><li><span style="color:#fe2c24;">注意数据路径分隔符使用【/】,不是【\】</span></li><li><span style="color:#fe2c24;">项目不要出现含有中文字符的目录文件或路径，否则会出现很多异常！</span></li></ul> 
<pre><code class="language-bash"># Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
# 数据路径
path: ""  # dataset root dir
train:
  - "D:/path/to/helmet/Helmet-Asian/trainval.txt"
  - "D:/path/to/helmet/Helmet-Europe/trainval.txt"

val:
  - "D:/path/to/helmet/Helmet-Asian/test.txt"

test:  # test images (optional)
data_type: voc

# Classes
nc: 2  # number of classes
names: { 'head': 0, "helmet": 1 }
</code></pre> 
<h4>（4）配置模型文件</h4> 
<p>官方YOLOv5给出了YOLOv5l,YOLOv5m,YOLOv5s等模型。考虑到手机端CPU/GPU性能比较弱鸡，直接部署yolov5s运行速度十分慢。所以本人在yolov5s基础上进行模型轻量化处理，即将yolov5s的模型的channels通道数全部都减少一半，并且模型输入由原来的640×640降低到416×416或者320×320，该轻量化的模型我称之为<strong>yolov5s05</strong>。从性能来看，<strong>yolov5s05比yolov5s快5多倍，而mAP下降了9%（</strong>0.93→0.84<strong>），对于手机端，这精度勉强可以接受。</strong></p> 
<p>下面是yolov5s05和yolov5s的参数量和计算量对比：</p> 
<table align="left" border="1" cellpadding="1" cellspacing="1"><tbody><tr><td><strong>模型</strong></td><td><strong>input-size</strong></td><td><strong>params(M)</strong></td><td><strong>GFLOPs</strong></td><td><strong>mAP0.5</strong></td></tr><tr><td><strong>yolov5s</strong></td><td>640×640</td><td>7.2</td><td>16.5</td><td> <p>0.93067</p> </td></tr><tr><td><strong>yolov5s05</strong></td><td>416×416</td><td>1.7</td><td>1.8</td><td> <p>0.87051</p> </td></tr><tr><td><strong>yolov5s05</strong></td><td>320×320</td><td>1.7</td><td>1.1</td><td> <p>0.84477</p> </td></tr></tbody></table> 
<p></p> 
<p></p> 
<p></p> 
<h4>（5）重新聚类Anchor（可选）</h4> 
<p>官方yolov5s的Anchor是基于COCO数据集进行聚类获得（详见models/yolov5s.yaml文件）</p> 
<p> <img alt="9a26d4fcecb443948888df947ce7f02a.png" src="https://images2.imgbox.com/3d/6e/9h3bN13O_o.png"> </p> 
<p>对于yolov5s05的Anchor，由于输入大小640缩小到320，其对应的Anchor也应该缩小一倍：</p> 
<p> <img alt="a7a01dc39f4e42e79a98a2d1115cce8d.png" src="https://images2.imgbox.com/1b/95/dC91ESZH_o.png"></p> 
<blockquote> 
 <p><strong>一点建议：</strong></p> 
 <ul><li>官方yolov5s的Anchor是基于COCO数据集进行聚类获得，不同数据集需要做适当的调整，其最优Anchor建议重新进行聚类 。</li><li>当然你要是觉得麻烦就跳过，不需要重新聚类Anchor，这个影响不是特别大。如果你需要重新聚类，请参考engine/kmeans_anchor/demo.py文件</li></ul> 
</blockquote> 
<h4><strong>（6）开始训练</strong></h4> 
<p>整套训练代码非常简单操作，填写好对应的数据路径，即可开始训练了。</p> 
<blockquote> 
 <ul><li>修改训练超参文件： data/hyps/hyp.scratch-v1.yaml （可以修改训练学习率，数据增强等方式，使用默认即可）</li><li>Linux系统终端运行，训练yolov5s或轻量化版本yolov5s05_416或者yolov5s05_320 (选择其中一个训练即可)</li></ul> 
</blockquote> 
<pre><code class="language-bash">#!/usr/bin/env bash

#--------------训练yolov5s--------------
# 输出项目名称路径
project="runs/yolov5s_640"
# 训练和测试数据的路径
data="engine/configs/voc_local.yaml"
# YOLOv5模型配置文件
cfg="yolov5s.yaml"
# 训练超参数文件
hyp="data/hyps/hyp.scratch-v1.yaml"
# 预训练文件
weights="engine/pretrained/yolov5s.pt"
python train.py --data $data --cfg $cfg --hyp $hyp --weights $weights --batch-size 64 --imgsz 640 --workers 12 --project $project


#--------------训练轻量化版本yolov5s05_416--------------
# 输出项目名称路径
project="runs/yolov5s05_416"
# 训练和测试数据的路径
data="engine/configs/voc_local.yaml"
# YOLOv5模型配置文件
cfg="yolov5s05_416.yaml"
# 训练超参数文件
hyp="data/hyps/hyp.scratch-v1.yaml"
# 预训练文件
weights="engine/pretrained/yolov5s.pt"
python train.py --data $data --cfg $cfg --hyp $hyp --weights $weights --batch-size 64 --imgsz 416 --workers 12 --project $project


#--------------训练轻量化版本yolov5s05_320--------------
# 输出项目名称路径
project="runs/yolov5s05_320"
# 训练和测试数据的路径
data="engine/configs/voc_local.yaml"
# YOLOv5模型配置文件
cfg="yolov5s05_320.yaml"
# 训练超参数文件
hyp="data/hyps/hyp.scratch-v1.yaml"
# 预训练文件
weights="engine/pretrained/yolov5s.pt"
python train.py --data $data --cfg $cfg --hyp $hyp --weights $weights --batch-size 64 --imgsz 320 --workers 12 --project $project

</code></pre> 
<ul><li>Windows系统终端运行，训练yolov5s或轻量化版本yolov5s05_416或者yolov5s05_320 (选择其中一个训练即可)</li></ul> 
<pre><code class="language-bash">#!/usr/bin/env bash

#--------------训练yolov5s--------------
python train.py --data engine/configs/voc_local.yaml --cfg yolov5s.yaml --hyp data/hyps/hyp.scratch-v1.yaml --weights engine/pretrained/yolov5s.pt --batch-size 64 --imgsz 640 --workers 12 --project runs/yolov5s_640



#--------------训练轻量化版本yolov5s05_416--------------
python train.py --data engine/configs/voc_local.yaml --cfg yolov5s05_416.yaml --hyp data/hyps/hyp.scratch-v1.yaml --weights engine/pretrained/yolov5s.pt --batch-size 64 --imgsz 416 --workers 12 --project runs/yolov5s05_416



#--------------训练轻量化版本yolov5s05_320--------------
python train.py --data engine/configs/voc_local.yaml --cfg yolov5s05_320.yaml --hyp data/hyps/hyp.scratch-v1.yaml --weights engine/pretrained/yolov5s.pt --batch-size 64 --imgsz 320 --workers 12 --project runs/yolov5s05_320
</code></pre> 
<ul><li>开始训练：</li></ul> 
<p><img alt="8d3f32b9f55d4f77831d932c3394b216.png" src="https://images2.imgbox.com/9d/2f/MUHpoY5q_o.png"></p> 
<blockquote> 
 <p>训练完成，可以得到yolov5s佩戴安全帽识别的mAP指标大约mAP_0.5=0.93；而</p> 
 <p>yolov5s05_416 mAP_0.5=0.87左右；yolov5s05_320 mAP_0.5=0.84左右</p> 
</blockquote> 
<h4>（7）可视化训练过程</h4> 
<pre>训练过程可视化工具是使用Tensorboard，使用方法，在终端输入：</pre> 
<pre><code class="language-bash"># 基本方法
tensorboard --logdir=path/to/log/
# 例如
tensorboard --logdir ./runs/yolov5s_640 </code></pre> 
<table align="left" border="1" cellpadding="1" cellspacing="1"><tbody><tr><td colspan="2"><img alt="fc76ae69486c4a18835630ded98c5c7c.png" src="https://images2.imgbox.com/ee/54/CVDNZBAT_o.png"></td></tr><tr><td colspan="2"><img alt="f782c25ecae242b2a6151d59ecb9717a.png" src="https://images2.imgbox.com/ef/c1/7fsg9B7Z_o.png"></td></tr><tr><td colspan="2"><img alt="17dc1ca003e84315bdbec3949ed48ff7.png" src="https://images2.imgbox.com/57/50/QjKgEzyG_o.png"></td></tr><tr><td colspan="2"><img alt="2e4b2f8357bd41b9b026dd52a6cf3898.png" src="https://images2.imgbox.com/05/b0/2bt00UqP_o.png"></td></tr></tbody></table> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p>当然，在输出目录，也保存很多性能指标的图片</p> 
<ul><li>这是训练epoch的可视化图，可以看到mAP随着Epoch训练，逐渐提高</li></ul> 
<p><img alt="ab08c6ff27b1456bbfa2b41b8b5fcffd.png" src="https://images2.imgbox.com/62/2d/75LjDNps_o.png"></p> 
<ul><li>这是每个类别的F1-Score分数</li></ul> 
<p><img alt="d52651cf04b64649b918fa93fc2610cf.png" src="https://images2.imgbox.com/de/d8/TM1U3MCe_o.png"></p> 
<ul><li>这是模型的PR曲线</li></ul> 
<p><img alt="04136a31e3694e4aa492284f95b4c0bf.png" src="https://images2.imgbox.com/75/6f/J8s5uEvL_o.png"></p> 
<ul><li>这是混淆矩阵：</li></ul> 
<p><img alt="64e74438407249478a55acf42fd081c7.png" src="https://images2.imgbox.com/8d/6e/tslOauLx_o.png"></p> 
<h4>（8）常见的错误</h4> 
<ul><li><a href="https://blog.csdn.net/guyuealian/article/details/128327221" title="YOLOv5 BUG修复记录">YOLOv5 BUG修复记录</a></li><li> 项目安装教程请参考：<a href="https://blog.csdn.net/guyuealian/article/details/129163343" title="项目开发使用教程和常见问题和解决方法">项目开发使用教程和常见问题和解决方法</a></li><li><strong>项目不要出现含有中文字符的目录文件或路径，否则会出现很多异常</strong>！！！！！！！！</li></ul> 
<hr> 
<h3>5. Python版本佩戴安全帽检测效果</h3> 
<p>demo.py文件用于推理和测试模型的效果，填写好配置文件，模型文件以及测试图片即可运行测试了</p> 
<ul><li>测试图片</li></ul> 
<pre><code class="language-bash"># 测试图片(Linux系统)
image_dir='data/helmet-test' # 测试图片的目录
weights="runs/model/yolov5s_640/weights/best.pt" # 模型文件
out_dir="runs/helmet-result" # 保存检测结果
python demo.py --image_dir $image_dir --weights $weights --out_dir $out_dir</code></pre> 
<p> Windows系统，请将$image_dir， $weights ，$out_dir等变量代替为对应的变量值即可，如</p> 
<pre><code class="language-bash"># 测试图片(Windows系统)
python demo.py --image_dir data/helmet-test --weights runs/model/yolov5s_640/weights/best.pt --out_dir runs/helmet-result
</code></pre> 
<ul><li>测试视频文件</li></ul> 
<pre><code class="language-bash"># 测试视频文件(Linux系统)
video_file="data/helmet-test.mp4" # path/to/video.mp4 测试视频文件，如*.mp4,*.avi等
weights="runs/model/yolov5s_640/weights/best.pt" # 模型文件
out_dir="runs/helmet-result" # 保存检测结果
python demo.py --video_file $video_file --weights $weights --out_dir $out_dir</code></pre> 
<pre><code class="language-bash"># 测试视频文件(Windows系统)
python demo.py --video_file data/helmet-test.mp4 --weights runs/model/yolov5s_640/weights/best.pt --out_dir runs/helmet-result
</code></pre> 
<p></p> 
<ul><li> 测试摄像头</li></ul> 
<pre><code class="language-bash"># 测试摄像头(Linux系统)
video_file=0 # 测试摄像头ID
weights="runs/model/yolov5s_640/weights/best.pt" # 模型文件
out_dir="runs/helmet-result" # 保存检测结果
python demo.py --video_file $video_file --weights $weights --out_dir $out_dir
</code></pre> 
<pre><code class="language-bash"># 测试摄像头(Windows系统)
python demo.py --video_file 0 --weights runs/model/yolov5s_640/weights/best.pt --out_dir runs/helmet-result
</code></pre> 
<p> 先展示一下Python版本佩戴安全帽检测和识别Demo效果（head表示(头部)未佩戴安全帽，helmet表示佩戴了安全帽）：</p> 
<table border="1" cellpadding="1" cellspacing="1"><tbody><tr><td><img alt="2a3893ba2e6643c6892e2f1997121e53.gif" src="https://images2.imgbox.com/71/9c/lMpjK9KO_o.gif"></td><td><img alt="c88c17bb66e3422caee42be7fd6aa9e4.jpeg" src="https://images2.imgbox.com/c7/30/HgU90JcB_o.jpg"></td></tr><tr><td><img alt="a977d91bd9f64ebabfd83469a75198e3.jpeg" src="https://images2.imgbox.com/90/57/EkUTQfHr_o.jpg"></td><td><img alt="2f7ec06c80e842e1ad5a819c0c4a2c12.jpeg" src="https://images2.imgbox.com/03/91/KKcKqqtW_o.jpg"></td></tr></tbody></table> 
<p></p> 
<p><img alt="3129504502cb4dfdb5970d48d5cc84e8.jpeg" src="https://images2.imgbox.com/a7/98/h6Ontl4Y_o.jpg"></p> 
<p>如果想进一步提高模型的性能，可以尝试：</p> 
<blockquote> 
 <ol><li>​增加训练的样本数据： 目前只有2W+的数据量，<strong>建议根据自己的业务场景，采集相关数据，提高模型泛化能力</strong></li><li>使用参数量更大的模型： 本教程使用的YOLOv5s，其参数量才7.2M，而YOLOv5x的参数量有86.7M，理论上其精度更高，但推理速度也较慢。</li><li>尝试不同数据增强的组合进行训练</li></ol> 
</blockquote> 
<hr> 
<h3>6. Android版本佩戴安全帽检测效果（即将发布）</h3> 
<p>已经完成Android版本的佩戴安全帽目标检测识别模型算法开发，APP在普通Android手机上可以达到实时的检测和识别效果，CPU(4线程)约30ms左右，GPU约25ms左右 ，基本满足业务的性能需求。</p> 
<p>Android版本佩戴安全帽识别：<a href="https://blog.csdn.net/guyuealian/article/details/127345231" title="https://blog.csdn.net/guyuealian/article/details/127345231">https://blog.csdn.net/guyuealian/article/details/127345231</a></p> 
<p> Android Demo效果：</p> 
<table border="1" cellpadding="1" cellspacing="1"><tbody><tr><td><img alt="84f3d8c2fb6744ec9f38da816c57e345.jpeg" src="https://images2.imgbox.com/54/ed/hQ8CNtIc_o.jpg"></td><td><img alt="040202d9e45646c2ad4bc0266e82c597.jpeg" src="https://images2.imgbox.com/74/fd/R7gFAA19_o.jpg"></td><td><img alt="fc715181c9884a6e861aec5bb6ffd2f5.gif" src="https://images2.imgbox.com/59/01/fzpqEmRT_o.gif"></td></tr></tbody></table> 
<p>【Android APP体验】<a href="https://download.csdn.net/download/guyuealian/86768318" title="https://download.csdn.net/download/guyuealian/86768318">https://download.csdn.net/download/guyuealian/86768318</a></p> 
<hr> 
<h3>7.项目源码下载</h3> 
<p>整套项目源码内容包含<strong>佩戴安全帽数据集</strong> +<strong> YOLOv5训练代码和测试代码</strong></p> 
<p>项目源码下载：<a href="https://mp.weixin.qq.com/s/7bKD5DFu92Ba_CcH99nHvg" rel="nofollow" title="YOLOv5实现佩戴安全帽检测和识别(含佩戴安全帽数据集+训练代码)">YOLOv5实现佩戴安全帽检测和识别(含佩戴安全帽数据集+训练代码)</a></p> 
<p><strong>（1）佩戴安全帽数据集,总共2W+图片</strong></p> 
<blockquote> 
 <ol><li>Helmet-Asian亚洲人佩戴安全帽数据集：总共有19000+图片，VOC的XML数据格式，可直接用于目标检测模型训练。</li><li>Helmet-Europe欧洲人佩戴安全帽数据集：总共有3000+图片，VOC的XML数据格式，可直接用于目标检测模型训练。</li></ol> 
</blockquote> 
<p>关于数据详细说明：<a href="https://blog.csdn.net/guyuealian/article/details/127331580" title="佩戴安全帽数据集使用说明和下载">佩戴安全帽数据集使用说明和下载</a></p> 
<p><strong>（2）YOLOv5训练代码和测试代码（Pytorch）</strong></p> 
<blockquote> 
 <ol><li>整套YOLOv5项目工程，含训练代码train.py和测试代码demo.py</li><li>支持高精度版本yolov5s训练和测试</li><li>支持轻量化版本yolov5s05_320和yolov5s05_416训练和测试</li><li>根据本篇博文说明，简单配置即可开始训练：train.py</li><li>源码包含了训练好的yolov5s,yolov5s05_416和yolov5s05_320模型，配置好环境，可直接运行demo.py</li><li>测试代码demo.py支持图片，视频和摄像头测试</li></ol> 
</blockquote> 
<p></p> 
<p></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f1007ed258768f88b28aa88be5f0c431/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">阿里云物联网平台使用指南</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/25833f6e6527cd3d878c800bfe47f9ec/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">SQL Server 高可用方案介绍</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>