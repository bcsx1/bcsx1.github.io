<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【百度PARL】强化学习笔记 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【百度PARL】强化学习笔记" />
<meta property="og:description" content="文章目录 强化学习基本知识一些框架Value-based的方法Q表格举个例子 强化的概念TD更新 Sarsa算法SampleSarsa Agent类 On_policy vs off_policy函数逼近与神经网络DQN算法DQN创新点DQN代码实现model.pyalgorithm.pyagent.py总结：举个例子 实战 视频：世界冠军带你从零实践强化学习
代码：github仓库
因项目需要，这系列课程只学到了DQN。本人首先先学习了李宏毅的policy-based的课程，然后再学习这里百度飞桨科科老师的强化学习课程，主要学习了value-based的内容。科科老师这里对代码逻辑的讲解更加清晰，非常的好。
强化学习基本知识 算法库 一些框架 PARL 对于一个新的example，只需要修改一下agent/model就可以了算法在parl文件夹中也将所有算法定义好了 第一部分总结 Value-based的方法 下图的过程是符合马尔科夫决策过程的，俗称MDP
如果状态转移概率和reward都是已知的，那么就称这个环境是已知的 model-based P函数和R函数已知可以直接用动态规划求解 model-free P函数和R函数未知试错探索，现实世界的环境往往未知我们主要学习这个用Q函数和V函数来表示 Q表格 反应在某个s下，哪个动作价值高
Q表格：指导每一个Step的动作选择，目标导向：未来的总收益
我们的收益要看的更远一些
但是有时候看的太远也不好，所以引入衰减因子 γ \gamma γ
举个例子 折扣因子 我们就是要求解Q表格 刚开始全部初始化为0，当足够多的与环境交互之后，Q表格就会更新足够完善 强化的概念 时序差分
主要特点是在估计当前策略的价值函数时，它不需要等到一个完整的序列（如一局游戏）结束后才更新价值估计，而是在每一步之后立即进行更新李宏毅讲过 在不断的重复试验之后，原本是要看到熊发怒才会瑟瑟发抖，不断试验之后，看到有熊爪就会瑟瑟发抖
意味着agent学会了预测熊发怒这一状态的价值，并将这种预期的负面价值向前传播到先前的状态（熊爪）。这种向前传播的过程是通过Temporal Difference Error来完成的，这个错误是实际奖励和智能体预测的未来奖励之间的差异。智能体使用这个TD错误来更新其关于当前状态和动作的价值估计，使得未来的决策更加准确。 下一个状态的价值，是可以不断强化影响上一个状态的价值
下一个状态的价值只与当前状态有关，历史的状态已经融合到当前状态 状态价值迭代 demohttps://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_td.html TD更新 拿下一步的Q值去更新这一步的Q值
刚开始 Q ( S t , A t ) Q(S_t,A_t) Q(St​,At​)初始化为0，其要去逼近Target、也就是未来收益之和 G t G_t Gt​。在做一个简单的数学变换我们可以发现 G t G_t Gt​ = R t &#43; 1 &#43; γ G t &#43; 1 R_{t&#43;1}&#43;{\gamma}G_{t&#43;1} Rt&#43;1​&#43;γGt&#43;1​因为 Q ( S t , A t ) Q(S_t,A_t) Q(St​,At​)要逼近 G t G_t Gt​所以差不多 Q ( S t &#43; 1 , A t &#43; 1 ) Q(S_{t&#43;1},A_{t&#43;1}) Q(St&#43;1​,At&#43;1​)要逼近 G t &#43; 1 G_{t&#43;1} Gt&#43;1​ α：学习率，决定了新信息覆盖旧信息的速度当前的Q值会向目标Q值逼近，而目标Q值是基于智能体获得的实际奖励和下一个状态-动作对的预期Q值计算得来的。右侧的图表示了状态和动作之间的转移，以及如何更新Q值。每次智能体在状态 ( $S_t $) 下采取动作 ( A t A_t At​ )，都会转移到新的状态 ( $S_{t&#43;1} KaTeX parse error: Can&#39;t use function &#39;\)&#39; in math mode at position 1: \̲)̲ 并采取新的动作 \( A_{t&#43;1} $)，同时接收奖励 ( $R_{t&#43;1} $)，然后基于这些信息来更新Q值。 所谓的软更新其实像一种误差，表示预期（即时奖励加上对下一状态的Q值的估计）与当前估计之间的差异" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/9834495cb8d3ecd47f262fc3bb2050a1/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-18T23:28:28+08:00" />
<meta property="article:modified_time" content="2023-12-18T23:28:28+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【百度PARL】强化学习笔记</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_5" rel="nofollow">强化学习基本知识</a></li><li><a href="#_19" rel="nofollow">一些框架</a></li><li><a href="#Valuebased_31" rel="nofollow">Value-based的方法</a></li><li><ul><li><a href="#Q_52" rel="nofollow">Q表格</a></li><li><ul><li><a href="#_68" rel="nofollow">举个例子</a></li></ul> 
   </li><li><a href="#_79" rel="nofollow">强化的概念</a></li><li><a href="#TD_99" rel="nofollow">TD更新</a></li></ul> 
  </li><li><a href="#Sarsa_134" rel="nofollow">Sarsa算法</a></li><li><ul><li><a href="#Sample_146" rel="nofollow">Sample</a></li><li><ul><li><a href="#Sarsa_Agent_165" rel="nofollow">Sarsa Agent类</a></li></ul> 
  </li></ul> 
  </li><li><a href="#On_policy_vs_off_policy_181" rel="nofollow">On_policy vs off_policy</a></li><li><a href="#_208" rel="nofollow">函数逼近与神经网络</a></li><li><a href="#DQN_229" rel="nofollow">DQN算法</a></li><li><ul><li><a href="#DQN_231" rel="nofollow">DQN创新点</a></li><li><a href="#DQN_290" rel="nofollow">DQN代码实现</a></li><li><ul><li><a href="#modelpy_292" rel="nofollow">model.py</a></li><li><a href="#algorithmpy_319" rel="nofollow">algorithm.py</a></li><li><a href="#agentpy_422" rel="nofollow">agent.py</a></li><li><a href="#_446" rel="nofollow">总结：举个例子</a></li></ul> 
   </li><li><a href="#_484" rel="nofollow">实战</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<blockquote> 
 <p>视频：<a href="https://www.bilibili.com/video/BV1yv411i7xd/" rel="nofollow">世界冠军带你从零实践强化学习</a><br> 代码：<a href="https://github.com/PaddlePaddle/PARL">github仓库</a><br> 因项目需要，这系列课程只学到了DQN。本人首先先学习了李宏毅的policy-based的课程，然后再学习这里百度飞桨科科老师的强化学习课程，主要学习了value-based的内容。科科老师这里对代码逻辑的讲解更加清晰，非常的好。</p> 
</blockquote> 
<h2><a id="_5"></a>强化学习基本知识</h2> 
<p><img src="https://images2.imgbox.com/34/2b/wx4IatRE_o.jpg" alt=""></p> 
<p><img src="https://images2.imgbox.com/ca/1b/oxdZ7Lo3_o.jpg" alt=""></p> 
<ul><li>算法库</li></ul> 
<p><img src="https://images2.imgbox.com/ea/d1/ixdEDdH7_o.jpg" alt=""></p> 
<h2><a id="_19"></a>一些框架</h2> 
<ul><li>PARL 
  <ul><li>对于一个新的example，只需要修改一下agent/model就可以了</li><li>算法在parl文件夹中也将所有算法定义好了</li></ul> </li></ul> 
<p><img src="https://images2.imgbox.com/1b/39/y4VgZzYP_o.jpg" alt=""></p> 
<ul><li>第一部分总结</li></ul> 
<p><img src="https://images2.imgbox.com/bc/27/Xf4sn9tV_o.jpg" alt=""></p> 
<h2><a id="Valuebased_31"></a>Value-based的方法</h2> 
<p>下图的过程是符合马尔科夫决策过程的，俗称MDP</p> 
<p><img src="https://images2.imgbox.com/94/b0/due2sJA1_o.jpg" alt=""></p> 
<ul><li>如果状态转移概率和reward都是已知的，那么就称这个环境是已知的</li></ul> 
<p><img src="https://images2.imgbox.com/c9/fc/R7vuLkk6_o.jpg" alt=""></p> 
<ul><li>model-based 
  <ul><li>P函数和R函数已知</li><li>可以直接用动态规划求解</li></ul> </li><li>model-free 
  <ul><li>P函数和R函数未知</li><li>试错探索，现实世界的环境往往未知</li><li>我们主要学习这个</li><li>用Q函数和V函数来表示</li></ul> </li></ul> 
<p><img src="https://images2.imgbox.com/c0/f4/JLJePjsn_o.jpg" alt=""></p> 
<h3><a id="Q_52"></a>Q表格</h3> 
<p>反应在某个s下，哪个动作价值高</p> 
<p><img src="https://images2.imgbox.com/c5/7d/HS5YfI39_o.jpg" alt=""></p> 
<p>Q表格：指导每一个Step的动作选择，目标导向：未来的总收益</p> 
<p>我们的收益要看的更远一些</p> 
<p><img src="https://images2.imgbox.com/ee/30/EtmaTZc6_o.jpg" alt=""></p> 
<p>但是有时候看的太远也不好，所以引入衰减因子<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         γ 
        
       
      
        \gamma 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0556em;">γ</span></span></span></span></span></p> 
<p><img src="https://images2.imgbox.com/78/5b/UttxAzL3_o.jpg" alt=""></p> 
<h4><a id="_68"></a>举个例子</h4> 
<p><img src="https://images2.imgbox.com/13/34/LrubfFyR_o.jpg" alt=""></p> 
<ul><li>折扣因子</li></ul> 
<p><img src="https://images2.imgbox.com/a3/32/w6R6bklJ_o.jpg" alt=""></p> 
<ul><li>我们就是要求解Q表格 
  <ul><li>刚开始全部初始化为0，当足够多的与环境交互之后，Q表格就会更新足够完善</li></ul> </li></ul> 
<h3><a id="_79"></a>强化的概念</h3> 
<ul><li> <p>时序差分</p> 
  <ul><li>主要特点是在估计当前策略的价值函数时，它不需要等到一个完整的序列（如一局游戏）结束后才更新价值估计，而是在每一步之后立即进行更新</li><li>李宏毅讲过</li></ul> </li><li> <p>在不断的重复试验之后，原本是要看到熊发怒才会瑟瑟发抖，不断试验之后，看到有熊爪就会瑟瑟发抖</p> 
  <ul><li>意味着agent学会了预测熊发怒这一状态的价值，并将这种预期的负面价值向前传播到先前的状态（熊爪）。这种向前传播的过程是通过<strong>Temporal Difference Error</strong>来完成的，这个错误是实际奖励和智能体预测的未来奖励之间的差异。智能体使用这个TD错误来更新其关于当前状态和动作的价值估计，使得未来的决策更加准确。</li></ul> </li><li> <p>下一个状态的价值，是可以不断强化影响上一个状态的价值</p> 
  <ul><li>下一个状态的价值只与当前状态有关，历史的状态已经融合到当前状态</li></ul> </li></ul> 
<p><img src="https://images2.imgbox.com/fb/2c/TT16Kimd_o.jpg" alt=""></p> 
<ul><li>状态价值迭代 
  <ul><li>demo</li><li>https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_td.html</li></ul> </li></ul> 
<h3><a id="TD_99"></a>TD更新</h3> 
<p>拿下一步的Q值去更新这一步的Q值</p> 
<ul><li>刚开始<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          Q 
         
        
          ( 
         
         
         
           S 
          
         
           t 
          
         
        
          , 
         
         
         
           A 
          
         
           t 
          
         
        
          ) 
         
        
       
         Q(S_t,A_t) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: -0.0576em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>初始化为0，其要去逼近Target、也就是未来收益之和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           G 
          
         
           t 
          
         
        
       
         G_t 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>。</li><li>在做一个简单的数学变换我们可以发现 
  <ul><li><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
        
         
          
           
           
             G 
            
           
             t 
            
           
          
         
           G_t 
          
         
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> = <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
        
         
          
           
           
             R 
            
            
            
              t 
             
            
              + 
             
            
              1 
             
            
           
          
            + 
           
          
            γ 
           
           
           
             G 
            
            
            
              t 
             
            
              + 
             
            
              1 
             
            
           
          
         
           R_{t+1}+{\gamma}G_{t+1} 
          
         
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8917em; vertical-align: -0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0077em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.0077em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.8917em; vertical-align: -0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0556em;">γ</span></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span></span></span></span></span></li><li>因为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
        
         
          
          
            Q 
           
          
            ( 
           
           
           
             S 
            
           
             t 
            
           
          
            , 
           
           
           
             A 
            
           
             t 
            
           
          
            ) 
           
          
         
           Q(S_t,A_t) 
          
         
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: -0.0576em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>要逼近<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
        
         
          
           
           
             G 
            
           
             t 
            
           
          
         
           G_t 
          
         
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span></li><li>所以差不多<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
        
         
          
          
            Q 
           
          
            ( 
           
           
           
             S 
            
            
            
              t 
             
            
              + 
             
            
              1 
             
            
           
          
            , 
           
           
           
             A 
            
            
            
              t 
             
            
              + 
             
            
              1 
             
            
           
          
            ) 
           
          
         
           Q(S_{t+1},A_{t+1}) 
          
         
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.0576em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>要逼近<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
        
         
          
           
           
             G 
            
            
            
              t 
             
            
              + 
             
            
              1 
             
            
           
          
         
           G_{t+1} 
          
         
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8917em; vertical-align: -0.2083em;"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span></span></span></span></span></li></ul> </li><li><em>α</em>：学习率，决定了新信息覆盖旧信息的速度</li><li>当前的Q值会向目标Q值逼近，而目标Q值是基于智能体获得的实际奖励和下一个状态-动作对的预期Q值计算得来的。</li><li>右侧的图表示了状态和动作之间的转移，以及如何更新Q值。每次智能体在状态 ( $S_t $) 下采取动作 ( <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           A 
          
         
           t 
          
         
        
       
         A_t 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> )，都会转移到新的状态 ( $S_{t+1} <span class="katex--inline">KaTeX parse error: Can't use function '\)' in math mode at position 1: \̲)̲ 并采取新的动作 \(</span> A_{t+1} $)，同时接收奖励 ( $R_{t+1} $)，然后基于这些信息来更新Q值。</li></ul> 
<p><img src="https://images2.imgbox.com/1a/73/r7uokjqt_o.jpg" alt=""></p> 
<p>所谓的软更新其实像一种误差，表示<strong>预期</strong>（即时奖励加上对下一状态的Q值的估计）与<strong>当前估计</strong>之间的差异</p> 
<p>预期反映了采取动作<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          A 
         
        
          t 
         
        
       
      
        A_t 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 并进入状态 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          S 
         
         
         
           t 
          
         
           + 
          
         
           1 
          
         
        
       
      
        S_{t+1} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8917em; vertical-align: -0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.0576em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 后的长期期望回报</p> 
<blockquote> 
 <p>在时序差分（TD）学习中，如果 ( <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           R 
          
          
          
            t 
           
          
            + 
           
          
            1 
           
          
         
        
          + 
         
        
          γ 
         
        
          Q 
         
        
          ( 
         
         
         
           S 
          
          
          
            t 
           
          
            + 
           
          
            1 
           
          
         
        
          , 
         
         
         
           A 
          
          
          
            t 
           
          
            + 
           
          
            1 
           
          
         
        
          ) 
         
        
       
         R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8917em; vertical-align: -0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0077em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.0077em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0556em;">γ</span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.0576em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> ) （也就是我们说的目标或者预期）比当前的 ( <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          Q 
         
        
          ( 
         
         
         
           S 
          
         
           t 
          
         
        
          , 
         
         
         
           A 
          
         
           t 
          
         
        
          ) 
         
        
       
         Q(S_t, A_t) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: -0.0576em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> ) 低，这并不能直接告诉我们是当前的动作 ($ A_t $) 有问题还是下一步的动作 ( <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           A 
          
          
          
            t 
           
          
            + 
           
          
            1 
           
          
         
        
       
         A_{t+1} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8917em; vertical-align: -0.2083em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span></span></span></span></span>) 有问题。这里涉及的是两个连续的决策（当前和未来）以及它们对长期回报的影响。</p> 
 <p>理解这个情况需要分析几个方面：</p> 
 <ol><li> <p><strong>即时奖励 ( <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
         
          
           
            
            
              R 
             
             
             
               t 
              
             
               + 
              
             
               1 
              
             
            
           
          
            R_{t+1} 
           
          
        </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8917em; vertical-align: -0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0077em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.0077em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span></span></span></span></span> )</strong>: 这是智能体在状态 ($ S_t$ ) 执行动作 ($ A_t$ ) 之后立即获得的奖励。如果这个奖励很低，它可能表明当前的动作并不理想。</p> </li><li> <p><strong>未来预期回报 ($ \gamma Q(S_{t+1}, A_{t+1}) $)</strong>: 这代表智能体预期在下一个状态 ( <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
        
         
          
           
           
             S 
            
            
            
              t 
             
            
              + 
             
            
              1 
             
            
           
          
         
           S_{t+1} 
          
         
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8917em; vertical-align: -0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.0576em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span></span></span></span></span> ) 执行动作 ( <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
        
         
          
           
           
             A 
            
            
            
              t 
             
            
              + 
             
            
              1 
             
            
           
          
         
           A_{t+1} 
          
         
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8917em; vertical-align: -0.2083em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span></span></span></span></span> ) 之后能够获得的折扣后的回报。如果这个值低，它可能意味着从当前状态 ( $S_t $) 到达的下一个状态 ( $S_{t+1} <span class="katex--inline">KaTeX parse error: Can't use function '\)' in math mode at position 1: \̲)̲ 不是一个有利的状态，或者在那…</span> A_{t+1} $) 不是最佳选择。</p> </li><li> <p><strong>TD误差</strong>: 如果 ( <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
        
         
          
           
           
             R 
            
            
            
              t 
             
            
              + 
             
            
              1 
             
            
           
          
            + 
           
          
            γ 
           
          
            Q 
           
          
            ( 
           
           
           
             S 
            
            
            
              t 
             
            
              + 
             
            
              1 
             
            
           
          
            , 
           
           
           
             A 
            
            
            
              t 
             
            
              + 
             
            
              1 
             
            
           
          
            ) 
           
          
         
           R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) 
          
         
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8917em; vertical-align: -0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0077em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.0077em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0556em;">γ</span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.0576em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> ) 比 ( <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
        
         
          
          
            Q 
           
          
            ( 
           
           
           
             S 
            
           
             t 
            
           
          
            , 
           
           
           
             A 
            
           
             t 
            
           
          
            ) 
           
          
         
           Q(S_t, A_t) 
          
         
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: -0.0576em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> ) 小，TD误差是负的，这表明智能体对当前状态-动作对的价值估计过高。智能体需要通过学习降低这一估计，以更准确地反映实际的长期回报。</p> </li><li> <p><strong>学习和策略改进</strong>: 这个信息（TD误差）被用来指导智能体如何调整其策略。如果TD误差反复为负，智能体会逐渐学习减少选择导致这种情况的动作的频率。相反，如果TD误差为正，智能体会增加选择那个动作的倾向。</p> </li></ol> 
 <p>在实际应用中，我们需要考虑整个学习过程，并且通常要运行多个episode来确定是否一种特定的动作序列通常导致负面的结果。只有在长时间和多次迭代的基础上，我们才能确定问题是否出在当前动作、下一动作，或者是整体策略的问题。</p> 
</blockquote> 
<h2><a id="Sarsa_134"></a>Sarsa算法</h2> 
<ul><li>伪代码</li></ul> 
<p><img src="https://images2.imgbox.com/82/02/oWcHADgE_o.jpg" alt=""></p> 
<p>不停的训练，Q就会收敛到某个状态</p> 
<p><strong>重点</strong>：注意Sarsa这里是根据next_obs先拿到next_action。这跟Q-learning很不一样</p> 
<p><img src="https://images2.imgbox.com/36/f8/iuhU8vGJ_o.jpg" alt=""></p> 
<h3><a id="Sample_146"></a>Sample</h3> 
<ul><li>predict函数 
  <ul><li>贪心算法，先提取出Q table中某个obs的一行，找出这行Q最大的格子。如果有多个格子，那就随机选取一个，并返回其对应的action</li></ul> </li><li>但是这样子agent不会探索，所以我们使用sample函数 
  <ul><li>除了我们能拿到最优的动作外，还有一定的概率能探索到别的action</li></ul> </li></ul> 
<p><img src="https://images2.imgbox.com/e9/87/49dVywdf_o.jpg" alt=""></p> 
<p>所以整个训练的代码是这样子的</p> 
<p>最重要的就是左边流程图红框框的这个</p> 
<p><img src="https://images2.imgbox.com/12/f6/3OFnBFoT_o.jpg" alt=""></p> 
<p>agent主要就是两个功能，一个是sample、一个是learn。learn后面会讲，对Q表格进行更新</p> 
<h4><a id="Sarsa_Agent_165"></a>Sarsa Agent类</h4> 
<ul><li>初始化 
  <ul><li>obs的维度和act维度</li></ul> </li></ul> 
<p><img src="https://images2.imgbox.com/46/cf/zXB3CPcl_o.jpg" alt=""></p> 
<ul><li>learn 更新Q表格的方法 
  <ul><li>就是完全按照那个公式来的，先求出目标Q，然后对当前Q进行修正</li></ul> </li></ul> 
<p><img src="https://images2.imgbox.com/60/74/V7wt9fTt_o.jpg" alt=""></p> 
<ul><li>结合上环境，具体例子，调包的代码</li></ul> 
<p><img src="https://images2.imgbox.com/0c/81/p3m2zFaR_o.jpg" alt=""></p> 
<h2><a id="On_policy_vs_off_policy_181"></a>On_policy vs off_policy</h2> 
<p><a href="https://www.zhihu.com/question/57159315/answer/465865135" rel="nofollow">强化学习中on-policy 与off-policy有什么区别？</a></p> 
<ul><li>目标策略 
  <ul><li>比如说Q表格训练完之后，我们对于一个s，去找到对应Q值最大的a，的这个决策过程，叫做目标策略</li></ul> </li><li>行为策略 
  <ul><li>进行数据的收集的策略是行为策略</li></ul> </li></ul> 
<p><img src="https://images2.imgbox.com/e4/74/WIY7olyO_o.jpg" alt=""></p> 
<p>q learning 并没有实际上要传进来的那个值</p> 
<p>传进来下一个next action</p> 
<p>q learning更大胆，默认自己选的就是最优的</p> 
<h2><a id="_208"></a>函数逼近与神经网络</h2> 
<ul><li>因为很多情况下，state太多了，Q表格存不下，这时候可以用值函数来近似</li></ul> 
<p><img src="https://images2.imgbox.com/f4/aa/8smMUW8d_o.jpg" alt=""></p> 
<p>复习一下Q-learning</p> 
<p>其实这里Q的更新就是用下一步的Q来更新上一步的Q，去逼近这个未来的Reward。</p> 
<p>其中对于action的选择，是有sample策略的</p> 
<p><img src="https://images2.imgbox.com/94/6f/pd93P6ns_o.jpg" alt=""></p> 
<ul><li>DQN的改进就是把Q表格给换成了神经网络 
  <ul><li>输入一个s，通过神经网络，输入所有的action的Q值</li></ul> </li></ul> 
<p><img src="https://images2.imgbox.com/7a/31/df7WQwGg_o.jpg" alt=""></p> 
<h2><a id="DQN_229"></a>DQN算法</h2> 
<h3><a id="DQN_231"></a>DQN创新点</h3> 
<p>用神经网络来代替Q表格，会引发两个问题，DQN使用两个方法解决了以下两个问题</p> 
<ol><li>经验回放：样本相关性 
  <ol><li>序列决策的样本关联</li><li>样本利用率低</li></ol> </li><li>固定Q目标 
  <ol><li>非平稳性：算法非平稳</li></ol> </li></ol> 
<ul><li>经验回放 
  <ul><li>不用连续数据训练</li></ul> </li></ul> 
<p><img src="https://images2.imgbox.com/75/43/2ToqVu2b_o.jpg" alt=""></p> 
<p><img src="https://images2.imgbox.com/9a/6b/znUCaJoY_o.jpg" alt=""></p> 
<ul><li>固定Q目标 
  <ul><li>解决了算法更新不平稳的问题</li></ul> </li></ul> 
<p><img src="https://images2.imgbox.com/b0/ba/YqvX36oU_o.jpg" alt=""></p> 
<ul><li> <p>在DQN中，如果我们用同一个网络来选择最大化动作和评估这个动作的Q值，会有一个问题：网络的微小更新可能会极大地影响这个最大化动作的选择，导致训练变得非常不稳定</p> </li><li> <p>为了解决这个问题，DQN采用了固定Q目标技巧。具体来说，DQN使用两个网络：一个是行为网络，用于选择动作；另一个是目标网络，用于计算Q目标值。目标网络的权重是行为网络权重的较老版本，不会在每一步更新。<strong>在一定的时间步后，行为网络的权重会被复制到目标网络</strong>。这样可以使训练过程更加稳定，因为目标Q值变化不会那么剧烈。</p> </li><li> <p>DQN流程图</p> </li></ul> 
<p><img src="https://images2.imgbox.com/00/a9/eiXy2K7r_o.jpg" alt=""></p> 
<p>PARL的DQN框架</p> 
<p><img src="https://images2.imgbox.com/f6/a7/Q1x6CQP5_o.jpg" alt=""></p> 
<p>用嵌套的方式来组成这个agent</p> 
<p>重点就是根据数据和模型结构来构建loss function这步做好了，就好用。</p> 
<p><img src="https://images2.imgbox.com/49/c8/sWPNvA7P_o.jpg" alt=""></p> 
<ul><li>训练文件目录</li></ul> 
<p><img src="https://images2.imgbox.com/b1/a0/cQ2dH0G3_o.jpg" alt=""></p> 
<blockquote> 
 <p>一些补充：</p> 
 <p>Q：他如何计算目标Q，跟他实际下一步执行哪个action是没有关系的</p> 
 <p>A：对的，你理解得很准确。在DQN算法中，计算目标Q值的过程与智能体实际执行的下一步动作是独立的。</p> 
 <p>在DQN中，目标Q值的计算方式是基于贪婪策略的，即选取下一个状态（s’）中具有最大预期回报的动作（a’）的Q值。具体来说，它使用目标网络来预测下一个状态的所有可能动作的Q值，并从中选择最大的Q值来构建目标Q值。这个过程是基于对最优行为的假设，不考虑智能体实际采取的动作。</p> 
 <p>这样做的原因在于，DQN旨在学习一个最优策略，这个策略可以告诉智能体在任何给定状态下应该采取什么动作以最大化长期收益。通过总是考虑最优动作的Q值，DQN试图引导智能体学习如何在任何情况下都做出最佳决策。</p> 
 <p>然而，这并不意味着智能体在实际的操作中总是选择最佳动作。在实际执行过程中，智能体通常会采用ϵ-greedy策略（即大部分时间选择最优动作，但有小概率随机选择一个动作）来平衡探索和利用。这样，智能体可以在执行过程中探索新的动作，而不是始终固守已知的最优动作。但在学习更新过程中，计算目标Q值时仍然是基于最优动作的假设。</p> 
</blockquote> 
<h3><a id="DQN_290"></a>DQN代码实现</h3> 
<h4><a id="modelpy_292"></a>model.py</h4> 
<p>主要就是实现value()函数，输出Q价值。</p> 
<p>定义来三层网络结构，act_dim就是最后输出动作有多少，这里维度就是多少</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> parl
<span class="token keyword">from</span> parl <span class="token keyword">import</span> layers  <span class="token comment"># 封装了 paddle.fluid.layers 的API</span>


<span class="token keyword">class</span> <span class="token class-name">Model</span><span class="token punctuation">(</span>parl<span class="token punctuation">.</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> act_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        hid1_size <span class="token operator">=</span> <span class="token number">128</span>
        hid2_size <span class="token operator">=</span> <span class="token number">128</span>
        <span class="token comment"># 3层全连接网络</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> layers<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>size<span class="token operator">=</span>hid1_size<span class="token punctuation">,</span> act<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> layers<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>size<span class="token operator">=</span>hid2_size<span class="token punctuation">,</span> act<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc3 <span class="token operator">=</span> layers<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>size<span class="token operator">=</span>act_dim<span class="token punctuation">,</span> act<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">value</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> obs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        h1 <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>obs<span class="token punctuation">)</span>
        h2 <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>h1<span class="token punctuation">)</span>
        Q <span class="token operator">=</span> self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>h2<span class="token punctuation">)</span>
        <span class="token keyword">return</span> Q
</code></pre> 
<h4><a id="algorithmpy_319"></a>algorithm.py</h4> 
<p>DQN的类继承PARL里的algorithm</p> 
<p>定义一个model，直接把前面定义的model拿过来，然后再deepcopy一下，作为目标网络</p> 
<p>再定义一些超参数</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> copy
<span class="token keyword">import</span> paddle<span class="token punctuation">.</span>fluid <span class="token keyword">as</span> fluid
<span class="token keyword">import</span> parl
<span class="token keyword">from</span> parl <span class="token keyword">import</span> layers


<span class="token keyword">class</span> <span class="token class-name">DQN</span><span class="token punctuation">(</span>parl<span class="token punctuation">.</span>Algorithm<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model<span class="token punctuation">,</span> act_dim<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">""" DQN algorithm
        
        Args:
            model (parl.Model): 定义Q函数的前向网络结构
            act_dim (int): action空间的维度，即有几个action
            gamma (float): reward的衰减因子
            lr (float): learning_rate，学习率.
        """</span>
        self<span class="token punctuation">.</span>model <span class="token operator">=</span> model
        self<span class="token punctuation">.</span>target_model <span class="token operator">=</span> copy<span class="token punctuation">.</span>deepcopy<span class="token punctuation">(</span>model<span class="token punctuation">)</span>

        <span class="token keyword">assert</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>act_dim<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">)</span>
        <span class="token keyword">assert</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>gamma<span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">)</span>
        <span class="token keyword">assert</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>lr<span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>act_dim <span class="token operator">=</span> act_dim
        self<span class="token punctuation">.</span>gamma <span class="token operator">=</span> gamma
        self<span class="token punctuation">.</span>lr <span class="token operator">=</span> lr
</code></pre> 
<ul><li>sync_target() 
  <ul><li>实现定期参数同步，将self.model的参数同步到self.target_model</li><li>调用PARL中已经实现好的api即可</li></ul> </li></ul> 
<pre><code class="prism language-python">    <span class="token keyword">def</span> <span class="token function">sync_target</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">""" 把 self.model 的模型参数值同步到 self.target_model
        """</span>
        self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>sync_weights_to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>target_model<span class="token punctuation">)</span>
</code></pre> 
<ul><li>predict() 
  <ul><li>使用model.value方法，来获取一批action在observation中对应的Q值</li><li>输出个数与输入的action个数一样</li></ul> </li></ul> 
<pre><code class="prism language-python">    <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> obs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">""" 使用self.model的value网络来获取 [Q(s,a1),Q(s,a2),...]
        """</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>value<span class="token punctuation">(</span>obs<span class="token punctuation">)</span>
</code></pre> 
<ul><li> <p>learn()</p> 
  <ul><li> <p>最核心的方法</p> </li><li> <p>分为三部分</p> 
    <ul><li>计算目标Q</li><li>计算预测Q</li><li>得到loss</li></ul> </li><li> <p>方法使用</p> 
    <ul><li>sample到的一批数据，作为数组直接传进来，（obs,action,reward,next_obs）</li></ul> </li></ul> </li></ul> 
<p><img src="https://images2.imgbox.com/42/a8/5tBZK2YL_o.jpg" alt=""></p> 
<ul><li> <p>对于获取traget Q</p> 
  <ul><li>按照公式计算</li><li>对于最后一条数据，通过传入的参数terminal来判断 
    <ul><li><code>terminal = layers.cast(terminal, dtype='float32') </code></li><li><code>target = reward + (1.0 - terminal) * self.gamma * best_v</code></li><li>这两行代码很巧妙的实现了ppt最上面的if。就是最后一步不需要后面的那一块j+1</li></ul> </li><li>加了一行阻止梯度传播 
    <ul><li>其实就是暂时固定计算target Q的那个网络参数，让他不要时刻更新</li></ul> </li></ul> </li><li> <p>对于下面这一块获取pred Q value</p> 
  <ul><li>输入obs后，会输出该obs下所有的actions的pred Q value，此时我们只需要某个action的pred Q value</li><li>这里就是把对应的这个action进行one_hot编码。然后与pred Q value数组<strong>按位相乘，再相加</strong>，就得到了。</li></ul> </li></ul> 
<pre><code class="prism language-python">pred_value <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>value<span class="token punctuation">(</span>obs<span class="token punctuation">)</span>  <span class="token comment"># 获取Q预测值</span>
        <span class="token comment"># 将action转onehot向量，比如：3 =&gt; [0,0,0,1,0]</span>
        action_onehot <span class="token operator">=</span> layers<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>action<span class="token punctuation">,</span> self<span class="token punctuation">.</span>act_dim<span class="token punctuation">)</span>
        action_onehot <span class="token operator">=</span> layers<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>action_onehot<span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">'float32'</span><span class="token punctuation">)</span>
        <span class="token comment"># 下面一行是逐元素相乘，拿到action对应的 Q(s,a)</span>
        <span class="token comment"># 比如：pred_value = [[2.3, 5.7, 1.2, 3.9, 1.4]], action_onehot = [[0,0,0,1,0]]</span>
        <span class="token comment">#  ==&gt; pred_action_value = [[3.9]]</span>
        pred_action_value <span class="token operator">=</span> layers<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span>
            layers<span class="token punctuation">.</span>elementwise_mul<span class="token punctuation">(</span>action_onehot<span class="token punctuation">,</span> pred_value<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li>计算loss</li></ul> 
<p>pred_action_value与targrt计算均方差，然后扔进优化器</p> 
<h4><a id="agentpy_422"></a>agent.py</h4> 
<p>核心的算法都在algorithm里面了，但是我们需要feed数据，这些数据就由agent来获取</p> 
<ul><li> <p>每run一次，就是更新一次</p> </li><li> <p><code>build_program</code> 方法：这个方法用于构建预测和学习的程序。</p> 
  <ul><li><code>self.pred_program</code>: 用于动作预测的程序，用来拿到perd的Q值的。</li><li><code>self.learn_program</code>: 用于训练（学习）的程序，定义每一个数据的type、shape等。</li></ul> </li><li> <p><code>learn方法</code>：这是智能体的学习方法。</p> 
  <ul><li>每隔一定步数（由 <code>update_target_steps</code> 定义），它会同步模型和目标模型的参数（这是DQN算法中常见的做法）。</li><li>该方法接收当前状态、动作、奖励、下一个状态和是否为终止状态作为输入，然后执行一次训练步骤。</li></ul> </li></ul> 
<p><img src="https://images2.imgbox.com/bc/6a/pOfRz25B_o.jpg" alt=""></p> 
<ul><li>sample和predict</li></ul> 
<p><img src="https://images2.imgbox.com/88/44/0wfSxnCx_o.jpg" alt=""></p> 
<h4><a id="_446"></a>总结：举个例子</h4> 
<p>让我们通过一个简单的强化学习场景来具体说明这个区别。假设我们正在训练一个智能体来玩迷宫游戏，智能体的目标是找到从起点到终点的最短路径。</p> 
<ul><li> <p>场景设定</p> 
  <ul><li> <p><strong>迷宫游戏</strong>：游戏中有墙壁、路径和目标。智能体的任务是找到从起点到终点的路径。</p> </li><li> <p><strong>智能体（Agent）</strong>：控制角色在迷宫中移动。</p> </li><li> <p><strong>算法（Algorithm）</strong>：决定如何根据当前位置和目标来选择动作。</p> </li></ul> </li><li> <p><code>Algorithm</code> 类中的 <code>learn</code> 和 <code>predict</code> 方法</p> 
  <ul><li><strong>Algorithm类</strong>：通常包含强化学习算法的核心逻辑，如Q学习、策略梯度等。它直接与神经网络模型交互，负责计算和更新值函数（例如Q值）或策略。</li></ul> 
  <ol><li><strong><code>predict</code> 方法</strong>：这个方法直接处理模型预测。在迷宫示例中，它可能接收当前位置的状态，并直接使用神经网络模型预测每个可能动作的Q值。</li><li><strong><code>learn</code> 方法</strong>：此方法执行学习过程的核心步骤，比如计算损失函数并更新模型参数。在迷宫示例中，它可能接收一批经历（状态、动作、奖励等）并执行反向传播来改善模型预测。</li></ol> </li><li> <p><code>Agent</code> 类中的 <code>learn</code> 和 <code>predict</code> 方法</p> 
  <ul><li><strong>Agent类</strong>：代表智能体，它是与环境交互的接口。<code>Agent</code> 通常封装了 <code>Algorithm</code>，管理与环境的交互、数据预处理、决策和学习过程的细节。</li></ul> 
  <ol><li><strong><code>predict</code> 方法</strong>：在迷宫游戏中，这个方法可能首先对状态进行预处理（比如归一化），然后调用 <code>Algorithm</code> 的 <code>predict</code> 方法来获取动作的Q值，并基于这些Q值选择动作（例如使用ϵ-greedy策略）。</li><li><strong><code>learn</code> 方法</strong>：这个方法可能管理学习过程中的一些高层逻辑，如确定何时同步目标网络的参数（在DQN中）。然后它会调用 <code>Algorithm</code> 的 <code>learn</code> 方法来实际更新模型。此外，它可能处理与学习相关的其他逻辑，比如更新ϵ值（探索率）。</li></ol> </li><li> <p>实例解释</p> 
  <ul><li> <p>当智能体在迷宫中探索时，它使用 <code>predict</code> 方法来决定下一步动作。<code>predict</code> 方法内部调用算法层的 <code>predict</code> 来评估当前状态下的每个可能动作，然后选择最佳动作。</p> </li><li> <p>当智能体获得一些经验（例如走了一段路径，得到了一些奖励或惩罚）后，它使用 <code>learn</code> 方法来更新其策略。<code>learn</code> 方法内部调用算法层的 <code>learn</code> 来实际进行学习，更新模型以改进智能体在未来做出决策的能力。</p> </li></ul> </li><li> <p>结论</p> </li></ul> 
<p>这个例子说明了Agent层如何处理高层逻辑和环境交互（如数据预处理和决定何时学习），而Algorithm层专注于实际的计算和模型更新。这种分层设计有助于代码的组织和复用，同时使智能体的行为和学习过程更加灵活和高效。</p> 
<h3><a id="_484"></a>实战</h3> 
<p><img src="https://images2.imgbox.com/df/a1/McCygaLy_o.jpg" alt=""></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/10c9da56f9a421b8d782a95004a7474b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">推荐几款值得收藏的3DMAX插件</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6baa5c1fa350ba54d3a0d50ef7a550f9/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【经典LeetCode算法题目专栏分类】【第6期】二分查找系列：x的平方根、有效完全平方数、搜索二位矩阵、寻找旋转排序数组最小值</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>