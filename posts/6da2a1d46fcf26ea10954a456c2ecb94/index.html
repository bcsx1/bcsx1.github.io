<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ã€æ•°æ®é›†ã€‘Learn2Reg2021 Task 01 â€”â€” 3D è…¹éƒ¨å¤šæ¨¡æ€ MR-CT - ç¼–ç¨‹éšæƒ³</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="ã€æ•°æ®é›†ã€‘Learn2Reg2021 Task 01 â€”â€” 3D è…¹éƒ¨å¤šæ¨¡æ€ MR-CT" />
<meta property="og:description" content="Reference Learn2Reg: Comprehensive Multi-Task Medical Image Registration Challenge, Dataset and Evaluation in the Era of Deep Learning ğŸ‘‰
Abstract Medical image registration plays a very important role in improving clinical workflows, computer-assisted interventions and diagnosis as well as for research studies involving e.g. morphological analysis. Besides ongoing research into new concepts for optimisation, similarity metrics, domain adaptation and deformation models, deep learning for medical registration is currently starting to show promising advances that could improve the robustness, generalisation, computation speed and accuracy of conventional algorithms to enable better practical translation." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/6da2a1d46fcf26ea10954a456c2ecb94/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-09T09:40:14+08:00" />
<meta property="article:modified_time" content="2023-07-09T09:40:14+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="ç¼–ç¨‹éšæƒ³" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">ç¼–ç¨‹éšæƒ³</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ã€æ•°æ®é›†ã€‘Learn2Reg2021 Task 01 â€”â€” 3D è…¹éƒ¨å¤šæ¨¡æ€ MR-CT</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h4><a id="Reference_1"></a>Reference</h4> 
<p>Learn2Reg: Comprehensive Multi-Task Medical Image Registration Challenge, Dataset and Evaluation in the Era of Deep Learning <a href="https://ieeexplore.ieee.org/ielx7/42/10057585/09925717.pdf" rel="nofollow">ğŸ‘‰</a></p> 
<h4><a id="Abstract_3"></a>Abstract</h4> 
<blockquote> 
 <p>Medical image registration plays a very important role in improving clinical workflows, computer-assisted interventions and diagnosis as well as for research studies involving e.g. morphological analysis. Besides ongoing research into new concepts for optimisation, similarity metrics, domain adaptation and deformation models, <mark>deep learning for medical registration is currently starting to show promising advances that could improve the robustness, generalisation, computation speed and accuracy of conventional algorithms to enable better practical translation</mark>. <strong>Nevertheless, before Learn2Reg there was no commonly used benchmark dataset to compare stateof-the-art learning based registration among another and with their conventional (not trained) counterparts</strong>. With few exceptions (CuRIOUS at MICCAI 2018/2019, the Continuous Registration Challenge at WBIR 2018 and Learn2Reg 2020) there has also been no comprehensive registration challenge covering different anatomical structures and evaluation metrics. We also believe that the entry barrier for new teams to contribute to this emerging field are higher than e.g. for segmentation, where standardised datasets (e.g. Medical Decathlon, BraTS) are easily available. <mark>In contrast, many registration tasks, require resampling from different voxel spacings, affine pre-registration and can lead to ambiguous and error-prone evaluation of whole deformation fields</mark>. We propose a simplified challenge design that removes many of the common pitfalls for learning and applying transformations. <strong>We will provide pre-preprocessed data (resample, crop, pre-align, etc.) that can be directly employed by most conventional and learning frameworks</strong>. Only docker containers that generate displacement fields in voxel dimensions in a standard orientation will have to be provided by participants and python code to test their application (on local machines) to training data will be provided as open-source along with all evaluation metrics. Our challenge will consist of three clinically relevant sub-tasks (datasets) that are complementary in nature. <strong>They can either be individually or comprehensively addressed by participants and cover both intra- and inter-patient alignment, CT, ultrasound and MRI modalities, neuro-, thorax and abdominal anatomies and the four of the imminent challenges of medical image registration:</strong></p> 
 <ul><li><strong>learning from small datasets</strong></li><li><strong>estimating large deformations</strong></li><li><strong>dealing with multi-modal scans</strong></li><li><strong>learning from noisy annotations</strong></li></ul> 
</blockquote> 
<blockquote> 
 <p><mark>é»„è‰²éƒ¨åˆ†</mark> æ˜¯å¯ä»¥ç›´æ¥å¼•ç”¨åœ¨è®ºæ–‡å†™ä½œä¸­çš„ï¼›<strong>é»‘ä½“éƒ¨åˆ†</strong> æ˜¯ <a href="https://learn2reg.grand-challenge.org/Learn2Reg2021/" rel="nofollow">Learn2Reg2021</a> çš„æ„ä¹‰å’Œæä¾›çš„æ•°æ®é›†ä»¥åŠæå‡ºçš„æŒ‘æˆ˜ã€‚</p> 
</blockquote> 
<hr> 
<h4><a id="Task_01_Abdominal_MRCT_14"></a>Task 01: Abdominal MR-CT</h4> 
<h5><a id="Abstract_15"></a>Abstract</h5> 
<p>äººä½“è…¹éƒ¨æ˜¯ä¸€ä¸ªé‡è¦è€Œå¤æ‚çš„èº«ä½“ç©ºé—´ã€‚è…¹éƒ¨ä»¥æ¨ªè†ˆä¸ºä¸Šç•Œï¼Œä»¥éª¨ç›†ä¸ºä¸‹ç•Œï¼Œä»¥è„Šæ¤ä¸ºæ”¯æ’‘ï¼Œä»¥è‚Œè‚‰è…¹å£ä¸ºä¿æŠ¤ï¼Œè…¹éƒ¨åŒ…å«è¡€æ¶²å‚¨å¤‡ã€æ’æ¯’ã€æ’å°¿ã€å†…åˆ†æ³ŒåŠŸèƒ½å’Œæ¶ˆåŒ–ç­‰å™¨å®˜ï¼Œå¹¶åŒ…æ‹¬è®¸å¤šé‡è¦çš„åŠ¨è„‰å’Œé™è„‰ã€‚</p> 
<p>è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ‰«æå’Œç£å…±æŒ¯å›¾åƒï¼ˆMRIï¼‰é€šå¸¸ç”¨äºè…¹éƒ¨ç›¸å…³ç–¾ç—…çš„è¯Šæ–­å’Œé¢„åæˆ–å¹²é¢„è®¡åˆ’ï¼›ç„¶è€Œï¼Œ<strong>é’ˆå¯¹è…¹éƒ¨çš„ç‰¹å®šå›¾åƒé…å‡†å·¥å…·å¾ˆå°‘ï¼Œå¹¶ä¸”å‡ ä¹æ²¡æœ‰ç®—æ³•èƒ½å¤Ÿå¤„ç†å¤šæ¨¡æ€é…å‡†ã€‚</strong></p> 
<blockquote> 
 <p>yet few specific image registration tools for the abdomen have been developed and <strong>nearly no algorithm is capable of dealing with multimodal alignment.</strong></p> 
</blockquote> 
<p>åœ¨è…¹éƒ¨ CT å’Œ MRI ä¸Šï¼Œä¸åŒè¡¨ç°ä¹‹é—´çš„å·®å¼‚ï¼ˆä¾‹å¦‚ï¼Œå¹´é¾„ã€æ€§åˆ«ã€èº«é«˜ã€æ­£å¸¸è§£å‰–å˜å¼‚å’Œç–¾ç—…çŠ¶æ€ï¼‰å¯ä»¥é€šè¿‡è§‚å¯Ÿæ¯ä¸ªå™¨å®˜çš„å¤§å°ã€å½¢çŠ¶å’Œå¤–è§‚æ¥é¢„æµ‹ã€‚ä½†æ˜¯ï¼Œ<strong>ä¸ªä½“æœ¬èº«ç”±äºè¯¸å¦‚ï¼šå§¿åŠ¿ã€å‘¼å¸å‘¨æœŸã€æ°´è‚¿ã€æ¶ˆåŒ–çŠ¶æ€ç­‰ï¼Œå¾ˆå®¹æ˜“æ”¹å˜è…¹éƒ¨å™¨å®˜çš„å½¢çŠ¶å’Œå™¨å®˜ä¹‹é—´çš„ä½ç½®å…³ç³»ï¼Œä½¿é…å‡†è¿›ä¸€æ­¥å¤æ‚åŒ–ã€‚</strong></p> 
<blockquote> 
 <p>On abdominal CT and MRI, inter-subject variability (e.g., age, gender, stature, normal anatomical variants, and disease status) can be observed in terms of the size, shape, and appearance of each organ. <strong>Soft anatomy deformation further complicates the registration by varying the inter-organ relationships, even within individuals (e.g., pose, respiratory cycle, edema, digestive status).</strong></p> 
</blockquote> 
<p>åœ¨ Learn2reg æŒ‘æˆ˜ä¸­ï¼Œ<strong>è¿™é¡¹ä»»åŠ¡æ˜¯å¯¹å‡ ä¸ªåˆ†ç¦»çš„åŒºåŸŸè¿›è¡Œé…å‡†</strong>ï¼Œè¿™äº›åŒºåŸŸå…·æœ‰è¾ƒå¤§çš„å™¨å®˜é—´å·®å¼‚å’Œå·¨å¤§çš„ä½“ç§¯å¯å˜æ€§ï¼šä»å‡ ç™¾ä¸ªä½“ç´ çš„å°å™¨å®˜åˆ°éå¸¸å¤§çš„å™¨å®˜ã€‚</p> 
<blockquote> 
 <p>aligning several disjunct regions with large inter-subject variations and<br> great variability in volume: from a few hundred voxels to very large organs.</p> 
</blockquote> 
<p><strong>å½“ä»…æœ‰ä¸€ç§æ¨¡æ€æœ‰å¤§é‡çš„æ ‡ç­¾æ—¶ï¼ŒåŸŸè‡ªé€‚åº”å¯ä»¥åœ¨å¤šæ¨¡æ€é…å‡†ä¸­èµ·å†³å®šæ€§ä½œç”¨</strong>ã€‚ä¸ºäº†æ¢ç´¢è¿™ç§å¤šæ¨¡æ€è¿ç§»å­¦ä¹ çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å°†åªæä¾›ç”¨äºè®­ç»ƒçš„ CT æ ‡ç­¾ï¼Œä½†åœ¨æµ‹è¯•ä¸­è¯„ä¼°æ¨¡æ€é—´ï¼ˆCT-MRï¼‰å’Œæ¨¡æ€å†…é…å‡†ã€‚</p> 
<blockquote> 
 <p><strong>Domain adaptation can play a decisive role when large labelled datasets are only available for one modality</strong>. To explore the challenges of this multimodal transfer learning, we will only provide CT labels for training, but evaluate both inter- and intra-modal registration at test.</p> 
</blockquote> 
<hr> 
<h5><a id="Keywords_32"></a>Keywords</h5> 
<p>intra-patient, CT, MRI, registration, multimodal</p> 
<hr> 
<h5><a id="challenge_36"></a>challenge</h5> 
<ul><li>Multimodal registration.</li><li>Learning from few/noisy annotations.</li><li>Learning with domain gaps.</li></ul> 
<hr> 
<h5><a id="Cohorts_42"></a>Cohorts</h5> 
<ul><li> <p><strong>Target cohorts</strong> - å“ªäº›ä¸»ä½“ / å¯¹è±¡å°†åœ¨æœ€ç»ˆçš„åº”ç”¨ä¸­è·å¾—æœ‰ç”¨çš„æ•°æ®ï¼Ÿ</p> <p>ä¸€æ–¹é¢ï¼Œæ¥å—å›¾åƒå¼•å¯¼æ‰‹æœ¯å¹²é¢„ã€æ´»æ£€æˆ–æ”¾å°„æ²»ç–—çš„æ‚£è€…å¯ä»¥å—ç›Šäºå¯å˜å½¢è§£å‰–å™¨å®˜å›¾è°±ï¼Œè¯¥å›¾è°±å¯æ•æ‰å±é™©å™¨å®˜å’Œé¶åŒºçš„ç©ºé—´å…³ç³»ã€‚å¦ä¸€æ–¹é¢ï¼Œå¯¹å¤§ç¾¤ä½“è¿›è¡Œå½¢çŠ¶åˆ†æå¯ä»¥æ·±å…¥äº†è§£ä¸å¸¸è§ç–¾ç—…ç›¸å…³çš„æµè¡Œç—…å­¦å·®å¼‚ã€‚</p> 
  <blockquote> 
   <p>On the one hand patients undergoing image-guided surgical interventions, biopsies or radiotherapy could benefit from a deformable anatomical organ atlas that captures spatial relations of organs-at-risk and target regions. On the other hand, shape analysis over large cohorts could provide insight into epidemiological difference in relation to common disease.</p> 
  </blockquote> </li><li> <p><strong>challenge cohort</strong> - è·å–è¿™äº›æ•°æ®çš„ä¸»ä½“ / å¯¹è±¡ã€‚</p> <p>åœ¨æ¥å—ç»“ç›´è‚ ç™ŒåŒ–ç–—è¯•éªŒçš„æ‚£è€…ä¸­ï¼Œè…¹éƒ¨ CT æ‰«æçš„åŸºçº¿æ—¶æ®µï¼ˆbaseline sessionsï¼‰æ˜¯ä»è½¬ç§»æ€§è‚ç™Œæ‚£è€…ä¸­éšæœºé€‰æ‹©çš„ï¼›å…¶ä½™çš„æ‰«ææ˜¯ä»ç–‘ä¼¼è…¹ç–çš„æœ¯åå›é¡¾æ€§é˜Ÿåˆ—ï¼ˆcohortï¼‰ä¸­è·å¾—çš„ã€‚é™„åŠ çš„éšè—æ•°æ®é›†æ˜¯ç ”ç©¶ä¸€èˆ¬äººç¾¤çš„å…¨èº« MRIã€‚</p> 
  <blockquote> 
   <p>Patients from an colorectal cancer chemotherapy trial, the baseline sessions of the abdominal CT scans were randomly selected from metastatic liver cancer patients; the remaining scans were acquired from a retrospective post-operative cohort with suspected ventral hernias. Additional hidden dataset of general population study with whole-body MRI.</p> 
  </blockquote> </li></ul> 
<hr> 
<h5><a id="Imaging_modalityies_54"></a>Imaging modality(ies)</h5> 
<p>Magnetic Resonance Imaging (MRI) &amp; Computed Tomography (CT)</p> 
<p><strong>fixed images: MR; moving images: CT</strong></p> 
<hr> 
<h5><a id="Context_information_60"></a>Context information</h5> 
<p>ä¸»è¦è€ƒè™‘<strong>æ¥è‡ªåŒä¸€æ‚£è€…è…¹éƒ¨çš„æˆå¯¹ CT å’Œ MRI æ‰«æã€‚é™„åŠ å¦å¤–ä¸¤ä¸ªæœªé…å¯¹çš„ CT (<a href="https://learn2reg.grand-challenge.org/Learn2Reg2020/" rel="nofollow">Task3 L2Râ€™20</a>) å’Œ MRI (CHAOS MR) æ•°æ®é›†ç”¨äºè¾…åŠ©è®­ç»ƒã€‚</strong></p> 
<p>122 CT/MR scans (16 CT-MR scan pairs (8 Training, 8 Test) + 90 unpaired CT (50)/MR (40) scans)</p> 
<ul><li>å¯¹äº CTï¼šé™„åŠ çš„å­é›†å°†åŒ…æ‹¬ 13 ä¸ªè¢«è®¤ä¸ºæ˜¯æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰çš„è…¹éƒ¨å™¨å®˜ï¼ŒåŒ…æ‹¬è„¾è„ã€å³è‚¾ã€å·¦è‚¾ã€èƒ†å›Šã€é£Ÿç®¡ã€è‚ã€èƒƒã€ä¸»åŠ¨è„‰ã€ä¸‹è…”é™è„‰ã€é—¨é™è„‰å’Œè„¾é™è„‰ã€èƒ°è…ºã€å·¦è‚¾ä¸Šè…ºå’Œå³è‚¾ä¸Šè…ºã€‚å¹¶éæ‰€æœ‰çš„ ROI éƒ½æ˜¯æœ‰æ‰‹åŠ¨åˆ†å‰²çš„ã€‚</li><li>å¯¹äº MRIï¼šåªæœ‰æ‰‹åŠ¨åˆ†å‰²è¾ƒå°‘æ•°é‡çš„å™¨å®˜ï¼Œå¹¶éƒ¨åˆ†æä¾›è®­ç»ƒæ•°æ®ã€‚<strong>æˆ‘ä»¬é¼“åŠ±ä½¿ç”¨å¤šæ¨¡æ€é¢†åŸŸé€‚åº”å­¦ä¹ çš„æ–¹æ³•è¿›è¡Œé…å‡†ã€‚</strong></li></ul> 
<blockquote> 
 <p>We encourage approaches that <strong>learn by multimodal domain adaptation.</strong></p> 
</blockquote> 
<hr> 
<h5><a id="Algorithm_target_71"></a>Algorithm target</h5> 
<blockquote> 
 <p>å³ï¼Œè¯´æ˜ç®—æ³•è®¾è®¡ç”¨äºå…³æ³¨çš„ç»“æ„ / ä¸»é¢˜ / å¯¹è±¡ / ç»„ä»¶ï¼ˆä¾‹å¦‚å¤§è„‘è‚¿ç˜¤ã€åŒ»ç–—å™¨æ¢°å°–ç«¯ã€æ‰‹æœ¯å®¤æŠ¤å£«ã€é€è§†æ‰«æä¸­çš„å¯¼ç®¡ï¼‰ã€‚</p> 
</blockquote> 
<p><strong>ç®—æ³•åº”è¯¥å…³æ³¨è…¹éƒ¨å™¨å®˜åœ¨å•æ¨¡å’Œå¤šæ¨¡æƒ…å†µä¸‹çš„é…å‡†ã€‚é‡ç‚¹åœ¨ç‰¹å®šçš„æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰çš„å¯¹é½æ–¹é¢ï¼Œä¾‹å¦‚ï¼Œè„¾è„ï¼Œå³è‚¾ï¼Œå·¦è‚¾å’Œè‚è„ï¼ˆå°±è¿™ 4 ä¸ª ROI æœ‰æ‰‹åŠ¨åˆ†å‰²æ ‡ç­¾ï¼‰</strong>ï¼›ä»¥åŠè¾ƒå°å™¨å®˜ï¼šèƒ†å›Šï¼Œé£Ÿç®¡ï¼Œèƒƒï¼Œä¸‹è…”é™è„‰ï¼Œé—¨è€³å’Œè„¾é™è„‰ï¼Œèƒ°è…ºçš„å¯¹é½ï¼›ä»¥åŠç”Ÿæˆå…·æœ‰ç©ºé—´å¹³æ»‘åº¦çš„åˆç†å˜å½¢ï¼ˆé›…å¯æ¯”è¡Œåˆ—å¼çš„ä½æ ‡å‡†åå·®ï¼‰ã€‚</p> 
<blockquote> 
 <p>Alignment of abdominal organs within and across modalities in a heterogenous patient cohort. <strong>The focus will be on the alignment of particular regions of interest (ROI), e.g. spleen, right kidney, left kidney and liver.</strong></p> 
</blockquote> 
<hr> 
<h5><a id="Training_and_test_case_characteristics_79"></a>Training and test case characteristics</h5> 
<p>ä¸€ä¸ª case æŒ‡çš„æ˜¯ä¸€å¯¹ CT/MR æ‰«æï¼Œæ¯å¯¹æ‰«ææ¥è‡ªä¸åŒçš„æ‚£è€…ï¼ˆæ‚£è€…é—´é…å‡†ï¼‰ã€‚æ‰€æœ‰ CT æ‰«æï¼ˆè®­ç»ƒå’Œæµ‹è¯•ï¼‰æ‰‹åŠ¨åˆ†å‰²æ ‡ç­¾ä¸€èµ·æä¾›ã€‚æ‰€æœ‰æˆå¯¹çš„ CT/MRI æ‰«æéƒ½å°†æ‰‹åŠ¨åˆ†å‰²æ ‡ç­¾ï¼ˆéšè—æµ‹è¯•æ ‡ç­¾ï¼‰ã€‚</p> 
<ul><li> <p>è¯´æ˜è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ¡ˆä¾‹çš„æ€»æ•°ã€‚</p> <p>Training: 20-25 paired MR/CT cases (40-50 scans) + 30 additional CT scans and ~30 additional MRI Test: 10 paired MR/CT cases (20 scans)</p> </li><li> <p>Mention further important characteristics of the training, validation and test cases</p> <p>åœ¨è¿™ä¸ª intra-subject é…å‡†ä»»åŠ¡ä¸­ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨å¤šä¸ªæˆå¯¹çš„ MR/CT æ‰«æè¿›è¡Œæœ‰ç›‘ç£å­¦ä¹ ã€‚é€šè¿‡æä¾›ä¸æˆå¯¹çš„ CT å’Œ MRI æ‰«æï¼Œè·¨é¢†åŸŸå­¦ä¹ å¯ä»¥æˆä¸ºè¿™é¡¹æŒ‘æˆ˜ä»»åŠ¡çš„ä¸€ä¸ªç»„æˆéƒ¨åˆ†ã€‚</p> 
  <blockquote> 
   <p>In this intra-subject registration task, a number of <strong>paired MR/CT scans can be directly employed for supervised training</strong>. Cross-domain learning can be an integral part of this challenge task with the provision of unpaired CT and MRI scans.</p> 
  </blockquote> </li></ul> 
<hr> 
<h5><a id="_92"></a>æ‰‹åŠ¨åˆ†å‰²æ ‡ç­¾çš„ç‰¹å¾</h5> 
<p>Annotation characteristics</p> 
<p>MRI å’Œé…å¯¹çš„ MRI/CTï¼š<strong>è‡³å°‘å››ä¸ªè…¹éƒ¨å™¨å®˜çš„æ‰‹åŠ¨ 3D ä½“ç´ åˆ†å‰²ï¼šè‚è„ï¼ˆ1ï¼‰ã€è„¾è„ï¼ˆ2ï¼‰ã€å³è‚¾ï¼ˆ3ï¼‰ã€å·¦è‚¾ï¼ˆ4ï¼‰</strong>ï¼Œæ ‡æ³¨æ˜¯æ¥è‡ªç»éªŒä¸°å¯Œçš„ç ”ç©¶ç”Ÿï¼Œå…·æœ‰ 3 å¹´ä»¥ä¸Šçš„åŒ»å­¦æˆåƒç»éªŒã€‚ITK-SNAP<sup class="footnote-ref"><a href="#fn1" rel="nofollow" id="fnref1">1</a></sup><br> <img src="https://images2.imgbox.com/d6/8f/IKN5Dcz0_o.png" alt="0"></p> 
<blockquote> 
 <p>MRI and paired MRI/CT: manual 3D voxel segmentation of at least four abdominal organs: liver(1), spleen(2), right kidney(3), left kidney(4),</p> 
</blockquote> 
<p>é™„åŠ  CTï¼š13 ä¸ªè…¹éƒ¨å™¨å®˜è¢«è§†ä¸ºæ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰ï¼ŒåŒ…æ‹¬è„¾è„ã€å³è‚¾ã€å·¦è‚¾ã€èƒ†å›Šã€é£Ÿç®¡ã€è‚ã€èƒƒã€ä¸»åŠ¨è„‰ã€ä¸‹è…”é™è„‰ã€é—¨é™è„‰å’Œè„¾é™è„‰ã€èƒ°è…ºã€å·¦è‚¾ä¸Šè…ºå’Œå³è‚¾ä¸Šè…ºã€‚</p> 
<blockquote> 
 <p>æ­£å¦‚ä¸€ä½æ”¾å°„ç§‘åŒ»ç”Ÿæ‰€å»ºè®®çš„ï¼Œç”±äºæ•°æ®é›†ä¸­ç¼ºä¹å®Œæ•´çš„å¤–è§‚ï¼Œå¿ƒè„è¢«æ’é™¤åœ¨å¤–ï¼Œå–è€Œä»£ä¹‹çš„æ˜¯è‚¾ä¸Šè…ºè¢«çº³å…¥ä¸´åºŠç ”ç©¶ã€‚</p> 
</blockquote> 
<hr> 
<h5><a id="_104"></a>æ•°æ®é¢„å¤„ç†</h5> 
<p>Data pre-processing method(s)</p> 
<p><strong>æä¾›ç›¸åŒä½“ç´ åˆ†è¾¨ç‡ï¼ˆ192 Ã— 160 Ã— 192ï¼‰å’Œä½“ç´ ç©ºé—´ç»´åº¦ï¼ˆ2mm voxel spacingï¼‰çš„é€šç”¨é¢„å¤„ç†ä»¥åŠä»¿å°„é¢„é…å‡†</strong>ï¼Œä»¥ä¾¿äºå…ˆå‰åœ¨å›¾åƒé…å‡†æ–¹é¢ç»éªŒä¸è¶³çš„å‚ä¸è€…ä½¿ç”¨åŸºäºå­¦ä¹ çš„ç®—æ³•ã€‚</p> 
<blockquote> 
 <p>Common pre-processing to same voxel resolutions and spatial dimensions as well as affine pre-registration will be provided to ease the use of learning-based algorithms for participants with little prior experience in image registration.</p> 
</blockquote> 
<hr> 
<h5><a id="_112"></a>è¯¯å·®æ¥æº</h5> 
<p>CTï¼šæ ‡æ³¨è€…ä¹‹é—´çš„å¹³å‡æ€»ä½“ DSC åˆ†æ•°ï¼ˆå³æ ‡æ³¨è€…é—´çš„è¯¯å·®ï¼‰ä¸º 0.87 Â± 0.13ï¼ˆ<strong>ä»…è€ƒè™‘è„¾è„ã€è‚¾è„å’Œè‚è„æ—¶ä¸º 0.95 Â± 0.04</strong>ï¼‰ã€‚MRIï¼šå¾…å®šã€‚</p> 
<hr> 
<h5><a id="_116"></a>æŒ‡æ ‡</h5> 
<ul><li>DSC (Dice similarity coefficient) of segmentations</li><li>HD95 (95% percentile of Haussdorff distance) of segmentations</li><li>Robustness: 30% lowest DSC of all cases</li><li>SD (standard deviation) of log Jacobian determinant</li><li>Run-time computation time</li></ul> 
<p>DSC æˆ– TRE åº¦é‡å‡†ç¡®åº¦ï¼›HD95 åº¦é‡å¯é æ€§ï¼›ä½¿ç”¨ç¨³å¥æ€§å¾—åˆ†ï¼ˆæœ€ä½å¹³å‡ DSC çš„ 30% æˆ–æœ€é«˜å¹³å‡ TRE çš„ 30%ï¼‰å¯¹å¼‚å¸¸å€¼è¿›è¡Œæƒ©ç½šï¼›å˜å½¢åœºçš„å¹³æ»‘åº¦ï¼ˆå¯¹æ•°é›…å¯æ¯”è¡Œåˆ—å¼çš„æ ‡å‡†å·®ï¼‰åœ¨é…å‡†ä¸­å¾ˆé‡è¦ï¼›è¿è¡Œæ—¶è®¡ç®—æ—¶é—´ä¸ä¸´åºŠåº”ç”¨ç›¸å…³ã€‚</p> 
<p>æˆ‘ä»¬è®¤ä¸ºé€†ä¸€è‡´æ€§ï¼ˆinverse consistencyï¼‰æ˜¯ä¸€ä¸ªé™„åŠ çš„åº¦é‡æ ‡å‡†ï¼Œè¿™åœ¨åŒ»å­¦å›¾åƒé…å‡†ä¸­æ˜¯æœ‰äº‰è®®çš„ã€‚æˆ‘ä»¬å†³å®šä¸ä½¿ç”¨å®ƒä½œä¸ºç«äº‰ï¼ˆæ’åï¼‰æŒ‡æ ‡ï¼Œè€Œæ˜¯å‡ºäºä¿¡æ¯çš„åŸå› ï¼ˆå³é€†ä¸€è‡´ç®—æ³•æ˜¯å¦æ›´ç¨³å¥çš„é—®é¢˜ï¼‰è®¡ç®—å®ƒã€‚</p> 
<hr> 
<h5><a id="Baselines_128"></a>Baselines</h5> 
<p>æˆ‘ä»¬å°†æä¾›å‡ ç§ baseline ç®—æ³•æ¥æ¯”è¾ƒä½ çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ï¼š</p> 
<ul><li><a href="https://github.com/multimodallearning/pdd_net">PDD-Net (MICCAI '19)</a> unsupervised training</li><li><a href="https://blog.csdn.net/zuzhiang/article/details/108601599">Voxelmorph (CVPRâ€™18)</a> with and without label supervision</li><li><a href="https://github.com/multimodallearning/pdd_net">Deeds</a></li><li>Elastix, NiftyReg, and/or <a href="https://blog.csdn.net/zuzhiang/article/details/104930000">ANTs</a> (where applicable)</li></ul> 
<hr class="footnotes-sep"> 
<section class="footnotes"> 
 <ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Paul A. Yushkevich, Joseph Piven, Heather Cody Hazlett, Rachel Gimpel Smith, Sean Ho, James C. Gee, and Guido Gerig. User-guided 3D active contour segmentation of anatomical structures: Significantly improved efficiency and reliability. Neuroimage. 2006 Jul 1; 31(3):1116-28. <a href="#fnref1" rel="nofollow" class="footnote-backref">â†©ï¸</a></p> </li></ol> 
</section>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b3344451774846adef68edcf73f4aade/" rel="prev">
			<span class="pager__subtitle">Â«&thinsp;Previous</span>
			<p class="pager__title">ã€æ•°æ®é›†ã€‘Learn2Reg2021 Task 03 â€”â€” é¢„å¤„ç†çš„ OASIS 3D è„‘éƒ¨ MRI</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/91972e55a871f927b096aa3413e6b65a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;Â»</span>
			<p class="pager__title">ã€Linux åŸºç¡€ã€‘vi ç¼–è¾‘å™¨</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 ç¼–ç¨‹éšæƒ³.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>