<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【数据集】Learn2Reg2021 Task 01 —— 3D 腹部多模态 MR-CT - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【数据集】Learn2Reg2021 Task 01 —— 3D 腹部多模态 MR-CT" />
<meta property="og:description" content="Reference Learn2Reg: Comprehensive Multi-Task Medical Image Registration Challenge, Dataset and Evaluation in the Era of Deep Learning 👉
Abstract Medical image registration plays a very important role in improving clinical workflows, computer-assisted interventions and diagnosis as well as for research studies involving e.g. morphological analysis. Besides ongoing research into new concepts for optimisation, similarity metrics, domain adaptation and deformation models, deep learning for medical registration is currently starting to show promising advances that could improve the robustness, generalisation, computation speed and accuracy of conventional algorithms to enable better practical translation." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/6da2a1d46fcf26ea10954a456c2ecb94/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-09T09:40:14+08:00" />
<meta property="article:modified_time" content="2023-07-09T09:40:14+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【数据集】Learn2Reg2021 Task 01 —— 3D 腹部多模态 MR-CT</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h4><a id="Reference_1"></a>Reference</h4> 
<p>Learn2Reg: Comprehensive Multi-Task Medical Image Registration Challenge, Dataset and Evaluation in the Era of Deep Learning <a href="https://ieeexplore.ieee.org/ielx7/42/10057585/09925717.pdf" rel="nofollow">👉</a></p> 
<h4><a id="Abstract_3"></a>Abstract</h4> 
<blockquote> 
 <p>Medical image registration plays a very important role in improving clinical workflows, computer-assisted interventions and diagnosis as well as for research studies involving e.g. morphological analysis. Besides ongoing research into new concepts for optimisation, similarity metrics, domain adaptation and deformation models, <mark>deep learning for medical registration is currently starting to show promising advances that could improve the robustness, generalisation, computation speed and accuracy of conventional algorithms to enable better practical translation</mark>. <strong>Nevertheless, before Learn2Reg there was no commonly used benchmark dataset to compare stateof-the-art learning based registration among another and with their conventional (not trained) counterparts</strong>. With few exceptions (CuRIOUS at MICCAI 2018/2019, the Continuous Registration Challenge at WBIR 2018 and Learn2Reg 2020) there has also been no comprehensive registration challenge covering different anatomical structures and evaluation metrics. We also believe that the entry barrier for new teams to contribute to this emerging field are higher than e.g. for segmentation, where standardised datasets (e.g. Medical Decathlon, BraTS) are easily available. <mark>In contrast, many registration tasks, require resampling from different voxel spacings, affine pre-registration and can lead to ambiguous and error-prone evaluation of whole deformation fields</mark>. We propose a simplified challenge design that removes many of the common pitfalls for learning and applying transformations. <strong>We will provide pre-preprocessed data (resample, crop, pre-align, etc.) that can be directly employed by most conventional and learning frameworks</strong>. Only docker containers that generate displacement fields in voxel dimensions in a standard orientation will have to be provided by participants and python code to test their application (on local machines) to training data will be provided as open-source along with all evaluation metrics. Our challenge will consist of three clinically relevant sub-tasks (datasets) that are complementary in nature. <strong>They can either be individually or comprehensively addressed by participants and cover both intra- and inter-patient alignment, CT, ultrasound and MRI modalities, neuro-, thorax and abdominal anatomies and the four of the imminent challenges of medical image registration:</strong></p> 
 <ul><li><strong>learning from small datasets</strong></li><li><strong>estimating large deformations</strong></li><li><strong>dealing with multi-modal scans</strong></li><li><strong>learning from noisy annotations</strong></li></ul> 
</blockquote> 
<blockquote> 
 <p><mark>黄色部分</mark> 是可以直接引用在论文写作中的；<strong>黑体部分</strong> 是 <a href="https://learn2reg.grand-challenge.org/Learn2Reg2021/" rel="nofollow">Learn2Reg2021</a> 的意义和提供的数据集以及提出的挑战。</p> 
</blockquote> 
<hr> 
<h4><a id="Task_01_Abdominal_MRCT_14"></a>Task 01: Abdominal MR-CT</h4> 
<h5><a id="Abstract_15"></a>Abstract</h5> 
<p>人体腹部是一个重要而复杂的身体空间。腹部以横膈为上界，以骨盆为下界，以脊椎为支撑，以肌肉腹壁为保护，腹部包含血液储备、排毒、排尿、内分泌功能和消化等器官，并包括许多重要的动脉和静脉。</p> 
<p>计算机断层扫描（CT）扫描和磁共振图像（MRI）通常用于腹部相关疾病的诊断和预后或干预计划；然而，<strong>针对腹部的特定图像配准工具很少，并且几乎没有算法能够处理多模态配准。</strong></p> 
<blockquote> 
 <p>yet few specific image registration tools for the abdomen have been developed and <strong>nearly no algorithm is capable of dealing with multimodal alignment.</strong></p> 
</blockquote> 
<p>在腹部 CT 和 MRI 上，不同表现之间的差异（例如，年龄、性别、身高、正常解剖变异和疾病状态）可以通过观察每个器官的大小、形状和外观来预测。但是，<strong>个体本身由于诸如：姿势、呼吸周期、水肿、消化状态等，很容易改变腹部器官的形状和器官之间的位置关系，使配准进一步复杂化。</strong></p> 
<blockquote> 
 <p>On abdominal CT and MRI, inter-subject variability (e.g., age, gender, stature, normal anatomical variants, and disease status) can be observed in terms of the size, shape, and appearance of each organ. <strong>Soft anatomy deformation further complicates the registration by varying the inter-organ relationships, even within individuals (e.g., pose, respiratory cycle, edema, digestive status).</strong></p> 
</blockquote> 
<p>在 Learn2reg 挑战中，<strong>这项任务是对几个分离的区域进行配准</strong>，这些区域具有较大的器官间差异和巨大的体积可变性：从几百个体素的小器官到非常大的器官。</p> 
<blockquote> 
 <p>aligning several disjunct regions with large inter-subject variations and<br> great variability in volume: from a few hundred voxels to very large organs.</p> 
</blockquote> 
<p><strong>当仅有一种模态有大量的标签时，域自适应可以在多模态配准中起决定性作用</strong>。为了探索这种多模态迁移学习的挑战，我们将只提供用于训练的 CT 标签，但在测试中评估模态间（CT-MR）和模态内配准。</p> 
<blockquote> 
 <p><strong>Domain adaptation can play a decisive role when large labelled datasets are only available for one modality</strong>. To explore the challenges of this multimodal transfer learning, we will only provide CT labels for training, but evaluate both inter- and intra-modal registration at test.</p> 
</blockquote> 
<hr> 
<h5><a id="Keywords_32"></a>Keywords</h5> 
<p>intra-patient, CT, MRI, registration, multimodal</p> 
<hr> 
<h5><a id="challenge_36"></a>challenge</h5> 
<ul><li>Multimodal registration.</li><li>Learning from few/noisy annotations.</li><li>Learning with domain gaps.</li></ul> 
<hr> 
<h5><a id="Cohorts_42"></a>Cohorts</h5> 
<ul><li> <p><strong>Target cohorts</strong> - 哪些主体 / 对象将在最终的应用中获得有用的数据？</p> <p>一方面，接受图像引导手术干预、活检或放射治疗的患者可以受益于可变形解剖器官图谱，该图谱可捕捉危险器官和靶区的空间关系。另一方面，对大群体进行形状分析可以深入了解与常见疾病相关的流行病学差异。</p> 
  <blockquote> 
   <p>On the one hand patients undergoing image-guided surgical interventions, biopsies or radiotherapy could benefit from a deformable anatomical organ atlas that captures spatial relations of organs-at-risk and target regions. On the other hand, shape analysis over large cohorts could provide insight into epidemiological difference in relation to common disease.</p> 
  </blockquote> </li><li> <p><strong>challenge cohort</strong> - 获取这些数据的主体 / 对象。</p> <p>在接受结直肠癌化疗试验的患者中，腹部 CT 扫描的基线时段（baseline sessions）是从转移性肝癌患者中随机选择的；其余的扫描是从疑似腹疝的术后回顾性队列（cohort）中获得的。附加的隐藏数据集是研究一般人群的全身 MRI。</p> 
  <blockquote> 
   <p>Patients from an colorectal cancer chemotherapy trial, the baseline sessions of the abdominal CT scans were randomly selected from metastatic liver cancer patients; the remaining scans were acquired from a retrospective post-operative cohort with suspected ventral hernias. Additional hidden dataset of general population study with whole-body MRI.</p> 
  </blockquote> </li></ul> 
<hr> 
<h5><a id="Imaging_modalityies_54"></a>Imaging modality(ies)</h5> 
<p>Magnetic Resonance Imaging (MRI) &amp; Computed Tomography (CT)</p> 
<p><strong>fixed images: MR; moving images: CT</strong></p> 
<hr> 
<h5><a id="Context_information_60"></a>Context information</h5> 
<p>主要考虑<strong>来自同一患者腹部的成对 CT 和 MRI 扫描。附加另外两个未配对的 CT (<a href="https://learn2reg.grand-challenge.org/Learn2Reg2020/" rel="nofollow">Task3 L2R’20</a>) 和 MRI (CHAOS MR) 数据集用于辅助训练。</strong></p> 
<p>122 CT/MR scans (16 CT-MR scan pairs (8 Training, 8 Test) + 90 unpaired CT (50)/MR (40) scans)</p> 
<ul><li>对于 CT：附加的子集将包括 13 个被认为是感兴趣区域（ROI）的腹部器官，包括脾脏、右肾、左肾、胆囊、食管、肝、胃、主动脉、下腔静脉、门静脉和脾静脉、胰腺、左肾上腺和右肾上腺。并非所有的 ROI 都是有手动分割的。</li><li>对于 MRI：只有手动分割较少数量的器官，并部分提供训练数据。<strong>我们鼓励使用多模态领域适应学习的方法进行配准。</strong></li></ul> 
<blockquote> 
 <p>We encourage approaches that <strong>learn by multimodal domain adaptation.</strong></p> 
</blockquote> 
<hr> 
<h5><a id="Algorithm_target_71"></a>Algorithm target</h5> 
<blockquote> 
 <p>即，说明算法设计用于关注的结构 / 主题 / 对象 / 组件（例如大脑肿瘤、医疗器械尖端、手术室护士、透视扫描中的导管）。</p> 
</blockquote> 
<p><strong>算法应该关注腹部器官在单模和多模情况下的配准。重点在特定的感兴趣区域（ROI）的对齐方面，例如，脾脏，右肾，左肾和肝脏（就这 4 个 ROI 有手动分割标签）</strong>；以及较小器官：胆囊，食管，胃，下腔静脉，门耳和脾静脉，胰腺的对齐；以及生成具有空间平滑度的合理变形（雅可比行列式的低标准偏差）。</p> 
<blockquote> 
 <p>Alignment of abdominal organs within and across modalities in a heterogenous patient cohort. <strong>The focus will be on the alignment of particular regions of interest (ROI), e.g. spleen, right kidney, left kidney and liver.</strong></p> 
</blockquote> 
<hr> 
<h5><a id="Training_and_test_case_characteristics_79"></a>Training and test case characteristics</h5> 
<p>一个 case 指的是一对 CT/MR 扫描，每对扫描来自不同的患者（患者间配准）。所有 CT 扫描（训练和测试）手动分割标签一起提供。所有成对的 CT/MRI 扫描都将手动分割标签（隐藏测试标签）。</p> 
<ul><li> <p>说明训练、验证和测试案例的总数。</p> <p>Training: 20-25 paired MR/CT cases (40-50 scans) + 30 additional CT scans and ~30 additional MRI Test: 10 paired MR/CT cases (20 scans)</p> </li><li> <p>Mention further important characteristics of the training, validation and test cases</p> <p>在这个 intra-subject 配准任务中，可以直接使用多个成对的 MR/CT 扫描进行有监督学习。通过提供不成对的 CT 和 MRI 扫描，跨领域学习可以成为这项挑战任务的一个组成部分。</p> 
  <blockquote> 
   <p>In this intra-subject registration task, a number of <strong>paired MR/CT scans can be directly employed for supervised training</strong>. Cross-domain learning can be an integral part of this challenge task with the provision of unpaired CT and MRI scans.</p> 
  </blockquote> </li></ul> 
<hr> 
<h5><a id="_92"></a>手动分割标签的特征</h5> 
<p>Annotation characteristics</p> 
<p>MRI 和配对的 MRI/CT：<strong>至少四个腹部器官的手动 3D 体素分割：肝脏（1）、脾脏（2）、右肾（3）、左肾（4）</strong>，标注是来自经验丰富的研究生，具有 3 年以上的医学成像经验。ITK-SNAP<sup class="footnote-ref"><a href="#fn1" rel="nofollow" id="fnref1">1</a></sup><br> <img src="https://images2.imgbox.com/d6/8f/IKN5Dcz0_o.png" alt="0"></p> 
<blockquote> 
 <p>MRI and paired MRI/CT: manual 3D voxel segmentation of at least four abdominal organs: liver(1), spleen(2), right kidney(3), left kidney(4),</p> 
</blockquote> 
<p>附加 CT：13 个腹部器官被视为感兴趣区域（ROI），包括脾脏、右肾、左肾、胆囊、食管、肝、胃、主动脉、下腔静脉、门静脉和脾静脉、胰腺、左肾上腺和右肾上腺。</p> 
<blockquote> 
 <p>正如一位放射科医生所建议的，由于数据集中缺乏完整的外观，心脏被排除在外，取而代之的是肾上腺被纳入临床研究。</p> 
</blockquote> 
<hr> 
<h5><a id="_104"></a>数据预处理</h5> 
<p>Data pre-processing method(s)</p> 
<p><strong>提供相同体素分辨率（192 × 160 × 192）和体素空间维度（2mm voxel spacing）的通用预处理以及仿射预配准</strong>，以便于先前在图像配准方面经验不足的参与者使用基于学习的算法。</p> 
<blockquote> 
 <p>Common pre-processing to same voxel resolutions and spatial dimensions as well as affine pre-registration will be provided to ease the use of learning-based algorithms for participants with little prior experience in image registration.</p> 
</blockquote> 
<hr> 
<h5><a id="_112"></a>误差来源</h5> 
<p>CT：标注者之间的平均总体 DSC 分数（即标注者间的误差）为 0.87 ± 0.13（<strong>仅考虑脾脏、肾脏和肝脏时为 0.95 ± 0.04</strong>）。MRI：待定。</p> 
<hr> 
<h5><a id="_116"></a>指标</h5> 
<ul><li>DSC (Dice similarity coefficient) of segmentations</li><li>HD95 (95% percentile of Haussdorff distance) of segmentations</li><li>Robustness: 30% lowest DSC of all cases</li><li>SD (standard deviation) of log Jacobian determinant</li><li>Run-time computation time</li></ul> 
<p>DSC 或 TRE 度量准确度；HD95 度量可靠性；使用稳健性得分（最低平均 DSC 的 30% 或最高平均 TRE 的 30%）对异常值进行惩罚；变形场的平滑度（对数雅可比行列式的标准差）在配准中很重要；运行时计算时间与临床应用相关。</p> 
<p>我们认为逆一致性（inverse consistency）是一个附加的度量标准，这在医学图像配准中是有争议的。我们决定不使用它作为竞争（排名）指标，而是出于信息的原因（即逆一致算法是否更稳健的问题）计算它。</p> 
<hr> 
<h5><a id="Baselines_128"></a>Baselines</h5> 
<p>我们将提供几种 baseline 算法来比较你的方法，包括：</p> 
<ul><li><a href="https://github.com/multimodallearning/pdd_net">PDD-Net (MICCAI '19)</a> unsupervised training</li><li><a href="https://blog.csdn.net/zuzhiang/article/details/108601599">Voxelmorph (CVPR’18)</a> with and without label supervision</li><li><a href="https://github.com/multimodallearning/pdd_net">Deeds</a></li><li>Elastix, NiftyReg, and/or <a href="https://blog.csdn.net/zuzhiang/article/details/104930000">ANTs</a> (where applicable)</li></ul> 
<hr class="footnotes-sep"> 
<section class="footnotes"> 
 <ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Paul A. Yushkevich, Joseph Piven, Heather Cody Hazlett, Rachel Gimpel Smith, Sean Ho, James C. Gee, and Guido Gerig. User-guided 3D active contour segmentation of anatomical structures: Significantly improved efficiency and reliability. Neuroimage. 2006 Jul 1; 31(3):1116-28. <a href="#fnref1" rel="nofollow" class="footnote-backref">↩︎</a></p> </li></ol> 
</section>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b3344451774846adef68edcf73f4aade/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【数据集】Learn2Reg2021 Task 03 —— 预处理的 OASIS 3D 脑部 MRI</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/91972e55a871f927b096aa3413e6b65a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【Linux 基础】vi 编辑器</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>