<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>人体关键点检测2：Pytorch实现人体关键点检测(人体姿势估计)含训练代码 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="人体关键点检测2：Pytorch实现人体关键点检测(人体姿势估计)含训练代码" />
<meta property="og:description" content="人体关键点检测2：Pytorch实现人体关键点检测(人体姿势估计)含训练代码 目录
人体关键点检测2：Pytorch实现人体关键点检测(人体姿势估计)含训练代码
1. 前言
2.人体关键点检测方法
(1)Top-Down(自上而下)方法
(2)Bottom-Up(自下而上)方法：
3.人体关键点检测数据集
4.人体检测模型训练
5.人体关键点检测模型训练
（1）项目安装
（2）准备Train和Test数据
（3）配置文件configs
（4）开始训练
（5）Tensorboard可视化训练过程
6.人体关键点检测检测模型效果
7.人体关键点检测(推理代码)下载
8.人体关键点检测(训练代码)下载
9.人体关键点检测C&#43;&#43;/Android版本
1. 前言 人体关键点检测（Human Keypoints Detection）又称为人体姿态估计2D Pose，是计算机视觉中一个相对基础的任务，是人体动作识别、行为分析、人机交互等的前置任务。一般情况下可以将人体关键点检测细分为单人/多人关键点检测、2D/3D关键点检测，同时有算法在完成关键点检测之后还会进行关键点的跟踪，也被称为人体姿态跟踪。
本项目将实现人体关键点检测算法，其中使用YOLOv5模型实现人体检测(Person Detection)，使用HRNet，LiteHRNet和Mobilenet-v2模型实现人体关键点检测。项目分为数据集说明，模型训练和C&#43;&#43;/Android部署等多个章节，本篇是项目《人体关键点检测(人体姿势估计)》系列文章之Pytorch实现人体关键点检测(人体姿势估计)；为了方便后续模型工程化和Android平台部署，项目支持高精度HRNet检测模型，轻量化模型LiteHRNet和Mobilenet模型训练和测试，并提供Python/C&#43;&#43;/Android多个版本；
轻量化Mobilenet-v2模型在普通Android手机上可以达到实时的检测效果，CPU(4线程)约50ms左右，GPU约30ms左右 ，基本满足业务的性能需求。下表格给出HRNet，以及轻量化模型LiteHRNet和Mobilenet的计算量和参数量，以及其检测精度
模型input-sizeparams(M)GFLOPsAPHRNet-w32192×25628.48M5734.05M0.7585LiteHRNet18192×2561.10M182.15M0.6237Mobilenet-v2192×2562.63M529.25M0.6181 先展示一下人体关键点检测效果：
Android人体关键点检测APP Demo体验(下载)：https://download.csdn.net/download/guyuealian/88610359
【尊重原创，转载请注明出处】https://blog.csdn.net/guyuealian/article/details/134837816
更多项目《人体关键点检测(人体姿势估计)》系列文章请参考：
人体关键点检测1：人体姿势估计数据集(含下载链接) https://blog.csdn.net/guyuealian/article/details/134703548人体关键点检测2：Pytorch实现人体关键点检测(人体姿势估计)含训练代码和数据集 https://blog.csdn.net/guyuealian/article/details/134837816人体关键点检测3：Android实现人体关键点检测(人体姿势估计)含源码 可实时检测 https://blog.csdn.net/guyuealian/article/details/134881797人体关键点检测4：C/C&#43;&#43;实现人体关键点检测(人体姿势估计)含源码 可实时检测 https://blog.csdn.net/guyuealian/article/details/134881831手部关键点检测1：手部关键点(手部姿势估计)数据集(含下载链接)https://blog.csdn.net/guyuealian/article/details/133277630手部关键点检测2：YOLOv5实现手部检测(含训练代码和数据集)https://blog.csdn.net/guyuealian/article/details/133279222手部关键点检测3：Pytorch实现手部关键点检测(手部姿势估计)含训练代码和数据集https://blog.csdn.net/guyuealian/article/details/133277726手部关键点检测4：Android实现手部关键点检测(手部姿势估计)含源码 可实时检测https://blog.csdn.net/guyuealian/article/details/133931698手部关键点检测5：C&#43;&#43;实现手部关键点检测(手部姿势估计)含源码 可实时检测https://blog.csdn.net/guyuealian/article/details/133277748 2.人体关键点检测方法 目前主流的人体关键点检测(人体姿势估计)方法主要两种：一种是Top-Down（自上而下）方法，另外一种是Bottom-Up（自下而上）方法；
(1)Top-Down(自上而下)方法 将人体检测和人体关键点检测(人体姿势估计)检测分离，在图像上首先进行人体目标检测，定位人体位置；然后crop每一个人体图像，再估计人体关键点；这类方法往往比较慢，但姿态估计准确度较高。目前主流模型主要有CPN，Hourglass，CPM，Alpha Pose，HRNet等。
(2)Bottom-Up(自下而上)方法： 先估计图像中所有人体关键点，然后在通过Grouping的方法组合成一个一个实例；因此这类方法在测试推断的时候往往更快速，准确度稍低。典型就是COCO2016年人体关键点检测冠军Open Pose。
通常来说，Top-Down具有更高的精度，而Bottom-Up具有更快的速度；就目前调研而言， Top-Down的方法研究较多，精度也比Bottom-Up（自下而上）方法高。本项目采用Top-Down(自上而下)方法，先使用YOLOv5模型实现人体检测，然后再使用HRNet进行人体关键点检测(人体姿势估计)；
本项目基于开源的HRNet进行改进，关于HRNet项目请参考GitHub
HRNet: https://github.com/leoxiaobin/deep-high-resolution-net.pytorch
3.人体关键点检测数据集 本项目主要使用COCO数据集和MPII数据集，关于人体关键点检测数据集说明，请参考《人体关键点检测1：人体姿势估计数据集》https://blog.csdn.net/guyuealian/article/details/134703548
4.人体检测模型训练 本项目采用Top-Down(自上而下)方法，使用YOLOv5模型实现人体目标检测，使用HRNet进行人体关键点检测(人体姿势估计)；关于人体检测模型训练方法，可参考 :
行人检测(人体检测)2：YOLOv5实现人体检测(含人体检测数据集和训练代码)
5.人体关键点检测模型训练 整套工程项目基本结构如下：
. ├── configs # 训练配置文件 ├── data # 一些数据 ├── libs # 一些工具库 ├── pose # 姿态估计模型文件 ├── work_space # 训练输出工作目录 ├── demo." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/7ae2cbf21bc60fd7c502cb9e9caf2e1d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-18T18:00:43+08:00" />
<meta property="article:modified_time" content="2023-12-18T18:00:43+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">人体关键点检测2：Pytorch实现人体关键点检测(人体姿势估计)含训练代码</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2 id="%E6%89%8B%E9%83%A8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B3%EF%BC%9APytorch%E5%AE%9E%E7%8E%B0%E6%89%8B%E9%83%A8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B(%E6%89%8B%E9%83%A8%E5%A7%BF%E5%8A%BF%E4%BC%B0%E8%AE%A1)%E5%90%AB%E8%AE%AD%E7%BB%83%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86">人体关键点检测2：Pytorch实现人体关键点检测(人体姿势估计)含训练代码</h2> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="%E6%89%8B%E9%83%A8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B3%EF%BC%9APytorch%E5%AE%9E%E7%8E%B0%E6%89%8B%E9%83%A8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B(%E6%89%8B%E9%83%A8%E5%A7%BF%E5%8A%BF%E4%BC%B0%E8%AE%A1)%E5%90%AB%E8%AE%AD%E7%BB%83%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86-toc" style="margin-left:0px;"><a href="#%E6%89%8B%E9%83%A8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B3%EF%BC%9APytorch%E5%AE%9E%E7%8E%B0%E6%89%8B%E9%83%A8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%28%E6%89%8B%E9%83%A8%E5%A7%BF%E5%8A%BF%E4%BC%B0%E8%AE%A1%29%E5%90%AB%E8%AE%AD%E7%BB%83%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86" rel="nofollow">人体关键点检测2：Pytorch实现人体关键点检测(人体姿势估计)含训练代码</a></p> 
<p id="1.%20%E5%89%8D%E8%A8%80-toc" style="margin-left:40px;"><a href="#1.%20%E5%89%8D%E8%A8%80" rel="nofollow">1. 前言</a></p> 
<p id="2.%20Fruit-Dataset%E6%B0%B4%E6%9E%9C%E6%95%B0%E6%8D%AE%E9%9B%86-toc" style="margin-left:40px;"><a href="#2.%20Fruit-Dataset%E6%B0%B4%E6%9E%9C%E6%95%B0%E6%8D%AE%E9%9B%86" rel="nofollow">2.人体关键点检测方法</a></p> 
<p id="(1)Top-Down(%E8%87%AA%E4%B8%8A%E8%80%8C%E4%B8%8B)%E6%96%B9%E6%B3%95-toc" style="margin-left:80px;"><a href="#%281%29Top-Down%28%E8%87%AA%E4%B8%8A%E8%80%8C%E4%B8%8B%29%E6%96%B9%E6%B3%95" rel="nofollow">(1)Top-Down(自上而下)方法</a></p> 
<p id="(2)Bottom-Up(%E8%87%AA%E4%B8%8B%E8%80%8C%E4%B8%8A)%E6%96%B9%E6%B3%95%EF%BC%9A-toc" style="margin-left:80px;"><a href="#%282%29Bottom-Up%28%E8%87%AA%E4%B8%8B%E8%80%8C%E4%B8%8A%29%E6%96%B9%E6%B3%95%EF%BC%9A" rel="nofollow">(2)Bottom-Up(自下而上)方法：</a></p> 
<p id="3.%E6%89%8B%E9%83%A8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86-toc" style="margin-left:40px;"><a href="#3.%E6%89%8B%E9%83%A8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86" rel="nofollow">3.人体关键点检测数据集</a></p> 
<p id="4.%E6%89%8B%E9%83%A8%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-toc" style="margin-left:40px;"><a href="#4.%E6%89%8B%E9%83%A8%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83" rel="nofollow">4.人体检测模型训练</a></p> 
<p id="4.%20%E5%9F%BA%E4%BA%8EYOLOv5%E7%9A%84%E6%89%8B%E5%8A%BF%E8%AF%86%E5%88%AB%E8%AE%AD%E7%BB%83-toc" style="margin-left:40px;"><a href="#4.%20%E5%9F%BA%E4%BA%8EYOLOv5%E7%9A%84%E6%89%8B%E5%8A%BF%E8%AF%86%E5%88%AB%E8%AE%AD%E7%BB%83" rel="nofollow">5.人体关键点检测模型训练</a></p> 
<p id="%EF%BC%881%EF%BC%89%E9%A1%B9%E7%9B%AE%E5%AE%89%E8%A3%85-toc" style="margin-left:80px;"><a href="#%EF%BC%881%EF%BC%89%E9%A1%B9%E7%9B%AE%E5%AE%89%E8%A3%85" rel="nofollow">（1）项目安装</a></p> 
<p id="%EF%BC%882%EF%BC%89%E5%87%86%E5%A4%87Train%E5%92%8CTest%E6%95%B0%E6%8D%AE-toc" style="margin-left:80px;"><a href="#%EF%BC%882%EF%BC%89%E5%87%86%E5%A4%87Train%E5%92%8CTest%E6%95%B0%E6%8D%AE" rel="nofollow">（2）准备Train和Test数据</a></p> 
<p id="%EF%BC%883%EF%BC%89%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6configs-toc" style="margin-left:80px;"><a href="#%EF%BC%883%EF%BC%89%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6configs" rel="nofollow">（3）配置文件configs</a></p> 
<p id="%EF%BC%884%EF%BC%89%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83-toc" style="margin-left:80px;"><a href="#%EF%BC%884%EF%BC%89%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83" rel="nofollow">（4）开始训练</a></p> 
<p id="%EF%BC%885%EF%BC%89Tensorboard%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B-toc" style="margin-left:80px;"><a href="#%EF%BC%885%EF%BC%89Tensorboard%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B" rel="nofollow">（5）Tensorboard可视化训练过程</a></p> 
<p id="4.%20%E5%9E%83%E5%9C%BE%E5%88%86%E7%B1%BB%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%E6%B5%8B%E8%AF%95%E6%95%88%E6%9E%9C-toc" style="margin-left:40px;"><a href="#4.%20%E5%9E%83%E5%9C%BE%E5%88%86%E7%B1%BB%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%E6%B5%8B%E8%AF%95%E6%95%88%E6%9E%9C" rel="nofollow">6.人体关键点检测检测模型效果</a></p> 
<p id="6.%E6%89%8B%E9%83%A8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B(%E6%8E%A8%E7%90%86%E4%BB%A3%E7%A0%81)-toc" style="margin-left:40px;"><a href="#6.%E6%89%8B%E9%83%A8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%28%E6%8E%A8%E7%90%86%E4%BB%A3%E7%A0%81%29" rel="nofollow">7.人体关键点检测(推理代码)下载</a></p> 
<p id="7.%E6%89%8B%E9%83%A8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B(%E8%AE%AD%E7%BB%83%E4%BB%A3%E7%A0%81)-toc" style="margin-left:40px;"><a href="#7.%E6%89%8B%E9%83%A8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%28%E8%AE%AD%E7%BB%83%E4%BB%A3%E7%A0%81%29" rel="nofollow">8.人体关键点检测(训练代码)下载</a></p> 
<p id="8.%E6%89%8B%E9%83%A8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8BC%2B%2B%2FAndroid%E7%89%88%E6%9C%AC-toc" style="margin-left:40px;"><a href="#8.%E6%89%8B%E9%83%A8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8BC%2B%2B%2FAndroid%E7%89%88%E6%9C%AC" rel="nofollow">9.人体关键点检测C++/Android版本</a></p> 
<hr> 
<h3 id="1.%20%E5%89%8D%E8%A8%80">1. 前言</h3> 
<p>人体关键点检测（Human Keypoints Detection）又称为人体姿态估计2D Pose，是计算机视觉中一个相对基础的任务，是人体动作识别、行为分析、人机交互等的前置任务。一般情况下可以将人体关键点检测细分为单人/多人关键点检测、2D/3D关键点检测，同时有算法在完成关键点检测之后还会进行关键点的跟踪，也被称为人体姿态跟踪。</p> 
<p>本项目将实现<strong>人体关键点检测</strong>算法，其中使用YOLOv5模型实现人体检测(Person Detection)，使用HRNet，LiteHRNet和Mobilenet-v2模型实现<strong>人体关键点检测</strong>。项目分为<strong>数据集说明，模型训练</strong>和<strong>C++/Android部署</strong>等多个章节，本篇是项目《<strong>人体关键点检测(人体姿势估计)</strong>》系列文章之<strong>Pytorch实现人体关键点检测(人体姿势估计)</strong>；为了方便后续模型工程化和Android平台部署，项目支持高精度HRNet检测模型，轻量化模型LiteHRNet和Mobilenet模型训练和测试，并提供Python/C++/Android多个版本；</p> 
<p><img alt="" height="421" src="https://images2.imgbox.com/db/a0/nZoNB8ge_o.png" width="600"></p> 
<p>轻量化Mobilenet-v2模型在普通Android手机上可以达到实时的检测效果，CPU(4线程)约50ms左右，GPU约30ms左右 ，基本满足业务的性能需求。下表格给出HRNet，以及轻量化模型LiteHRNet和Mobilenet的计算量和参数量，以及其检测精度</p> 
<table align="center" border="1" cellpadding="1" cellspacing="1"><tbody><tr><td style="text-align:center;"><strong>模型</strong></td><td style="text-align:center;"><strong>input-size</strong></td><td style="text-align:center;"><strong>params(M)</strong></td><td style="text-align:center;"><strong>GFLOPs</strong></td><td style="text-align:center;"><strong>AP</strong></td></tr><tr><td style="text-align:center;">HRNet-w32</td><td style="text-align:center;">192×256</td><td style="text-align:center;">28.48M</td><td style="text-align:center;">5734.05M</td><td style="text-align:center;">0.7585</td></tr><tr><td style="text-align:center;">LiteHRNet18</td><td style="text-align:center;">192×256</td><td style="text-align:center;">1.10M</td><td style="text-align:center;">182.15M</td><td style="text-align:center;">0.6237</td></tr><tr><td style="text-align:center;">Mobilenet-v2</td><td style="text-align:center;">192×256</td><td style="text-align:center;">2.63M</td><td style="text-align:center;">529.25M</td><td style="text-align:center;">0.6181</td></tr></tbody></table> 
<p>先展示一下<strong>人体关键点检测</strong>效果：</p> 
<p><img alt="" height="333" src="https://images2.imgbox.com/63/c7/u0JGQfnb_o.gif" width="600"></p> 
<p></p> 
<p></p> 
<p>Android<strong>人体关键点检测</strong>APP Demo体验(<a class="link-info" href="https://download.csdn.net/download/guyuealian/88610359" title="下载">下载</a>)：<a href="https://download.csdn.net/download/guyuealian/88610359" title="https://download.csdn.net/download/guyuealian/88610359">https://download.csdn.net/download/guyuealian/88610359</a></p> 
<p>【尊重原创，转载请注明出处】<a href="https://blog.csdn.net/guyuealian/article/details/134837816" title="https://blog.csdn.net/guyuealian/article/details/134837816">https://blog.csdn.net/guyuealian/article/details/134837816</a></p> 
<hr> 
<p>更多项目《<strong>人体关键点检测(人体姿势估计)</strong>》系列文章请参考：</p> 
<ul><li>人体关键点检测1：人体姿势估计数据集(含下载链接) <a href="https://blog.csdn.net/guyuealian/article/details/134703548" title="https://blog.csdn.net/guyuealian/article/details/134703548">https://blog.csdn.net/guyuealian/article/details/134703548</a></li><li>人体关键点检测2：Pytorch实现人体关键点检测(人体姿势估计)含训练代码和数据集 <a href="https://blog.csdn.net/guyuealian/article/details/134837816" title="https://blog.csdn.net/guyuealian/article/details/134837816">https://blog.csdn.net/guyuealian/article/details/134837816</a></li><li>人体关键点检测3：Android实现人体关键点检测(人体姿势估计)含源码 可实时检测 <a href="https://blog.csdn.net/guyuealian/article/details/134881797" title="https://blog.csdn.net/guyuealian/article/details/134881797">https://blog.csdn.net/guyuealian/article/details/134881797</a></li><li>人体关键点检测4：C/C++实现人体关键点检测(人体姿势估计)含源码 可实时检测 <a href="https://blog.csdn.net/guyuealian/article/details/134881831" title="https://blog.csdn.net/guyuealian/article/details/134881831">https://blog.csdn.net/guyuealian/article/details/134881831</a></li><li>手部关键点检测1：手部关键点(手部姿势估计)数据集(含下载链接)<a href="https://blog.csdn.net/guyuealian/article/details/133277630" title="https://blog.csdn.net/guyuealian/article/details/133277630">https://blog.csdn.net/guyuealian/article/details/133277630</a></li><li>手部关键点检测2：YOLOv5实现手部检测(含训练代码和数据集)<a href="https://blog.csdn.net/guyuealian/article/details/133279222" title="https://blog.csdn.net/guyuealian/article/details/133279222">https://blog.csdn.net/guyuealian/article/details/133279222</a></li><li>手部关键点检测3：Pytorch实现手部关键点检测(手部姿势估计)含训练代码和数据集<a href="https://blog.csdn.net/guyuealian/article/details/133277726" title="https://blog.csdn.net/guyuealian/article/details/133277726">https://blog.csdn.net/guyuealian/article/details/133277726</a></li><li>手部关键点检测4：Android实现手部关键点检测(手部姿势估计)含源码 可实时检测<a href="https://blog.csdn.net/guyuealian/article/details/133931698" title="https://blog.csdn.net/guyuealian/article/details/133931698">https://blog.csdn.net/guyuealian/article/details/133931698</a></li><li>手部关键点检测5：C++实现手部关键点检测(手部姿势估计)含源码 可实时检测<a href="https://blog.csdn.net/guyuealian/article/details/133277748" title="https://blog.csdn.net/guyuealian/article/details/133277748">https://blog.csdn.net/guyuealian/article/details/133277748</a></li></ul> 
<p>  <img alt="" height="338" src="https://images2.imgbox.com/0e/32/5Plrhi3e_o.gif" width="600"></p> 
<p><img alt="" height="338" src="https://images2.imgbox.com/c5/a2/qUWgYg62_o.gif" width="600"></p> 
<hr> 
<h3 id="2.%20Fruit-Dataset%E6%B0%B4%E6%9E%9C%E6%95%B0%E6%8D%AE%E9%9B%86">2.<strong>人体关键点检测</strong>方法</h3> 
<p>目前主流的<strong>人体关键点检测(人体姿势估计)</strong>方法主要两种：一种是<strong><strong>Top-Down</strong></strong>（自上而下）方法，另外一种是<strong><strong>Bottom-Up</strong></strong>（自下而上）方法；</p> 
<h4 id="(1)Top-Down(%E8%87%AA%E4%B8%8A%E8%80%8C%E4%B8%8B)%E6%96%B9%E6%B3%95"><strong><strong>(1)Top-Down(</strong></strong>自上而下<strong><strong>)</strong></strong>方法</h4> 
<blockquote> 
 <p>将人体检测和<strong>人体关键点检测(人体姿势估计)</strong>检测分离，在图像上首先进行人体目标检测，定位人体位置；然后crop每一个人体图像，再估计人体关键点；这类方法往往比较慢，但姿态估计准确度较高。目前主流模型主要有CPN，Hourglass，CPM，Alpha Pose，HRNet等。</p> 
</blockquote> 
<h4 id="(2)Bottom-Up(%E8%87%AA%E4%B8%8B%E8%80%8C%E4%B8%8A)%E6%96%B9%E6%B3%95%EF%BC%9A"><strong><strong>(2)Bottom-Up(</strong></strong>自下而上<strong><strong>)</strong></strong>方法：</h4> 
<blockquote> 
 <p>先估计图像中所有人体关键点，然后在通过Grouping的方法组合成一个一个实例；因此这类方法在测试推断的时候往往更快速，准确度稍低。典型就是COCO2016年人体关键点检测冠军Open Pose。</p> 
</blockquote> 
<p>通常来说，T<strong>op-Down具有更高的精度，而Bottom-Up具有更快的速度；</strong>就目前调研而言， Top-Down的方法研究较多，精度也比<strong><strong>Bottom-Up</strong></strong>（自下而上）方法高。本项目采用<strong><strong>Top-Down(</strong></strong>自上而下<strong><strong>)</strong></strong>方法，先使用YOLOv5模型实现人体检测，然后再使用HRNet进行人体关键点检测(人体姿势估计)；</p> 
<p>本项目基于开源的HRNet进行改进，关于HRNet项目请参考GitHub</p> 
<blockquote> 
 <p>HRNet: https://github.com/leoxiaobin/deep-high-resolution-net.pytorch</p> 
</blockquote> 
<hr> 
<h3 id="3.%E6%89%8B%E9%83%A8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86">3.人体关键点检测数据集</h3> 
<p>本项目主要使用COCO数据集和MPII数据集，关于人体关键点检测数据集说明，请参考《<strong>人体关键点检测1：人体姿势估计数据集</strong>》<a href="https://blog.csdn.net/guyuealian/article/details/134703548" title="https://blog.csdn.net/guyuealian/article/details/134703548">https://blog.csdn.net/guyuealian/article/details/134703548</a></p> 
<hr> 
<h3 id="4.%E6%89%8B%E9%83%A8%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83">4.人体检测模型训练</h3> 
<p>本项目采用<strong><strong>Top-Down(</strong></strong>自上而下<strong><strong>)</strong></strong>方法，使用YOLOv5模型实现人体目标检测，使用HRNet进行<strong>人体关键点检测(人体姿势估计)</strong>；关于人体检测模型训练方法，可参考 :</p> 
<p><a href="https://blog.csdn.net/guyuealian/article/details/128954588" title="行人检测(人体检测)2：YOLOv5实现人体检测(含人体检测数据集和训练代码)">行人检测(人体检测)2：YOLOv5实现人体检测(含人体检测数据集和训练代码)</a></p> 
<hr> 
<h3 id="4.%20%E5%9F%BA%E4%BA%8EYOLOv5%E7%9A%84%E6%89%8B%E5%8A%BF%E8%AF%86%E5%88%AB%E8%AE%AD%E7%BB%83">5.人体关键点检测模型训练</h3> 
<p> 整套工程项目基本结构如下：</p> 
<pre><code class="language-bash">.
├── configs              # 训练配置文件
├── data                 # 一些数据
├── libs                 # 一些工具库
├── pose                 # 姿态估计模型文件
├── work_space           # 训练输出工作目录
├── demo.py              # 模型推理demo文件
├── README.md            # 项目工程说明文档
├── requirements.txt     # 项目相关依赖包
└── train.py             # 训练文件</code></pre> 
<h4 id="%EF%BC%881%EF%BC%89%E9%A1%B9%E7%9B%AE%E5%AE%89%E8%A3%85">（1）项目安装</h4> 
<p>推荐使用Python3.8或Python3.7，更高版本可能存在版本差异问题，项目依赖python包请参考requirements.txt，使用pip安装即可<strong>，项目代码都在Ubuntu系统和Windows系统验证正常运行，请放心使用；若出现异常，大概率是相关依赖包版本没有完全对应</strong></p> 
<pre><code class="language-bash">numpy==1.21.6
matplotlib==3.2.2
Pillow==8.4.0
bcolz==1.2.1
easydict==1.9
onnx==1.8.1
onnx-simplifier==0.2.28
onnxoptimizer==0.2.0
onnxruntime==1.6.0
opencv-contrib-python==4.5.2.52
opencv-python==4.5.1.48
pandas==1.1.5
PyYAML==5.3.1
scikit-image==0.17.2
scikit-learn==0.24.0
scipy==1.5.4
seaborn==0.11.2
sklearn==0.0
tensorboard==2.5.0
tensorboardX==2.1
torch==1.7.1+cu110
torchvision==0.8.2+cu110
tqdm==4.55.1
xmltodict==0.12.0
pycocotools==2.0.2
pybaseutils==0.9.4
basetrainer
</code></pre> 
<p>项目安装教程请参考（<strong>初学者入门，麻烦先看完下面教程，配置好Python开发环境</strong>）：</p> 
<blockquote> 
 <ul><li>推荐使用Python3.8或Python3.7，更高版本可能存在版本差异问题</li><li><a href="https://blog.csdn.net/guyuealian/article/details/129163343" title="项目开发使用教程和常见问题和解决方法">项目开发使用教程和常见问题和解决方法</a></li><li><strong>视频教程：</strong><a href="https://www.bilibili.com/video/BV1sY411c7JS/?spm_id_from=333.999.0.0" rel="nofollow" title="1 手把手教你安装CUDA和cuDNN(1)">1 手把手教你安装CUDA和cuDNN(1)</a></li><li><strong>视频教程：</strong><a href="https://www.bilibili.com/video/BV1q5411d7GD/?spm_id_from=333.999.0.0" rel="nofollow" title="2 手把手教你安装CUDA和cuDNN(2)">2 手把手教你安装CUDA和cuDNN(2)</a></li><li><strong>视频教程：</strong><a href="https://www.bilibili.com/video/BV18Y411c7Qt/?spm_id_from=333.999.0.0" rel="nofollow" title="3 如何用Anaconda创建pycharm环境">3 如何用Anaconda创建pycharm环境</a></li><li><strong>视频教程：</strong><a href="https://www.bilibili.com/video/BV1SY4y1z7eS/?spm_id_from=333.999.0.0" rel="nofollow" title="4 如何在pycharm中使用Anaconda创建的python环境">4 如何在pycharm中使用Anaconda创建的python环境</a></li></ul> 
</blockquote> 
<h4 id="%EF%BC%882%EF%BC%89%E5%87%86%E5%A4%87Train%E5%92%8CTest%E6%95%B0%E6%8D%AE">（2）准备Train和Test数据</h4> 
<p>下载<strong>COCO数据集</strong>或者<strong>MPII数据集</strong><span style="color:#fe2c24;">（建议使用COCO数据集）</span>，然后：</p> 
<ul><li>COCO数据集下载并解压到本地，存储目录结构参考如下（原始图片目录和标注信息文件在同一级目录）</li></ul> 
<pre><code class="language-bash">─── COCO
    ├── train2017
    │   ├── images                           # COCO训练集原始图片目录
    │   └── person_keypoints_train2017.json  # COCO训练集标注信息文件
    └── val2017
        ├── images                           # COCO验证集原始图片目录
        └── person_keypoints_val2017.json    # COCO验证集标注信息文件

</code></pre> 
<ul><li>MPII数据集下载并解压到本地，存储目录结构参考如下</li></ul> 
<pre><code class="language-bash">─── MPII
    ├── images      # MPII数据集原始图片目录
    ├── train.json  # MPII训练集标注信息文件
    └── valid.json  # MPII训练集标注信息文件

</code></pre> 
<h4 id="%EF%BC%883%EF%BC%89%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6configs">（3）配置文件configs</h4> 
<p>项目支持HRNet以及轻量化模型LiteHRNet和Mobilenet模型训练，并提供对应的配置文件；你需要修改对应配置文件的数据路径；本篇以训练HRNet-w32为例子，其配置文件在configs/coco/hrnet/w32_adam_192_192.yaml，修改该文件的训练数据集路径TRAIN_FILE（支持多个数据集训练）和测试数据集TEST_FILE的数据路径为你本地数据路径，其他参数保持默认即可，如下所示：</p> 
<pre><code class="language-python">WORKERS: 8
PRINT_FREQ: 10
DATASET:
  DATASET: 'custom_coco'
  TRAIN_FILE:
    - 'D:/COCO/train2017/person_keypoints_train2017.json'
  TEST_FILE: 'D:/COCO/val2017/person_keypoints_val2017.json'
  FLIP: true
  ROT_FACTOR: 45
  SCALE_FACTOR: 0.3
  SCALE_RATE: 1.25
  JOINT_IDS: [0,1]
  FLIP_PAIRS: [ ]
  SKELETON: [ ]</code></pre> 
<p>配置文件的一些参数说明，请参考</p> 
<table align="center" border="1" cellpadding="1" cellspacing="1"><tbody><tr><td><strong>参数</strong></td><td><strong>类型</strong></td><td><strong>参考值</strong></td><td><strong>说明</strong></td></tr><tr><td><strong>WORKERS</strong></td><td>int</td><td>8</td><td>数据加载处理的进程数</td></tr><tr><td><strong>PRINT_FREQ</strong></td><td>int</td><td>10</td><td>打印LOG信息的间隔</td></tr><tr><td><strong>DATASET</strong></td><td>str</td><td>custom_coco</td><td>数据集类型，目前仅支持COCO数据格式</td></tr><tr><td><strong>TRAIN_FILE</strong></td><td>List</td><td>-</td><td>训练数据集文件列表(COCO数据格式)，支持多个数据集</td></tr><tr><td><strong>TEST_FILE</strong></td><td>string</td><td>-</td><td>测试数据集文件(COCO数据格式),仅支持单个数据集</td></tr><tr><td><strong>FLIP</strong></td><td>bool</td><td>True</td><td>是否翻转图片进行测试，可提高测试效果</td></tr><tr><td><strong>ROT_FACTOR</strong></td><td>float</td><td>45</td><td>训练数据随机旋转的最大角度，用于数据增强</td></tr><tr><td><strong>SCALE_FACTOR</strong></td><td>float</td><td>1.25</td><td>图像缩放比例因子</td></tr><tr><td><strong>SCALE_RATE</strong></td><td>float</td><td>0.25</td><td>图像缩放率</td></tr><tr><td><strong>JOINT_IDS</strong></td><td>list</td><td>[ ]</td><td>[ ]表示所有关键点，也可以指定需要训练的关键点序号ID</td></tr><tr><td><strong>FLIP_PAIRS</strong></td><td>list</td><td>[ ]</td><td>图像翻转时，关键点不受翻转影响的ID号</td></tr><tr><td><strong>SKELETON</strong></td><td>list</td><td>[ ]</td><td>关键点连接线的序列列表，用于可视化效果</td></tr></tbody></table> 
<h4 id="%EF%BC%884%EF%BC%89%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83">（4）开始训练</h4> 
<p>修改好配置文件后，就可以开始准备训练了：</p> 
<ul><li>训练高精度模型HRNet-w48或者HRNet-w32</li></ul> 
<pre><code class="language-bash"># 高精度模型：HRNet-w32
python train.py  -c "configs/coco/hrnet/w48_adam_192_192.yaml" --workers=8 --batch_size=32 --gpu_id=0 --work_dir="work_space/person"
# 高精度模型：HRNet-w48
python train.py  -c "configs/coco/hrnet/w32_adam_192_192.yaml" --workers=8 --batch_size=32 --gpu_id=0 --work_dir="work_space/person"
</code></pre> 
<ul><li>训练轻量化模型LiteHRNet</li></ul> 
<pre><code class="language-bash"># 轻量化模型：LiteHRNet
python train.py  -c "configs/coco/litehrnet/litehrnet18_192_192.yaml" --workers=8 --batch_size=32 --gpu_id=0 --work_dir="work_space/person"
</code></pre> 
<ul><li>训练轻量化模型Mobilenetv2</li></ul> 
<pre><code class="language-bash"># 轻量化模型：Mobilenet
python train.py  -c "configs/coco/mobilenet/mobilenetv2_192_192.yaml" --workers=8 --batch_size=32 --gpu_id=0 --work_dir="work_space/person"
</code></pre> 
<p>下表格给出HRNet，以及轻量化模型LiteHRNet和Mobilenet的计算量和参数量，以及其检测精度AP； 高精度检测模型HRNet-w32，<strong>AP可以达到0.7585</strong>，但其参数量和计算量比较大，不合适在移动端部署；LiteHRNet18和Mobilenet-v2参数量和计算量比较少，合适在移动端部署；虽然LiteHRNet18的理论计算量和参数量比Mobilenet-v2低，但在实际测试中，发现Mobilenet-v2运行速度更快。轻量化Mobilenet-v2模型在普通Android手机上可以达到实时的检测效果，CPU(4线程)约50ms左右，GPU约30ms左右 ，基本满足业务的性能需求</p> 
<table align="center" border="1" cellpadding="1" cellspacing="1"><tbody><tr><td style="text-align:center;"><strong>模型</strong></td><td style="text-align:center;"><strong>input-size</strong></td><td style="text-align:center;"><strong>params(M)</strong></td><td style="text-align:center;"><strong>GFLOPs</strong></td><td style="text-align:center;"><strong>AP</strong></td></tr><tr><td style="text-align:center;">HRNet-w32</td><td style="text-align:center;">192×256</td><td style="text-align:center;">28.48M</td><td style="text-align:center;">5734.05M</td><td style="text-align:center;">0.7585</td></tr><tr><td style="text-align:center;">LiteHRNet18</td><td style="text-align:center;">192×256</td><td style="text-align:center;">1.10M</td><td style="text-align:center;">182.15M</td><td style="text-align:center;">0.6237</td></tr><tr><td style="text-align:center;">Mobilenet-v2</td><td style="text-align:center;">192×256</td><td style="text-align:center;">2.63M</td><td style="text-align:center;">529.25M</td><td style="text-align:center;">0.6181</td></tr></tbody></table> 
<h4 id="%EF%BC%885%EF%BC%89Tensorboard%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B">（5）Tensorboard可视化训练过程</h4> 
<pre>训练过程可视化工具是使用Tensorboard，使用方法，在终端输入：
</pre> 
<pre><code class="language-bash"># 基本方法
tensorboard --logdir=path/to/log/
# 例如
tensorboard --logdir="work_space/person/hrnet_w32_16_192_256_mpii_20231127_113836_6644/log"</code></pre> 
<p>点击终端TensorBoard打印的链接，即可在浏览器查看训练LOG信息等：</p> 
<p><img alt="" height="226" src="https://images2.imgbox.com/2c/9c/Io7welW8_o.png" width="337"><img alt="" height="226" src="https://images2.imgbox.com/e6/6b/wNu4oTqS_o.png" width="333"></p> 
<p><img alt="" height="227" src="https://images2.imgbox.com/2e/94/anZLAKuq_o.png" width="338"><img alt="" height="228" src="https://images2.imgbox.com/d2/b2/L1MDHcK6_o.png" width="338"></p> 
<hr> 
<h3 id="4.%20%E5%9E%83%E5%9C%BE%E5%88%86%E7%B1%BB%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%E6%B5%8B%E8%AF%95%E6%95%88%E6%9E%9C">6.人体关键点检测<strong>检测</strong>模型效果</h3> 
<p>demo.py文件用于推理和测试模型的效果，填写好配置文件，模型文件以及测试图片即可运行测试了；demo.py命令行参数说明如下：</p> 
<table><thead><tr><th><strong>参数</strong></th><th><strong>类型</strong></th><th><strong>参考值</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td><strong>-c,--config_file</strong></td><td>str</td><td>-</td><td>配置文件</td></tr><tr><td><strong>-m,--model_file</strong></td><td>str</td><td>-</td><td>模型文件</td></tr><tr><td><strong>target</strong></td><td>str</td><td>-</td><td>骨骼点类型，如hand,coco_person,mpii_person</td></tr><tr><td><strong>image_dir</strong></td><td>str</td><td>data/image</td><td>测试图片的路径</td></tr><tr><td><strong>video_file</strong></td><td>str,int</td><td>-</td><td>测试的视频文件</td></tr><tr><td><strong>out_dir</strong></td><td>str</td><td>output</td><td>保存结果，为空不保存</td></tr><tr><td><strong>threshold</strong></td><td>float</td><td>0.3</td><td>关键点检测置信度</td></tr><tr><td><strong>device</strong></td><td>str</td><td>cuda:0</td><td>GPU ID</td></tr><tr><td></td><td></td><td></td><td></td></tr></tbody></table> 
<p>下面以运行HRNet-w32为样例，其他模型修改--config_file或者--model_file即可</p> 
<ul><li>测试图片</li></ul> 
<pre><code class="language-bash">python demo.py -c work_space/person/hrnet_w32_17_192_256_custom_coco_20231115_092948_1789/w32_adam_192_192.yaml -m work_space/person/hrnet_w32_17_192_256_custom_coco_20231115_092948_1789/model/best_model_195_0.7585.pth --image_dir data/test_images --out_dir output
</code></pre> 
<ul><li>测试视频文件</li></ul> 
<pre><code class="language-bash">python demo.py -c work_space/person/hrnet_w32_17_192_256_custom_coco_20231115_092948_1789/w32_adam_192_192.yaml -m work_space/person/hrnet_w32_17_192_256_custom_coco_20231115_092948_1789/model/best_model_195_0.7585.pth --video_file data/video-test.mp4 --out_dir output
</code></pre> 
<ul><li> 测试摄像头</li></ul> 
<pre><code class="language-bash">python demo.py -c work_space/person/hrnet_w32_17_192_256_custom_coco_20231115_092948_1789/w32_adam_192_192.yaml -m work_space/person/hrnet_w32_17_192_256_custom_coco_20231115_092948_1789/model/best_model_195_0.7585.pth --video_file 0 --out_dir output
</code></pre> 
<p>项目同时支持MPII数据集格式人体关键点检测</p> 
<ul><li>测试图片（MPII格式的人体关键点检测）</li></ul> 
<pre><code class="language-bash">python demo.py -c work_space/person/hrnet_w32_16_192_256_mpii_20231127_113836_6644/w32_adam_192_192.yaml -m work_space/person/hrnet_w32_16_192_256_mpii_20231127_113836_6644/model/best_model_148_89.4041.pth --image_dir data/test_images --out_dir output --target mpii_person
</code></pre> 
<p>运行效果（支持单人和多人人体关键点检测）：</p> 
<p><img alt="" height="821" src="https://images2.imgbox.com/f7/5f/P58P1j0f_o.png" width="512"></p> 
<p><img alt="" height="743" src="https://images2.imgbox.com/84/22/IcRnHoOR_o.png" width="512"></p> 
<p><img alt="" height="284" src="https://images2.imgbox.com/5c/88/NK46t0cU_o.gif" width="512"><img alt="" height="288" src="https://images2.imgbox.com/5a/62/eyezbikb_o.gif" width="512"></p> 
<hr> 
<h3 id="6.%E6%89%8B%E9%83%A8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B(%E6%8E%A8%E7%90%86%E4%BB%A3%E7%A0%81)">7.<strong>人体关键点</strong>检测(推理代码)下载</h3> 
<p>人体关键点检测推理代码<strong>下载地址</strong>：<a href="https://mp.weixin.qq.com/s/4bujKQ6h1znYFFrF1pRoVw" rel="nofollow" title="Pytorch实现人体关键点检测(人体姿势估计)推理代码">Pytorch实现人体关键点检测(人体姿势估计)推理代码</a></p> 
<p>资源内容包含：</p> 
<blockquote> 
 <ol><li>提供YOLOv5人体检测推理代码（不包含训练代码）</li><li>提供人体关键点检测(人体姿势估计)推理代码demo.py（不包含训练代码）</li><li>提供高精度版本HRNet人体关键点检测(人体姿势估计)（不包含训练代码）</li><li>提供轻量化模型LiteHRNet,以及Mobilenet-v2人体关键点检测(人体姿势估计)（不包含训练代码）</li><li>提供训练好的模型：HRNet-w32,LiteHRNet和Mobilenet-v2模型权重文件，配置好环境，可直接运行demo.py</li><li>推理代码demo.py支持图片，视频和摄像头测试</li></ol> 
</blockquote> 
<p> 如果你需要配套的训练数据集和训练代码，请查看下面部分</p> 
<hr> 
<h3 id="7.%E6%89%8B%E9%83%A8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B(%E8%AE%AD%E7%BB%83%E4%BB%A3%E7%A0%81)">8.<strong>人体关键点检测</strong>(训练代码)下载</h3> 
<p>人体关键点检测训练代码<strong>下载地址</strong>： <a href="https://mp.weixin.qq.com/s/CqctDsA1wQrPiN_M-HSW9Q" rel="nofollow" title="Pytorch实现人体关键点检测(人体姿势估计)训练代码">Pytorch实现人体关键点检测(人体姿势估计)训练代码</a></p> 
<p>资源内容包含：</p> 
<blockquote> 
 <ol><li>提供YOLOv5人体检测推理代码</li><li>提供整套完整的项目工程代码，包含人体关键点检测(人体姿势估计)的<strong>训练代码</strong>train.py和<strong>推理测试</strong>代码demo.py</li><li>提供高精度版本HRNet人体关键点检测(人体姿势估计)<strong>训练和测试</strong>代码</li><li>提供轻量化模型LiteHRNet以及Mobilenet-v2人体关键点检测(人体姿势估计)<strong>训练和测试</strong>代码</li><li>项目代码支持MPII数据集和COCO数据集人体关键点检测模型训练和测试</li><li>根据本篇博文说明，简单配置即可开始训练：train.py</li><li>提供训练好的模型：HRNet-w32,LiteHRNet和Mobilenet-v2模型权重文件，配置好环境，可直接运行demo.py</li><li>推理代码demo.py支持图片，视频和摄像头测试</li></ol> 
</blockquote> 
<hr> 
<h3 id="8.%E6%89%8B%E9%83%A8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8BC%2B%2B%2FAndroid%E7%89%88%E6%9C%AC">9.<strong>人体关键点检测</strong>C++/Android版本</h3> 
<ul><li>人体关键点检测3：Android实现人体关键点检测(人体姿势估计)含源码 可实时检测 <a href="https://blog.csdn.net/guyuealian/article/details/134881797" title="https://blog.csdn.net/guyuealian/article/details/134881797">https://blog.csdn.net/guyuealian/article/details/134881797</a></li><li>人体关键点检测4：C/C++实现人体关键点检测(人体姿势估计)含源码 可实时检测 <a href="https://blog.csdn.net/guyuealian/article/details/134881797" title="https://blog.csdn.net/guyuealian/article/details/134881797">https://blog.csdn.net/guyuealian/article/details/134881797</a></li></ul> 
<p> Android<strong>人体关键点检测</strong>APP Demo体验(<a class="link-info" href="https://download.csdn.net/download/guyuealian/88610359" title="下载">下载</a>)：<a href="https://download.csdn.net/download/guyuealian/88610359" title="https://download.csdn.net/download/guyuealian/88610359">https://download.csdn.net/download/guyuealian/88610359</a> </p> 
<p><img alt="" height="580" src="https://images2.imgbox.com/8a/22/CNAgnF7e_o.gif" width="266">  <img alt="" height="580" src="https://images2.imgbox.com/c6/9a/lx6qYAKH_o.gif" width="266"></p> 
<p></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e996b440b3a9afc1a7bf984f69e3d7c4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">手部关键点检测5：C&#43;&#43;实现手部关键点检测(手部姿势估计)含源码 可实时检测</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9a379e2cbfd3978ecd7a733f84df0bf1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Vue集成UEditor puls富文本编辑器</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>