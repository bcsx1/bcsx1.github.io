<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>（三）Flink1.15 发布最新版本说明 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="（三）Flink1.15 发布最新版本说明" />
<meta property="og:description" content="官网 https://nightlies.apache.org/flink/flink-docs-release-1.15/release-notes/flink-1.15
变化的依赖概况 在Flink 1.15中有几个变化，当从早期版本升级时，需要更新依赖项名称，主要包括从非Scala模块中选择排除Scala依赖项，以及重新组织表模块。 一个快速的依赖变化清单如下:
对以下模块的任何依赖都需要更新，以不再包含后缀:
flink-cep flink-clients flink-connector-elasticsearch-base flink-connector-elasticsearch6 flink-connector-elasticsearch7 flink-connector-gcp-pubsub flink-connector-hbase-1.4 flink-connector-hbase-2.2 flink-connector-hbase-base flink-connector-jdbc flink-connector-kafka flink-connector-kinesis flink-connector-nifi flink-connector-pulsar flink-connector-rabbitmq flink-container flink-dstl-dfs flink-gelly flink-hadoop-bulk flink-kubernetes flink-runtime-web flink-sql-connector-elasticsearch6 flink-sql-connector-elasticsearch7 flink-sql-connector-hbase-1.4 flink-sql-connector-hbase-2.2 flink-sql-connector-kafka flink-sql-connector-kinesis flink-sql-connector-rabbitmq flink-state-processor-api flink-statebackend-rocksdb flink-streaming-java flink-test-utils flink-yarn flink-table-api-java-bridge flink-table-runtime flink-sql-client flink-orc flink-orc-nohive flink-parquet 对于Table / SQL用户，新的模块flink-table-planner-loader取代了flink- Table - planner_1 .12，并且避免了Scala后缀的需要。 为了向后兼容，用户仍然可以将其与位于opt/中的flink-table-planner_2.12进行交换。 Flink-table-uber被分为flink-table-api-java-uber, flink-table-planner(-loader)和flink-table-runtime。 Scala用户需要显式地添加对flink-table-api-scala或flink-table-api-scala-bridge的依赖。
所涉及问题的详细情况如下。
添加对opting-out Scala的支持 Java DataSet/-Stream api现在独立于Scala，不再传递地依赖于它。具体如下：
如果你只打算在Java类型中使用Java api，那么你可以通过从发行版的lib/目录中删除Flink -scala jar来选择加入一个没有scala的Flink。 然后你就可以使用任何Scala版本和Scala库了。 你可以把Scala本身捆绑到你的user-jar中; 或者放到发行版的lib/目录中。如果您依赖于Scala api，而没有对它们的显式依赖，那么在构建项目时可能会遇到问题。 您可以通过向您正在使用的api添加显式依赖来解决这个问题。 这将主要影响Scala DataStream/CEP api的用户。 许多模块已经失去了它们的Scala后缀。 当混合来自不同Flink版本(例如，一个旧的连接器)的依赖关系时，建议进一步注意，因为您现在可能会拉入单个模块的多个版本(这在以前是通过名称相等来防止的)。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/a9d90656bd3a4d17a3319d02d83e61d2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-05-18T22:28:56+08:00" />
<meta property="article:modified_time" content="2022-05-18T22:28:56+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">（三）Flink1.15 发布最新版本说明</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p>官网 https://nightlies.apache.org/flink/flink-docs-release-1.15/release-notes/flink-1.15</p> 
</blockquote> 
<h2><a id="_2"></a>变化的依赖概况</h2> 
<p>在Flink 1.15中有几个变化，当从早期版本升级时，需要更新依赖项名称，主要包括从非Scala模块中选择排除Scala依赖项，以及重新组织表模块。 一个快速的依赖变化清单如下:<br> 对以下模块的任何依赖都需要更新，以不再包含后缀:</p> 
<pre><code class="prism language-java">flink<span class="token operator">-</span>cep
flink<span class="token operator">-</span>clients
flink<span class="token operator">-</span>connector<span class="token operator">-</span>elasticsearch<span class="token operator">-</span>base
flink<span class="token operator">-</span>connector<span class="token operator">-</span>elasticsearch6
flink<span class="token operator">-</span>connector<span class="token operator">-</span>elasticsearch7
flink<span class="token operator">-</span>connector<span class="token operator">-</span>gcp<span class="token operator">-</span>pubsub
flink<span class="token operator">-</span>connector<span class="token operator">-</span>hbase<span class="token operator">-</span><span class="token number">1.4</span>
flink<span class="token operator">-</span>connector<span class="token operator">-</span>hbase<span class="token operator">-</span><span class="token number">2.2</span>
flink<span class="token operator">-</span>connector<span class="token operator">-</span>hbase<span class="token operator">-</span>base
flink<span class="token operator">-</span>connector<span class="token operator">-</span>jdbc
flink<span class="token operator">-</span>connector<span class="token operator">-</span>kafka
flink<span class="token operator">-</span>connector<span class="token operator">-</span>kinesis
flink<span class="token operator">-</span>connector<span class="token operator">-</span>nifi
flink<span class="token operator">-</span>connector<span class="token operator">-</span>pulsar
flink<span class="token operator">-</span>connector<span class="token operator">-</span>rabbitmq
flink<span class="token operator">-</span>container
flink<span class="token operator">-</span>dstl<span class="token operator">-</span>dfs
flink<span class="token operator">-</span>gelly
flink<span class="token operator">-</span>hadoop<span class="token operator">-</span>bulk
flink<span class="token operator">-</span>kubernetes
flink<span class="token operator">-</span>runtime<span class="token operator">-</span>web
flink<span class="token operator">-</span>sql<span class="token operator">-</span>connector<span class="token operator">-</span>elasticsearch6
flink<span class="token operator">-</span>sql<span class="token operator">-</span>connector<span class="token operator">-</span>elasticsearch7
flink<span class="token operator">-</span>sql<span class="token operator">-</span>connector<span class="token operator">-</span>hbase<span class="token operator">-</span><span class="token number">1.4</span>
flink<span class="token operator">-</span>sql<span class="token operator">-</span>connector<span class="token operator">-</span>hbase<span class="token operator">-</span><span class="token number">2.2</span>
flink<span class="token operator">-</span>sql<span class="token operator">-</span>connector<span class="token operator">-</span>kafka
flink<span class="token operator">-</span>sql<span class="token operator">-</span>connector<span class="token operator">-</span>kinesis
flink<span class="token operator">-</span>sql<span class="token operator">-</span>connector<span class="token operator">-</span>rabbitmq
flink<span class="token operator">-</span>state<span class="token operator">-</span>processor<span class="token operator">-</span>api
flink<span class="token operator">-</span>statebackend<span class="token operator">-</span>rocksdb
flink<span class="token operator">-</span>streaming<span class="token operator">-</span>java
flink<span class="token operator">-</span>test<span class="token operator">-</span>utils
flink<span class="token operator">-</span>yarn
flink<span class="token operator">-</span>table<span class="token operator">-</span>api<span class="token operator">-</span>java<span class="token operator">-</span>bridge
flink<span class="token operator">-</span>table<span class="token operator">-</span>runtime
flink<span class="token operator">-</span>sql<span class="token operator">-</span>client
flink<span class="token operator">-</span>orc
flink<span class="token operator">-</span>orc<span class="token operator">-</span>nohive
flink<span class="token operator">-</span>parquet
</code></pre> 
<p>对于Table / SQL用户，新的模块flink-table-planner-loader取代了flink- Table - planner_1 .12，并且避免了Scala后缀的需要。 为了向后兼容，用户仍然可以将其与位于opt/中的flink-table-planner_2.12进行交换。 Flink-table-uber被分为flink-table-api-java-uber, flink-table-planner(-loader)和flink-table-runtime。 Scala用户需要显式地添加对flink-table-api-scala或flink-table-api-scala-bridge的依赖。</p> 
<p>所涉及问题的详细情况如下。</p> 
<h3><a id="optingout_Scala_51"></a>添加对opting-out Scala的支持</h3> 
<p>Java DataSet/-Stream api现在独立于Scala，不再传递地依赖于它。具体如下：</p> 
<ul><li>如果你只打算在Java类型中使用Java api，那么你可以通过从发行版的lib/目录中删除Flink -scala jar来选择加入一个没有scala的Flink。 然后你就可以使用任何Scala版本和Scala库了。 你可以把Scala本身捆绑到你的user-jar中; 或者放到发行版的lib/目录中。</li><li>如果您依赖于Scala api，而没有对它们的显式依赖，那么在构建项目时可能会遇到问题。 您可以通过向您正在使用的api添加显式依赖来解决这个问题。 这将主要影响Scala DataStream/CEP api的用户。</li></ul> 
<p>许多模块已经失去了它们的Scala后缀。 当混合来自不同Flink版本(例如，一个旧的连接器)的依赖关系时，建议进一步注意，因为您现在可能会拉入单个模块的多个版本(这在以前是通过名称相等来防止的)。</p> 
<h3><a id="tableflinktableplannerloader_58"></a>重新组织table模块和介绍flink-table-planner-loader</h3> 
<p>新的模块flink-table-planner-loader取代了flink-table-planner_2.12，并且避免了Scala后缀的需要。 它包含在Flink发行版的lib/下。 为了向后兼容，用户仍然可以将其与位于opt/中的flink-table-planner_2.12进行交换。 因此，flink-table-uber被分为flink-table-api-java-uber、flink-table-planner(-loader)和flink-table-runtime。 flink-sql-client不再有Scala后缀。</p> 
<p>建议让新项目在提供的范围内依赖于flink-table-planner-loader(没有Scala后缀)。</p> 
<p>请注意，该发行版默认情况下不包含Scala API。 Scala用户需要显式地添加对flink-table-api-scala或flink-table-api-scala-bridge的依赖。</p> 
<h3><a id="flinktableruntime_flinkscala_65"></a>从flink-table-runtime #中移除flink-scala的依赖</h3> 
<p>flink-table-runtime不再有Scala后缀了。如果case类的遗留类型系统(基于TypeInformation)仍然在Table API中使用，请确保包含flink-scala。</p> 
<h3><a id="Flinktable_uber_jarflinkconnectorfiles_68"></a>Flink-table uber jar不应该包括flink-connector-files依赖</h3> 
<p>表文件系统连接器不再是Flink -table-uber JAR的一部分，而是一个专用的(但可移动的)Flink -connector-files JAR，位于Flink发行版的/lib目录中。</p> 
<h2><a id="JDK_71"></a>JDK升级</h2> 
<p>Java 8的支持现在已弃用，并将在未来的版本中删除(FLINK-25247)。我们建议所有用户迁移到Java 11。</p> 
<p>在Flink docker镜像中的默认Java版本现在是Java 11 (Flink -25251)。有些图像是用Java 8构建的，标记为“java8”。</p> 
<h2><a id="scala_211_76"></a>不再支持scala 2.11</h2> 
<p>对Scala 2.11的支持已经在FLINK-20845中移除。所有(过渡地)依赖于Scala的Flink依赖都以它们所针对的Scala版本作为后缀，例如Flink -stream -scala_2.12。用户应该更新所有Flink依赖项，将“2.11”改为“2.12”。</p> 
<p>Scala版本(2.11、2.12等)之间不是二进制兼容的。这也意味着，如果你正在升级到Flink Scala 2.12应用程序，你不能保证可以从Flink Scala 2.11应用程序的保存点恢复。这取决于您在应用程序中使用的数据类型。</p> 
<p>Scala Shell/REPL已经在FLINK-24360中被移除。</p> 
<h2><a id="Table_API__SQL_83"></a>Table API &amp; SQL</h2> 
<h3><a id="_84"></a>默认情况下禁用遗留类型转换行为</h3> 
<p>默认情况下，已禁用遗留类型转换行为。这可能对极端情况(字符串解析、数字溢出、字符串表示、varchar/二进制精度)有影响。设置table.exec。legacy-cast-behaviour=ENABLED，恢复旧的行为。</p> 
<h3><a id="Sink_CHARVARCHAR_87"></a>当输出到Sink #时强制CHAR/VARCHAR精度</h3> 
<p>在进入表接收器之前，默认情况下，CHAR/VARCHAR长度是强制的(修剪/填充)</p> 
<h3><a id="Scala_Table_API_90"></a>支持Scala Table API表函数中的新类型推断</h3> 
<p>使用Scala隐式转换调用的表函数已经更新，以使用新的类型系统和新的类型推断。用户被要求更新他们的udf或使用已弃用的TableEnvironment。registerFunction通过名称调用函数来临时恢复旧的行为。</p> 
<h3><a id="TableConfig_93"></a>将执行器配置传播到TableConfig</h3> 
<p>flink-conf.yaml和其他配置(例如CLI)现在被传播到TableConfig。尽管直接在TableConfig中设置的配置仍然优先，但是如果表配置意外地设置在其他层中，这种更改可能会产生副作用。</p> 
<h3><a id="pre_FLIP84_96"></a>删除pre FLIP-84方法</h3> 
<p>先前已弃用的方法TableEnvironment.execute, Table.insertInto, TableEnvironment.fromTableSource, TableEnvironment.sqlUpdate, and TableEnvironment.explain 已经被移除. 请使用 TableEnvironment.executeSql, TableEnvironment.explainSql, TableEnvironment.createStatementSet, as well as Table.executeInsert, Table.explain and Table.execute and the newly introduces classes TableResult, ResultKind, StatementSet and ExplainDetail.</p> 
<h3><a id="_99"></a>修复解析器生成器警告</h3> 
<p>STATEMENT现在是一个保留关键字。使用反引号转义表、字段和其他引用。</p> 
<h3><a id="DataStreamTransformationuid_102"></a>为DataStream/Transformation提供程序公开uid生成器</h3> 
<p>用于表连接器的DataStreamScanProvider和DataStreamSinkProvider收到了一个额外的方法，该方法可能会破坏之前使用lambdas的实现。我们建议将静态类作为替代并增强将来的健壮性。</p> 
<h3><a id="_105"></a>添加新的语句集语法</h3> 
<p>建议将语句集更新为新的SQL语法:</p> 
<pre><code class="prism language-sql"><span class="token keyword">EXECUTE</span> STATEMENT <span class="token keyword">SET</span> <span class="token keyword">BEGIN</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token keyword">END</span><span class="token punctuation">;</span>
<span class="token keyword">EXPLAIN</span> STATEMENT <span class="token keyword">SET</span> <span class="token keyword">BEGIN</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token keyword">END</span><span class="token punctuation">;</span>
</code></pre> 
<h3><a id="_111"></a>检查并可能修复所有聚合函数的十进制精度和刻度</h3> 
<p>这将使用retraction和AVG()改变十进制SUM()的结果。部分行为恢复到与1.13相同，使整体行为与Hive / Spark一致。</p> 
<h3><a id="DecodingFormat_114"></a>阐明DecodingFormat的语义及其数据类型</h3> 
<p>DecodingFormat接口用于可投射和非投射格式，导致不一致的实现。FileSystemTableSource已经进行了更新，以区分这两个接口。为FileSystemTableSource实现自定义格式的用户可能需要验证实现，并确保在必要时实现ProjectableDecodingFormat。</p> 
<h3><a id="_117"></a>在过滤器之前下推分区</h3> 
<p>这可能会对现有的表源实现产生影响，因为下推过滤器可能不再包含分区谓词。但是，实现分区下推和过滤器下推的表源的连接器实现变得更容易了。</p> 
<h3><a id="Flink_SQL_SUM_120"></a>Flink SQL SUM()导致精度错误</h3> 
<p>这将在1.14.0和1.14.1之间更改十进制SUM()的结果。将1.13的行为恢复为与Hive/Spark一致。</p> 
<h3><a id="TableResultprint_123"></a>在TableResult#print中使用新的类型转换规则</h3> 
<p>来自DDL结果的布尔列的字符串表示(true/false -&gt; true/false)，以及DQL结果中的行列(+I[…-&gt;(…))已更改为可打印。</p> 
<h3><a id="DATETIME_126"></a>将字符串转换为DATE和TIME允许不完整字符串</h3> 
<p>将不完整字符串(如“12”)转换为TIME的默认值已从12:01:01更改为12:00:00。</p> 
<h3><a id="STRINGTIMESTAMP_LTZ_129"></a>从STRING转换到TIMESTAMP_LTZ会丢失小数秒</h3> 
<p>STRING到TIMESTAMP(_LTZ)类型转换现在考虑的是小数秒。以前，任何精度的小数秒都被忽略。</p> 
<h3><a id="Table_API_132"></a>在Table API中使用时，使用统一接收器框架构建的接收器不接收时间戳</h3> 
<p>如果使用了新的接收接口(例如Kafka)，这将为拓扑添加一个额外的操作符。当从1.14保存点恢复时，可能会导致1.14.1出现问题。一种解决方法是在接收之前将时间属性转换为SQL语句中的常规时间戳。</p> 
<h3><a id="SQLSTRINGVARCHAR2000_135"></a>SQL函数应该返回STRING而不是VARCHAR(2000)</h3> 
<p>在1.14中返回VARCHAR(2000)的函数现在返回最大长度的VARCHAR。特别是包括:</p> 
<pre><code class="prism language-sql">SON_VALUE
CHR
REVERSE
SPLIT_INDEX
REGEXP_EXTRACT
PARSE_URL
FROM_UNIXTIME
DECODE
DATE_FORMAT
CONVERT_TZ
</code></pre> 
<h3><a id="IS_JSONAPI_149"></a>支持IS JSON的表API</h3> 
<p>这个问题增加了IS JSON的表API。注意，IS JSON不再返回NULL，但总是FALSE(即使参数是NULL)。</p> 
<h3><a id="Flink_SQLupsert_into_152"></a>在Flink SQL中禁用upsert into语法</h3> 
<p>禁用UPSERT INTO语句。在以前的版本中，UPSERT INTO语法错误地暴露了出来，没有详细讨论。从这个版本开始，每个UPSERT INTO都将抛出一个异常。UPSERT INTO的用户应该使用文档中的INSERT INTO语句。</p> 
<h3><a id="RuntimeException_while_resolving_method_booleanValue_in_class_class_javamathBigDecimal_155"></a>RuntimeException: while resolving method ‘booleanValue’ in class class java.math.BigDecimal</h3> 
<p>十进制数字类型不再允许转换为布尔值。</p> 
<h3><a id="Upsert_materializer_158"></a>不会为所有接收器提供程序插入Upsert materializer</h3> 
<p>此问题旨在修复各种主要的关键问题，有效地使它无法使用此功能。更改可能会影响那些不正确管道的保存点向后兼容性。同样，在这些变更之后，产生的变更日志流也可能不同。以前正确的管道应该可以从保存点恢复。</p> 
<h3><a id="Propagate_unique_keys_for_fromChangelogStream_161"></a>Propagate unique keys for fromChangelogStream</h3> 
<p>StreamTableEnvironment.fromChangelogStream might produce a different stream because primary keys were not properly considered before.</p> 
<h3><a id="TableResultprint_164"></a>TableResult#print()应该使用内部数据类型</h3> 
<p>Table#print的结果已经修改为更接近实际的SQL数据类型。例如，decimal可以正确打印前导/尾随零。</p> 
<h2><a id="Connectors_167"></a>Connectors连接器</h2> 
<h3><a id="Remove_MapR_filesystem_168"></a>Remove MapR filesystem</h3> 
<p>对MapR文件系统的支持已被删除。</p> 
<h3><a id="flinkconnectortestingflinkconnectortestutils_171"></a>合并flink-connector-testing到flink-connector-test-utils</h3> 
<p>flink-connector-testing模块已经被删除，用户应该改用flink-connector-test-utils模块。</p> 
<h3><a id="_174"></a>通过元数据支持分区键(对于文件系统连接器)</h3> 
<p>现在实现的格式BulkWriterFormatFactory不再需要实现分区键读取，因为它在内部由FileSystemTableSource.</p> 
<h3><a id="_ElasticSearch_Sink__Unified_Sink_API_FLIP143_177"></a>将 ElasticSearch Sink 移植到新的 Unified Sink API (FLIP-143)</h3> 
<p>ElasticsearchXSinkBuilder使用支持 DataStream API 的批处理和流模式的新统一接收器接口取代ElasticsearchSink.Builder并提供至少一次写入。<br> 对于使用旧 ElasticsearchSink 接口 ( org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSink) 并依赖于自己的 elasticsearch-rest-high-level-client 版本的 Elasticsearch 7 用户，由于内部更改，需要将客户端依赖项更新为 &gt;= 7.14.0 的版本。</p> 
<h3><a id="_Table_API__181"></a>减少 Table API 连接器中的遗留问题</h3> 
<p>旧的 JDBC 连接器（connector.type=jdbc在 DDL 中表示）已被删除。如果尚未完成，用户需要升级到较新的堆栈（connector=jdbc在 DDL 中表示）。</p> 
<h3><a id="_184"></a>可扩展统一接收器使用新指标来捕获传出记录</h3> 
<p>引入了新的指标numRecordsSend，numRecordsSendErrors供用户监控发送到外部系统的记录数量。numRecordsOut应该用于监视接收器任务之间传输的记录数。</p> 
<p>连接器开发人员在构建接收器连接器时应注意这些指标 numRecordsOut、numRecordsSend 和 numRecordsSendErrors 的使用。有关详细信息，请参阅新的 Kafka Sink。此外，由于 numRecordsOut 现在只计算接收器任务之间发送的记录，并且 numRecordsOutErrors 是为计算发送到外部系统的记录而设计的，因此我们不推荐使用 numRecordsOutErrors 并建议使用 numRecordsSendErrors 代替。</p> 
<h2><a id="Runtime__Coordination_189"></a>Runtime &amp; Coordination</h2> 
<h3><a id="_190"></a>整合清理阶段的重试策略</h3> 
<p>将重试逻辑添加到已完成作业的清理步骤。此功能改变了 Flink 作业的清理方式。不是尝试一次清理作业，而是重复此步骤，直到成功。用户旨在修复阻止 Flink 完成作业清理的问题。可以配置和禁用重试功能。</p> 
<h3><a id="TaskManager_and_JobManager_193"></a>TaskManager and JobManager之间引入显式关闭信号</h3> 
<p>TaskManagers JobManager现在在关闭时显式发送信号。这减少了反应模式下的缩减延迟（之前绑定到心跳超时）。</p> 
<h3><a id="TaskManagerJobMetricGroup__196"></a>TaskManagerJobMetricGroup使用最后一个插槽而不是任务 释放</h3> 
<p>TaskManager 上的作业指标现在在最后一个插槽被释放时被删除，而不是最后一个任务。这意味着它们可能会被报告比以前更长的时间，并且当 TaskManager 上没有任务运行时。</p> 
<h3><a id="_JobMaster__199"></a>通过异常历史记录访​​问 JobMaster 初始化期间发生的错误</h3> 
<p>修复了故障转移未列在异常历史记录中但作为根本原因的问题。JobMaster 如果在初始化期间发生故障，则可能会发生这种情况。</p> 
<h3><a id="_ResourceManagerDispatcherResourceManagerComponent__202"></a>如果没有前导 ResourceManager，DispatcherResourceManagerComponent 无法注销应用程序</h3> 
<p>实现了一个新的多组件领导选举服务，每个 Flink 进程只运行一次领导选举。如果这会导致任何问题，那么您可以high-availability.use-old-ha-services: true在 中设置flink-conf.yaml 使用旧的高可用性服务。</p> 
<h3><a id="_205"></a>允许幂等作业取消</h3> 
<p>尝试取消FINISHED/FAILED作业现在返回 409 Conflict 而不是 404 Not Found。</p> 
<h3><a id="_Dispatcher_208"></a>将异步保存点操作缓存移动到 Dispatcher</h3> 
<p>JobManagers现在可以查询所有保存点操作的状态，无论哪个接收JobManager到初始请求。</p> 
<h3><a id="_JobSchedulingStatus_211"></a>每个作业的待机模式调度程序不知道作业的 JobSchedulingStatus</h3> 
<p>通过引入新的 JobResultStore 组件，Flink 可以将作业的清理状态持久化到文件系统中，解决了在应用模式下作业完成但在清理过程中失败时重新提交作业的问题。（见FLINK-25431）</p> 
<h3><a id="_shuffle__214"></a>更改阻塞 shuffle 的一些默认配置值以获得更好的可用性</h3> 
<p>从 1.15 开始，sort-shuffle 成为默认的阻塞 shuffle 实现，并且默认启用 shuffle 数据压缩。这些变化仅影响批处理作业，更多信息请参考官方文档<a href="https://nightlies.apache.org/flink/flink-docs-release-1.15" rel="nofollow">【链接】</a>。</p> 
<h2><a id="_217"></a>检查点</h2> 
<h3><a id="FLIP193_219"></a>FLIP-193：快照所有权</h3> 
<p>从保存点或保留的外部检查点恢复时，您可以选择要执行操作的模式。您可以从CLAIM, NO_CLAIM, LEGACY（旧行为）中进行选择。</p> 
<p>在CLAIM模式下，Flink 拥有快照的所有权，并可能会在某个时间点尝试删除快照。另一方面，该NO_CLAIM 模式将确保 Flink 不依赖于任何属于初始快照的文件的存在。</p> 
<p>有关更详尽的描述，请参阅<a href="https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/ops/state/savepoints/#restore-mode" rel="nofollow">文档</a>。</p> 
<h3><a id="_statebackend__226"></a>支持本机保存点（无需修改 statebackend 特定的快照策略）</h3> 
<p>获取保存点时，您可以指定二进制格式。您可以选择原生（特定于特定状态后端）或规范（在所有状态后端统一）。</p> 
<h3><a id="_JM__229"></a>防止 JM在 检查点中止时丢弃状态</h3> 
<p>共享状态跟踪更改为使用检查点 ID 而不是引用计数。共享状态不再在流产时清理（而是在归并或工作终止时）。</p> 
<p>这可能会导致丢弃已中止检查点的状态出现延迟。</p> 
<h3><a id="_234"></a>引入增量/完整检查点大小统计信息</h3> 
<p>在每个检查点中引入持久字节的度量（通过 REST API 和 UI），这可以帮助用户了解在基于增量或更改日志的检查点期间保留了多少数据大小。</p> 
<h3><a id="_237"></a>默认启用最终检查点</h3> 
<p>在 1.15 中我们默认启用了部分任务完成后的检查点支持，并让任务在退出前等待最终检查点，以确保所有数据都已提交。</p> 
<p>但是，值得注意的是，此更改会强制任务在退出之前等待另一个检查点。换句话说，此更改将阻止任务，直到下一个检查点被触发并完成。如果检查点间隔较长，任务的执行时间也会大大延长。在最坏的情况下，如果检查点间隔是Long.MAX_VALUE，那么这些任务实际上会被永远阻塞。</p> 
<p>具体参考<a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/checkpointing/#checkpointing-with-parts-of-the-graph-finished-beta" rel="nofollow">文档</a></p> 
<h3><a id="_API__DataStream_API_244"></a>将状态处理器 API 迁移到 DataStream API</h3> 
<p>State Processor API 已从 Flinks 的旧 DataSet API 迁移到现在运行在正在BATCH执行的 DataStreams 上。</p> 
<h3><a id="_RocksDB__flink__247"></a>将 RocksDB 的日志默认重定位到 flink 日志目录下</h3> 
<p>RocksDB 的内部日志默认保存在 flink 的日志目录下。</p> 
<h2><a id="_250"></a>依赖升级</h2> 
<h3><a id="_hadoop__285_251"></a>将支持的最低 hadoop 版本升级到 2.8.5</h3> 
<p>现在支持的最低 Hadoop 客户端版本是 2.8.5（Flink 运行时依赖的版本）。客户端仍然可以与旧版本的服务器通信，因为二进制协议应该是向后兼容的。</p> 
<h3><a id="_Elasticsearch__254"></a>将 Elasticsearch 接收器更新到最新的次要版本</h3> 
<p>连接器使用的 Elasticsearch 库分别升级到 7.15.2 和 6.8.20。</p> 
<p>对于使用旧 ElasticsearchSink 接口org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSink（elasticsearch-rest-high-level-client</p> 
<h3><a id="_Zookeeper_34__259"></a>放弃对 Zookeeper 3.4 的支持</h3> 
<p>已放弃对使用 Zookeeper 3.4 进行 HA 的支持。依赖 Zookeeper 的用户需要升级到 3.5/3.6。默认情况下，Flink 现在使用 Zookeeper 3.5 客户端。</p> 
<h3><a id="Kafka_262"></a>升级Kafka依赖</h3> 
<p>Kafka 连接器现在默认使用 Kafka 客户端 2.8.1。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/51566a69c589efbb341195b761f3d1b4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">论文R语言复现 | 基于 EM 算法的高斯混合模型参数估计</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4dc6ccb0a4f3abb695277205e19eeaee/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">win10，edge打不开，点了没反应</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>