<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>（2021，FastGAN）用于高保真 few-shot 图像合成的更快、更稳定的 GAN 训练 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="（2021，FastGAN）用于高保真 few-shot 图像合成的更快、更稳定的 GAN 训练" />
<meta property="og:description" content="Towards faster and stabilized gan training for high-fidelity few-shot image synthesis
公众号：EDPJ
目录
0. 摘要
1. 简介
2. 相关工作
3. 方法
3.1 跳跃层通道激励 3.2 自监督判别器 4. 实验 4.1 图像合成性能
4.2 更多分析与应用
5. 结论
参考
附录
A. 跳层激励带来的性能提升
B. 判别器的特征提取性能
C. 不同分辨率下的风格混合 S. 总结
S.1 核心思想
S.2 模块架构
S.3 Loss
S.4 分析
0. 摘要 在高保真图像上训练生成对抗网络 (GAN) 通常需要大规模 GPU 和大量训练图像。 在本文中，我们以最小的计算成本研究了 GAN 的 few-shot 图像合成任务。 我们提出了一种轻量级 GAN 结构，可在 1024x1024 分辨率上获得卓越的质量。 值得注意的是，该模型仅需在单个 RTX-2080 GPU 上从头开始进行几个小时的训练即可收敛，并且具有一致的性能，即使训练样本少于 100 个也是如此。 两种技术设计构成了我们的工作，一个跳跃层通道激励模块（skip-layer channel-wise excitation module）和一个作为特征编码器训练的自监督鉴别器。 通过涵盖各种图像域的 13 个数据集，当数据和计算预算有限时，我们展示了我们的模型与最先进的 StyleGAN2 相比的卓越性能。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/0271f79fd5a617559b39484fd6521483/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-03T16:38:15+08:00" />
<meta property="article:modified_time" content="2023-06-03T16:38:15+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">（2021，FastGAN）用于高保真 few-shot 图像合成的更快、更稳定的 GAN 训练</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p style="text-align:center;"><strong>Towards faster and stabilized gan training for high-fidelity few-shot image synthesis</strong></p> 
<p style="text-align:center;"><strong>公众号：EDPJ</strong></p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="-toc" style="margin-left:0px;"><a href="#0.%20%E6%91%98%E8%A6%81" rel="nofollow">0. 摘要</a></p> 
<p id="1.%20%E7%AE%80%E4%BB%8B-toc" style="margin-left:0px;"><a href="#1.%20%E7%AE%80%E4%BB%8B" rel="nofollow">1. 简介</a></p> 
<p id="2.%20%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C-toc" style="margin-left:0px;"><a href="#2.%20%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C" rel="nofollow">2. 相关工作</a></p> 
<p id="3.%20%E6%96%B9%E6%B3%95-toc" style="margin-left:0px;"><a href="#3.%20%E6%96%B9%E6%B3%95" rel="nofollow">3. 方法</a></p> 
<p id="3.1%20%E8%B7%B3%E8%B7%83%E5%B1%82%E9%80%9A%E9%81%93%E6%BF%80%E5%8A%B1%C2%A0-toc" style="margin-left:40px;"><a href="#3.1%20%E8%B7%B3%E8%B7%83%E5%B1%82%E9%80%9A%E9%81%93%E6%BF%80%E5%8A%B1%C2%A0" rel="nofollow">3.1 跳跃层通道激励 </a></p> 
<p id="3.2%20%E8%87%AA%E7%9B%91%E7%9D%A3%E5%88%A4%E5%88%AB%E5%99%A8%C2%A0-toc" style="margin-left:40px;"><a href="#3.2%20%E8%87%AA%E7%9B%91%E7%9D%A3%E5%88%A4%E5%88%AB%E5%99%A8%C2%A0" rel="nofollow">3.2 自监督判别器 </a></p> 
<p id="4.%20%E5%AE%9E%E9%AA%8C%C2%A0-toc" style="margin-left:0px;"><a href="#4.%20%E5%AE%9E%E9%AA%8C%C2%A0" rel="nofollow">4. 实验 </a></p> 
<p id="%E2%80%8B%E7%BC%96%E8%BE%91-toc" style="margin-left:40px;"><a href="#4.1%20%E5%9B%BE%E5%83%8F%E5%90%88%E6%88%90%E6%80%A7%E8%83%BD" rel="nofollow">4.1 图像合成性能</a></p> 
<p id="4.2%20%E6%9B%B4%E5%A4%9A%E5%88%86%E6%9E%90%E4%B8%8E%E5%BA%94%E7%94%A8-toc" style="margin-left:40px;"><a href="#4.2%20%E6%9B%B4%E5%A4%9A%E5%88%86%E6%9E%90%E4%B8%8E%E5%BA%94%E7%94%A8" rel="nofollow">4.2 更多分析与应用</a></p> 
<p id="5.%20%E7%BB%93%E8%AE%BA-toc" style="margin-left:0px;"><a href="#5.%20%E7%BB%93%E8%AE%BA" rel="nofollow">5. 结论</a></p> 
<p id="%E5%8F%82%E8%80%83-toc" style="margin-left:0px;"><a href="#%E5%8F%82%E8%80%83" rel="nofollow">参考</a></p> 
<p id="%E9%99%84%E5%BD%95-toc" style="margin-left:0px;"><a href="#%E9%99%84%E5%BD%95" rel="nofollow">附录</a></p> 
<p id="A.%20%E8%B7%B3%E5%B1%82%E6%BF%80%E5%8A%B1%E5%B8%A6%E6%9D%A5%E7%9A%84%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87-toc" style="margin-left:40px;"><a href="#A.%20%E8%B7%B3%E5%B1%82%E6%BF%80%E5%8A%B1%E5%B8%A6%E6%9D%A5%E7%9A%84%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87" rel="nofollow">A. 跳层激励带来的性能提升</a></p> 
<p id="B.%20%E5%88%A4%E5%88%AB%E5%99%A8%E7%9A%84%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E6%80%A7%E8%83%BD-toc" style="margin-left:40px;"><a href="#B.%20%E5%88%A4%E5%88%AB%E5%99%A8%E7%9A%84%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E6%80%A7%E8%83%BD" rel="nofollow">B. 判别器的特征提取性能</a></p> 
<p id="C.%20%E4%B8%8D%E5%90%8C%E5%88%86%E8%BE%A8%E7%8E%87%E4%B8%8B%E7%9A%84%E9%A3%8E%E6%A0%BC%E6%B7%B7%E5%90%88%C2%A0-toc" style="margin-left:40px;"><a href="#C.%20%E4%B8%8D%E5%90%8C%E5%88%86%E8%BE%A8%E7%8E%87%E4%B8%8B%E7%9A%84%E9%A3%8E%E6%A0%BC%E6%B7%B7%E5%90%88%C2%A0" rel="nofollow">C. 不同分辨率下的风格混合 </a></p> 
<p id="S.%20%E6%80%BB%E7%BB%93-toc" style="margin-left:0px;"><a href="#S.%20%E6%80%BB%E7%BB%93" rel="nofollow">S. 总结</a></p> 
<p id="S.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-toc" style="margin-left:40px;"><a href="#S.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3" rel="nofollow">S.1 核心思想</a></p> 
<p id="S.2%20%E6%A8%A1%E5%9D%97%E6%9E%B6%E6%9E%84-toc" style="margin-left:40px;"><a href="#S.2%20%E6%A8%A1%E5%9D%97%E6%9E%B6%E6%9E%84" rel="nofollow">S.2 模块架构</a></p> 
<p id="S.3%C2%A0%C2%A0Loss-toc" style="margin-left:40px;"><a href="#S.3%C2%A0%C2%A0Loss" rel="nofollow">S.3  Loss</a></p> 
<p id="S.4%20%E5%88%86%E6%9E%90-toc" style="margin-left:40px;"><a href="#S.4%20%E5%88%86%E6%9E%90" rel="nofollow">S.4 分析</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="0.%20%E6%91%98%E8%A6%81">0. 摘要</h2> 
<p>在高保真图像上训练生成对抗网络 (GAN) 通常需要大规模 GPU 和大量训练图像。 在本文中，我们以最小的计算成本研究了 GAN 的 few-shot 图像合成任务。 我们提出了一种轻量级 GAN 结构，可在 1024x1024 分辨率上获得卓越的质量。 值得注意的是，该模型仅需在单个 RTX-2080 GPU 上从头开始进行几个小时的训练即可收敛，并且具有一致的性能，即使训练样本少于 100 个也是如此。 两种技术设计构成了我们的工作，一个跳跃层通道激励模块（skip-layer channel-wise excitation module）和一个作为特征编码器训练的自监督鉴别器。 通过涵盖各种图像域的 13 个数据集，当数据和计算预算有限时，我们展示了我们的模型与最先进的 StyleGAN2 相比的卓越性能。</p> 
<h2 id="1.%20%E7%AE%80%E4%BB%8B">1. 简介</h2> 
<p><img alt="" height="268" src="https://images2.imgbox.com/32/be/Esn8m8MW_o.png" width="608"></p> 
<p>在本文中，我们的目标是在高分辨率图像上学习无条件 GAN，计算成本低且训练样本少。 如图 2 中总结的那样，这些训练条件使模型面临过度拟合和模式崩溃的高风险。 为了在苛刻的训练条件下训练 GAN，我们需要一个可以快速学习的生成器 G 和一个可以持续提供有用信号来训练 G 的鉴别器 D。为了应对这些挑战，我们将我们的贡献总结为：</p> 
<ul><li>我们设计了 Skip-Layer channel-wise Excitation (SLE) 模块，它利用低尺度激活来修改高尺度特征图上的通道响应。SLE 允许在整个模型权重中使用更稳健的梯度流，以加快训练速度。 它还会导致自动学习风格 / 内容解耦，如同 StyleGAN2。</li><li>我们提出了一个自我监督的鉴别器 D，它被训练为一个带有额外解码器的特征编码器。 我们强制 D 从输入图像中学习覆盖更多区域的更具描述性的特征图，从而产生更全面的信号来训练 G。我们测试了 D 的多种自监督策略，其中我们表明自动编码效果最好。</li><li>我们基于所提出的两种技术构建了一个计算高效的 GAN 模型，并展示了该模型在多个高保真数据集上的稳健性，如图 1 所示。</li></ul> 
<p><img alt="" height="411" src="https://images2.imgbox.com/12/e6/0KHRfo9V_o.png" width="1200"></p> 
<h2 id="2.%20%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C">2. 相关工作</h2> 
<p><strong>加速 GAN 训练</strong>：加速 GAN 的训练已经从不同的角度进行了探讨。</p> 
<ul><li>Ngxande 等人建议通过深度（depth-wise）卷积减少计算时间。</li><li>Zhong 等人将 GAN 目标调整为 min-max-min 问题以获得更短的优化路径。</li><li>Sinha 等人建议通过核心集选择来准备每 batch 训练样本，利用更好的数据准备来实现更快的收敛。</li></ul> 
<p>然而，这些方法对训练速度的提升有限。 此外，在缩短的训练时间内，合成质量并没有提高。</p> 
<p><strong>在高分辨率上训练 GAN</strong>：GAN 的高分辨率训练可能会出现问题。 首先，增加的模型参数导致更严格的梯度流来优化 G。其次，由 1024x1024 分辨率的图像形成的目标分布非常稀疏，使得 GAN 更难收敛。 一些工作开发多尺度 GAN 结构以缓解梯度流问题，其中 G 输出图像并同时接收来自多个分辨率的反馈。 然而，所有这些方法都进一步增加了计算成本，消耗了更多的 GPU 内存和训练时间。 </p> 
<p><strong>稳定 GAN 训练</strong>：G 上的模式崩溃是训练 GAN 时面临的一大挑战。 鉴于更少的训练样本和更低的计算预算（更小的 batch 大小），它变得更具挑战性。 由于 D 更有可能在数据集上过度拟合，因此无法提供有意义的梯度来训练 G。</p> 
<p>先前的工作通过为 D 寻求良好的正则化来解决过度拟合问题，包括：不同的目标函数； 正则化梯度； 标准化模型权重；扩充训练数据。 然而，当训练批量大小有限时，这些方法的效果会迅速下降，因为在训练迭代中很难计算出适当的批量统计数据来进行正则化（归一化）。</p> 
<p>同时，正如 Tran 等人研究的那样，D 的自我监督已被证明是稳定 GAN 训练的有效方法。然而，先前工作中的辅助自监督任务限制了使用场景和图像域。 此外，之前的工作仅研究低分辨率图像（32x32 到 128x128），并且没有限制计算资源。</p> 
<h2 id="3.%20%E6%96%B9%E6%B3%95">3. 方法</h2> 
<p>我们的模型采用简约设计。 特别是，我们在 G 中的每个分辨率上使用单个 conv 层，并且在 G 和 D 中的高分辨率 (512x512) 上仅对 conv 层应用三个（输入和输出）通道。图 3 图 4 说明了我们的 G 和 D 的模型结构，以及组件层和前向流的描述。 这些结构设计使我们的 GAN 比 SOTA 模型小得多，训练速度也快得多。 同时，由于我们提出的两种技术的紧凑尺寸，我们的模型在小型数据集上保持稳健。</p> 
<h3 id="3.1%20%E8%B7%B3%E8%B7%83%E5%B1%82%E9%80%9A%E9%81%93%E6%BF%80%E5%8A%B1%C2%A0">3.1 跳跃层通道激励 </h3> 
<p>为了合成更高分辨率的图像，生成器 G 不可避免地需要变得更深，具有更多的卷积层，以符合上采样需求。 具有更多卷积层的更深模型导致 GAN 的训练时间更长，这是由于模型参数数量增加和通过 G 的梯度流更弱。为了更好地训练深度模型，He 等人设计残差结构（ResBlock），利用跳层连接加强层间梯度信号。 然而，虽然 ResBlock 已广泛用于 GAN 中，但它也增加了计算成本。 </p> 
<p>我们将具有两种独特设计的跳跃连接思想重新表述为跳跃层激励模块 (SLE)。</p> 
<ul><li>首先，ResBlock 将 skip-connection 实现为来自不同 conv 层的激活之间的逐元素加法。 它要求激活的空间维度相同。 我们不是加法，而是在激活之间应用逐通道乘法，从而消除了繁重的卷积计算（因为激活的一侧现在的空间维度为 1x1）。</li><li>其次，在之前的 GAN 工作中，skip-connections 仅在同一分辨率内使用。 相比之下，我们在具有更大范围（例如，8x8 和 128x128、16x16 和 256x256）的分辨率之间执行跳跃连接，因为不再需要相等的空间维度。</li><li>这两种设计使得SLE继承了ResBlock的优点，具有快捷的梯度流，同时没有额外的计算负担。</li></ul> 
<p><img alt="" height="709" src="https://images2.imgbox.com/2f/c6/GPJu8iBa_o.png" width="1200"></p> 
<p>形式上，我们将 Skip-Layer Excitation 模块定义为：</p> 
<p class="img-center"><img alt="" height="30" src="https://images2.imgbox.com/bf/d3/rzP2DK1H_o.png" width="589"></p> 
<p>这里 x 和 y 是 SLE 模块的输入和输出特征图，函数 F 包含对 x_low 的操作，Wi 表示要学习的模块权重。 图 3 中的左侧面板显示了实践中的 SLE 模块，其中 x_low 和 x_high 分别是 8×8 和 128x128 分辨率的特征图。</p> 
<ul><li>F 中的自适应平均池化层首先沿空间维度将 xlow 下采样为 4x4，然后 conv 层进一步将其下采样为 1x1。</li><li>LeakyReLU 用于对非线性建模，并且，另一个转换层将 x_low 投影为与 x_high 具有相同的通道大小。</li><li>最后，在通过 Sigmoid 函数进行门控（gating）操作后，F 的输出沿通道维度乘以 x_high，生成与 x_high 形状相同的 y。</li></ul> 
<p>SLE 部分类似于 Hu 等人提出的 Squeeze-and-Excitation 模块 (SE)。但是，SE 作为自门控（self-gating）模块在一个特征图中运行。 相比之下，SLE 在彼此远离的特征图之间执行。SLE 像 SE 一样带来了 channel-wise 特征重新校准的好处，并且也像 ResBlock 一样加强了整个模型的梯度流。 SLE 中的通道乘法也与实例规范化（Instance Normalization）相吻合，后者广泛用于风格迁移（style-transfer）。 同样，我们表明 SLE 使 G 能够自动分离内容和样式属性，就像 StyleGAN 一样。 当 SLE 在高分辨率特征图上执行时，改变这些特征图被证明更有可能改变生成图像的风格属性。 通过用另一个合成样本替换 SLE 中的 x_low，我们的 G 可以生成内容不变的图像，但与新替换图像具有相同的风格。</p> 
<h3 id="3.2%20%E8%87%AA%E7%9B%91%E7%9D%A3%E5%88%A4%E5%88%AB%E5%99%A8%C2%A0">3.2 自监督判别器 </h3> 
<p>我们为 D 提供强正则化的方法非常简单。 我们将 D 视为编码器并使用小型解码器对其进行训练。 这种自动编码训练迫使 D 提取图像特征，解码器用该特征可以提供良好重建的。 解码器与 D 一起优化了一个简单的重建损失，它只在真实样本上训练：</p> 
<p><img alt="" height="41" src="https://images2.imgbox.com/79/c2/6nx2ue8n_o.png" width="920"></p> 
<p>其中 f 是来自 D 的中间特征图，函数 G 包含对 f 和解码器的处理，函数 T 表示对来自真实图像 I_real 的样本 x 的处理。 </p> 
<p><img alt="" height="409" src="https://images2.imgbox.com/4b/12/Z34IhQ7z_o.png" width="1200"></p> 
<p>我们的自监督 D 如图 4 所示，我们在两个尺度上为特征图使用了两个解码器：16x16 上的 f1 和 8x8 上的 f2。</p> 
<ul><li>解码器只有四个 conv 层来生成 128x128 分辨率的图像，导致很少的额外计算（比其他正则化方法少得多）。</li><li>我们随机裁剪 f1 的高度和宽度的 1/8 ，然后以相同比例裁剪真实图像获得 I_part。 我们调整真实图像的大小以获得 I。</li><li>解码器从裁剪的 f1 中产生 I'_part，从 f2 中产生 I'。</li><li>最后，D 和解码器一起训练以最小化等式 2 中的损失。通过将 I'_part 匹配到 I_part 并将 I' 匹配到 I。</li></ul> 
<p>这种重建训练确保 D 从输入中提取更全面的 representation，涵盖整体成分（来自 f2）和详细纹理（来自 f1）。 请注意，G 和 T 中的处理不限于裁剪； 更多的操作仍有待探索以获得更好的性能。 我们采用的自动编码方法是一种典型的自监督学习方法，已被广泛认为可以提高模型的鲁棒性和泛化能力。 在 GAN 的背景下，我们发现通过自我监督训练策略的正则化 D 显着提高了 G 的合成质量，其中自动编码带来的性能提升最大。</p> 
<p>尽管我们对 D 的自我监督策略以自动编码器 (AE) 的形式出现，但这种方法与试图结合 GAN 和 AE 的工作有根本的不同。 后者的工作主要是在从 D 学习到的隐空间上训练 G 作为解码器，或者将 D 的对抗训练作为 AE 训练之外的补充损失。 相比之下，我们的模型是一个纯 GAN，具有更简单的训练模式，自编码训练只对D进行正则化，不涉及G。</p> 
<p>总而言之，我们使用了对抗损失的铰链（hinge）版本来迭代训练我们的 D 和 G。我们发现不同的 GAN 损失几乎没有性能差异，而铰链损失计算最快： </p> 
<p><img alt="" height="60" src="https://images2.imgbox.com/20/46/He6N2PU5_o.png" width="837"></p> 
<h2 id="4.%20%E5%AE%9E%E9%AA%8C%C2%A0">4. 实验 </h2> 
<p>基线模型是我们从基于 DCGAN 的各种 GAN 技术中集成的最强表现者：</p> 
<ul><li>谱归一化</li><li>G 上的指数移动平均优化</li><li>可鉴别增强（DA）</li><li>G 中使用 GLU 而不是 ReLU</li></ul> 
<p>我们使用两种建议的技术在基线上构建我们的模型：跳层激励模块和自监督鉴别器。</p> 
<p id="%E2%80%8B%E7%BC%96%E8%BE%91"><img alt="" height="220" src="https://images2.imgbox.com/5f/f8/GXGFCUB1_o.png" width="906"></p> 
<p>表 1 显示了使用 PyTorch 实现的 Nvidia RTX 2080-Ti GPU 模型的归一化 cc 数据。 重要的是，具有 1/4 参数的 slimed StyleGAN2 无法收敛于 1024x1024 分辨率的测试数据集。 我们在以下实验中与具有 1/2 参数（如果没有特别提及）的 StyleGAN2 进行比较。 </p> 
<h3 id="4.1%20%E5%9B%BE%E5%83%8F%E5%90%88%E6%88%90%E6%80%A7%E8%83%BD">4.1 图像合成性能</h3> 
<p><strong>Few-shot generation</strong>：对于某个角色、某个流派或某个主题，收集大规模的图像数据集是昂贵的，甚至是不可能的。 在那些 few-shot 数据集上，数据高效模型对于图像生成任务变得特别有价值。 在表 2 和表 3 中，我们表明我们的模型不仅在 few-shot 数据集上实现了卓越的性能，而且比比较方法的计算效率更高。 我们在训练期间每 10k 次迭代保存检查点，并从检查点报告最佳 FID（至少在所有数据集上对 Style-GAN2 训练 15 小时后发生）。 在 12 个数据集中，我们的模型在其中 10 个上表现最好。</p> 
<p><img alt="" height="235" src="https://images2.imgbox.com/b2/f3/Tp47gotX_o.png" width="916"></p> 
<p><img alt="" height="199" src="https://images2.imgbox.com/85/bc/Ek2cZakH_o.png" width="907"></p> 
<p>请注意，由于在 1024x1024 分辨率下训练时 StyleGAN2 的 VRAM 要求，我们必须在 RTX TITAN GPU 上训练表 3 中的模型。 实际上，2080-TI 和 TITAN 具有相似的性能，我们的模型在两个 GPU 上同时运行。 </p> 
<p><strong>从头开始训练与微调</strong>：在样本很少的数据集上，从预训练的 GAN 进行微调一直是图像生成任务的首选方法 。 然而，它的性能在很大程度上取决于新数据集和可用的预训练模型之间的语义一致性。 根据 Zhao 等人的说法，在大多数情况下，当新数据集的内容偏离原始数据集时，微调的效果比从头开始训练更差。 我们从表 2 和表 3 中确认了当前微调方法的局限性，其中，我们使用 Mo 等人的 Freeze-D 方法对在 FFHQ 上训练的 StyleGAN2 进行微调。在所有测试的数据集中，只有 Obaxx 和 Skull 支持微调方法，这是有道理的，因为这两组数据与 FFHQ 的内容最相似 。</p> 
<p><strong>模块消融研究</strong>：我们对表 2 中提出的两个模块进行了实验，其中 SLE（跳过）和 decoding-on-D（解码）都可以分别提升模型性能。 说明两个模块在提升模型性能上是相互正交的，其中自监督D的贡献最大。 重要的是，基线模型和 StyleGAN2 在列出的训练时间后迅速发散。 相比之下，我们的模型不太可能在测试数据集中出现模式崩溃。 与通常在训练 10 小时后模型崩溃的基线模型不同，我们的模型保持良好的合成质量，即使在训练 20 小时后也不会崩溃。 我们认为是 D 上的解码正则化防止了模型发散。 </p> 
<p><img alt="" height="206" src="https://images2.imgbox.com/d0/45/kqVrQzEW_o.png" width="866"></p> 
<p><strong>使用更多图像进行训练</strong>：为了进行更全面的评估，我们还在具有更多训练样本的数据集上测试了我们的模型，如表 4 所示，我们在两个 TITAN RTX GPU 上以 batch 大小为 16 的艺术和照片数据集训练完整的 StyleGAN2 大约五天，并使用 Zhao 等人在 FFHQ 上的最新官方数据。相反，我们仅训练我们的模型 24 小时，在单个 2080-Ti GPU 上 batch 大小为 8。 具体来说，对于具有全部 70000 张图像的 FFHQ，我们使用更大的 batch 32 训练我们的模型，以反映我们模型的最佳性能。 </p> 
<p>在这个测试中，我们遵循通过生成 50k 图像来计算 FID 的常见做法，并使用整个训练集作为参考分布。 请注意，与我们的模型相比，StyleGAN2 的参数增加了一倍多，并且在 FFHQ 上使用更大的 batch 进行训练。 当给予足够的训练样本和计算能力时，这些因素有助于其更好的性能。 同时，我们的模型在所有测试中都与 StyleGAN2 保持一致，计算预算低得多，即使在更大规模的数据集上也显示出令人信服的性能，并且与基线模型相比具有一致的性能提升。</p> 
<p class="img-center"><img alt="" height="833" src="https://images2.imgbox.com/ef/52/T2zWXu09_o.png" width="1156"></p> 
<p class="img-center"><img alt="" height="865" src="https://images2.imgbox.com/53/ff/IXACAWcp_o.png" width="1156"></p> 
<p class="img-center"><img alt="" height="133" src="https://images2.imgbox.com/cb/f8/Ao76Fslq_o.png" width="1183"></p> 
<p><strong>定性结果</strong>：从图 6 中的定性比较，我们的模型的优势变得更加清晰。在相同的 batch 大小和训练时间下，StyleGAN2 收敛速度较慢或出现模式崩溃。 相比之下，我们的模型始终生成令人满意的图像。 请注意，我们的模型在 Flower、Shell 和 Pokemon 上的最佳结果只需要三个小时的训练，而对于其余三个数据集，最好的性能是在训练八小时时达到的。 对于 “Shell”、“anime face” 和 “Pokemon” 上的 StyleGAN2，图 6 中显示的图像来自最佳 epoch，它们与表 2 和表 3 中的分数相匹配。对于其余数据集，由于训练时间更长，StyleGAN2 的质量提升也有限。 </p> 
<h3 id="4.2%20%E6%9B%B4%E5%A4%9A%E5%88%86%E6%9E%90%E4%B8%8E%E5%BA%94%E7%94%A8">4.2 更多分析与应用</h3> 
<p><strong>使用回溯（back-tracking）测试模式崩溃</strong>：从训练有素的 GAN 中，可以获取真实图像并将其逆映射回 G 的隐空间中的向量，从而通过改变回溯向量来编辑图像的内容。 尽管有各种回溯方法，泛化良好的 G 对于良好的逆映射同样重要。 为此，我们表明我们的模型虽然在有限的图像样本上进行了训练，但在真实图像回溯上仍然获得了理想的性能。</p> 
<p class="img-center"><img alt="" height="241" src="https://images2.imgbox.com/59/02/i6aM8Znz_o.png" width="488"></p> 
<p>在表 5 中，我们以 9:1 的训练/测试比例拆分每个数据集中的图像，并在训练集上训练 G。 在对隐向量进行 1000 次迭代的相同更新后（以防止向量远离正态分布），我们计算来自测试集的所有图像与它们来自 G 的逆映射之间的重建误差。 随着更多的训练迭代，基线模型的性能越来越差，这反映了 G 上的模式崩溃。相比之下，我们的模型在更多的训练迭代中提供了更好的重建和一致的性能。 图 5 显示了给定真实图像的回溯示例（中间面板中最左边和最右边的样本）。 来自回溯隐向量的平滑插值也表明我们的 G 几乎没有模式崩溃。 </p> 
<p><img alt="" height="891" src="https://images2.imgbox.com/61/7f/zl5LRhpR_o.png" width="1060"></p> 
<p>此外，我们在附录 D 中展示了定性比较，其中我们的模型保持了良好的生成，而 StyleGAN2 和基线模型崩溃了。</p> 
<p class="img-center"><img alt="" height="205" src="https://images2.imgbox.com/3c/1b/7Y6VFfCP_o.png" width="490"></p> 
<p><strong>D 的自我监督方法和泛化能力</strong>：除了 D 的自动编码训练外，我们还证明了 D 与其他常见的自我监督策略也提高了 GAN 在我们的训练设置中的表现。 我们测试了五种自我监督设置，如表 6 所示，与基线模型相比，它们都带来了显着的性能提升。 具体来说，</p> 
<ul><li>设置 a 是指 contrastive learning，我们把每一个真实的图像作为一个独特的类，让 D 对它们进行分类。</li><li>对于设置 b，我们训练 D 来预测真实图像的原始纵横比（aspect-ratio），因为当输入 D 时它们被重塑为正方形。</li><li>设置 c 是我们在模型中使用的方法，它将 D 作为编码器和解码器进行训练重建真实图像。</li></ul> 
<p>为了更好地验证自我监督对 D 的好处，所有测试都是在具有 10000 张图像的完整训练集上进行的，batch 大小为 8 以与表 4 一致。我们还尝试使用更大的  batch 16 进行训练，结果与 batch 为 8 一致。 </p> 
<p>有趣的是，根据表 6，虽然设置 c 表现最好，但将其与其余两个设置结合使用会导致明显的性能下降。 类似的行为可以在其他一些自我监督设置中找到，例如，和 Chen 等人 (2019) 一样，在艺术绘画和 FFHQ 数据集上执行“旋转预测”任务，我们观察到与基线模型相比性能下降。 我们假设原因是，自动编码迫使 D 关注输入图像的更多区域，从而提取更全面的特征图来描述输入图像（为了更好的重建）。 相反，分类任务并不能保证 D 覆盖整个图像，该任务促使 D 只关注小区域，因为模型可以从图像的小区域中找到类线索。 专注于有限的区域（即对有限的图像模式做出反应）是一种典型的过度拟合行为，这在普通 GAN 中也广泛发生在 D 中。 更多讨论可以在附录 B 中找到。 </p> 
<p><img alt="" height="778" src="https://images2.imgbox.com/a5/e3/yQC4l4Oo_o.png" width="1200"></p> 
<p><strong>像 StyleGAN 这样的风格混合</strong>。 通过通道激励模块，我们的模型获得了与 StyleGAN 相同的功能：它学习以无监督的方式从 G 的不同尺度的卷积层中分离图像的高级语义属性（风格和内容）。 样式混合结果如图 7 所示，其中前三个数据集为 256x256 分辨率，后三个数据集为 1024x 1024 分辨率。 虽然 StyleGAN2 在底部高分辨率数据集上收敛，但我们的模型成功地学习了“excited” 层上沿通道维度的样式 representations（即，对于 256x256、512x512 分辨率的特征图）。 有关 SLE 和样式混合的更多信息，请参阅附录 A 和 C。 </p> 
<h2 id="5.%20%E7%BB%93%E8%AE%BA">5. 结论</h2> 
<p>我们介绍了两种技术，可以在给定一百张高保真图像和有限的计算资源的情况下稳定 GAN 训练并提高合成质量。 在具有不同内容变化的 13 个数据集上，我们表明跳跃层通道激励机制 (SLE) 和鉴别器上的自监督正则化显着提高了 GAN 的综合性能。 这两种提议的技术都需要对普通 GAN 进行微小改动，从而通过理想的即插即用特性增强 GAN 的实用性。 我们希望这项工作能够有益于 GAN 的下游任务，并为未来的研究提供新的研究视角。</p> 
<h2 id="%E5%8F%82%E8%80%83">参考</h2> 
<p>Liu B, Zhu Y, Song K, et al. Towards faster and stabilized gan training for high-fidelity few-shot image synthesis[C]//International Conference on Learning Representations. 2021.</p> 
<p></p> 
<h2 id="%E9%99%84%E5%BD%95">附录</h2> 
<h3 id="A.%20%E8%B7%B3%E5%B1%82%E6%BF%80%E5%8A%B1%E5%B8%A6%E6%9D%A5%E7%9A%84%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87">A. 跳层激励带来的性能提升</h3> 
<p><img alt="" height="634" src="https://images2.imgbox.com/8e/93/msbEpSq1_o.png" width="1200"></p> 
<p>我们的主要观察是，SLE 加速了 GAN 的收敛，其中最显着的效果发生在训练开始时。 在前 20000 次迭代中，生成器 G 能够更快地收敛并达到基线模型需要更多训练迭代才能达到的良好点。 另一方面，虽然 SLE 在 G 上提供了更快的收敛，但 SLE 的整体模型行为似乎很好地遵循了基线模型，整体性能略好。 </p> 
<p>换句话说，两个模型的线在图 8 的每个子图中是平行的。具体来说，在 Shell 上（左下），具有 SLE 的模型在 60000 次迭代训练后也崩溃了，就像基线模型一样。 而在其余三个数据集上，FID 的改进速度要慢得多，并且在后半训练迭代中几乎停止变化。 我们认为这种模型行为是有道理的，因为 SLE 模块既没有增加模型容量（参数增加很少），也没有对 GAN 的训练施加任何明确的正则化或指导。 因此，在模型达到良好的收敛状态后，SLE 不太可能产生很大的差异。 </p> 
<p>另一方面，SLE 很好地加快了 G 的收敛速度，提高了 G 的性能。</p> 
<h3 id="B.%20%E5%88%A4%E5%88%AB%E5%99%A8%E7%9A%84%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E6%80%A7%E8%83%BD">B. 判别器的特征提取性能</h3> 
<p>在这里，我们继续讨论鉴别器 D 的自监督自动编码训练的有效性。具体来说，我们探索鉴别器 D 上的特征提取行为与 GAN 的综合性能之间的关系。 通过特征提取性能，我们指的是 D 提取的特征图覆盖输入图像信息的全面程度。 这种特征提取性能可以通过自动编码训练轻松检查。</p> 
<ul><li>详细地说，我们采用在 GAN 中训练的 D 并固定它，然后为 D 训练一个解码器，它试图从 D 编码的特征图中重建图像。</li><li>直觉是，如果 D 关注输入图像的所有区域 ，并以最少的信息丢失对图像进行编码，则解码器更容易重建由 D 编码的图像。</li><li>相反，如果 D 过度拟合并且仅关注图像的有限局部模式，则它的输出丢失特征图信息，因此解码器无法从 D 的输出特征图中重建图像。 </li></ul> 
<p><img alt="" height="343" src="https://images2.imgbox.com/b8/ea/ZL1ofcdg_o.png" width="1033"></p> 
<p>我们为解码器提取倒数第二层的激活，这是 D 确定图像真伪的激活。 表 7 显示了结果，我们在其中训练所有解码器进行相同的 100000 次迭代（所有解码器都收敛）。 请注意，D 的这种特征提取性能并不一定意味着 G 具有更好的合成性能。此外，来自 StyleGAN2 的 D 与来自基线的 D 不可比，因为它们具有完全不同的模型结构和复杂性。</p> 
<p>相反，根据表 7，我们可以获得一些有趣的信息。</p> 
<ul><li>首先，GAN 训练实际上使 D 作为特征编码器的性能变差。 根据行 3 (StyleGAN2) 和 4（基线），我们发现与随机初始化的 D（第 6 列和第 8 列）相比，GAN 训练后的 D 提取的有意义的特征较少。 这意味着在 GAN 训练导致 D 找到真假样本之间的判别特征的同时，它也有效地让 D 忽略了输入图像中的相当多的信息。</li><li>其次，我们将基线模型与具有自我监督学习指导的模型进行比较（第 4、5、6、7 行）。 它表明，与 D 上的随机初始化相比，D 上的自我监督确实导致了更具描述性的特征提取。此外，对比学习也可能导致过度拟合，因为对于分类任务，只有部分图像（一些局部模式）可能就足够了 。 相比之下，重建任务更有可能让 D 从输入图像中覆盖更多信息。 令我们惊讶的是，结合自动编码训练和对比学习导致 D 的性能更差。这表明分类目标影响自动编码目标并以负面方式改变 D 的行为。</li><li>最后但同样重要的是，我们确实发现 D 上更好的特征提取性能会导致 GAN 的更好合成性能。 StyleGAN2 和基线模型似乎都是如此。 对于在 FFHQ 上训练的 StyleGAN2，用更多数据训练的 D 确实比仅在 1000 张图像上训练的 D 从输入图像中保留了更多信息。 对于我们的基线模型，D 上的特征提取性能与各自的 FID 分数非常吻合。 此外，与随机初始化和普通 GAN 训练相比，自监督方法都有效地让 D 从图像中提取了更多信息。</li></ul> 
<p>除了观察之外，我们想强调的是，实验主要是在少样本数据集上进行的。 结果没有给出 D 上的特征提取性能与 GAN 的合成性能之间关系的全貌，需要在更大规模的数据集上进一步研究。 然而，实验确实验证了 D 上的自我监督策略的有效性，以增强 GAN 在少样本数据集上的性能。</p> 
<h3 id="C.%20%E4%B8%8D%E5%90%8C%E5%88%86%E8%BE%A8%E7%8E%87%E4%B8%8B%E7%9A%84%E9%A3%8E%E6%A0%BC%E6%B7%B7%E5%90%88%C2%A0">C. 不同分辨率下的风格混合 </h3> 
<p>在这里，我们展示了关于我们模型的风格混合性能的更多定性结果。 对于在 1024x1024 分辨率上训练的模型，有三个 SLE 层，我们可以在生成的样本之间交换特征图，并且有两个 SLE 层用于 256x256 分辨率的模型。</p> 
<p><img alt="" height="732" src="https://images2.imgbox.com/48/2d/bk7Aij55_o.png" width="1192"></p> 
<p>上图显示了在 1024×1024 分辨率下对艺术绘画和 FFHQ 进行的 1000 个样本训练，以及分辨率为 256x256 的 100 个 Obaxx 样本。 在每一行中，我们将第一行和第一列相应图像 SLE 层中的 x_low 交换。当在所有分辨率上完成特征图交换时，可以获得最佳的样式混合结果。 而引起最多风格变化的最有效图层是 128 分辨率上的图层。 在 256x256 分辨率下，模型表现相同，而 SLE 在较低分辨率下的风格差异最大。 </p> 
<p>在艺术绘画数据上，该模型在风格混合方面表现良好，不仅可以控制颜色，还可以控制纹理。 这些模型在风格混合的合成图像中迁移了扁平或尖笔触的风格。 但是，该模型在 FFHQ 数据上的表现不佳。 在某些情况下，甚至连头发的颜色都无法正确迁移。 我们推测 FFHQ 的性能较差是由于训练样本有限和背景差异很大。 鉴于有限的训练样本，前端面部和背景内容之间没有明确的关系，这使模型对解耦更详细的风格属性感到困惑。 相比之下，艺术绘画在每个图像中都有一致的风格线索，并且场景中每个对象之间都有明显的联系，这可以说比 FFHQ 数据更容易。 另一方面，该模型在 Obaxx 上表现出色，训练图像数量甚至更少，只有 100 张。 它成功地迁移了面部属性和背景风格。 在 256x256 分辨率上学习是一项更简单的任务，模型容量仅在 100 个样本上就足够了。</p> 
<p></p> 
<h2 id="S.%20%E6%80%BB%E7%BB%93">S. 总结</h2> 
<h3 id="S.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3">S.1 核心思想</h3> 
<p>为了减小 GAN 的 few-shot 图像合成的计算成本，作者提出一种轻量化 GAN 结构（FastGAN），包含：一个跳跃层通道激励模块（skip-layer channel-wise excitation module，SLE），一个作为特征编码器的自监督鉴别器。这两种技术都需要对普通 GAN 进行微小改动，从而通过理想的即插即用特性增强 GAN 的实用性。</p> 
<p>FsatGAN 使用更小的模型（更少的参数）获得与 StyleGAN2 相比优越的性能，且收敛也更快。</p> 
<h3 id="S.2%20%E6%A8%A1%E5%9D%97%E6%9E%B6%E6%9E%84">S.2 模块架构</h3> 
<p><img alt="" height="709" src="https://images2.imgbox.com/0e/ed/JrJfyfHH_o.png" width="1200"></p> 
<p><strong>跳跃层通道激励 SLE（G）</strong>。该模块定义为：</p> 
<p class="img-center"><img alt="" height="30" src="https://images2.imgbox.com/ac/b7/Cu49wLzZ_o.png" width="589"></p> 
<p>它利用低尺度特征（激活，activation）来修改高尺度特征图上的通道响应。其中，x 和 y 分别是模块的输入和输出特征图。函数 F 包含对 x_low 的操作，Wi 表示要学习的模块权重。<img alt="" height="409" src="https://images2.imgbox.com/00/ad/xRWeY77r_o.png" width="1200"></p> 
<p><strong>自监督鉴别器（D）</strong>。模块结构如图 4 所示。将 D 视为编码器并用解码器对其进行训练。 这种自动编码训练迫使 D 提取图像特征，解码器用该特征良好重建。 解码器与 D 一起优化重建损失，它只在真实样本上训练：</p> 
<p><img alt="" height="41" src="https://images2.imgbox.com/47/ee/LsdHaCx1_o.png" width="920"></p> 
<p>其中 f 是来自 D 的中间特征图，函数 G 包含对 f 和解码器的处理，函数 T 表示对来自真实图像 I_real 的样本 x 的处理（例如，裁剪。但不限于此）。  </p> 
<h3>S.3  Loss</h3> 
<p><img alt="" height="60" src="https://images2.imgbox.com/c9/82/1nkXRJOl_o.png" width="837"></p> 
<h3 id="S.4%20%E5%88%86%E6%9E%90">S.4 分析</h3> 
<p><strong>SLE 与 ResBlock 的区别。</strong></p> 
<ul><li>相比于 ResBlock 将 skip-connection 实现为来自不同 conv 层的激活之间的逐元素加法（要求激活的空间维度相同），SLE 使用逐通道乘法，从而消除了繁重的卷积计算。</li><li>相比于 skip-connection 仅在同一分辨率内使用。SLE 在更大范围（例如，8x8 和 128x128、16x16 和 256x256）的分辨率之间执行跳跃连接，因为不再需要相等的空间维度。</li></ul> 
<p><strong>SLE 与其他技术的关系</strong>。</p> 
<ul><li>SLE 中的通道乘法与实例规范化（Instance Normalization）吻合，后者广泛用于风格迁移（style-transfer）。</li><li>SLE 像 StyleGAN 一样，使 G 能够自动分离内容（低级特征）和风格（高级特征）。</li></ul> 
<p><strong>自监督鉴别器与自动编码（AE）的区别</strong>。</p> 
<ul><li>AE 在从 D 学习到的隐空间上训练 G 作为解码器，或者把 D 的对抗训练作为 AE 训练之外的补充损失。</li><li>本文的模型是一个纯 GAN，具有更简单的训练模式，自编码训练只对 D 进行正则化，不涉及G。 </li></ul> 
<p></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6e4abe5125e85cea1324391e6213f68c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">（2022，FreGAN）利用频率分量在有限数据下训练 GAN</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8f9648540a50b82ff273eef1827aecd5/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【Java】volatile和内存屏障</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>