<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【具身智能评估8】BEHAVIOR-1K: A Benchmark for Embodied AI with 1,000 Everyday Activities and ... - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【具身智能评估8】BEHAVIOR-1K: A Benchmark for Embodied AI with 1,000 Everyday Activities and ..." />
<meta property="og:description" content="论文标题：BEHAVIOR-1K: A Benchmark for Embodied AI with 1,000 Everyday Activities and Realistic Simulation
论文作者：Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Martín-Martín, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, Mona Anvari, Minjune Hwang, Manasi Sharma, Arman Aydin, Dhruva Bansal, Samuel Hunter, Kyu-Young Kim, Alan Lou, Caleb R Matthews, Ivan Villa-Renteria, Jerry Huayang Tang, Claire Tang, Fei Xia, Silvio Savarese, Hyowon Gweon, Karen Liu, Jiajun Wu, Li Fei-Fei" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/c38556283168c0390b60352f9ff10250/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-19T04:30:00+08:00" />
<meta property="article:modified_time" content="2023-12-19T04:30:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【具身智能评估8】BEHAVIOR-1K: A Benchmark for Embodied AI with 1,000 Everyday Activities and ...</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p>论文标题：BEHAVIOR-1K: A Benchmark for Embodied AI with 1,000 Everyday Activities and Realistic Simulation<br> 论文作者：Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Martín-Martín, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, Mona Anvari, Minjune Hwang, Manasi Sharma, Arman Aydin, Dhruva Bansal, Samuel Hunter, Kyu-Young Kim, Alan Lou, Caleb R Matthews, Ivan Villa-Renteria, Jerry Huayang Tang, Claire Tang, Fei Xia, Silvio Savarese, Hyowon Gweon, Karen Liu, Jiajun Wu, Li Fei-Fei<br> 论文原文：https://proceedings.mlr.press/v205/li23a.html（论文+附录分别下载）<br> 论文出处：PMLR 2023<br> 论文被引：52（12/18/2023）<br> 论文代码：https://github.com/StanfordVL/OmniGibson，236 star<br> 项目主页：https://behavior.stanford.edu/behavior-1k<br> BDDL：https://github.com/StanfordVL/bddl/tree/master/bddl/activity_definitions</p> 
</blockquote> 
<h2><a id="Abstract_11"></a>Abstract</h2> 
<p>我们推出的 BEHAVIOR-1K 是以人为中心的机器人技术综合模拟基准。BEHAVIOR-1K 包括两个部分，由 “what do you want robots to do for you?” 的广泛调查结果指导和推动。第一部分是对 <strong>1000 种日常活动</strong>的定义，以 <strong>50 个场景</strong>（房屋、花园、餐厅、办公室等）为基础，其中有 <strong>5000 多个标注了丰富物理和语义属性的物体</strong>。其次是 <strong>OMNIGIBSON</strong>，这是一个新颖的<strong>模拟环境，通过对刚体、可变形体和液体进行逼真的物理模拟和渲染来支持这些活动</strong>。我们的实验表明，BEHAVIOR-1K 中的活动是长视距的（long-horizon），并且依赖于复杂的操作技能，这两点对于最先进的机器人学习解决方案来说仍然是一个挑战。为了校准 BEHAVIOR-1K 从模拟到现实的差距，我们进行了一项初步研究，将在模拟公寓中使用移动机械手学习到的解决方案迁移到现实世界中。我们希望 BEHAVIOR-1K 的人类基础性、多样性和现实性能使其在具身人工智能和机器人学习研究中发挥重要作用。</p> 
<p><strong>Keywords</strong>: Embodied AI Benchmark, Everyday Activities, Mobile Manipulation</p> 
<h2><a id="1_Introduction_17"></a>1 Introduction</h2> 
<p>基准测试为计算机视觉[1-11] 和自然语言处理[12-16] 带来了进步，受此启发，机器人学界开发了多个模拟基准[17-30]。这些基准的更广泛目标是<strong>推动通用、高效机器人的发展，为人们的日常生活带来重大益处，即服务于人类需求、目标和价值观 的以人为本的人工智能</strong> [31-34]。尽管这些基准令人鼓舞，但其中的任务和活动都是由研究人员设计的，是否能满足人类的实际需求尚不清楚。</p> 
<p>我们认为，以人为本的机器人基准不仅应针对人类需求而设计，还应源于人类需求：<strong>人类希望机器人为他们完成哪些日常活动？</strong> 为此，我们对 <strong>1461 名参与者</strong>进行了广泛调查（见第 2 章），根据参与者希望将这些活动委托给机器人的意愿，对各种日常活动进行排序。我们还请<strong>外行注释者提供这些活动的定义</strong>。调查显示，<strong>人们希望机器人完成的活动具有系统性</strong>，但更重要的是，调查突出了我们<strong>在设计机器人基准时应优先考虑的两个关键因素</strong>：</p> 
<ul><li><strong>场景、物体和活动类型的多样性</strong></li><li><strong>底层模拟环境的真实性</strong></li></ul> 
<p>调查显示，最需要的活动从 "清洗地板 "到 “清洁浴缸” 不等。显然，这些<strong>活动的多样性远远超出了现实世界中机器人挑战所能提供的范围</strong> [35-42]。开发仿真环境是一种自然的替代方案：人们可以在多种场景、物体和条件下高效、安全地训练和测试机器人智能体。然而，要使这种范式（paradigm）发挥作用，必须真实地模拟活动，准确再现机器人在真实世界中可能遇到的情况。虽然在特定领域的逼真度方面已经取得了重大进展 [43 - 45]，但由于提供逼真的模型和模拟功能需要付出大量努力，因此要在各种活动中实现逼真度仍然是一项巨大的挑战。</p> 
<p>在这项工作中，我们介绍了 BEHAVIOR-1K，一个在虚拟、交互和生态环境中的 1000 个日常家庭活动的基准（BEHAVIOR-100 [27] 的下一代）。BEHAVIOR-1K 包括两个新的组成部分，以满足对多样性和真实性的要求：</p> 
<ul><li>多样化的 BEHAVIOR-1K 数据集</li><li>逼真的 OMNIGIBSON 仿真环境</li></ul> 
<p>BEHAVIOR-1K DATASET 是一个大型数据集，其中包括：</p> 
<ul><li>1）1000 个活动的常识性知识库，包括谓词逻辑定义（初始条件和目标条件）、所涉及的物体、物体属性及其状态转换；</li><li>2）高质量 3D 资产，包括 50 个场景和 5000 多个物体模型，并附有丰富的物理和语义注释。</li></ul> 
<p>BEHAVIOR-1K DATASET 中的所有活动都在一个新颖的<strong>仿真环境 OMNIGIBSON</strong> 中实例化，我们<strong>在 Nvidia 的 Omniverse 和 PhysX 5 [46] 的基础上构建了这一环境，以便对刚体、可变形体和流体进行逼真的物理仿真和渲染</strong>。OMNIGIBSON 通过一组扩展物体状态（如温度、切换、浸泡和污浊度）扩展了 Omniverse 的功能。它还能生成有效的初始活动配置，并根据活动定义判别有效的目标解决方案。<strong>凭借所有这些逼真的模拟功能，OMNIGIBSON 可支持 BEHAVIOR-1K DATASET 中的 1000 种不同活动</strong>。</p> 
<p>我们在 BEHAVIOR-1K 的几个活动中评估了最先进的强化学习算法 [47 , 48]，既包括原始动作空间中的视觉运动控制，也包括利用基于采样的运动规划的动作基元（action primitives） [49]。我们的分析表明，<strong>即使是 BEHAVIOR-1K 中的一个活动，对于当前的人工智能算法来说也是极具挑战性的</strong>。具体来说，困难部分来自 <strong>BEHAVIOR-1K 活动的长度和所需物理操作的复杂性</strong>。为了校准 BEHAVIOR-1K <strong>从模拟到现实的差距</strong>，我们进行了一项初步研究，<strong>将在模拟公寓中使用移动机械手学到的解决方案迁移到现实世界中</strong>。我们希望，BEHAVIOR-1K 基准、我们的调查和分析将有助于支持和指导未来具身人工智能智能体和机器人的开发。</p> 
<h2><a id="2_Creating_a_Benchmark_Grounded_in_Human_Needs_A_Survey_Study_42"></a>2 Creating a Benchmark Grounded in Human Needs: A Survey Study</h2> 
<p>大量机器人研究都希望满足人类的需求，但这些需求通常都是假设或推测的。以人为本的开发需要直接了解人类对自主智能体的需求[31]。为了创建一个能反映这些需求的基准，我们针对<strong>美国普通民众</strong>进行了一项调查，询问：<code>what do you want robots to do for you?</code> <strong>该调查从时间使用调查[50-52]（记录人们如何花费时间）和WikiHow文章[53]中获取了约2000项活动</strong>。我们在 Amazon Mechanical Turk 上进行调查，共有 1461 名受访者（人口统计数据见附录 A.3），每项活动有 50 个 10 分的 Likert 量表回答。</p> 
<p><img src="https://images2.imgbox.com/7e/b5/pMJn4kss_o.png" alt="在这里插入图片描述"></p> 
<p>图 1（左）总结了调查结果，其中我们根据人类偏好得分对活动进行了排名。<strong>完整的排序活动列表可在我们的网站上找到</strong>。<strong>分布显示出较大的统计离散性（基尼系数=0.158）：人类希望机器人执行的活动范围很广，从清洁杂务到烹饪大餐。像 “擦洗浴室地板” 这样的繁琐任务得分最高，而像游戏这样的娱乐活动得分最低</strong>。在众多类别中，有大约 200 项清洁活动和 200 多项烹饪活动。</p> 
<p><strong>BEHAVIOR-1K 的活动包括人类偏好分数最高的 909 项活动，以及 BEHAVIOR-100 [27] 中的 91 项活动，共计排名最高的 1000 项活动</strong>。BEHAVIOR-1K 有别于其他具身人工智能基准，它从时间使用调查中获取数据，并利用调查数据对人类认为最重要和最有用的活动进行优先排序，同时还包含了大量不同的活动。</p> 
<h2><a id="3_Related_Work_Embodied_AI_Benchmarks_53"></a>3 Related Work: Embodied AI Benchmarks</h2> 
<p><img src="https://images2.imgbox.com/24/b9/bS377vP6_o.png" alt="在这里插入图片描述"></p> 
<p>我们在表 1 中提供了 BEHAVIOR-1K 与其他人工智能模拟基准[17- 26]之间的广泛比较。我们将许多有助于提高多样性和真实性的因素纳入其中，并观察到 BEHAVIOR-1K 在这方面迈出了重要一步。首先，其他基准都没有将普通人的需求作为其活动集的基础。其他基准通常只针对相对有限的一组活动，而且其模拟器只在这些任务的相关方面逼真。事实上，我们经常可以看到多样性与现实性之间的权衡。例如，</p> 
<ul><li>VirtualHome [20] 和 ALFRED [20 , 21] 等遵循指令的基准在场景、物体和状态变化的数量上是多样的，但提供的<strong>低层次（low-level）物理逼真度却很有限</strong>。</li><li>另一方面，Habitat 2.0 HAB[26]、TDW Transport[19]和 SAPIEN ManiSkill[54，55] 等家庭重组基准支持逼真的动作执行和精确的刚体物理模拟，但<strong>只包括少数任务</strong>。</li><li>同样，SoftGym [45] 和 RFUniverse [56] 的模拟功能与 OMNIGIBSON 最为接近，因此也最逼真，但它们也<strong>缺乏支持以人为中心的通用机器人开发所需的任务多样性</strong>。</li></ul> 
<p>与我们最相似的基准是上一代的 BEHAVIOR-100 [27]。BEHAVIOR-100 提出了一些有益的设计方案，<strong>我们在 BEHAVIOR-1K 中继承了这些方案，如活动源（ATUS [ 50 ]）、活动定义逻辑语言和评估指标</strong>。然而，BEHAVIOR-100在支持以人为本的人工智能模拟基准所需的<strong>多样性和真实性方面存在不足</strong>，而BEHAVIOR-1K在这些方面达到了无与伦比的水平。<strong>BEHAVIOR-100 包含 100 个由研究人员选择的活动，而我们的 BEHAVIOR-1K 则将多样性提高了一个数量级，达到了 1000 个活动，这些活动都是通过我们独特的调查以人类需求为基础的</strong>。此外，BEHAVIOR-100 只包括 15 个场景（所有房屋）和 300 多个物体类别，而 BEHAVIOR-1K 则增加到 50 个场景（房屋、商店、餐馆、办公室等）和 1200 多个物体类别。在逼真度方面，BEHAVIOR-1K 扩展了 OMNIGIBSON 可模拟的物理状态和过程：流体、柔性材料、混合物质等。OMNIGIBSON 为 BEHAVIOR-1K 提供的渲染逼真度也明显高于 BEHAVIOR-100 和其他基准测试（见图 3）。</p> 
<p><img src="https://images2.imgbox.com/fe/e8/d9ukQ2Js_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="4_BEHAVIOR1K_DATASET_69"></a>4 BEHAVIOR-1K DATASET</h2> 
<p>一旦活动来源反映了人类的需求，就需要对它们进行具体定义，并按照它们在现实世界中发生的方式进行实例化。我们建立了 BEHAVIOR-1K 数据集，其中包括一个包含相关物体和物体状态的众包活动定义知识库，以及一个大规模的高质量交互式 3D 模型库。</p> 
<p>我们以 BEHAVIOR Domain Definition Language（行为领域定义语言，BDDL）[27] 的形式对活动的具体定义进行众包。BDDL 以谓词逻辑为基础，旨在让普通人也能描述特定活动的具体初始条件和目标条件。与几何、图像/视频或经验目标规范不同[17, 18]，BDDL 的定义以物体和物体状态为单位，允许注释者在直观的语义层面进行定义。语义符号还能捕捉到多个物理状态可能是活动的有效初始化和解决方案这一事实。有关定义示例，请参见附录（附录需要从PMLR单独下载）中的 listing 1、2 和 3。</p> 
<p><img src="https://images2.imgbox.com/8f/a5/5nCVlzZu_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>[27] BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments</p> 
</blockquote> 
<p>活动定义所基于的物体和物体状态空间都经过注释，在生态学上是可信的。物体空间（object spaces）来自于 1000 个活动的 5000 篇 WikiHow 文章，并映射到 1484 个 WordNet [57] 词组。通过群众工作者、学生和 GPT-3 [58]，我们还将每个物体与完全可模拟的物体状态联系起来：例如，苹果与煮熟和切片相关联，但与 "打开 " 无关。许多物体-属性对还增加了参数，例如 “苹果的熟制温度”，利用 OMNIGIBSON 的连续扩展状态，使活动特别逼真。最后，注释者和研究人员还创建了过渡规则，例如将西红柿和盐变成调味汁，或要求用砂纸除锈。最终，我们建立了一个包含数以万计元素的知识库，其中包含 1000 个生态上合理的活动定义。我们通过让五位经验丰富的机器学习注释员验证所有类型注释的子集来确保注释质量，并获得了极高的认可率（&gt;96.8%）。有关知识库的更多详情，请参见附录 B。</p> 
<p>这些活动定义的多样性需要不同的物体和场景模型。<strong>除了 BEHAVIOR-100 [ 27] 中的 15 个房屋场景外，我们还获得了 35 个完全交互式场景，这些场景类型多种多样，如花园、办公室、餐厅和商店等，都是日常活动中必不可少的场景</strong>。与其他基准相比，这是前所未有的（见表 1）。我们还获取了活动所需的 1,200 多个类别中的 5,000 多个物体实例，并为每个物体标注了丰富的物理属性（如摩擦、质量、衔接）和语义属性（如类别）。具有代表性的场景和物体模型见图 2。有关三维模型的更多详情，请参阅附录 D。</p> 
<p><img src="https://images2.imgbox.com/53/4b/TFfICJgv_o.png" alt="在这里插入图片描述"></p> 
<p>图 2：BEHAVIOR-1K 的要素。我们的基准包括两个要素：</p> 
<p>BEHAVIOR-1K DATASET 和 OMNIGIBSON。</p> 
<ul><li>左图：BEHAVIOR-1K DATASET 包括 1,000 个 BDDL 活动定义（左上角）、50 个逼真多样的场景（右上角）和 5,000 多个在知识库中标注了属性的物体（下图）。</li><li>右图： OMNIGIBSON 提供了逼真模拟 1000 种活动所需的功能，包括热效应（如火/蒸汽/烟雾）（左上）、流体动力学（左下）、过渡规则功能机（中上）、可变形体/布（中下）、逼真照明和反射（右上）以及透明渲染（右下）。它们共同构成了日常活动（如模拟烹饪晚餐）的具体、逼真的实例。</li></ul> 
<h2><a id="5_OMNIGIBSON_Instantiating_BEHAVIOR1K_with_Realistic_Simulation_94"></a>5 OMNIGIBSON: Instantiating BEHAVIOR-1K with Realistic Simulation</h2> 
<p>BEHAVIOR-100 是在 iGibson 2.0 中实现的[ 59]；然而，BEHAVIOR-1K 中各种活动的真实模拟超出了 iGibson 2.0 的能力范围。我们提出了一种新型仿真环境 OMNIGIBSON，它提供了支持和实例化 BEHAVIOR-1K 的必要功能。OMNIGIBSON 建立在 NVIDIA Omniverse 和 PhysX 5 的基础之上，不仅能模拟刚体，还能模拟可变形物体、流体和柔性材料（见图 4），同时生成高度逼真的光线追踪或路径追踪虚拟图像（见图 3）。与其他基准相比，这些功能大大提高了 BEHAVIOR-1K 的逼真度。</p> 
<p><img src="https://images2.imgbox.com/4e/7b/iXcZzh70_o.png" alt="在这里插入图片描述"></p> 
<p>与 BEHAVIOR-100 类似，<strong>OMNIGIBSON 也会根据启发式方法（例如，当靠近热源并打开开关时，温度会升高）模拟额外的、非运动式的扩展物体状态（例如温度、浸湿程度）</strong>。OMNIGIBSON 还实现了<strong>生成无限有效物理配置的功能，以满足作为逻辑谓词的活动初始条件（如食物已冷冻），并根据物体的物理状态（姿势和关节配置）和扩展状态评估其目标条件（如食物已煮熟并在盘子上，布已折叠）</strong>。OMNIGIBSON 本机支持场景初始化过程中的随机化，并能对物体模型及其姿势/状态进行采样。关于 OMNIGIBSON 所支持的扩展物体状态和逻辑谓词的全部细节，请参见附录 E.1。</p> 
<p><strong>许多日常任务难以模拟，因为它们需要模拟复杂的物理过程，例如折叠毛巾或倒一杯水。OMNIGIBSON 支持对液体、可变形物体和布料进行逼真的模拟，从而使这些任务得以实现（见图 2）</strong>。事实上，如果没有这些功能，BEHAVIOR-1K 中一半以上的活动都无法模拟，这说明这些功能对于捕捉日常活动是多么重要。OMNIGIBSON 还能捕捉 Omniverse 本身无法模拟的多种物理过程，例如<strong>烤馅饼</strong>或<strong>搅碎蔬菜</strong>。除了上述扩展状态外，我们还设计了一个模块化的转换器（Transition Machine），用于<strong>在满足特定条件时指定物体组之间的自定义转换</strong>。例如，放在烤箱中的面团达到一定温度阈值后，就会变成馅饼。这进一步扩展了 OMNIGIBSON 的能力，使其能够模拟复杂、逼真的活动，<strong>否则就很难用物理方法完全模拟这些活动</strong>。</p> 
<h2><a id="6_Experiments_Evaluating_Embodied_AI_Solutions_in_BEHAVIOR1K_107"></a>6 Experiments: Evaluating Embodied AI Solutions in BEHAVIOR-1K</h2> 
<p>在实验中，我们旨在回答三个问题：</p> 
<ul><li>现有的基于视觉的机器人学习算法在 BEHAVIOR-1K 中表现如何？</li><li>对于当前的人工智能来说，哪些活动元素最容易出问题？</li><li>BEHAVIOR-1K/OMNIGIBSON 中模拟现实差距的主要来源是什么？</li></ul> 
<p>我们的目标是指出有希望的研究方向，以提高人工智能在模拟 BEHAVIOR-1K 活动中的表现，并最终提高其在现实世界中的表现。</p> 
<h3><a id="61_Evaluating_BEHAVIOR1K_Solutions_in_OMNIGIBSON_117"></a>6.1 Evaluating BEHAVIOR-1K Solutions in OMNIGIBSON</h3> 
<h4><a id="Experimental_Setups_119"></a>Experimental Setups</h4> 
<p>我们选择了三种范例活动进行实验：</p> 
<ul><li>收集垃圾（Collect Trash），即智能体收集空瓶子和杯子并将其扔进垃圾桶（刚体操作）；</li><li>存储装饰（Store Decoration），即智能体将物品存储到抽屉中（铰接物体操作）；</li><li>擦桌子（Clean Table），即智能体用一块浸湿的布擦拭脏桌子（柔性材料和液体操作）。</li></ul> 
<p>我们根据最先进的强化学习算法（RL）[60] 评估了三种不同的基线：</p> 
<ul><li>RL-VMC，一种视觉运动控制（从图像到底层关节指令）的 RL 解决方案，基于软演员批判（Soft Actor-Critic, SAC）[48]；</li><li>RL-Prim.，一种 RL 解决方案，基于 PPO [47]，利用一组基于采样运动规划器的动作基元[61, 62, 49]（取、放、推、导航、沾和擦）。</li><li>RL-Prim.Hist. 是 RL-Prim. 的一种变体，它将历史观察结果（3 个步骤）作为额外输入，以帮助区分外观相似的状态。</li></ul> 
<p>所有智能体都是<strong>通过稀疏的任务成功奖励进行训练的</strong>，没有任何奖励工程。按照 BEHAVIOR-100 [27]提出的指标，我们在表 2 和表 3 中报告了成功率和效率指标（行进距离、投入时间和造成的混乱），并在附录表 A.13 中报告了成功得分 Q。</p> 
<p><img src="https://images2.imgbox.com/22/53/17ovT9Lm_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/b1/73/s0eCh7cQ_o.png" alt="在这里插入图片描述"></p> 
<p>抓取（Grasping）本身就是一个具有挑战性的研究课题。为了便于实验，我们采用了一种辅助拾取基元（pick primitive），当所有手指都与物体接触时，如果要求抓取，该基元就会在物体和抓取器之间建立刚性连接，这是之前研究中使用的 StickyMitten 的一种更为严格的形式 [26 , 63 , 64]。此外，为了加快训练速度，动作基元只检查最终配置的可行性（如可触及性、碰撞），例如拾取时的抓取姿势或导航时的期望位置。如果运动学上可行，动作基元将直接把机器人状态设置为最终配置，并从那时起继续模拟。我们在评估中对这些假设和简化的效果进行了消融分析（见表 4）。有关训练和评估设置的更多详情，请参阅附录 F。</p> 
<h4><a id="Results_Task_Completion_143"></a>Results: Task Completion</h4> 
<p>表 2 列出了各种基线方法的任务成功率。在我们的活动中，极长的时间跨度导致视觉-运动控制（RL-VMC）策略在所有三个活动中都失败了，这可能是由于信用分配[65]、深度探索[66 , 67]和梯度消失[68]等问题造成的。我们<strong>采用时间扩展行动基元（RL-Prim.和 RL-Prim.Hist.）的基线取得了更好的成功，在所有三个活动中都达到了 40% 以上的成功率</strong>。我们发现，时间跨度较长的活动更具挑战性：执行 6 个基元步骤的最佳序列即可完成 CleanTable，而收集垃圾至少需要 16 个基元步骤。这支持了一种观点，即某种形式的行动空间抽象对于解决 BEHAVIOR-1K 的长周期活动是必要的，正如其他报告所指出的那样[27, 26 , 69]。<strong>在分析记忆的作用时，我们观察到从 RL-Prim.Hist. 到 RL-Prim.Hist. 的性能提升非常明显，尤其是在收集垃圾（CollectTrash）等具有别名观测的长视距活动中</strong>。在这项任务中，当机器人观察垃圾桶时，它需要额外的信息来了解哪个位置已经被清理过，以便继续前往其他位置。<strong>我们的研究结果表明，在长视距 BEHAVIOR-1K 活动中，记忆将对人工智能起到至关重要的作用</strong>。</p> 
<h4><a id="Results_Efficiency_147"></a>Results: Efficiency</h4> 
<p>除了成功与否，效率也是评估人工智能的关键：在模拟中成功的策略，如果耗时过长或浪费过多能源，在现实世界中可能是不可行的。在表 3 中，我们用 Srivastava 等人[27]提出的三个效率指标来报告结果。我们发现，使用记忆（RL-Prim.Hist.）可以提高所有指标的效率：导航距离（Dist. Nav.）、模拟时间（Sim. Time）和运动物体错位（Kin. Dis.），即机器人运动造成的物体位移量。</p> 
<p>我们还评估了在训练过程中引入的物理和执行（抓取、运动执行）简化对 RL-Prim 性能的影响程度。结果见表 4。我们观察到，在评估过程中<strong>启用完全基于物理的抓取功能后，性能急剧下降</strong>。因此，抓取是任何人工智能任务的关键组成部分，研究人员在训练过程中简化其执行时应小心谨慎。虽然 OMNIGIBSON 支持完全基于物理的抓取功能，但<strong>为任意物体设计一个可利用完全基于物理的抓取功能的拾取动作基元本身就是一个有待解决的研究问题，我们将其留待今后的工作中解决</strong>。相比之下，在评估过程中启用全轨迹运动执行后，性能下降幅度要小得多。这一结果支持了我们的假设，即<strong>在评估过程中，假设运动规划有可能在自由空间中提供可行路径，那么加速训练过程就是合理的</strong>。</p> 
<h3><a id="62_Evaluating_BEHAVIOR1K_Solutions_on_a_Real_Robot_153"></a>6.2 Evaluating BEHAVIOR-1K Solutions on a Real Robot</h3> 
<p>我们用一个真实机器人进行了一系列实验，以回答这样一个问题：我们的真实模拟与现实世界之间存在差异的主要原因是什么？为此，我们在 “收集垃圾” 活动中使用了模拟公寓的真实场景。我们对公寓进行了扫描，并将其转换成一个虚拟的交互式场景。</p> 
<ul><li>我们使用了一个真实的双手动移动机械手 Tiago</li><li>利用其板载传感器的 RGB-D 图像和 YOLOv3 物体检测器 [70 , 71] 在三维空间中定位物体，以便进行操作。</li><li>在导航方面，机器人利用基于两个激光雷达传感器和公寓地图的粒子过滤器[72]进行定位。</li><li>动作基元采用与模拟[62 , 49]中相同的基于采样的运动规划算法，并进行了额外的调整。</li></ul> 
<p>我们评估了在现实世界中选择动作基元的两种策略：</p> 
<ul><li>一种是基于人类输入的最优策略；</li><li>另一种是在 OMNIGIBSON 中训练的基于视觉的策略（RL-Prim.） 。</li></ul> 
<p>为了促进从模拟到真实的转换，在训练过程中，我们还根据先前的研究成果[73]对观察结果进行了基于图像的数据增强（详情请参见附录 G）。通过最优策略，我们评估了模拟机器人与真实机器人在执行方面的差距；通过学习的策略，我们还评估了视觉感知方面的差距。我们在模拟（50 次运行，成功率为 40%）和真实世界中使用最优策略（27 次运行，成功率为 22%）和训练策略（26 次运行，成功率为 0%）取得了不同的成功率，这表明模拟与真实之间存在差距，我们将在下文进行分析。</p> 
<p><img src="https://images2.imgbox.com/e2/11/5hxzTvF8_o.png" alt="在这里插入图片描述"></p> 
<p>图 5：模拟与真实之间差距的特征：左图：模拟场景与真实场景的并排对比，包括机器人获取的虚拟图像和真实图像。虽然高分辨率图像极为相似，但木质纹理和摄像头属性的不匹配导致了机器人视觉输入的巨大差距。右图：在模拟（S，左）和真实世界中，OMNIGIBSON 的训练策略（R-TP，中）和优化策略（R-OP，右）由于执行（实色）或感知（条纹）而导致的故障源。在模拟过程中，由于没有对抓取动作进行全面模拟（见第 6.1 节），策略失败（即选择了错误的动作基元）占主导地位。在真实机器人上，抓取是导致错误的主要原因之一，感知问题也是如此（使用训练视觉策略时出现策略错误，使用最优策略时出现物体检测错误）。</p> 
<p>失败案例如图 5（右图）所示。我们观察到，<strong>模拟中的大多数失败案例都是由于视觉策略（感知）造成的，而其他失败案例则是由于位置基元和基于采样的运动规划器中的随机性造成的</strong>。之所以没有一个失败是由抓取造成的，是因为在模拟中我们使用了辅助拾取基元进行评估。在现实世界中，抓取是相当困难的，在训练有素的策略和最优策略中，抓取造成的失败约占 40%。对于已学习的策略，44% 的错误来自于<strong>视觉策略选择了错误的动作基元，这是由于模拟图像和真实图像之间存在差异</strong>。<strong>视觉差异是由于未建模的影响造成的，例如真实摄像机的动态范围较差</strong>（见图 5 左侧和中间）以及<strong>不完美的物体建模</strong>（例如精确的木质纹理和桌子的表面反射率），<strong>这些都可以通过更有针对性的域随机化来缓解</strong>。有趣的是，<strong>真实机器人的几次操作失败都是由于前一时间步中的导航不准确导致机器人底座位置不合适造成的</strong>。这种复合误差源在模拟中并不存在，因为我们假定定位和执行都是完美的。我们相信，这一分析提供了有关 OMNIGIBSON BEHAVIOR-1K 模拟与实际差距的主要来源和严重程度的相关信息，并为未来的研究方向提供了启示。我们计划利用其中的一些见解来创建新颖的仿真解决方案，从而在 BEHAVIOR-1K 方面取得进展。</p> 
<h2><a id="7_Discussion_and_Limitations_176"></a>7 Discussion and Limitations</h2> 
<p>我们介绍了 BEHAVIOR-1K，它是具身人工智能和机器人研究的基准，可真实模拟 1000 种基于人类需求的各种活动。BEHAVIOR-1K 包含两个要素：</p> 
<ul><li>BEHAVIOR-1K DATASET（日常活动语义知识库）和大型三维模型库；</li><li>OMNIGIBSON（仿真环境），为刚性/可变形物体、柔性材料和流体提供逼真的渲染和物理效果。</li></ul> 
<p>在评估中，我们发现 BEHAVIOR-1K 是一个极具挑战性的基准：<strong>自主解决这 1000 个活动超出了当前最先进的人工智能算法的能力</strong>。我们研究并尝试用动作基元来解决少数活动，以便深入了解最具挑战性的部分，为其他研究人员提供一个起点来研究我们的基准。同样，我们还<strong>通过创建真实世界模拟公寓的数字孪生体，以及使用模拟和真实移动机械手对模拟和真实世界中的策略进行严格评估和分析，探索了模拟与真实之间差距的根源</strong>。</p> 
<p>局限性：我们从底层物理和渲染引擎 Nvidia’s Omniverse 继承了一些限制。在 OMNIGIBSON 中，我们在渲染速度与视觉逼真度（光线追踪）之间进行了权衡，对于一个包含约 60 个物体的房屋场景，渲染速度约为 60 fps（iGibson 2.0 中的渲染速度约为 100 fps [59]）。我们正在积极进行性能优化。另一个限制因素是，<strong>我们只包括不需要与人类互动的活动</strong>。对人类（行为、动作、外观）的真实模拟极具挑战性，也是一个开放的研究领域。我们计划在技术更加成熟时将模拟人纳入其中。最后，OMNIGIBSON 仍有改进的余地，以进一步促进模拟与现实之间的转换，例如纳入感知和执行的噪声模型。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c653a92e919f2094b4d38edef62288d4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【具身智能评估7】ProcTHOR: Large-Scale Embodied AI Using Procedural Generation</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/775bb358f6d569d17a1a7896018f97d3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【具身智能评估9】Open X-Embodiment: Robotic Learning Datasets and RT-X Models</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>