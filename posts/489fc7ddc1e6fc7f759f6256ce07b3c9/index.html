<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>第九章：RefineNet——多路径细化网络用于高分辨率语义分割 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="第九章：RefineNet——多路径细化网络用于高分辨率语义分割" />
<meta property="og:description" content="&amp;原文信息 原文题目：《RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation》
原文引用：Lin G, Milan A, Shen C, et al. Refinenet: Multi-path refinement networks for high-resolution semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 1925-1934.
原文链接：https://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_RefineNet_Multi-Path_Refinement_CVPR_2017_paper.pdfhttps://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_RefineNet_Multi-Path_Refinement_CVPR_2017_paper.pdf
0.摘要 最近，非常深的卷积神经网络（CNN）在目标识别方面表现出色，并且也是密集分类问题（如语义分割）的首选。然而，在深度CNN中，重复的子采样操作（如池化或卷积跳跃）会导致初始图像分辨率的显著降低。因此，我们提出了RefineNet，这是一个通用的多路径细化网络，明确利用沿着降采样过程可用的所有信息，通过长程残差连接实现高分辨率预测。通过这种方式，能够直接利用较早卷积层的细粒度特征来精确调整捕获高级语义特征的深层。RefineNet的各个组件都采用了残差连接，遵循恒等映射的思想，从而实现了有效的端到端训练。此外，我们引入了链式残差池化，以高效地捕捉丰富的背景上下文。我们进行了全面的实验，并在七个公共数据集上取得了最新的最优结果。特别是，在具有挑战性的PASCAL VOC 2012数据集上，我们取得了83.4的交并比分数，这是迄今为止报道的最好结果。
1.引言 语义分割是图像理解中的一个关键组成部分。任务是为图像中的每个像素分配一个唯一的标签（或类别），可以被视为一种密集分类问题。所谓的对象解析问题通常可以被视为语义分割。最近，深度学习方法，特别是卷积神经网络（CNN），例如VGG [42]、残差网络 [24]，在识别任务中显示出了显著的结果。然而，当涉及到密集预测任务，如密集深度或法线估计 [13,33,34]和语义分割 [36,5]时，这些方法显示出明显的局限性。多个阶段的空间池化和卷积步幅通常会使最终图像预测在每个维度上减少32倍，从而丧失了许多更精细的图像结构。
解决这个限制的一种方法是学习反卷积滤波器作为上采样操作[38,36]，以生成高分辨率的特征图。反卷积操作无法恢复在卷积前向阶段的降采样操作后丢失的低级视觉特征。因此，它们无法输出准确的高分辨率预测。低级视觉信息对于边界或细节的准确预测至关重要。最近由Chen等人提出的DeepLab方法[6]采用了扩张（或膨胀）卷积来考虑更大的感受野，而不对图像进行降采样。DeepLab被广泛应用，并代表了语义分割的最新性能。然而，这种策略至少有两个限制。首先，它需要在通常具有高维特征的大量详细（高分辨率）特征图上进行卷积，这在计算上是昂贵的。而且，大量的高维和高分辨率特征图也需要巨大的GPU内存资源，特别是在训练阶段。这阻碍了高分辨率预测的计算，并且通常将输出大小限制为原始输入的1/8。其次，扩张卷积引入了特征的粗糙子采样，可能导致重要细节的丢失。
另一种方法利用中间层的特征来生成高分辨率的预测，例如[36]中的FCN方法和[22]中的超级像素方法。这些方法的直觉是，中间层的特征被期望描述对象部分的中级表示，同时保留空间信息。这些信息被认为是早期卷积层的特征的补充，早期卷积层的特征编码低级的空间视觉信息，如边缘、角点、圆等，以及深层的高级特征，编码高级语义信息，包括对象或类别级别的证据，但缺乏强的空间信息。
我们认为所有层次的特征对于语义分割都是有帮助的。高级语义特征有助于图像区域的类别识别，而低级视觉特征有助于生成高分辨率预测的清晰、详细的边界。如何有效地利用中间层特征仍然是一个开放的问题，值得更多的关注。为此，我们提出了一种新颖的网络架构，有效地利用多层次特征生成高分辨率的预测。我们的主要贡献如下：
1.我们提出了一个多路径细化网络（RefineNet），它利用多个抽象级别的特征进行高分辨率的语义分割。RefineNet以递归的方式将低分辨率（粗糙的）语义特征与细粒度的低级特征相结合，生成高分辨率的语义特征图。我们的模型具有灵活性，可以轻松地级联和修改。
2.我们的级联RefineNet可以有效地进行端到端的训练，这对于最佳预测性能至关重要。具体而言，RefineNet中的所有组件都使用残差连接[24]和恒等映射[25]，使得梯度可以通过短程和长程残差连接直接传播，从而实现有效和高效的端到端训练。
3.我们提出了一个新的网络组件，称为串联残差池化，它能够从大范围的图像区域中捕获背景上下文。它通过使用多个窗口大小高效地池化特征，并通过残差连接和可学习权重将它们融合在一起。
4.我们提出的RefineNet在包括PASCAL VOC 2012、PASCAL-Context、NYUDv2、SUN-RGBD、Cityscapes、ADE20K和对象解析人体部位数据集在内的7个公共数据集上取得了最新的最佳性能。特别是在PASCAL VOC 2012数据集上，我们的IoU得分达到了83.4，远远超过当前最好的方法DeepLab。
为了促进未来的研究，我们发布了RefineNet的源代码和训练模型。
图1.我们方法在对象解析任务（左）和语义分割（右）上的示例结果。
1.1.相关工作 在近年来，卷积神经网络（CNN）已成为语义分割最成功的方法。早期的方法[18,23]是基于区域提议的方法，通过对区域提议进行分类来生成分割结果。最近，完全卷积网络（FCNN）[36,5,10]展示了有效的特征生成和端到端训练，并因此成为语义分割最受欢迎的选择。FCNN还被广泛应用于其他密集预测任务，例如深度估计[15,13,33]、图像恢复[14]和图像超分辨率[12]。这里提出的方法也基于完全卷积风格的网络。
基于FCNN的方法通常存在低分辨率预测的限制。有许多提出的技术来解决这个限制，并旨在生成高分辨率的预测结果。在[5]中，采用了基于空洞卷积的方法DeepLab-CRF，直接输出中等分辨率的分数图，然后利用颜色对比信息应用密集CRF方法[27]来优化边界。CRF-RNN [47]通过实现循环层来扩展这种方法，以端到端学习密集CRF和FCNN。解卷积方法[38,2]学习解卷积层来上采样低分辨率的预测结果。在[34]中的深度估计方法采用超像素池化来输出高分辨率的预测结果。
存在一些利用中间层特征进行分割的方法。Long等人的FCN方法[36]在中间层添加预测层，以生成多分辨率的预测分数。他们对多分辨率的分数进行平均，生成最终的预测掩码。他们的系统采用阶段性训练而非端到端训练。Hypercolumn方法[22]合并了来自中间层的特征，并学习密集分类层。该方法也采用阶段性训练策略而非端到端训练。SegNet [2]和U-Net [40]都在解卷积架构中应用跳过连接，以利用中间层的特征。
尽管存在一些相关工作，如何有效地利用中间层特征仍然是一个开放的问题。我们提出了一种新颖的网络架构RefineNet来解决这个问题。RefineNet的网络架构与现有方法有所不同。它由一些特别设计的组件组成，能够通过利用低级视觉特征来改进粗糙的高级语义特征。特别是，RefineNet采用了短程和长程的残差连接，并使用恒等映射，实现了整个系统的有效端到端训练，从而帮助实现卓越的性能。全面的实证结果清楚地验证了我们的新颖网络架构在利用中间层特征方面的有效性。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/489fc7ddc1e6fc7f759f6256ce07b3c9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-02T20:22:07+08:00" />
<meta property="article:modified_time" content="2023-08-02T20:22:07+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">第九章：RefineNet——多路径细化网络用于高分辨率语义分割</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>&amp;原文信息</h2> 
<p><span style="color:#4da8ee;">原文题目：《RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation》</span></p> 
<p><span style="color:#4da8ee;">原文引用：Lin G, Milan A, Shen C, et al. Refinenet: Multi-path refinement networks for high-resolution semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 1925-1934.</span></p> 
<p><span style="color:#4da8ee;">原文链接：</span><a class="has-card" href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_RefineNet_Multi-Path_Refinement_CVPR_2017_paper.pdf" rel="nofollow" title="https://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_RefineNet_Multi-Path_Refinement_CVPR_2017_paper.pdf"><span class="link-card-box"><span class="link-title">https://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_RefineNet_Multi-Path_Refinement_CVPR_2017_paper.pdf</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/30/2d/QwhYog2J_o.png" alt="icon-default.png?t=N6B9">https://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_RefineNet_Multi-Path_Refinement_CVPR_2017_paper.pdf</span></span></a></p> 
<h2>0.摘要</h2> 
<p>        最近，非常深的卷积神经网络（CNN）在目标识别方面表现出色，并且也是密集分类问题（如语义分割）的首选。然而，<span style="color:#fe2c24;">在深度CNN中，重复的子采样操作（如池化或卷积跳跃）会导致初始图像分辨率的显著降低。</span>因此，我们提出了RefineNet，<span style="color:#fe2c24;">这是一个通用的多路径细化网络，明确利用沿着降采样过程可用的所有信息，通过长程残差连接实现高分辨率预测</span>。<u>通过这种方式，能够直接利用较早卷积层的细粒度特征来精确调整捕获高级语义特征的深层。</u>RefineNet的各个组件都采用了残差连接，遵循恒等映射的思想，从而实现了有效的端到端训练。此外，我们引入了链式残差池化，以高效地捕捉丰富的背景上下文。我们进行了全面的实验，并在七个公共数据集上取得了最新的最优结果。特别是，在具有挑战性的PASCAL VOC 2012数据集上，我们取得了83.4的交并比分数，这是迄今为止报道的最好结果。</p> 
<h2>1.引言</h2> 
<p>        <span style="color:#fe2c24;">语义分割是图像理解中的一个关键组成部分。任务是为图像中的每个像素分配一个唯一的标签（或类别），可以被视为一种密集分类问题。</span>所谓的对象解析问题通常可以被视为语义分割。最近，深度学习方法，特别是卷积神经网络（CNN），例如VGG [42]、残差网络 [24]，在识别任务中显示出了显著的结果。然而，当涉及到密集预测任务，如密集深度或法线估计 [13,33,34]和语义分割 [36,5]时，这些方法显示出明显的局限性。<span style="color:#fe2c24;">多个阶段的空间池化和卷积步幅通常会使最终图像预测在每个维度上减少32倍，从而丧失了许多更精细的图像结构。</span></p> 
<p>        <span style="color:#fe2c24;">解决这个限制的一种方法是学习反卷积滤波器作为上采样操作[38,36]，以生成高分辨率的特征图。</span><span style="background-color:#ff9900;">反卷积操作无法恢复在卷积前向阶段的降采样操作后丢失的低级视觉特征。因此，它们无法输出准确的高分辨率预测。低级视觉信息对于边界或细节的准确预测至关重要。</span><span style="background-color:#a2e043;">最近由Chen等人提出的DeepLab方法[6]采用了扩张（或膨胀）卷积来考虑更大的感受野，而不对图像进行降采样。DeepLab被广泛应用，并代表了语义分割的最新性能。</span><span style="background-color:#38d8f0;">然而，这种策略至少有两个限制。首先，它需要在通常具有高维特征的大量详细（高分辨率）特征图上进行卷积，这在计算上是昂贵的。而且，大量的高维和高分辨率特征图也需要巨大的GPU内存资源，特别是在训练阶段。这阻碍了高分辨率预测的计算，并且通常将输出大小限制为原始输入的1/8。其次，扩张卷积引入了特征的粗糙子采样，可能导致重要细节的丢失。</span></p> 
<p>        <span style="color:#fe2c24;">另一种方法利用中间层的特征来生成高分辨率的预测，例如[36]中的FCN方法和[22]中的超级像素方法。这些方法的直觉是，中间层的特征被期望描述对象部分的中级表示，同时保留空间信息。这些信息被认为是早期卷积层的特征的补充，早期卷积层的特征编码低级的空间视觉信息，如边缘、角点、圆等，以及深层的高级特征，编码高级语义信息，包括对象或类别级别的证据，但缺乏强的空间信息。</span></p> 
<p>        <span style="color:#fe2c24;"><strong>我们认为所有层次的特征对于语义分割都是有帮助的。高级语义特征有助于图像区域的类别识别，而低级视觉特征有助于生成高分辨率预测的清晰、详细的边界。如何有效地利用中间层特征仍然是一个开放的问题，值得更多的关注。</strong></span>为此，我们提出了一种新颖的网络架构，有效地利用多层次特征生成高分辨率的预测。我们的主要贡献如下：</p> 
<p>        1.我们提出了一个<u>多路径细化网络（RefineNet），它利用多个抽象级别的特征进行高分辨率的语义分割。RefineNet以递归的方式将低分辨率（粗糙的）语义特征与细粒度的低级特征相结合，生成高分辨率的语义特征图。我们的模型具有灵活性，可以轻松地级联和修改。</u></p> 
<p>        2.我们的级联RefineNet可以有效地进行端到端的训练，这对于最佳预测性能至关重要。具体而言，<u>RefineNet中的所有组件都使用残差连接[24]和恒等映射[25]，使得梯度可以通过短程和长程残差连接直接传播，从而实现有效和高效的端到端训练。</u></p> 
<p>         3.我们提出了一个新的网络组件，称为<u>串联残差池化，它能够从大范围的图像区域中捕获背景上下文。它通过使用多个窗口大小高效地池化特征，并通过残差连接和可学习权重将它们融合在一起。</u></p> 
<p>        4.我们提出的RefineNet在包括PASCAL VOC 2012、PASCAL-Context、NYUDv2、SUN-RGBD、Cityscapes、ADE20K和对象解析人体部位数据集在内的7个公共数据集上取得了最新的最佳性能。特别是在PASCAL VOC 2012数据集上，我们的IoU得分达到了83.4，远远超过当前最好的方法DeepLab。</p> 
<p>        为了促进未来的研究，我们发布了RefineNet的源代码和训练模型。</p> 
<p><img alt="" height="780" src="https://images2.imgbox.com/9d/75/Uz0fAPnE_o.png" width="1200"></p> 
<p style="text-align:center;">图1.我们方法在对象解析任务（左）和语义分割（右）上的示例结果。</p> 
<h3>1.1.相关工作</h3> 
<p>        在近年来，卷积神经网络（CNN）已成为语义分割最成功的方法。<span style="background-color:#ff9900;">早期的方法[18,23]是基于区域提议的方法，通过对区域提议进行分类来生成分割结果。</span><span style="background-color:#ffd900;">最近，完全卷积网络（FCNN）[36,5,10]展示了有效的特征生成和端到端训练，</span>并因此成为语义分割最受欢迎的选择。FCNN还被广泛应用于其他密集预测任务，例如深度估计[15,13,33]、图像恢复[14]和图像超分辨率[12]。这里提出的方法也基于完全卷积风格的网络。</p> 
<p>       <span style="color:#fe2c24;"> 基于FCNN的方法通常存在低分辨率预测的限制</span>。有许多提出的技术来解决这个限制，并旨在生成高分辨率的预测结果。<span style="background-color:#ff9900;">在[5]中，采用了基于空洞卷积的方法DeepLab-CRF，直接输出中等分辨率的分数图，然后利用颜色对比信息应用密集CRF方法[27]来优化边界。</span><span style="background-color:#a2e043;">CRF-RNN [47]通过实现循环层来扩展这种方法，以端到端学习密集CRF和FCNN。解卷积方法[38,2]学习解卷积层来上采样低分辨率的预测结果。</span><span style="background-color:#4da8ee;">在[34]中的深度估计方法采用超像素池化来输出高分辨率的预测结果</span>。</p> 
<p>        <span style="color:#fe2c24;">存在一些利用中间层特征进行分割的方法。</span><span style="background-color:#38d8f0;">Long等人的FCN方法[36]在中间层添加预测层，以生成多分辨率的预测分数。他们对多分辨率的分数进行平均，生成最终的预测掩码。他们的系统采用阶段性训练而非端到端训练。</span><span style="background-color:#4da8ee;">Hypercolumn方法[22]合并了来自中间层的特征，并学习密集分类层。该方法也采用阶段性训练策略而非端到端训练。</span><span style="background-color:#ffd900;">SegNet [2]和U-Net [40]都在解卷积架构中应用跳过连接，以利用中间层的特征。</span></p> 
<p>        尽管存在一些相关工作，如何有效地利用中间层特征仍然是一个开放的问题。我们提出了一种新颖的网络架构RefineNet来解决这个问题。<span style="color:#fe2c24;">RefineNet的网络架构与现有方法有所不同。它由一些特别设计的组件组成，能够通过利用低级视觉特征来改进粗糙的高级语义特征。特别是，RefineNet采用了短程和长程的残差连接，并使用恒等映射，实现了整个系统的有效端到端训练，从而帮助实现卓越的性能。全面的实证结果清楚地验证了我们的新颖网络架构在利用中间层特征方面的有效性。</span></p> 
<h2>2.背景</h2> 
<p>        在介绍我们的方法之前，我们首先详细回顾完全卷积网络（FCN）用于语义分割[36]的结构，并讨论最近的空洞卷积技术[6]，该技术专门设计用于生成高分辨率的预测结果。非常深的卷积神经网络（CNN）在目标识别问题上显示出出色的性能。具体而言，最近提出的残差网络（ResNet）[24]在早期架构上显示出了巨大的改进，而且已经公开提供了在ImageNet识别任务上预训练的ResNet模型。因此，在接下来的内容中，我们将采用ResNet作为我们进行语义分割的基本构建模块。然而，请注意，将其替换为任何其他深度网络也是简单直接的。<span style="color:#fe2c24;">由于语义分割可以视为密集分类问题，因此可以很容易地修改ResNet模型以适应这个任务。这是通过将单标签预测层替换为密集预测层来实现的，该层在每个像素点上输出每个类别的分类置信度。</span>这个方法在图2(a)中有所说明。正如可以看到的，在ResNet的前向传播过程中，特征图（层的输出）的分辨率会降低，而特征深度，即每层的特征图数目（或通道），会增加。前者是由于卷积和池化操作中的步幅造成的。</p> 
<p>        根据输出特征图的分辨率，ResNet层可以自然地分为4个块，如图2(a)所示。<span style="color:#fe2c24;">通常情况下，步幅被设置为2，因此当从一个块传递到下一个块时，特征图的分辨率减少到一半。</span><u>这种顺序的子采样有两个效果：首先，它增加了深层卷积的感受野，使得滤波器能够捕获更多的全局和上下文信息，这对于高质量的分类是至关重要的；其次，为了保持训练的高效性和可行性，这是必要的，因为每个层都包含大量的滤波器，因此产生的输出具有相应数量的通道。因此，特征图的通道数和分辨率之间存在权衡。</u><span style="color:#fe2c24;">通常情况下，最终的特征图输出在每个空间维度上比原始图像小32倍（但具有数千个通道）。这个低分辨率的特征图会丢失早期低级滤波器捕捉到的重要视觉细节，导致分割地图较粗糙。这个问题是基于深度CNN的分割方法的一个众所周知的局限性。</span></p> 
<p>        <span style="color:#fe2c24;">为了避免降低分辨率同时保持较大的感受野，一种替代方法是使用扩张（atrous）卷积。这种方法在[6]中介绍，并在语义分割中显示出最先进的性能。</span><u>子采样操作被移除（步幅从2更改为1），并且第一个块之后的所有卷积层都使用扩张卷积。这种扩张卷积实际上是一个子采样卷积核，它可以增加滤波器的感受野大小，而不增加必须学习的权重的数量（参见图2(b)中的示例）。即使如此，这种方法在内存方面有很大的成本，因为与图像子采样方法不同，我们必须在更高的分辨率下保留大量的特征图。</u><strong><span style="color:#fe2c24;">例如，如果我们保留所有通道的所有层的分辨率至少为原始图像分辨率的1/4，并且考虑到典型的滤波器通道数为1024，那么我们可以看到即使是高端GPU的内存容量也很快被非常深的网络所耗尽。因此，在实践中，扩张卷积方法通常使用深度网络时的分辨率预测不超过原始图像的1/8而不是1/4</span>。</strong>与扩张卷积方法相比，本文提出了一种方法，既能享受降低分辨率的内存和计算优势，又能产生高效且高分辨率的分割预测，如下一节所述。</p> 
<h2>3.提出的方法</h2> 
<p>        我们提出了一个新的框架，通过使用通用的构建块RefineNet，提供了多条路径，从不同分辨率和可能的长程连接中吸收信息。图2(c)展示了一种可能的构建块排列方式，以实现我们对高分辨率语义分割的目标。</p> 
<h3>3.1.多路精细化</h3> 
<p>        如前所述，我们旨在利用多级特征进行高分辨率预测，并通过长程残差连接进行信息融合。<span style="color:#fe2c24;">RefineNet提供了一种通用的方法，将粗粒度的高级语义特征与细粒度的低级特征融合，生成高分辨率的语义特征图。</span><u>设计的一个关键方面是确保梯度能够轻松地通过网络向后传播，一直到早期的低级层，通过长程残差连接，确保整个网络可以端到端地进行训练。</u><span style="background-color:#ffd900;">对于我们的标准多路径架构，我们根据特征图的分辨率将预训练的ResNet（在ImageNet上训练）分成4个块，并使用4个级联的RefineNet单元的架构，每个单元直接连接到一个ResNet块的输出以及级联中的前一个RefineNet块。然而，请注意，这样的设计并不唯一。</span>实际上，我们灵活的架构可以简单地探索不同的变体。例如，一个RefineNet块可以接受来自多个ResNet块的输入。在第4.3节中，我们将分析一个2级联版本、一个单块方法以及一个2尺度7路径架构。</p> 
<p>        我们将连接到ResNet中的第m个块的RefineNet块表示为RefineNet-m。<span style="color:#fe2c24;">在实践中，每个ResNet输出都通过一个卷积层进行维度适应。尽管所有的RefineNet都共享相同的内部结构，但它们的参数是独立的，这样可以更灵活地适应不同级别的细节。</span><u>根据图2(c)中的示意图自底向上，我们从ResNet中的最后一个块开始，将ResNet块4的输出连接到RefineNet-4。在这里，RefineNet-4只有一个输入，并且RefineNet-4作为一组额外的卷积层，将预训练的ResNet权重适应到我们的任务上，即语义分割。在下一个阶段，RefineNet-4的输出和ResNet块3的输出被作为2个路径的输入传递给RefineNet-3。RefineNet-3的目标是利用来自ResNet块3的高分辨率特征来优化RefineNet-4在前一个阶段输出的低分辨率特征图。类似地，RefineNet-2和RefineNet-1通过融合后面层的高级信息和前面层的高分辨率但低级别的特征来重复这个逐阶段的优化过程。</u><span style="color:#fe2c24;">作为最后一步，最终的高分辨率特征图被送入一个密集的softmax层，以生成密集的分数图形式的最终预测。然后，使用双线性插值将该分数图上采样以匹配原始图像的大小。请注意，整个网络可以高效地进行端到端的训练。</span></p> 
<p>        重要的是要注意，<strong>我们在ResNet块和RefineNet模块之间引入了长程残差连接。在前向传播过程中，这些长程残差连接传递了编码视觉细节的低级特征，用于优化粗糙的高级特征图。在训练步骤中，长程残差连接允许直接的梯度传播到早期的卷积层，这有助于有效的端到端训练。</strong></p> 
<p class="img-center"><img alt="" height="165" src="https://images2.imgbox.com/06/3e/LBnDR8tc_o.png" width="686"></p> 
<p style="text-align:center;"> 图2.全卷积方法进行密集分类的比较。标准的多层CNN，如ResNet (a)，在特征图的缩小过程中会丢失细节结构。扩张卷积 (b)通过引入空洞滤波器来弥补这个缺点，但是其训练计算量大，在现代GPU上很快达到内存极限。我们提出的被称为RefineNet (c)的架构利用不同阶段的卷积中的各个细节级别，并将它们融合起来，以获得高分辨率的预测，而无需维护大型的中间特征图。详见正文和图3。</p> 
<p style="text-align:center;"><img alt="" height="1022" src="https://images2.imgbox.com/6e/e6/DqgxKLGr_o.png" width="1200"></p> 
<p style="text-align:center;"></p> 
<p style="text-align:center;">图3.我们的多路径细化网络架构RefineNet的各个组件。RefineNet中的组件使用具有恒等映射的残差连接，使得梯度可以直接在网络中局部传播，并通过长程残差连接直接传递到输入路径，从而实现整个系统的有效端到端训练。</p> 
<h3>3.2.细化网络</h3> 
<p>        图3(a)展示了一个RefineNet块的结构。在图2(c)中显示的多路径概述中，<u>RefineNet-1只有一个输入路径，而所有其他的RefineNet块都有两个输入。然而，请注意，我们的架构是通用的，每个Refine块可以很容易地修改为接受任意数量的具有任意分辨率和深度的特征图</u>。</p> 
<p><strong>残差卷积单元</strong>。<u>每个RefineNet块的第一部分由一个自适应卷积集组成，主要对我们的任务微调预训练的ResNet权重。为此，每个输入路径依次通过两个残差卷积单元（RCU），这是原始ResNet [24]中卷积单元的简化版本，在其中批量归一化层被移除（参见图3(b)）。在我们的实验中，RefineNet-4的每个输入路径的滤波器数量设置为512，其余的为256。</u></p> 
<p><strong>多分辨率融合</strong>。<span style="background-color:#a2e043;">所有路径的输入通过多分辨率融合块融合成一个高分辨率特征图，如图3(c)所示。这个块首先应用卷积进行输入适应，生成具有相同特征维度（在输入中最小的维度）的特征图，然后将所有（较小的）特征图上采样到输入的最大分辨率上。最后，所有特征图通过求和进行融合。该块中的输入适应也有助于在不同路径上适当地重新缩放特征值，这对于后续的求和融合非常重要。如果只有一个输入路径（例如，图2(c)中RefineNet-4的情况），输入路径将直接通过此块而不进行任何更改。</span></p> 
<p><strong>链式残差池化</strong>。输出特征图然后通过链式残差池化块，如图3(d)所示。<span style="color:#fe2c24;">提出的链式残差池化旨在从大范围的图像区域中捕获背景上下文。</span><u>它能够高效地汇聚具有多个窗口大小的特征，并使用可学习的权重将它们融合在一起。</u><span style="background-color:#38d8f0;">具体而言，该组件是由多个池化块组成的链，每个池化块包括一个最大池化层和一个卷积层。一个池化块将前一个池化块的输出作为输入。因此，当前的池化块能够重复使用前一个池化操作的结果，从而在不使用大型池化窗口的情况下访问大范围的特征。在我们的实验中，如果没有进一步指定，我们使用两个步长为1的池化块。所有池化块的输出特征图通过残差连接与输入特征图进行融合，通过求和运算。</span>请注意，<u>我们选择使用残差连接也在这个构建块中持续存在，这再次在训练过程中促进了梯度传播。在一个池化块中，每个池化操作后面都是卷积层，用于加权求和融合。预期这个卷积层将在训练过程中学习适应池化块的重要性。</u></p> 
<p><strong>输出卷积</strong>。每个RefineNet块的最后一步是另一个残差卷积单元（RCU）。这导致每个块之间有三个RCU的序列。为了反映这种行为，在最后的RefineNet-1块中，我们在最后的softmax预测步骤之前放置了两个额外的RCU。这里的目标是对多路径融合的特征图进行非线性操作，以生成用于进一步处理或最终预测的特征。通过这个块后，特征维度保持不变。</p> 
<h3>3.3.在RefineNet中的恒等映射</h3> 
<p>        <span style="color:#fe2c24;">需要注意的是，RefineNet中的所有卷积组件都是受到残差连接背后的思想的启发，并遵循恒等映射的规则。</span><span style="background-color:#a2e043;">这使得梯度能够有效地通过RefineNet进行反向传播，并促进级联多路径细化网络的端到端学习。使用具有恒等映射的残差连接使得梯度能够直接从一个块传播到任何其他块，正如He等人最近展示的那样。这个概念鼓励保持捷径连接的清晰信息路径，以确保这些连接不会被任何非线性层或组件“阻塞”。</span>相反，<span style="color:#38d8f0;">非线性操作被放置在主要信息路径的分支上。我们遵循这个指导方针来开发RefineNet中的各个组件，包括所有的卷积单元。正是这个特定的策略使得多级联的RefineNet能够有效地进行训练。需要注意的是，我们在链式残差池化块中包含了一个非线性激活层（ReLU）。我们观察到这个ReLU对于后续的池化操作的有效性很重要，它还使得模型对学习率的变化不太敏感。我们观察到，在每个RefineNet块中只有一个ReLU并不会显著降低梯度流的有效性。</span></p> 
<p>        在RefineNet中，<strong>我们同时使用了短程和长程的残差连接。短程残差连接指的是在一个RCU或残差池化组件中的局部快捷连接，而长程残差连接指的是RefineNet模块和ResNet块之间的连接。通过长程残差连接，梯度可以直接传播到ResNet的早期卷积层，从而实现了所有网络组件的端到端训练。</strong></p> 
<p>        <u>融合模块将多个快捷路径的信息进行融合，可以将其视为对多个残差连接进行求和融合，并进行必要的维度或分辨率适应。在这个方面，多分辨率融合模块的作用类似于传统ResNet中的“求和”融合。在RefineNet中有一些层，特别是在融合模块中，执行线性特征转换操作，如线性特征降维或双线性上采样。这些层被放置在快捷路径上，与ResNet中的情况类似。在ResNet中，当一个快捷连接穿过两个块时，它会在快捷路径中包含一个卷积层进行线性特征维度的适应，确保特征维度与下一个块中的求和匹配。由于这些层只使用线性变换，梯度仍然可以有效地通过这些层传播。</u></p> 
<p style="text-align:center;">表1.在Person-Part数据集上的对象解析结果。我们的方法取得了最佳性能（加粗显示）。</p> 
<p style="text-align:center;"><img alt="" height="247" src="https://images2.imgbox.com/4e/f8/jYDeBZIX_o.png" width="1050"></p> 
<p></p> 
<p style="text-align:center;">表2.在NYUDv2和Person-Part上的消融实验。初始化、链式池化和多尺度评估的结果如下：</p> 
<p style="text-align:center;"><img alt="" height="302" src="https://images2.imgbox.com/06/89/cUwp1TEc_o.png" width="1060"></p> 
<p></p> 
<p style="text-align:center;">表3.在NYUDv2（40类）上的分割结果。 请注意，由于缺乏具体的结果数据，此处无法提供具体的数值。</p> 
<p style="text-align:center;"><img alt="" height="265" src="https://images2.imgbox.com/d2/2b/22hBdaso_o.png" width="1066"></p> 
<p></p> 
<h2>4.实验</h2> 
<p>        <span style="color:#fe2c24;">为了展示我们方法的有效性，我们在七个公共数据集上进行了全面的实验，其中包括六个用于室内和室外场景的语义分割的常用数据集，以及一个用于对象解析的数据集称为Person-Part。分割质量通过交并比（IoU）得分[16]、像素准确度和平均准确度[36]来衡量所有类别。</span><u>与文献中通常做的一样，我们在训练过程中应用简单的数据增强。具体来说，我们对图像进行随机缩放（范围从0.7到1.3）、随机裁剪和水平翻转。</u><span style="background-color:#ffd900;">如果没有进一步说明，我们会应用测试时的多尺度评估，这是分割方法中的常见做法[10,6]。对于多尺度评估，我们在不同尺度上对同一图像的预测进行平均，得到最终的预测结果。我们还进行了消融实验，以检查模型的各个组件和几个变体对结果的影响。我们的系统是基于MatConvNet [44]构建的。</span></p> 
<p><img alt="" height="790" src="https://images2.imgbox.com/90/63/fIpx2kyl_o.png" width="1200"></p> 
<p style="text-align:center;"> 图4.我们在Person-Parts数据集上的预测示例。</p> 
<h3>4.1.对象解析</h3> 
<p>        首先，我们展示了在对象解析任务上的结果，该任务包括识别和分割对象的部分。我们在Person-Part数据集[8,7]上进行实验，该数据集为六个人体部分（包括头部、躯干、上/下臂和上/下腿）提供了像素级标签。图像的其余部分被视为背景。数据集包含1717个训练图像和1818个测试图像。我们对该数据集使用了4个连接的池化块。我们将结果与一些最先进的方法进行了比较，这些方法列在表1中。结果清楚地表明了与先前工作相比的改进。特别是，我们在相同的ResNet初始化下，显著优于最近的基于扩张卷积用于高分辨率分割的DeepLab-v2方法[6]。在表2中，<span style="color:#fe2c24;">我们进行了一项消融实验，以量化以下因素的影响：网络深度、连接的残差池化和多尺度评估（Msc Eva），如前所述。该实验表明，这三个因素都可以提高整体性能。</span>图4展示了我们在该数据集上进行对象解析的定性示例。</p> 
<h3>4.2.语义分割</h3> 
<p>        我们现在描述了我们在六个公共基准上进行的密集语义标记实验，并展示了我们的RefineNet在所有数据集上优于先前的方法。</p> 
<p><strong>NYUDv2</strong>. NYUDv2数据集[41]包含1449张显示室内场景的RGB-D图像。我们使用[19]中提供的分割标签，其中所有标签都映射到40个类别。我们使用标准的训练/测试划分，分别使用795张和654张图像进行训练和测试。我们仅使用RGB图像训练模型，不使用深度信息。定量结果如表3所示。我们的RefineNet在NYUDv2上取得了最先进的结果。与上述对象解析任务类似，我们还在NYUDv2数据集上进行了消融实验，以评估不同设置的影响。结果如表2所示。再次，这项研究证明了添加所提出的连接残差池化组件和更深网络的好处，两者在IoU度量下都能持续改善性能。</p> 
<p><strong>PASCAL VOC 2012</strong> [16]是一个众所周知的分割数据集，包括20个物体类别和一个背景类别。该数据集被划分为训练集、验证集和测试集，分别包含1464、1449和1456张图像。由于测试集标签不公开，所有报告的结果都来自于VOC评估服务器。按照常规做法[5,6,47,35]，训练集使用[21]中提供的额外注释的VOC图像以及来自MS COCO数据集[31]的训练数据进行增强。我们将我们的RefineNet与PASCAL VOC 2012测试集上的一些竞争方法进行比较，展示了优越的性能。我们在这个数据集上使用了4个池化块。我们还使用了稠密CRF [27]对这个数据集进行进一步的细化，这在验证集上产生了0.1%的微小改善。由于这种稠密CRF后处理只对我们的高分辨率预测带来非常小的改进，我们没有在其他数据集上应用它。每个类别的详细结果和平均IoU分数如表4所示。我们取得了83.4的IoU得分，这是迄今为止在这个具有挑战性的数据集上报告的最好结果。我们在几乎所有类别中都表现优于竞争方法。特别是，我们明显优于DeepLab-v2 [6]，其使用与我们初始化相同的ResNet-101网络，差距接近3个百分点。选择的预测示例如图5所示。</p> 
<p><strong>Cityscapes</strong> [9]是一个关于来自50个不同欧洲城市的街景图像的最新数据集。该数据集提供了对道路、汽车、行人、自行车、天空等的细粒度像素级注释。提供的训练集有2975张图像，验证集有500张图像。总共有19个类别用于训练和评估。测试集的真实标签由组织者保留，我们在他们的评估服务器上评估我们的方法。测试结果如表5所示。在这个具有挑战性的环境中，我们的架构再次超越了先前的方法。图6展示了一些测试图像及其真实语义地图和我们预测的语义地图。</p> 
<p><strong>PASCAL-Context</strong>. PASCAL-Context [37]数据集为PASCAL VOC图像提供了整个场景的分割标签。我们使用包含60个类别（59个物体类别加背景）的分割标签进行评估，同时使用提供的训练/测试划分。训练集包含4998张图像，测试集包含5105张图像。结果如表6所示。即使没有额外的训练数据，并且使用与DeepLab相同的底层ResNet架构（101层），我们的表现优于先前由DeepLab达到的最先进水平。</p> 
<p><strong>SUN-RGBD</strong> [43]是一个分割数据集，包含大约10,000张室内RGB-D图像，并为37个类别提供像素级标注掩码。结果如表7所示。尽管我们在训练中没有使用深度信息，但我们的方法在所有评估指标上都明显优于所有现有方法。</p> 
<p><strong>ADE20K MIT</strong> [48]是一个新发布的用于场景解析的数据集，提供了超过20K个场景图像的150个类别的稠密标签。这些类别包括各种对象（如人、车等）和物质（如天空、道路等）。提供的包含2000张图像的验证集用于定量评估。结果如表8所示。我们的方法明显优于[48]中描述的基线方法。</p> 
<p style="text-align:center;">表4.在PASCAL VOC 2012测试集上的结果（IoU得分）</p> 
<p style="text-align:center;"><img alt="" height="414" src="https://images2.imgbox.com/bc/b1/67paYRR5_o.png" width="1200"></p> 
<p style="text-align:center;"> <img alt="" height="777" src="https://images2.imgbox.com/00/f2/eeGbF4OO_o.png" width="1200"></p> 
<p style="text-align:center;"> 图5.PASCAL VOC 2012上的分割示例</p> 
<p style="text-align:center;">表5.在Cityscapes测试集上的分割结果</p> 
<p style="text-align:center;"><img alt="" height="356" src="https://images2.imgbox.com/87/53/IOmEYN9I_o.png" width="1200"></p> 
<p style="text-align:center;"><img alt="" height="867" src="https://images2.imgbox.com/a6/95/SPZfAJCU_o.png" width="1200"></p> 
<p style="text-align:center;"> 图6.我们在Cityscapes数据集上的预测示例</p> 
<p style="text-align:center;">表6.在PASCAL-Context数据集（60个类别）上的分割结果。DeepLab-v2使用来自COCO数据集的额外训练数据（约100K张图像），而我们只使用了VOC训练图像。</p> 
<p style="text-align:center;"><img alt="" height="350" src="https://images2.imgbox.com/c2/64/Dzsj7m3J_o.png" width="1200"></p> 
<p style="text-align:center;"> 表7.在SUN-RGBD数据集（37个类别）上的分割结果。我们的RefineNet明显优于现有的方法。<img alt="" height="408" src="https://images2.imgbox.com/f0/5b/ZfmfodQ1_o.png" width="1200"></p> 
<p style="text-align:center;"> 表8.在ADE20K数据集（150个类别）验证集上的分割结果。我们的方法取得了最佳的性能。<img alt="" height="347" src="https://images2.imgbox.com/92/32/1MUPCAFH_o.png" width="1200"></p> 
<p style="text-align:center;"> 表9.在NYUDv2数据集上对4种级联RefineNet的评估结果<img alt="" height="364" src="https://images2.imgbox.com/05/02/vq6Pnytg_o.png" width="1200"></p> 
<h3> 4.3.级联RefineNet的变体</h3> 
<p>        如前所述，我们的RefineNet非常灵活，可以以多种方式级联生成不同的架构。在这里，我们讨论了我们RefineNet的几种变体，如表9所列。<u>单一的RefineNet模型仅由一个RefineNet块组成，它从ResNet的四个块中获取所有四个输入，并在一个过程中融合所有分辨率的特征图。2级联版本与我们主要模型（4级联）类似于图2(c)，但只使用了两个RefineNet模块而不是四个。对于2尺度模型，输入图像被缩放为1.2倍和0.6倍，并分别输入到两个独立的4级联ResNet中。这些变体的详细架构在补充文档中有描述。表9中的评估结果表明，4级联版本比2级联和1级联版本具有更好的性能，并且使用2尺度的图像输入和2个ResNet比使用1尺度输入更好。这是由于网络的更大容量所致。然而，它也导致训练时间更长。因此，我们在所有实验中都采用了单尺度4级联版本作为标准架构。</u></p> 
<h2>5.结论</h2> 
<p>        我们提出了RefineNet，一种用于语义分割和物体解析的新型多路径细化网络。级联的架构能够有效地结合高级语义和低级特征，生成高分辨率的分割图。我们的设计选择受到了恒等映射的启发，这有助于在长距离连接上进行梯度传播，从而实现有效的端到端学习。实验证明，我们的方法在语义标记的最新技术水平上取得了新的突破。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2d80f60dca32284c0257e556d39753f6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">JessIbuca 插件报错 window.Jessibuca is not a constructor</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/91c6a40befb6b12779b8620e37770e03/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【Java多线程】【锁策略】</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>