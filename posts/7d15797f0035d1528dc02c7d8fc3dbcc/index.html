<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>推荐系统与知识图谱(2) - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="推荐系统与知识图谱(2)" />
<meta property="og:description" content="推荐系统与知识图谱(2) Ref：如何将知识图谱特征学习应用到推荐系统？
将知识图谱作为辅助信息引入到推荐系统中可以有效地解决传统推荐系统存在的稀疏性和冷启动问题，近几年有很多研究人员在做相关的工作。目前，将知识图谱特征学习应用到推荐系统中主要通过三种方式——依次学习、联合学习、以及交替学习。
依次学习（one-by-one learning）。首先使用知识图谱特征学习得到实体向量和关系向量，然后将这些低维向量引入推荐系统，学习得到用户向量和物品向量；
联合学习（joint learning）。将知识图谱特征学习和推荐算法的目标函数结合，使用端到端（end-to-end）的方法进行联合学习； 交替学习（alternate learning）。将知识图谱特征学习和推荐算法视为两个分离但又相关的任务，使用多任务学习（multi-task learning）的框架进行交替学习。 依次学习 Deep Knowledge-Aware Network (DKN) 我们以新闻推荐[1]为例来介绍依次学习。如下图所示，新闻标题和正文中通常存在大量的实体，实体间的语义关系可以有效地扩展用户兴趣。然而这种语义关系难以被传统方法（话题模型、词向量）发掘。
为了将知识图谱引入特征学习，遵循依次学习的框架，我们首先需要提取知识图谱特征。该步骤的方法如下：
实体连接（entity linking）。即从文本中发现相关词汇，并与知识图谱中的实体进行匹配；
知识图谱构建。根据所有匹配到的实体，在原始的知识图谱中抽取子图。子图的大小会影响后续算法的运行时间和效果：越大的子图通常会学习到更好的特征，但是所需的运行时间越长；
知识图谱特征学习。使用知识图谱特征学习算法（如TransE等）进行学习得到实体和关系向量。
需要注意的是，为了更准确地刻画实体，我们额外地使用一个实体的上下文实体特征（contextual entity embeddings）。一个实体e的上下文实体是e的所有一跳邻居节点，e的上下文实体特征为e的所有上下文实体特征的平均值：
下图的绿色椭圆框内即为“Fight Club”的上下文实体。
得到实体特征后，我们的第二步是构建推荐模型，该模型是一个基于CNN和注意力机制的新闻推荐算法：
基于卷积神经网络的文本特征提取：将新闻标题的词向量（word embedding）、实体向量（entity embedding）和实体上下文向量（context embedding）作为多个通道（类似于图像中的红绿蓝三通道），在CNN的框架下进行融合；
基于注意力机制的用户历史兴趣融合：在判断用户对当前新闻的兴趣时，使用注意力网络（attention network）给用户历史记录分配不同的权重。
该模型在新闻推荐上取得了很好的效果：DKN取得了0.689的F1值和0.659的AUC值，并在p=0.1水平上比其它方法取得了显著的提升。
我们也可以通过注意力权重的可视化结果看出，注意力机制的引入对模型的最后输出产生了积极的影响。由于注意力机制的引入，DKN可以更好地将同类别的新闻联系起来，从而提高了最终的正确预测的数量：
依次学习的优势在于知识图谱特征学习模块和推荐系统模块相互独立。在真实场景中，特别是知识图谱很大的情况下，进行一次知识图谱特征学习的时间开销会很大，而一般而言，知识图谱远没有推荐模块更新地快。因此我们可以先通过一次训练得到实体和关系向量，以后每次推荐系统模块需要更新时都可以直接使用这些向量作为输入，而无需重新训练。
依次学习的缺点也正在于此：因为两个模块相互独立，所以无法做到端到端的训练。通常来说，知识图谱特征学习得到的向量会更适合于知识图谱内的任务，比如连接预测、实体分类等，并非完全适合特定的推荐任务。在缺乏推荐模块的监督信号的情况下，学习得到的实体向量是否真的对推荐任务有帮助，还需要通过进一步的实验来推断。
联合学习 联合学习的核心是将推荐算法和知识图谱特征学习的目标融合，并在一个端到端的优化目标中进行训练。我们以CKE[2]和Ripple Network[3]为例介绍联合学习。
Collaborative Knowledge base Embedding (CKE) 在推荐系统中存在着很多与知识图谱相关的信息，以电影推荐为例：
结构化知识（structural knowledge），例如导演、类别等；
图像知识（visual knowledge），例如海报、剧照等；
文本知识（textual knowledge），例如电影描述、影评等。 CKE是一个基于协同过滤和知识图谱特征学习的推荐系统：
CKE使用如下方式进行三种知识的学习：
结构化知识学习：TransR。TransR是一种基于距离的翻译模型，可以学习得到知识实体的向量表示；
文本知识学习：去噪自编码器。去噪自编码器可以学习得到文本的一种泛化能力较强的向量表示； 图像知识学习：卷积-反卷积自编码器。卷积-反卷积自编码器可以得到图像的一种泛化能力较强的向量表示。 我们将三种知识学习的目标函数与推荐系统中的协同过滤结合，得到如下的联合损失函数：
使用诸如随机梯度下降（SGD）的方法对上述损失函数进行训练，我们最终可以得到用户/物品向量，以及实体/关系向量。CKE在电影推荐和图书推荐上取得了很高的Recall值和MAP值：
Ripple Network Ripple的中文翻译为“水波”，顾名思义，Ripple Network模拟了用户兴趣在知识图谱上的传播过程，整个过程类似于水波的传播：
一个用户的兴趣以其历史记录中的实体为中心，在知识图谱上向外逐层扩散；
一个用户的兴趣在知识图谱上的扩散过程中逐渐衰减。
下图展示了用户兴趣在知识图谱上扩散的过程。以一个用户看过的“Forrest Gump”为中心，用户的兴趣沿着关系边可以逐跳向外扩展，并在扩展过程中兴趣强度逐渐衰减。
下图展示了Ripple Network的模型。对于给定的用户u和物品v，我们将历史相关实体集合V中的所有实体进行相似度计算，并利用计算得到的权重值对V中实体在知识图谱中对应的尾节点进行加权求和。求和得到的结果可以视为v在u的一跳相关实体中的一个响应。该过程可以重复在u的二跳、三跳相关实体中进行，如此，v在知识图谱上便以V为中心逐层向外扩散。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/7d15797f0035d1528dc02c7d8fc3dbcc/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-01-04T16:44:32+08:00" />
<meta property="article:modified_time" content="2021-01-04T16:44:32+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">推荐系统与知识图谱(2)</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2><a href="https://www.cnblogs.com/niuxichuan/p/9318122.html" rel="nofollow" id="cb_post_title_url">推荐系统与知识图谱(2)</a></h2> 
<p>Ref：<a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649443804&amp;idx=1&amp;sn=79be363f8b08a9b591cadb3f8dd73904&amp;chksm=82c0a458b5b72d4e21d5edc7359d99ccde3c92d967825a6cbb4c354af51b0c28a25be682251b&amp;scene=21#wechat_redirect" rel="nofollow">如何将知识图谱特征学习应用到推荐系统？</a></p> 
<p>将知识图谱作为辅助信息引入到推荐系统中可以有效地解决传统推荐系统存在的稀疏性和冷启动问题，近几年有很多研究人员在做相关的工作。目前，<strong>将知识图谱特征学习应用到推荐系统中主要通过三种方式——依次学习、联合学习、以及交替学习</strong>。</p> 
<ul><li> <p><strong>依次学习</strong>（one-by-one learning）。首先使用知识图谱特征学习得到实体向量和关系向量，然后将这些低维向量引入推荐系统，学习得到用户向量和物品向量；</p> </li></ul> 
<p><img alt="" src="https://images2.imgbox.com/bc/b1/ohCpIrky_o.png"></p> 
<ul><li><strong>联合学习</strong>（joint learning）。将知识图谱特征学习和推荐算法的目标函数结合，使用端到端（end-to-end）的方法进行联合学习；</li></ul> 
<p><img alt="" src="https://images2.imgbox.com/b0/d0/27Tbfo8u_o.png"></p> 
<ul><li><strong>交替学习</strong>（alternate learning）。将知识图谱特征学习和推荐算法视为两个分离但又相关的任务，使用多任务学习（multi-task learning）的框架进行交替学习。</li></ul> 
<p><img alt="" src="https://images2.imgbox.com/cd/12/hclKK1Px_o.png"></p> 
<h3><strong><strong><strong><strong><strong><strong>依次学习</strong></strong></strong></strong></strong></strong></h3> 
<h4><strong>Deep Knowledge-Aware Network (DKN)</strong></h4> 
<p>我们以新闻推荐[1]为例来介绍依次学习。如下图所示，新闻标题和正文中通常存在大量的实体，实体间的语义关系可以有效地扩展用户兴趣。然而这种语义关系难以被传统方法（话题模型、词向量）发掘。</p> 
<p><img alt="" src="https://images2.imgbox.com/cc/9d/EuGBBpnm_o.png"></p> 
<p>为了将知识图谱引入特征学习，遵循依次学习的框架，我们<strong>首先需要</strong><strong>提取知识图谱特征</strong>。该步骤的方法如下：</p> 
<p> </p> 
<ul><li> <p><strong>实体连接</strong>（entity linking）。即从文本中发现相关词汇，并与知识图谱中的实体进行匹配；</p> </li></ul> 
<ul><li> <p><strong>知识图谱构建</strong>。根据所有匹配到的实体，在原始的知识图谱中抽取子图。子图的大小会影响后续算法的运行时间和效果：越大的子图通常会学习到更好的特征，但是所需的运行时间越长；</p> </li></ul> 
<ul><li> <p><strong>知识图谱特征学习</strong>。使用知识图谱特征学习算法（如TransE等）进行学习得到实体和关系向量。</p> </li></ul> 
<p><img alt="" src="https://images2.imgbox.com/3d/d2/DEXfFYy7_o.png"></p> 
<p>需要注意的是，<strong>为了更准确地刻画实体，我们额外地使用一个实体的上下文实体特征（contextual entity embeddings）</strong>。一个实体e的上下文实体是e的所有一跳邻居节点，e的上下文实体特征为e的所有上下文实体特征的平均值：</p> 
<p><img alt="" src="https://images2.imgbox.com/6d/e5/KVDaVB3W_o.png"></p> 
<p>下图的绿色椭圆框内即为“Fight Club”的上下文实体。</p> 
<p> <img alt="" src="https://images2.imgbox.com/ec/ed/gv150ljC_o.png"></p> 
<p>得到实体特征后，我们的<strong>第二步是构建推荐模型</strong>，该模型是一个基于CNN和注意力机制的新闻推荐算法：</p> 
<p> </p> 
<ul><li> <p><strong>基于卷积神经网络的文本特征提取</strong>：将新闻标题的词向量（word embedding）、实体向量（entity embedding）和实体上下文向量（context embedding）作为多个通道（类似于图像中的红绿蓝三通道），在CNN的框架下进行融合；</p> </li></ul> 
<ul><li> <p><strong>基于注意力机制的用户历史兴趣融合</strong>：在判断用户对当前新闻的兴趣时，使用注意力网络（attention network）给用户历史记录分配不同的权重。</p> </li></ul> 
<p><img alt="" src="https://images2.imgbox.com/e5/f9/h3DbbJzq_o.png"></p> 
<p>该模型在新闻推荐上取得了很好的效果：DKN取得了0.689的F1值和0.659的AUC值，并在p=0.1水平上比其它方法取得了显著的提升。</p> 
<p><img alt="" src="https://images2.imgbox.com/64/03/W2amVDxI_o.png"></p> 
<p>我们也可以通过注意力权重的可视化结果看出，注意力机制的引入对模型的最后输出产生了积极的影响。由于注意力机制的引入，DKN可以更好地将同类别的新闻联系起来，从而提高了最终的正确预测的数量：</p> 
<p><img alt="" src="https://images2.imgbox.com/03/76/Q6zWmr33_o.png"></p> 
<p><strong>依次学习的优势在于知识图谱特征学习模块和推荐系统模块相互独立</strong>。在真实场景中，特别是知识图谱很大的情况下，进行一次知识图谱特征学习的时间开销会很大，而一般而言，知识图谱远没有推荐模块更新地快。因此我们可以先通过一次训练得到实体和关系向量，以后每次推荐系统模块需要更新时都可以直接使用这些向量作为输入，而无需重新训练。</p> 
<p><strong> </strong></p> 
<p><strong>依次学习的缺点</strong>也正在于此：因为两个模块相互独立，所以<strong>无法做到端到端的训练</strong>。通常来说，知识图谱特征学习得到的向量会更适合于知识图谱内的任务，比如连接预测、实体分类等，并非完全适合特定的推荐任务。在缺乏推荐模块的监督信号的情况下，学习得到的实体向量是否真的对推荐任务有帮助，还需要通过进一步的实验来推断。</p> 
<h3><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>联合学习</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></h3> 
<p>联合学习的核心是将推荐算法和知识图谱特征学习的目标融合，并在一个端到端的优化目标中进行训练。我们以CKE[2]和Ripple Network[3]为例介绍联合学习。</p> 
<h4><strong>Collaborative Knowledge base Embedding (CKE)</strong></h4> 
<p>在推荐系统中存在着很多与知识图谱相关的信息，以电影推荐为例：</p> 
<ul><li> <p><strong>结构化知识</strong>（structural knowledge），例如导演、类别等；</p> </li><li> <p><strong>图像知识</strong>（visual knowledge），例如海报、剧照等；</p> </li><li> <p><strong>文本知识</strong>（textual knowledge），例如电影描述、影评等。 </p> </li></ul> 
<p>CKE是一个基于协同过滤和知识图谱特征学习的推荐系统：</p> 
<p><img alt="" src="https://images2.imgbox.com/18/2d/LzzvEn2w_o.png"></p> 
<p>CKE使用如下方式进行三种知识的学习：</p> 
<ul><li> <p><strong>结构化知识学习：TransR</strong>。TransR是一种基于距离的翻译模型，可以学习得到知识实体的向量表示；</p> </li></ul> 
<p><img alt="" src="https://images2.imgbox.com/ca/37/FVoINGm9_o.png"></p> 
<ul><li><strong>文本知识学习：去噪自编码器</strong>。去噪自编码器可以学习得到文本的一种泛化能力较强的向量表示；</li></ul> 
<p>　　<img alt="" src="https://images2.imgbox.com/c4/18/JfKojd08_o.png"></p> 
<ul><li><strong>图像知识学习：卷积-</strong><strong>反卷积自编码器</strong>。卷积-反卷积自编码器可以得到图像的一种泛化能力较强的向量表示。</li></ul> 
<p><img alt="" src="https://images2.imgbox.com/73/08/NygwsmTO_o.png"></p> 
<p>我们将三种知识学习的目标函数与推荐系统中的协同过滤结合，得到如下的联合损失函数：</p> 
<p><img alt="" src="https://images2.imgbox.com/7f/c7/HlyPl01N_o.png"></p> 
<p>使用诸如随机梯度下降（SGD）的方法对上述损失函数进行训练，我们最终可以得到用户/物品向量，以及实体/关系向量。CKE在电影推荐和图书推荐上取得了很高的Recall值和MAP值：</p> 
<p><img alt="" src="https://images2.imgbox.com/fa/f0/RXRMJCts_o.png"></p> 
<p><img alt="" src="https://images2.imgbox.com/8b/bc/9p2phIGA_o.png"></p> 
<h4><strong>Ripple Network</strong></h4> 
<p> </p> 
<p>Ripple的中文翻译为“水波”，顾名思义，<strong>Ripple Network模拟了用户兴趣在知识图谱上的传播过程，整个过程类似于水波的传播</strong>：</p> 
<ul><li> <p>一个用户的兴趣以其历史记录中的实体为中心，在知识图谱上<strong>向外逐层扩散</strong>；</p> </li></ul> 
<ul><li> <p>一个用户的兴趣在知识图谱上的扩散过程中<strong>逐渐衰减</strong>。</p> </li></ul> 
<p>下图展示了用户兴趣在知识图谱上扩散的过程。以一个用户看过的“Forrest Gump”为中心，用户的兴趣沿着关系边可以逐跳向外扩展，并在扩展过程中兴趣强度逐渐衰减。</p> 
<p><img alt="" src="https://images2.imgbox.com/cf/a4/jMdCWA2y_o.png"></p> 
<p>下图展示了Ripple Network的模型。对于给定的用户u和物品v，我们将历史相关实体集合V中的所有实体进行相似度计算，并利用计算得到的权重值对V中实体在知识图谱中对应的尾节点进行加权求和。求和得到的结果可以视为v在u的一跳相关实体中的一个响应。该过程可以重复在u的二跳、三跳相关实体中进行，如此，v在知识图谱上便以V为中心逐层向外扩散。</p> 
<p><img alt="" src="https://images2.imgbox.com/47/38/QXqJzeLl_o.png"></p> 
<p>最终得到的推荐算法和知识图谱特征学习的联合损失函数如下：</p> 
<p> <img alt="" src="https://images2.imgbox.com/0d/ec/7pZrhOd1_o.png"></p> 
<p>类似于CKE，我们在该损失函数上训练即可得到物品向量和实体向量。需要注意的是，Ripple Network中没有对用户直接使用向量进行刻画，而是用用户点击过的物品的向量集合作为其特征。Ripple Network在电影、图书和新闻的点击率预测上取得了非常好的效果：</p> 
<p> <img alt="" src="https://images2.imgbox.com/83/73/7M0QjYC6_o.png"></p> 
<p>我们将Ripple Network的计算结果可视化如下。可以看出，知识图谱连接了用户的历史兴趣和推荐结果，其中的若干条高分值的路径可以视为对推荐结果的解释：</p> 
<p><img alt="" src="https://images2.imgbox.com/85/e7/WwXwtKXF_o.png"></p> 
<p>联合学习的优劣势正好与依次学习相反。<strong>联合学习是一种端到端的训练方式</strong>，推荐系统模块的监督信号可以反馈到知识图谱特征学习中，这对于提高最终的性能是有利的。但是需要注意的是，两个模块在最终的目标函数中结合方式以及权重的分配都需要精细的实验才能确定。<strong>联合学习潜在的问题是训练开销较大，特别是一些使用到图算法的模型</strong>。</p> 
<h3><strong><strong>交替学习</strong></strong></h3> 
<h4><strong>Multi-task Learning for KG enhanced Recommendation (MKR)</strong></h4> 
<p> </p> 
<p>推荐系统和知识图谱特征学习的交替学习类似于多任务学习的框架。该方法的出发点是推荐系统中的物品和知识图谱中的实体存在重合，因此两个任务之间存在相关性。将推荐系统和知识图谱特征学习视为两个分离但是相关的任务，采用多任务学习的框架，可以有如下优势：</p> 
<ul><li> <p>两者的可用信息可以互补；</p> </li><li> <p>知识图谱特征学习任务可以帮助推荐系统摆脱局部极小值；</p> </li><li> <p>知识图谱特征学习任务可以防止推荐系统过拟合；</p> </li><li> <p>知识图谱特征学习任务可以提高推荐系统的泛化能力。</p> </li></ul> 
<p>MKR[4]的模型框架如下，其中左侧是推荐任务，右侧是知识图谱特征学习任务。推荐部分使用用户和物品的特征表示作为输入，预测的点击概率作为输出。知识图谱特征学习部分使用一个三元组的头结点和关系表示作为输入，预测的尾节点表示作为输出。</p> 
<p><img alt="" src="https://images2.imgbox.com/6b/eb/zlab8srB_o.png"></p> 
<p>由于推荐系统中的物品和知识图谱中的实体存在重合，所以两个任务并非相互独立。我们在两个任务中设计了交叉特征共享单元（cross-feature-sharing units）作为两者的连接纽带。</p> 
<p> </p> 
<p>交叉特征共享单元是一个可以让两个任务交换信息的模块。由于物品向量和实体向量实际上是对同一个对象的两种描述，他们之间的信息交叉共享可以让两者都获得来自对方的额外信息，从而弥补了自身的信息稀疏性的不足。</p> 
<p><img alt="" src="https://images2.imgbox.com/84/b2/pbXjh9nm_o.png"></p> 
<p>MKR的整体损失函数如下：</p> 
<p><img alt="" src="https://images2.imgbox.com/c1/fe/ZqcKOIHG_o.png"></p> 
<p>在实际操作中，我们采用<strong>交替训练</strong>的方式：固定推荐系统模块的参数，训练知识图谱特征学习模块的参数；然后固定知识图谱特征学习模块的参数，训练推荐系统模块的参数：</p> 
<p><img alt="" src="https://images2.imgbox.com/18/29/3fsfX8Za_o.png"></p> 
<p>MKR在电影、图书和新闻推荐上也取得了不错的效果，其F1@K指标在绝大多数情况下都超过了baseline方法：</p> 
<p> <img alt="" src="https://images2.imgbox.com/73/7f/z8GKRO95_o.png"></p> 
<p><strong>交替学习是一种较为创新和前沿的思路</strong>，其中如何设计两个相关的任务以及两个任务如何关联起来都是值得研究的方向。从实际运用和时间开销上来说，<strong>交替学习是介于依次学习和联合学习中间的</strong>：训练好的知识图谱特征学习模块可以在下一次训练的时候继续使用（不像联合学习需要从零开始），但是依然要参与到训练过程中来（不像依次学习中可以直接使用实体向量）。</p> 
<p> </p> 
<p>知识图谱作为推荐系统的一种新兴的辅助信息，近年来得到了研究人员的广泛关注。未来，<strong>知识图谱和时序模型的结合</strong>、<strong>知识图谱和基于强化学习的推荐系统的结合</strong>、以及<strong>知识图谱和其它辅助信息在推荐系统中的结合</strong>等相关问题仍然值得更多的研究。欢迎感兴趣的同学通过留言与我们互动沟通。</p> 
<p> </p> 
<p> </p> 
<p><strong>参考文献</strong></p> 
<p>[1] DKN: Deep Knowledge-Aware Network for News Recommendation.</p> 
<p>[2] Collaborative knowledge base embedding for recommender systems.</p> 
<p>[3] Ripple Network: Propagating User Preferences on the Knowledge Graph for Recommender Systems.</p> 
<p>[4] MKR: A Multi-Task Learning Approach for Knowledge Graph Enhanced Recommendation.</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f93bec8442e7c42b8a8304de258aa69c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Selenium新开一个窗口</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d95a99b17b6e9becbb3a06dc2bd346e3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">SpringBoot启动完成前后执行某个方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>