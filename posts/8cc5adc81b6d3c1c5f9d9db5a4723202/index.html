<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AI换脸项目faceswap研究报告 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="AI换脸项目faceswap研究报告" />
<meta property="og:description" content="缘起
deepfakes是利用AI技术换脸的开源项目，目前基于deepfakes的开源项目很多，而faceswap认可度很高，到目前为止有28.5千Star，可以说是换脸这类项目最火的了。小弟在当下有换脸需求，选取了这个项目进行研究尝试，将自己的经验和体会记录下来，希望对你有所帮助！
感谢公司、领导、一切关心支持我的人！
效果
先上一些效果，增加一些趣味性！
官方换脸效果1
官方效果2，小扎在国会舌战群儒的场景
第一次尝试换脸，演讲者左右换姿势，似乎是为不冷落两边的观众
另外一个换脸后的效果
尝试了很多此换脸，不在此一一列举。
&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;
准备
需要一块好的nvidia显卡，最低是1080或以上，众所周知，机器学习是费时的，一块好的显卡可以帮助你节省大量的时间，我用的机器配置了4块TITAN xp显卡，这个项目训练时大部分小弟用到其中两张显卡，在训练villain模型时，用到了其中三张，villain很吃显存，基本上一天可以训练出一个比较清晰的模型。
如果只有CPU，我建议还是放弃尝试，据说训练时间是以周为单位的，训练出一个结果，可能需要数周，那就需要极大的耐性。训练中你机器CPU100%的被烧数周，不能干其他事情。而训练一个模型可能并不让人满意，每次调整参数，又的继续等待数周。大概你周游完欧洲各国后，回家一看结果还没出来！
如果在linux上运行程序，需要一些ffmpeg知识、一些shell脚本经验
该项目在windows上可以直接运行图形，我只有ssh命令行连接的ubuntu服务器，所以自己写了写脚本来处理，很羡慕GUI版本
后来突然领悟可以在我的mac笔记本上运行图形界面，用来查看配置选项。
图形界面是这样：
能运行图形界面操作起来更方便，但是如果像我一样在服务器上通过ssh登陆，也无妨。
在本地调整好参数，同步到服务器上就行啦。
安装好CUDA 10.0 和 anaconda，这个到处都是教程，在此不累叙。
安装faceswap
下载项目https://github.com/deepfakes/faceswap
需要注意的是这个项目经常更新，有时可能不稳定，所以我clone了一份在自己的github里https://github.com/hnjiakai/faceswap ，幸运的是我用clone的这个版本，训练和转换等整个过程都很顺利，没有出现论坛里讨论的各种奇奇怪怪的问题。
1、利用conda创建好运行faceswap的虚拟环境
conda create -n face tensorflow=1.14 python=3.6
2、切换到虚拟环境
conda activate face 3、安装依赖环境
pip -r requirements.txt
conda install opencv 这些其实已经完整99%的安装，剩下1%用到时提示缺少可以用conda install xxx安装。详细的安装过程可以参照https://github.com/deepfakes/faceswap/blob/master/INSTALL.md
人脸资源准备
A人脸，以后简称A脸，从需要变脸的视频中得到，我用ffmpeg将视频A转换成图片到A-src文件夹，然后利用faceswap抽取人脸到A-desc文件夹为后续训练做准备。
B人脸，以后简称B脸，从另外一个视频中得到，同样用ffmpeg抽取帧B-src文件夹，然后利用faceswap抽取人脸到B-desc文件夹。
抽取的配置图：
抽取人脸的配置截图中，红框部分做调整就可以，其他默认就OK
1&#43;&#43;&#43;&#43;抽取帧的ffmpeg命令
ffmpeg -i a_movie.mp4 -qscale:v 2 -f image2 -r 1 zhubo-src/img_%d.jpg
更多ffmpeg用法请看https://blog.csdn.net/jiakai82/article/details/103288726
2&#43;&#43;&#43;&#43;利用faceswap抽取人脸图片
A脸提取" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/8cc5adc81b6d3c1c5f9d9db5a4723202/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-01-10T11:27:37+08:00" />
<meta property="article:modified_time" content="2020-01-10T11:27:37+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AI换脸项目faceswap研究报告</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><strong>缘起</strong></p> 
<p>deepfakes是利用AI技术换脸的开源项目，目前基于deepfakes的开源项目很多，而faceswap认可度很高，到目前为止有28.5千Star，可以说是换脸这类项目最火的了。小弟在当下有换脸需求，选取了这个项目进行研究尝试，将自己的经验和体会记录下来，希望对你有所帮助！</p> 
<p>感谢公司、领导、一切关心支持我的人！</p> 
<p><strong>效果</strong></p> 
<p>先上一些效果，增加一些趣味性！</p> 
<p>官方换脸效果1</p> 
<p><img alt="" class="has" height="123" src="https://images2.imgbox.com/9c/5a/p1IOlPoX_o.gif" width="200"></p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/87/9c/c7lJKMyk_o.png" width="200"></p> 
<p>官方效果2，小扎在国会舌战群儒的场景</p> 
<p><img alt="" class="has" height="180" src="https://images2.imgbox.com/58/ed/tx5tWcTs_o.gif" width="320"></p> 
<p>第一次尝试换脸，演讲者左右换姿势，似乎是为不冷落两边的观众</p> 
<p><img alt="" class="has" height="132" src="https://images2.imgbox.com/8c/bf/BWMiuvEH_o.gif" width="212"></p> 
<p>另外一个换脸后的效果</p> 
<p><img alt="" class="has" height="182" src="https://images2.imgbox.com/48/f1/14355Rrk_o.gif" width="212"></p> 
<p>尝试了很多此换脸，不在此一一列举。</p> 
<p>+++++++++++++++++++++++</p> 
<p><strong>准备</strong></p> 
<p>需要一块好的nvidia显卡，最低是1080或以上，众所周知，机器学习是费时的，一块好的显卡可以帮助你节省大量的时间，我用的机器配置了4块TITAN xp显卡，这个项目训练时大部分小弟用到其中两张显卡，在训练villain模型时，用到了其中三张，villain很吃显存，基本上一天可以训练出一个比较清晰的模型。</p> 
<p>如果只有CPU，我建议还是放弃尝试，据说训练时间是以周为单位的，训练出一个结果，可能需要数周，那就需要极大的耐性。训练中你机器CPU100%的被烧数周，不能干其他事情。而训练一个模型可能并不让人满意，每次调整参数，又的继续等待数周。大概你周游完欧洲各国后，回家一看结果还没出来！</p> 
<p>如果在linux上运行程序，需要一些ffmpeg知识、一些shell脚本经验</p> 
<p>该项目在windows上可以直接运行图形，我只有ssh命令行连接的ubuntu服务器，所以自己写了写脚本来处理，很羡慕GUI版本</p> 
<p>后来突然领悟可以在我的mac笔记本上运行图形界面，用来查看配置选项。</p> 
<p>图形界面是这样：</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/67/32/NsYDa7CF_o.png" width="800"></p> 
<p>能运行图形界面操作起来更方便，但是如果像我一样在服务器上通过ssh登陆，也无妨。</p> 
<p>在本地调整好参数，同步到服务器上就行啦。</p> 
<p>安装好CUDA 10.0  和 anaconda，这个到处都是教程，在此不累叙。</p> 
<p><strong>安装faceswap</strong></p> 
<p>下载项目<a href="https://github.com/deepfakes/faceswap">https://github.com/deepfakes/faceswap</a></p> 
<p>需要注意的是这个项目经常更新，有时可能不稳定，所以我clone了一份在自己的github里<a href="https://github.com/hnjiakai/faceswap">https://github.com/hnjiakai/faceswap</a> ，幸运的是我用clone的这个版本，训练和转换等整个过程都很顺利，没有出现论坛里讨论的各种奇奇怪怪的问题。</p> 
<p>1、利用conda创建好运行faceswap的虚拟环境</p> 
<p>conda create -n face tensorflow=1.14 python=3.6</p> 
<p>2、切换到虚拟环境</p> 
<p>conda activate face </p> 
<p>3、安装依赖环境</p> 
<p>pip -r requirements.txt</p> 
<p>conda install opencv </p> 
<p>这些其实已经完整99%的安装，剩下1%用到时提示缺少可以用conda install xxx安装。详细的安装过程可以参照<a href="https://github.com/deepfakes/faceswap/blob/master/INSTALL.md">https://github.com/deepfakes/faceswap/blob/master/INSTALL.md</a></p> 
<p><strong>人脸资源准备</strong></p> 
<p>A人脸，以后简称A脸，从需要变脸的视频中得到，我用ffmpeg将视频A转换成图片到A-src文件夹，然后利用faceswap抽取人脸到A-desc文件夹为后续训练做准备。</p> 
<p>B人脸，以后简称B脸，从另外一个视频中得到，同样用ffmpeg抽取帧B-src文件夹，然后利用faceswap抽取人脸到B-desc文件夹。</p> 
<p>抽取的配置图：</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/f8/c8/zWGi3ulU_o.png" width="800"></p> 
<p>抽取人脸的配置截图中，红框部分做调整就可以，其他默认就OK</p> 
<p>1++++抽取帧的ffmpeg命令</p> 
<p>ffmpeg -i a_movie.mp4 -qscale:v 2 -f image2 -r 1 zhubo-src/img_%d.jpg</p> 
<p>更多ffmpeg用法请看<a href="https://blog.csdn.net/jiakai82/article/details/103288726">https://blog.csdn.net/jiakai82/article/details/103288726</a></p> 
<p>2++++利用faceswap抽取人脸图片</p> 
<p>A脸提取</p> 
<p>faceswap.py extract -i A-src -o A-desc -D s3fd -A fan -M extended </p> 
<p>其中-M extended我选择是增强，其他参数选择默认就行，我尝试了很多参数，走了不少弯路，最后发现这个最适合我</p> 
<p>B脸提取</p> 
<p>faceswap.py extract -i B-src -o B-desc -D s3fd -A fan -M extended </p> 
<p>抽取时的截图：</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/53/bd/J9DV1ar3_o.png" width="800"></p> 
<p>最下方会显示抽取进度，可以看到当前进度8%。</p> 
<p>实际操作过后，抽取思路其实比较清晰了，有上面这些其实已经够啦。</p> 
<p>在抽取过程中您仍然可能碰到各种问题，可以在下面连接里获取更多信息</p> 
<p>最全的人脸抽取指导在这个<a href="https://forum.faceswap.dev/viewtopic.php?f=5&amp;t=27" rel="nofollow">https://forum.faceswap.dev/viewtopic.php?f=5&amp;t=27</a></p> 
<p>个人觉得-M extended 选项最有用，可以根据自己需要调整extended或者是其他，剩余的选项其实大多默认就行</p> 
<p>extended增强模版，用意在尽量包含眉毛。</p> 
<p>A-src A-desc B-src B-desc 这四个目录最好换成你的工作目录路径，便于管理</p> 
<p>我的某个模型的工作目录大概这样</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/b4/85/3ELgD8X9_o.png" width="800"></p> 
<p>目录文件一目了然，train.ini放置的是本模型训练的配置文件，my_conf.sh放置的是训练时的命令行参数配置信息，</p> 
<p>train75-original，75表示的是比例、original表示的是模型插件名。</p> 
<p>我使用的人脸全都是正面的，项目需求是这样安排的，视频中始终只有一人，所以不用考虑人脸清理等操作，也节省了不少时间。</p> 
<p>下图是需要换脸的A脸其中的一张</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/60/59/kEyaqYGN_o.png" width="200"></p> 
<p>A脸我大概收集了5000张，是从一段3分钟视频中截取</p> 
<p>下图是B脸图片其中的一张</p> 
<p><img alt="" class="has" height="197" src="https://images2.imgbox.com/ef/c8/mkBWL35Q_o.png" width="200"></p> 
<p>B脸图片大概也收集了5000张，同样是从另外一段三分钟视频中截取。</p> 
<p>人脸准备很快就说完了，其实为了整理人脸，费了不少时间，包括人脸视频选取，下载处理，美容等。</p> 
<p>在没有美容前，B脸偏暗，是这样的：</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/19/54/bVAIUdpZ_o.png" width="200"></p> 
<p>训练出来的结果总是灰头土脸，后来提亮肤色后，好很多。</p> 
<p><strong>参数调整</strong></p> 
<p>现在人脸图片已经准备好了，接下来就是准备训练train了。</p> 
<p>train的最全指南在这里<a href="https://forum.faceswap.dev/viewtopic.php?f=6&amp;t=146" rel="nofollow">https://forum.faceswap.dev/viewtopic.php?f=6&amp;t=146</a></p> 
<p>+++++++++++++++++++++++++++++++++++++++++</p> 
<p>用默认参数完全可以开始训练了，但是考虑到训练是个漫长的过程，在调整好你想要的参数后再训练，可以节省无谓的时间浪费。</p> 
<p>train的参数大概有几十个吧，这么多参数各种组合起来数量还是很多的，如果都要实验一遍的话，可以玩很久。</p> 
<p>+++++主要参数</p> 
<p>train的参数有很多，包括Plugins部分的参数（Configure Train Plugins ）和train阶段时用到的命令行参数，大部分参数其实默认就挺好，有些特定参数，做些调整，</p> 
<p>从而可以训练出更好的结果。</p> 
<p>首先说下Plugins参数</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/78/0a/BEU0QeQ3_o.png" width="500"></p> 
<p>Global部分，最有调整性的是Coverage，我尝试了很多，从62%到100%都有，最后得出结果是68%到75%之间的效果最好</p> 
<p>Coverage并不是越大越好，自己可以尝试调整。</p> 
<p>MaskType用none，其他我也有尝试，但是并没有什么进步，还是回到none。</p> 
<p>Train Plugins之model标签：</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/76/37/Ug1NmHEM_o.png" width="500"></p> 
<p>我最常用的是original和villain，GPU内存够用就没勾选 lowmem。</p> 
<p>下面这些选项可以根据自己需要调整，我基本不调整他</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/10/f8/SwswuJr1_o.png" width="500"></p> 
<p>下面是命令行参数选项卡</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/e4/89/oW283ABz_o.png" width="800"></p> 
<p>我更关心红色部分，这是些主要的选项</p> 
<p>各个选项都很好理解，其中WriteImage可以将图片中间结果保存，可以随时查看训练结果，觉得满意啦就可以停止训练</p> 
<p>图片是这样的：</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/5b/af/btmUAGm7_o.png" width="800"></p> 
<p>在选择好你的参数后，将faceswap项目目录下的config里的几个.ini文件，拷贝到服务器上，如果是本地训练可以忽略</p> 
<p>点击generate按钮，会生成命令行，可以粘贴到服务器上运行，如果是本地直接点击train按钮运行</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/6d/5a/ClRSfPiA_o.png" width="800"></p> 
<p><strong>训练</strong></p> 
<p>上面已经开始训练了，训练中可以随时查看faceswap目录下的training_preview.jpg文件，觉得不错就可以停止。</p> 
<p>重要的事情再说一遍，看到training_preview.jpg文件，效果不错了，就可以停止了</p> 
<p>++++++++并不是迭代次数越多越好，也不是看lose越小越好</p> 
<p>肉眼直接看training_preview.jpg图片是最好的方法</p> 
<p>model文件夹里的文件结构</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/14/ae/UXjwL1bx_o.png" width="800"></p> 
<p>如果中间训练时出错，无法继续训练，可以通过.bk文件还原</p> 
<p>为了防止过分迭代，比如，可以加入-it 100000，迭代10万次会自动停止</p> 
<p>接下来是个漫长的过程，时间长短取决于你的GPU性能。。。。。。。。。。。。。。。。</p> 
<p><strong>转换</strong></p> 
<p>在觉得训练结果已经不错，或者想看看结果如何，都可以停止训练，开始转换人脸，就是将A脸换成B脸</p> 
<p>train是个很重要的过程，有很多参数，convert也同样挺重要，也有很多参数，不同参数影响结果也挺大</p> 
<p>conver命令行参数：</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/c9/c5/xG2a5Bq0_o.png" width="800"></p> 
<p>在尝试很多参数变更后，最后回到调整红色部分的主要参数</p> 
<p>ColorAdjustment 主要调整avg-color和seamless-clone</p> 
<p>ColorAdjustment如果支持组合模式，我觉得可能更完美些，已经向作者提出建议</p> 
<p>++单纯用seamless-clone五官抖动厉害，单独用其他的融合很不完美</p> 
<p>seamless-clone效果图：</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/ac/f5/k7HB644h_o.png" width="200"></p> 
<p>不用seamless-clone效果更不好</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/a1/21/WhuAKDwO_o.png" width="200"></p> 
<p>MaskType主要用到extended和predicted</p> 
<p>extended能解决眉毛问题，但是下巴有问题</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/bd/34/57G7mobM_o.png" width="200"></p> 
<p>predicted眉毛有问题</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/ce/ab/hePXQIjk_o.png" width="200"></p> 
<p>++extended能够将眉毛包含覆盖，predicted要差些，眉毛总有问题</p> 
<p>++++如果换脸时有多人，可以用排除法，或者包含法，避免替换了额外的人脸。</p> 
<p>我尝试很多组合，大概训练了2位数那么多的模型，花费了无数时间，但是始终没找到完美的组合！</p> 
<p>这些模型占用磁盘达到383GB，GPU在日夜不停的训练</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/d6/3d/zRN45FlY_o.png" width="800"></p> 
<p>所以我觉得，faceswap完成完美的替换在某些情况下还是比较困难的。</p> 
<p>faceswap有很多参数可以选择，可玩性很强，可选很多，容易陷入选择泥团。</p> 
<p>虽然有不完美但是faceswap是个不平凡的项目！</p> 
<p><strong>合成视频</strong></p> 
<p>转换后的图片，我通过ffmpeg合成视频，暂时没考虑语音，语音也可以通过ffmpeg后期合成</p> 
<p>"ffmpeg -i ./converted-$convert_name/img_%0d.png -c:v libx264 -vf "fps=25,format=yuv420p" convert-movie-$convert_name-$dname.mp4"</p> 
<p>合成了文章开头那些视频</p> 
<p>+++++++++</p> 
<p><strong>总结</strong></p> 
<p>这次换的有瑕疵，原因有多种可能：</p> 
<p>第一、可能是选择的AB肤色有关系，图片中女性脸都非常白，稍有差池就能看到暇疵，我觉得肤色比较黑的可能更好；</p> 
<p>第二、可能也和算法有很大关系，期待作者出更完美的算法，包括在学习部分和脸部融合部分，在convert阶段，如果支持多种融合的组合，比如semaless-clone加vag-color组合，等，效果可能更好，我以向作者提出这个建议，他们考虑会加上，但是可能时间比较长；</p> 
<p>第三、可能是要找到适合AB脸的参数，需要更多的尝试，比较费时，每个换脸的参数可能都不同；</p> 
<p>第四、可能是人脸始终是正脸，光影很单调，我觉得如果是经常左右动的脸（有些演讲人会），可能更好些；</p> 
<p>机器学习是个学习过程，没有完美答案，也不能1+1=2那么直接，更多是概率，这次没成功可能下次会成功。</p> 
<p>时间有限，关注那些重要的参数。</p> 
<p>小弟接触换脸时间不长，能力也有限，希望大牛不吝赐教，万分感谢！</p> 
<p><strong>感谢</strong></p> 
<p>感谢公司、感谢领导、感谢一切关心支持我的人！</p> 
<p><strong>后记</strong></p> 
<p>期待faceswap的进步，让换脸更完美！</p> 
<p>近闻微软和北大搞出了个新的换脸算法，<a href="https://mp.weixin.qq.com/s/gGCyMq4PM_Whv-Ssiwt-HA" rel="nofollow">https://mp.weixin.qq.com/s/gGCyMq4PM_Whv-Ssiwt-HA</a>，据说可以秒杀faceswap！目前只有论文没有代码，希望有大佬可以开发出来并开源，造福万千程序员！</p> 
<p> </p> 
<p><strong>免责声明</strong></p> 
<p>文中人物图片来自网络，如果有侵权行为，请留言作者删除。</p> 
<p> </p> 
<p>全文结束</p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/539c842ff08cfb496b5a9fdd974f9bf3/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">通过SNMP查询交换机的MAC表以及核心交换机的ARP表，实现查询对应关系</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b32b06945896704697e42cdbfcb02584/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">VS2017&#43;Qt5.12 编译报错E2512	功能测试宏的参数必须是简单标识符	 的解决方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>