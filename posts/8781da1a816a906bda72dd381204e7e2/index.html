<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>测试阶段模型自适应方法总结 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="测试阶段模型自适应方法总结" />
<meta property="og:description" content="©PaperWeekly 原创 · 作者 | 张一帆
单位 | 中科院自动化所博士生
研究方向 | 计算机视觉
Domain Generalization（DG：域泛化）一直以来都是各大顶会的热门研究方向。DA 假设我们有多个个带标签的训练集（源域），这时候我们想让模型在另一个数据集上同样表现很好（目标域），但是在训练过程中根本不知道目标域是什么，这个时候如何提升模型泛化性呢？核心在于如何利用多个源域带来的丰富信息。
DG 最困难的地方在于 test-sample 的不可知，训练时不可用，近期有一系列方法开始尝试假设 test sample 以 online 的形式出现，然后利用其信息增强泛化性，下表总结了 test time daptation 方法与传统 DA，DG 方法的区别。
传统 DG 方法就是在源域 finetune 预训练模型，然后部署时不经过任何调整。DA 方法可以根据无标签的目标域数据在训练时调整模型，test-time training 方法在测试时会有一些无监督损失比如检测旋转角度等，然后对每个 test sample 也会进行旋转角度的检测，本文所述的 fully test-time adaptation 在 training 的时候不需要无监督损失，而只需要在 test 的时候进行 adaptation。
T3A
论文标题：
Test-Time Classifier Adjustment Module for Model-Agnostic Domain Generalization
论文来源：
NeurIPS 2021 Spotlight
论文链接：
https://proceedings.neurips.cc/paper/2021/hash/1415fe9fea0fa1e45dddcff5682239a0-Abstract.html
代码链接：
https://github.com/matsuolab/T3A
以往的 DG 方法都致力于研究如何利用好手头的域信息，这篇文章另辟蹊径。在 test 的阶段，我们在依然会选择更新模型头部的linear层。但是这与直接使用 test 的数据进行训练不同，主要的差异在于，本文假设在测试时，模型通常部署在某些环境中，并且必须在不断出现的各种示例上工作良好，样本连续到来是本文场景的主要特征，在拿到样本后模型需要立刻给出决策并更新。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/8781da1a816a906bda72dd381204e7e2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-09-16T10:37:00+08:00" />
<meta property="article:modified_time" content="2022-09-16T10:37:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">测试阶段模型自适应方法总结</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/db/79/9PMDcz7p_o.gif" alt="1d3d9b99f25963ea14dbb464db6b10ad.gif"></p> 
 <p style="text-align:right;"><strong>©PaperWeekly 原创 </strong><strong>·</strong> <strong>作者 | </strong>张一帆</p> 
 <p style="text-align:right;"><strong>单位 |</strong> 中科院自动化所博士生</p> 
 <p style="text-align:right;"><strong>研究方向 | </strong>计算机视觉</p> 
 <p>Domain Generalization（DG：域泛化）一直以来都是各大顶会的热门研究方向。DA 假设我们有多个个带标签的训练集（源域），这时候我们想让模型在另一个数据集上同样表现很好（目标域），但是在训练过程中根本不知道目标域是什么，这个时候如何提升模型泛化性呢？核心在于如何利用多个源域带来的丰富信息。</p> 
 <p>DG 最困难的地方在于 test-sample 的不可知，训练时不可用，近期有一系列方法开始尝试假设 test sample 以 online 的形式出现，然后利用其信息增强泛化性，下表总结了 test time daptation 方法与传统 DA，DG 方法的区别。<br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/d5/7f/VnFtxc1r_o.png" alt="de78b0ab26ad2e60a9cd3e38e78b12fa.png"></p> 
 <p>传统 DG 方法就是在源域 finetune 预训练模型，然后部署时不经过任何调整。DA 方法可以根据无标签的目标域数据在训练时调整模型，test-time training 方法在测试时会有一些无监督损失比如检测旋转角度等，然后对每个 test sample 也会进行旋转角度的检测，本文所述的 fully test-time adaptation 在 training 的时候不需要无监督损失，而只需要在 test 的时候进行 adaptation。</p> 
 <p style="text-align:left;"><img src="https://images2.imgbox.com/64/8b/GWzEnWwa_o.png" alt="1f31fd8450a6ef374285aa3c3e1e0cc0.png"></p> 
 <p style="text-align:left;"><strong>T3A</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/8f/db/23pI26Ti_o.png" alt="1c504313a08051b7d1aa91bcdb2d53c6.png"></p> 
 <p style="text-align:left;"><strong>论文标题：</strong></p> 
 <p style="text-align:left;">Test-Time Classifier Adjustment Module for Model-Agnostic Domain Generalization</p> 
 <p style="text-align:left;"><strong>论文来源：</strong></p> 
 <p style="text-align:left;">NeurIPS 2021 Spotlight</p> 
 <p style="text-align:left;"><strong>论文链接：</strong></p> 
 <p style="text-align:left;">https://proceedings.neurips.cc/paper/2021/hash/1415fe9fea0fa1e45dddcff5682239a0-Abstract.html</p> 
 <p style="text-align:left;"><strong>代码链接：</strong></p> 
 <p style="text-align:left;">https://github.com/matsuolab/T3A</p> 
 <p>以往的 DG 方法都致力于研究如何利用好手头的域信息，这篇文章另辟蹊径。在 test 的阶段，我们在依然会选择更新模型头部的linear层。但是这与直接使用 test 的数据进行训练不同，主要的差异在于，本文假设在测试时，模型通常部署在某些环境中，并且必须在不断出现的各种示例上工作良好，<strong>样本连续到来</strong>是本文场景的主要特征，在拿到样本后模型需要立刻给出决策并更新。</p> 
 <p>本文的方法被称之为 test-time templates adjuster（T3A），接下来我们对其 intuition 和具体实现思路进行介绍。</p> 
 <p>首先，我们知道模型的分类是通过样本特征  和分类层对于  类的权重  进行点积然后 softmax 得到的，也就是说分类层的权重在这里作为一个 <strong>prototype</strong>，预测是在衡量样本到每个 prototype 之间的相似度。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/f3/d0/y7utvulr_o.png" alt="3e863a4b63f5b59df9ae1e72a8c65110.png"></p> 
 <p>作者维护了  个不同的 support set，对于每一个到来的样本 ，首先根据上式得到初始的预测结果，然后更新 support set。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/d2/76/pdg9vsPl_o.png" alt="8d06cd2564d976421d4989a77c75567e.png"></p> 
 <p>初始化的 support set 就是源域训练的分类器对每个类的权重 。然后我们可以根据更新的 support set 完成预测：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/4c/d4/yCblbiKT_o.png" alt="ae271f7033364753022b64c832934284.png"></p> 
 <p>这里新的 prototype 。但是这里还有一个问题，就是一旦模型初始时分配了错误的标签，那么这个噪音会一直存在，因此作者使用了预测的熵来排除不可信的 pseudo-labeled data，熵的定义如下</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/8e/15/UlZ5Sz1k_o.png" alt="1785f4de3e649216c3f03fad60133f0a.png"></p> 
 <p>也就是说在使用上述公式进行预测之前，作者首先会对 support set 进行过滤。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/78/16/L6KEtQdP_o.png" alt="c64b2a37af4fd15a78e8bbcc1b70f0ed.png"></p> 
 <p>这里  是  中第  大的熵值， 是一个超参。</p> 
 <p>T3A 有如下几个良好的性质：</p> 
 <p>1. T3A 隐式降低预测熵：预测熵一定程度上表征了 DG 的难度；看不见的区域的熵往往大于可见区域的熵。使用 T3A 大大降低了模型在 unseen dataset 上的预测熵。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/3b/a7/q04Edmxm_o.png" alt="f11801da6d7dba217dbdff205ea8bf22.png"></p> 
 <p>2. 计算效率高：唯一的计算开销是最后一个线性层的前向传播的成本，与特征提取器的前向和后向传播相比，这通常可以忽略不计。</p> 
 <p>3. 性能提升明显，如上图 c 所示，在各个数据集上都取得了明显的提升，而而且和传统的一些算法正交。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/d0/30/cq929t35_o.png" alt="05c9238f8ff6f459e335916b29f923da.png"></p> 
 <p style="text-align:left;"><img src="https://images2.imgbox.com/2c/b6/l0yl73I9_o.png" alt="d8dc744494e061e3f2c4d6df6d5c8e82.png"></p> 
 <p style="text-align:left;"><strong>DomainBed</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/d2/b9/uZNRLVVn_o.png" alt="544034e50361c5307940ad8738f58507.png"></p> 
 <p style="text-align:left;"><strong>论文标题：</strong></p> 
 <p style="text-align:left;">Adaptive Methods for Real-World Domain Generalization</p> 
 <p style="text-align:left;"><strong>论文来源：</strong></p> 
 <p style="text-align:left;">CVPR 2021</p> 
 <p style="text-align:left;"><strong>论文链接：</strong></p> 
 <p style="text-align:left;">https://arxiv.org/abs/2103.15796</p> 
 <p style="text-align:left;"><strong>代码链接：</strong></p> 
 <p style="text-align:left;">https://github.com/abhimanyudubey/DomainBed</p> 
 <p>本文的 intuition 在于，ERM 本身是学一个在多个 source domain  上表现很好的分类器 ，即：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/c6/34/iFKzNj7c_o.png" alt="cb4b7be822d26405045504d517e04356.png"></p> 
 <p>问题在于，在源域上表现好并不意味着在目标域上效果好，即本文所说的 “adaptivity gap”，如下图所示目标域上的最优分类器和源域训练出来的分类器差距是很大的。因此本文的目标是分类器既要依赖输入 x，也要依赖域 D，即 ，进而确保对于任何的测试集 ，我们学到的分类器  和 target domain 的分类器  不要太远。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/48/ee/GONKeFhq_o.png" alt="31f4ec118837e09e0d8d9713d05ee089.png"></p> 
 <p>具体实现分为如下几步：</p> 
 <p><strong>1. </strong><strong>Computing domain embeddings: </strong>这一步我们需要对每个 domain 计算一个嵌入，具体来说，我们每个 domain 给少量几个 data sample，然后训练一个神经网络  我们有：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/fa/ac/pCZfMXzI_o.png" alt="2f25c1f3d4ec7d77dbe3eca7b8a55241.png"></p> 
 <p>这样我们就可以计算每个 data 属于某 domain 的概率：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/94/38/3Mwwuvlj_o.png" alt="6d67226e816dfc014129b9a1f5d95065.png"></p> 
 <p>对 ，整个过程的训练目标为：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/0d/16/jfXC3EWD_o.png" alt="a5ff5320eb2d2c0f14e680510dfc4e4a.png"></p> 
 <p><strong>2. ERM using augmented inputs: </strong>在第一步对每个 domain，我们能够得到一个 embedding，本文的另一个特殊点在于，网络的输入并不只是 x，而是 ，具体而言， 首先通过一个 encode 得到 feature ，<strong>然后特征和域特征 concatenate 起来</strong>， 一起作为分类器的输入，这里的训练目标就是简单的交叉熵损失。</p> 
 <p><strong>3. Inference: </strong>测试的时候同样的需要先得到 domain embedding 和 image embedding，将二者结合起来一起输入分类器得到最终结果。</p> 
 <p>文章在传统的 DG benchmark 和更大的真实数据集上都进行了实验，与传统 ERM 相结合就能够取得不错的结果。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/68/f8/lPZpGsOU_o.png" alt="c48f838ce3787076557006fd45a7b1ab.png"></p> 
 <p style="text-align:left;"><img src="https://images2.imgbox.com/63/25/Rb3KSsd9_o.png" alt="11e460973300319f88ff5c92fe04e897.png"></p> 
 <p style="text-align:left;"><strong>Tent</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/64/c8/HBHB45a2_o.png" alt="8dadcc5942ba4fe869cc38d5b3a6140a.png"></p> 
 <p style="text-align:left;"><strong>论文标题：</strong></p> 
 <p style="text-align:left;">Tent: Fully Test-Time Adaptation by Entropy Minimization</p> 
 <p style="text-align:left;"><strong>论文来源：</strong></p> 
 <p style="text-align:left;">ICLR 2021 Spotlight</p> 
 <p style="text-align:left;"><strong>论文链接：</strong></p> 
 <p style="text-align:left;">https://arxiv.org/abs/2006.10726</p> 
 <p style="text-align:left;"><strong>代码链接：</strong></p> 
 <p style="text-align:left;">https://github.com/DequanWang/tent</p> 
 <p>本文并不仅仅关注 DG 问题，所提出的方法可以 general 到任意场景。本文的 setting 在测试过程中，模型必须在给定参数和目标数据的情况下进行适应。这种测试时间适应设置不能依赖于源数据或监督信息，因为当模型第一次遇到新的测试数据是在它可以被收集和注释之前，而如果每次遇到新的 test data 都和源域数据合并重新训练的话需要的成本太高了。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/fa/a9/kMsvzHvS_o.png" alt="6678219524f352afb01dbf5ff4844448.png"></p> 
 <p>本文最主要的 intuition 是<strong>最小化模型预测的 entropy</strong>，具体实现如下所示。</p> 
 <p>测试时的训练目标即：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/22/9e/dzppkzps_o.png" alt="1ab4f53266c756a547e4f4a0df5f6c55.png"></p> 
 <p>这里的  是预测出的标签， 是预测结果的 entropy。但是更新模型参数  会有如下问题，因为  是源域训练结果的参数，直接 alter 这个参数会导致模型发散，而且  的维度太高了，这就导致了整个优化过程过于敏感并且不高效。</p> 
 <p>为了稳定性和效率，本文只更新线性（尺度和位移）和低维（通道）的参数，即如下的归一化和转化参数：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/fd/01/IJlxUD0G_o.png" alt="1384070d32ece7c1f9180ac0f2bc59a3.png"></p> 
 <p>实现上只需重新定义源模型的归一化层。在测试过程中更新所有层和通道的归一化统计数据和仿射参数</p> 
 <p>实验在 CIFAR-10-C， CIFAR-100-C，ImageNet-C 的各种有 Corruption 的数据集上都表现得非常好。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/34/cc/6Lp8vMhY_o.png" alt="6589d5f532a5c263582133b29c2da1a8.png"></p> 
 <p style="text-align:left;"><img src="https://images2.imgbox.com/ab/ab/oHu9o9ga_o.png" alt="d2fe47e2aae48d914e50bee9c3929c1c.png"></p> 
 <p style="text-align:left;"><strong>TADE</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/bf/c4/reEYgHtw_o.png" alt="dc91d5f584627b0955e281167b0c9f28.png"></p> 
 <p style="text-align:left;"><strong>论文标题：</strong></p> 
 <p style="text-align:left;">Test-Agnostic Long-Tailed Recognition y Test-Time Aggregating Diverse Experts with Self-Supervision</p> 
 <p style="text-align:left;"><strong>论文来源：</strong></p> 
 <p style="text-align:left;">ICCV 2021</p> 
 <p style="text-align:left;"><strong>论文链接：</strong></p> 
 <p style="text-align:left;">https://arxiv.org/pdf/2107.09249.pdf</p> 
 <p style="text-align:left;"><strong>代码链接：</strong></p> 
 <p style="text-align:left;">https://github.com/Vanint/TADE-AgnosticLT</p> 
 <p>这篇文章的主题并不是 DG，而是长尾分布。这项工作研究了一个更实际的任务设置，称为测试不可知性长尾识别，其中<strong>训练类分布是长尾的，而测试类分布是未知的</strong>，可以任意倾斜。除了类不平衡的问题之外，这个任务还带来了另一个挑战:在训练样本和测试样本之间的类分布转移是未知的。</p> 
 <p>为了处理这个任务，本文提出了一种新的方法，称为测试时间聚合多样化专家 Test-time Aggregating Diverse Experts，顾名思义，也是用到了 test-time adaptation 的技术，因此我们对它的方法做一个简单介绍。</p> 
 <p>本文的主要出发点如下所示（a）现有的长尾识别方法旨在训练在类分布均匀的测试数据上表现良好的模型。然而，产生的模型可能无法处理任意倾斜的实际测试类分布。（b）我们的方法试图学习一个多专家模型，不同的专家熟练地处理不同的类分布。通过在测试时合理地聚集这些专家，我们的方法能够处理任何未知的测试类分布。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/bc/49/jU3GrkDJ_o.png" alt="9702d0f8c5e19927173e66ffaff91e81.png"></p> 
 <h5>4.1 Test-time Self-supervised Aggregation</h5> 
 <p>我们主要对其 test-time adapt 的部分进行介绍并假设以及通过训练得到了多个专家模型 ，其中每个专家擅长处理不同的类。本文的关键见解是，强大的专家应该更稳定地预测来自他们技能阶层的样本，即使这些样本受到了干扰。为了验证这一点，我们通过比较样本的两个增强视图的预测之间的余弦相似度来估计专家的预测稳定性。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/92/13/TY5Q4YXn_o.png" alt="11953ce05d012e8fa3d1c1a9f3d6e6e4.png"></p> 
 <p><strong>Prediction stability maximization: </strong>本文设计了一种新的自我监督方法，即预测稳定性最大化，通过最大化未标记测试样本的模型预测稳定性来学习专家（带有冻结参数）的聚集权值。如上图所示，该方法由以下三个主要组件组成。</p> 
 <p>1. Data view generation：像 moco v2 一样生成数据增强样本；</p> 
 <p>2. Learnable aggregation weight：给定一组可学习的权重 ，我们根据对每个 expert 的结果的加权组合得到最终结果 ；</p> 
 <p>3. Objective function：对每个测试样本我们最大化他的预测稳定性，从而更新加权权重。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/84/49/dUvYzroc_o.png" alt="ac3fa96d2074b61ca90c9d86df0d5be6.png"></p> 
 <p>所提出的方法总体称为 TADE，在多个数据集上都取得了不错的效果</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/91/b6/ZalYJ2Up_o.png" alt="ceda1bd165a78794afe9668915405f20.png"></p> 
 <p style="text-align:left;"><img src="https://images2.imgbox.com/56/7e/WsuO7pkH_o.png" alt="09e0a8bcd658fd7573f9ccedf1c25797.png"></p> 
 <p style="text-align:left;"><strong>Conclusion</strong></p> 
 <p>测试分布相对于训练分布的偏移不管是在哪个领域都是很常见的事情，传统的 DG 任务假设测试数据不可见因此一定不能使用，但是近期很多的方法开始尝试利用 online 的 test sample，目前这类方法相比于传统方法还比较少，有较多的改进空间。</p> 
 <p><strong>更多阅读</strong></p> 
 <p><a href="" rel="nofollow"><img src="https://images2.imgbox.com/0e/f4/vpChN2Sc_o.png" alt="a83af6a17180fc88f79efaf402894cc8.png"></a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow"><img src="https://images2.imgbox.com/8e/0e/TCM52gRe_o.png" alt="2b7140c249170ba921b200d7e6f40e12.png"></a></p> 
 <p style="text-align:center;"><a href="" rel="nofollow"><img src="https://images2.imgbox.com/47/01/iiAWbkB8_o.png" alt="354cf281a61c2c3f05dd5687ef678f25.png"></a></p> 
 <p><img src="https://images2.imgbox.com/b3/48/KK5reHKt_o.gif" alt="9fdaadef761f18044e6cab313be33b4b.gif"></p> 
 <p><strong>#投 稿 通 道#</strong></p> 
 <p><strong> 让你的文字被更多人看到 </strong></p> 
 <p>如何才能让更多的优质内容以更短路径到达读者群体，缩短读者寻找优质内容的成本呢？<strong>答案就是：你不认识的人。</strong></p> 
 <p>总有一些你不认识的人，知道你想知道的东西。PaperWeekly 或许可以成为一座桥梁，促使不同背景、不同方向的学者和学术灵感相互碰撞，迸发出更多的可能性。 </p> 
 <p>PaperWeekly 鼓励高校实验室或个人，在我们的平台上分享各类优质内容，可以是<strong>最新论文解读</strong>，也可以是<strong>学术热点剖析</strong>、<strong>科研心得</strong>或<strong>竞赛经验讲解</strong>等。我们的目的只有一个，让知识真正流动起来。</p> 
 <p>📝 <strong>稿件基本要求：</strong></p> 
 <p>• 文章确系个人<strong>原创作品</strong>，未曾在公开渠道发表，如为其他平台已发表或待发表的文章，请明确标注 </p> 
 <p>• 稿件建议以 <strong>markdown</strong> 格式撰写，文中配图以附件形式发送，要求图片清晰，无版权问题</p> 
 <p>• PaperWeekly 尊重原作者署名权，并将为每篇被采纳的原创首发稿件，提供<strong>业内具有竞争力稿酬</strong>，具体依据文章阅读量和文章质量阶梯制结算</p> 
 <p>📬 <strong>投稿通道：</strong></p> 
 <p>• 投稿邮箱：hr@paperweekly.site </p> 
 <p>• 来稿请备注即时联系方式（微信），以便我们在稿件选用的第一时间联系作者</p> 
 <p>• 您也可以直接添加小编微信（<strong>pwbot02</strong>）快速投稿，备注：姓名-投稿</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/8b/5c/h3tL11Wm_o.png" alt="264f59d9e983b1ff7fb63f3693f29a80.png"></p> 
 <p style="text-align:center;"><strong>△长按添加PaperWeekly小编</strong></p> 
 <p style="text-align:center;">🔍</p> 
 <p style="text-align:center;">现在，在<strong>「知乎」</strong>也能找到我们了</p> 
 <p style="text-align:center;">进入知乎首页搜索<strong>「PaperWeekly」</strong></p> 
 <p style="text-align:center;">点击<strong>「关注」</strong>订阅我们的专栏吧</p> 
 <p>·</p> 
 <p style="text-align:right;"><img src="https://images2.imgbox.com/50/fe/Ss97wpRO_o.jpg" alt="5a8ee36f91cfb38507287555e3ab30c8.jpeg"></p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/802ae4a3db3c09d1539e5d56ae0953dc/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">洛谷：P1462 通往奥格瑞玛的道路</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8466212235be20be8e52ae12bd0d3379/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">unity关于纹理、着色器和材质的介绍</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>