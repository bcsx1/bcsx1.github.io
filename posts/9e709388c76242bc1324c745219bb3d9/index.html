<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>（2020，latent space）解释用于语义面部编辑的GAN的隐空间 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="（2020，latent space）解释用于语义面部编辑的GAN的隐空间" />
<meta property="og:description" content="Interpreting the Latent Space of GANs for Semantic Face Editing
公众号：EDPJ
目录
0. 摘要
1. 介绍
1.1 相关工作
2. InterFaceGAN 的框架
2.1 隐空间中的语义
2.2 隐空间中的操作
3. 实验
3.1 隐空间分离
3.2 隐空间操控 3.3. 条件操作
3.4 在 StyleGAN 上的结果
3.5 真实图像操作
4. 结论
一些想法
参考
0. 摘要 虽然生成对抗网络（Generative Adversarial Networks，GANs）是高保真度图像合成领域的先进技术，但是GAN如何把从随机分布中采样的隐编码（latent code）映射为逼真的图像，人们对此还没有充分的认知。先前的工作假设由GAN学到的隐空间（latent space）遵从一个分布式的表示（distributed representation），但可以进行向量运算。在本文，提出了一个新的架构，叫做InterFaceGAN，通过解释由GAN学到的隐语义（latent semantics），进行语义的面部编辑。在该框架中，我们详细地研究隐空间中的不同语义是如何被编码的。我们发现，在线性变形（linear transformations）之后，训练良好的生成模型的 latent code 学习到了一个分离的（disentangled）表示（representation）。我们研究分离各种语义，并设法将一些纠缠的语义与子空间投影分离，从而更精确地控制面部属性。除了操纵性别、年龄、表情和是否戴眼镜外，我们甚至可以改变面部姿势（face pose）并修复 GAN 模型意外生成的伪像（artifacts）。当与 GAN 逆映射（inversion）或一些编码器相关模型相结合时，我们所提出的方法可处理真实图像。研究结果表明，学习自发地合成面部图像会产生一种解耦的、可控的面部属性 representation。
1. 介绍 GAN 的原理：通过对抗性的训练，学习一个从隐分布（latent distribution）到真实数据的映射。在学习了这种非线性映射之后，GAN 能够从随机采样的 latent code 中生成逼真的图像。 然而，对于语义是如何在潜在空间中产生和组织的，人们并不确定。以人脸合成为例，当对 latent code 进行采样以生成图像时，code 如何能够确定输出人脸的各种语义属性（例如，性别和年龄），以及这些属性如何相互纠缠？" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/9e709388c76242bc1324c745219bb3d9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-02T17:55:04+08:00" />
<meta property="article:modified_time" content="2023-06-02T17:55:04+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">（2020，latent space）解释用于语义面部编辑的GAN的隐空间</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p style="text-align:center;"><strong>Interpreting the Latent Space of GANs for Semantic Face Editing</strong></p> 
<p style="text-align:center;">公众号：EDPJ</p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="-toc" style="margin-left:0px;"><a href="#0.%20%E6%91%98%E8%A6%81" rel="nofollow">0. 摘要</a></p> 
<p id="1.%20%E4%BB%8B%E7%BB%8D-toc" style="margin-left:0px;"><a href="#1.%20%E4%BB%8B%E7%BB%8D" rel="nofollow">1. 介绍</a></p> 
<p id="1.1%20%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C-toc" style="margin-left:40px;"><a href="#1.1%20%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C" rel="nofollow">1.1 相关工作</a></p> 
<p id="2.%C2%A0InterFaceGAN%20%E7%9A%84%E6%A1%86%E6%9E%B6-toc" style="margin-left:0px;"><a href="#2.%C2%A0InterFaceGAN%20%E7%9A%84%E6%A1%86%E6%9E%B6" rel="nofollow">2. InterFaceGAN 的框架</a></p> 
<p id="2.1%20%E9%9A%90%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%9A%84%E8%AF%AD%E4%B9%89-toc" style="margin-left:40px;"><a href="#2.1%20%E9%9A%90%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%9A%84%E8%AF%AD%E4%B9%89" rel="nofollow">2.1 隐空间中的语义</a></p> 
<p id="2.2%C2%A0%E9%9A%90%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%9A%84%E6%93%8D%E4%BD%9C-toc" style="margin-left:40px;"><a href="#2.2%C2%A0%E9%9A%90%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%9A%84%E6%93%8D%E4%BD%9C" rel="nofollow">2.2 隐空间中的操作</a></p> 
<p id="3.%20%E5%AE%9E%E9%AA%8C-toc" style="margin-left:0px;"><a href="#3.%20%E5%AE%9E%E9%AA%8C" rel="nofollow">3. 实验</a></p> 
<p id="3.1%C2%A0%20%E9%9A%90%E7%A9%BA%E9%97%B4%E5%88%86%E7%A6%BB-toc" style="margin-left:40px;"><a href="#3.1%C2%A0%20%E9%9A%90%E7%A9%BA%E9%97%B4%E5%88%86%E7%A6%BB" rel="nofollow">3.1  隐空间分离</a></p> 
<p id="3.2%20%E9%9A%90%E7%A9%BA%E9%97%B4%E6%93%8D%E6%8E%A7%C2%A0-toc" style="margin-left:40px;"><a href="#3.2%20%E9%9A%90%E7%A9%BA%E9%97%B4%E6%93%8D%E6%8E%A7%C2%A0" rel="nofollow">3.2 隐空间操控 </a></p> 
<p id="3.3.%20%E6%9D%A1%E4%BB%B6%E6%93%8D%E4%BD%9C-toc" style="margin-left:40px;"><a href="#3.3.%20%E6%9D%A1%E4%BB%B6%E6%93%8D%E4%BD%9C" rel="nofollow">3.3. 条件操作</a></p> 
<p id="3.4%20%E5%9C%A8%20StyleGAN%20%E4%B8%8A%E7%9A%84%E7%BB%93%E6%9E%9C-toc" style="margin-left:40px;"><a href="#3.4%20%E5%9C%A8%20StyleGAN%20%E4%B8%8A%E7%9A%84%E7%BB%93%E6%9E%9C" rel="nofollow">3.4 在 StyleGAN 上的结果</a></p> 
<p id="3.5%20%E7%9C%9F%E5%AE%9E%E5%9B%BE%E5%83%8F%E6%93%8D%E4%BD%9C-toc" style="margin-left:40px;"><a href="#3.5%20%E7%9C%9F%E5%AE%9E%E5%9B%BE%E5%83%8F%E6%93%8D%E4%BD%9C" rel="nofollow">3.5 真实图像操作</a></p> 
<p id="4.%20%E7%BB%93%E8%AE%BA-toc" style="margin-left:0px;"><a href="#4.%20%E7%BB%93%E8%AE%BA" rel="nofollow">4. 结论</a></p> 
<p id="%E4%B8%80%E4%BA%9B%E6%83%B3%E6%B3%95-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E4%BA%9B%E6%83%B3%E6%B3%95" rel="nofollow">一些想法</a></p> 
<p id="%E5%8F%82%E8%80%83-toc" style="margin-left:0px;"><a href="#%E5%8F%82%E8%80%83" rel="nofollow">参考</a></p> 
<hr id="hr-toc"> 
<h2>0. 摘要</h2> 
<p style="text-align:justify;">虽然生成对抗网络（Generative Adversarial Networks，GANs）是高保真度图像合成领域的先进技术，但是GAN如何把从随机分布中采样的隐编码（latent code）映射为逼真的图像，人们对此还没有充分的认知。先前的工作假设由GAN学到的隐空间（latent space）遵从一个分布式的表示（distributed representation），但可以进行向量运算。在本文，提出了一个新的架构，叫做InterFaceGAN，通过解释由GAN学到的隐语义（latent semantics），进行语义的面部编辑。在该框架中，我们详细地研究隐空间中的不同语义是如何被编码的。我们发现，在线性变形（linear transformations）之后，训练良好的生成模型的 latent code 学习到了一个分离的（disentangled）表示（representation）。我们研究分离各种语义，并设法将一些纠缠的语义与子空间投影分离，从而更精确地控制面部属性。除了操纵性别、年龄、表情和是否戴眼镜外，我们甚至可以改变面部姿势（face pose）并修复 GAN 模型意外生成的伪像（artifacts）。当与 GAN 逆映射（inversion）或一些编码器相关模型相结合时，我们所提出的方法可处理真实图像。研究结果表明，学习自发地合成面部图像会产生一种解耦的、可控的面部属性 representation。</p> 
<h2 id="1.%20%E4%BB%8B%E7%BB%8D" style="text-align:justify;">1. 介绍</h2> 
<p style="text-align:justify;">GAN 的原理：通过对抗性的训练，学习一个从隐分布（latent distribution）到真实数据的映射。在学习了这种非线性映射之后，GAN 能够从随机采样的 latent code 中生成逼真的图像。 然而，对于语义是如何在潜在空间中产生和组织的，人们并不确定。以人脸合成为例，当对 latent code 进行采样以生成图像时，code 如何能够确定输出人脸的各种语义属性（例如，性别和年龄），以及这些属性如何相互纠缠？</p> 
<p style="text-align:justify;">现有工作通常侧重于提高 GAN 的合成质量，但是，很少有人研究 GAN 在 latent space 方面实际学到的内容。 Radford 等人，最先发现 latent space 中的向量算术特性。 最近的一项工作进一步表明，GAN 生成器中间层的一些单元，专门合成某些视觉概念，例如生成客厅时生成的沙发和电视。 即便如此，对于 GAN 如何连接 latent space 和图像的语义空间（semantic space），以及如何将 latent code 用于图像编辑，还缺乏足够的了解。</p> 
<p style="text-align:justify;">在本文中，我们提出了一个框架 InterFaceGAN，即 Interpreting Face GANs 的缩写，以识别在训练良好的人脸合成模型的 latent space 中编码的语义，然后利用它们进行语义人脸编辑。除了向量运算之外，该框架还提供了理论分析和实验结果，以验证线性子空间是否与 latent space 中出现的不同的真假语义对齐。 我们进一步研究了不同语义之间的分离，并表明我们可以通过线性子空间投影，分离一些纠缠的属性（例如，老年人比年轻人更可能戴眼镜）。 这些分离的语义可以使用任何给定的 GAN 模型精确控制面部属性，而无需重新训练。</p> 
<p class="img-center"><img alt="" height="277" src="https://images2.imgbox.com/9e/6a/SijUGZzf_o.png" width="650"></p> 
<p>本文贡献如下所示：</p> 
<ul><li style="text-align:justify;">我们提出 InterFaceGAN 来探索单个或多个语义是如何在 GAN 的 latent space 中编码的，例如 PGGAN 和 StyleGAN，并观察到 GAN 自发地学习与特定属性对应的各种 latent subspace。线性变换分离可这些属性的 representation。</li><li style="text-align:justify;">我们展示了 InterFaceGAN 可以使用任何固定的（无需重新训练）预训练 GAN 模型进行语义人脸编辑。 部分结果如上图所示。 除了性别、年龄、表情和眼镜的存在，我们还可以明显地改变面部姿势或纠正 GAN 产生的一些伪像。</li><li style="text-align:justify;">我们使用 GAN inversion和与编码器相关模型将 InterFaceGAN 扩展到真实图像编辑。 我们通过简单地改变 latent code 成功地操纵了真实面孔的属性，即使使用的不是专门为编辑任务设计的 GAN。</li></ul> 
<h3 id="1.1%20%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C" style="text-align:justify;">1.1 相关工作</h3> 
<p style="text-align:justify;"><strong>GAN</strong>。GAN 通常将采样的 latent code 作为输入进行图像合成。 为了使 GAN 适用于真实图像处理，现有方法把 latent space 到图像空间的映射反转或学习与 GAN 训练相关的附加编码器。</p> 
<p style="text-align:justify;">虽然取得了巨大的成功，但是， GAN 如何学习把输入 latent space 与真实视觉世界中的语义联系起来，在这方面所做的研究很少。</p> 
<p style="text-align:justify;"><strong>GAN的 latent space 的研究</strong>。GAN 的潜在空间通常被视为黎曼流形（Riemannian manifold）。 先前的工作重点是，研究如何通过 latent space 中的插值，使输出图像从一种合成平滑地变化到另一种合成，而不管图像是否是语义可控的。</p> 
<p style="text-align:justify;">GLO 同时优化生成器和 latent code 以学习更好的 latent space。 然而，关于训练有素的 GAN 如何能够在 latent space 内编码不同语义的研究仍然缺失。</p> 
<p style="text-align:justify;">一些工作已经观察到矢量算术性质。 除此之外，这项工作从单个语义的属性和多个语义的分离两个方面，对 latent space 中 code 的语义进行了详细分析。</p> 
<p style="text-align:justify;">一些并行工作还探索了 GAN 学习的 latent semantics。</p> 
<ul><li style="text-align:justify;">Jahanian 等人研究了 GAN 在相机运动和图像色调方面的可控性。</li><li style="text-align:justify;">Goetschalckx 等人提高了输出图像的可记忆性。</li><li style="text-align:justify;">Yang 等人探索了场景合成的深度生成表示（deep generative representations）中的层次语义（hierarchical semantics）。</li></ul> 
<p style="text-align:justify;"><strong>使用 GAN 进行语义面部编辑</strong>。语义面部编辑旨在操纵给定图像的面部属性。与可以任意生成图像的无条件 GAN 相比，语义编辑期望模型仅更改目标属性而保留输入图像的其他属性。</p> 
<p style="text-align:justify;">为了实现这一目标，当前的方法需要精心设计 loss function、引入额外的属性标签或特征或特殊架构来训练新的模型。然而，这些模型的合成分辨率和质量远远落后于原生 GAN，如 PGGAN 和 StyleGAN。</p> 
<p style="text-align:justify;">与以往基于学习的方法不同，本工作探索了固定 GAN 模型的 latent space 内的可解释语义，并通过改变 latent code 将不受约束的 GAN 转变为可控的 GAN。</p> 
<h2 id="2.%C2%A0InterFaceGAN%20%E7%9A%84%E6%A1%86%E6%9E%B6" style="text-align:justify;">2. InterFaceGAN 的框架</h2> 
<h3 id="2.1%20%E9%9A%90%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%9A%84%E8%AF%AD%E4%B9%89">2.1 隐空间中的语义</h3> 
<p>给定一个训练有素的 GAN 模型，生成器可以被表述为确定性函数 g：Z→X。这里，<img alt="Z \subseteq \mathop R\nolimits^d" class="mathcode" src="https://images2.imgbox.com/18/f8/srnyQZ9X_o.png">表示 d 维的 latent space，其中，通常使用高斯分布 <img alt="N(0,\mathop I\nolimits_d )" class="mathcode" src="https://images2.imgbox.com/22/a2/i9lnW4b3_o.png">。X 表示图像空间，其中，每一个样本 x 包含确定的语义信息，例如面部模型的性别、年龄。假定有一个语义分数函数<img alt="\mathop f\nolimits_S :X \to S,S \in \mathop R\nolimits^m" class="mathcode" src="https://images2.imgbox.com/76/77/7wGAIu0M_o.png">表示有 m 个语义的 semantic space。我们可以用<img alt="s = \mathop f\nolimits_S (g(z))" class="mathcode" src="https://images2.imgbox.com/40/cc/Csfi0p3h_o.png">连接 latent space Z 和 semantic space S，其中，s 和 z 分别表示语义分数和采样的 latent code。</p> 
<p class="img-center"><img alt="" height="93" src="https://images2.imgbox.com/a8/3a/0TyTo9pP_o.png" width="461"></p> 
<p><strong>单语义</strong>。人们广泛观察到，当对两个 latent code z1 和 z2 进行线性插值时，相应合成的外观会不断变化。 它隐含地意味着图像中包含的语义也在逐渐变化。 根据性质 1（如上图所示），z1 和 z2 之间的线性插值在 Z 中形成一个方向，从而定义了一个超平面。 因此，我们假设对于任何二元语义（例如，男性与女性），在 latent space 中存在一个超平面作为分离边界。 当 latent code 在超平面的同一侧移动时语义保持不变，但在跨越边界时变成另一语义。</p> 
<p>给定一个具有单位法向量<img alt="\mathop {n \in R}\nolimits^d" class="mathcode" src="https://images2.imgbox.com/5b/2a/U6uoFZki_o.png">的超平面，定义一个样本 z 到该平面的距离为：</p> 
<p class="img-center"><img alt="" height="34" src="https://images2.imgbox.com/54/d3/9TmXK8tr_o.png" width="294"></p> 
<p>该距离不是一个严格定义的距离，因为可能是负数。当 z 在边界附近移向并穿过超平面时，“距离”和语义分数都会相应变化。 而正是在“距离”改变其数字符号的时候，语义属性发生了逆转。 因此，我们期望这两者线性相关</p> 
<p class="img-center"><img alt="" height="32" src="https://images2.imgbox.com/92/2f/j684370S_o.png" width="311"></p> 
<p>其中 f(·) 是特定语义的评分函数，<img alt="\lambda &gt; 0" class="mathcode" src="https://images2.imgbox.com/8a/ae/mT59kJqR_o.png">是一个标量，衡量语义随着距离的变化而变化的速度。 </p> 
<p class="img-center"><img alt="" height="131" src="https://images2.imgbox.com/7e/9b/2gKGvqty_o.png" width="461"></p> 
<p>根据属性 2（如上图所示），从<img alt="N(0,\mathop I\nolimits_d )" class="mathcode" src="https://images2.imgbox.com/5f/63/rOdkG0q0_o.png">中抽取的随机样本很可能位于足够靠近给定超平面的位置。 因此，相应的语义可以通过由法向量 n 定义的线性子空间来建模。 </p> 
<p><strong>多语义</strong>。考虑有 m 不同语义的情况，有</p> 
<p class="img-center"><img alt="" height="33" src="https://images2.imgbox.com/c1/97/iYJ5f6Wb_o.png" width="331"></p> 
<p>其中 s = [s_1, . . . , s_m]^T 表示语义分数，<img alt="\Lambda = diag(\mathop \lambda \nolimits_1 , \cdots ,\mathop \lambda \nolimits_m )" class="mathcode" src="https://images2.imgbox.com/c1/7e/kptrMqkv_o.png">是包含线性系数的对角矩阵，N = [n_1, . . . . , n_m] 表示分离边界。已知随机样本 z 的分布，即<img alt="N(0,\mathop I\nolimits_d )" class="mathcode" src="https://images2.imgbox.com/01/f0/9ycJnhyI_o.png">，我们可以轻松计算语义分数 s 的均值和协方差矩阵，如下所示</p> 
<p class="img-center"><img alt="" height="95" src="https://images2.imgbox.com/f0/5f/Js6wsBuV_o.png" width="413"></p> 
<p>因此我们有<img alt="s \sim N(0,\mathop \Sigma \nolimits_s )" class="mathcode" src="https://images2.imgbox.com/59/a7/AkydbJWA_o.png">，这是一个多元正态分布。 当且仅当<img alt="\mathop \Sigma \nolimits_s" class="mathcode" src="https://images2.imgbox.com/5a/89/jpKREhPB_o.png">是对角矩阵时，s 的不同条目才相互分离，这需要法向量 {n_1, . . . , n_m} 相互正交。 如果这个条件不成立，一些语义会相互关联，<img alt="\mathop n\nolimits_i^T \mathop n\nolimits_j" class="mathcode" src="https://images2.imgbox.com/93/da/PK8PPAX1_o.png">可以用来衡量第 i 个和第 j 个语义之间的联系。 </p> 
<h3 id="2.2%C2%A0%E9%9A%90%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%9A%84%E6%93%8D%E4%BD%9C">2.2 隐空间中的操作</h3> 
<p>在这一部分中，介绍如何使用在 latent space 中建立的语义进行图像编辑。</p> 
<p><strong>单属性操作</strong>。根据等式（2），要操纵合成图像的属性，我们可以使用<img alt="\mathop z\nolimits_{edit} = z + \alpha n" class="mathcode" src="https://images2.imgbox.com/fc/83/LXF9QiFj_o.png">编辑原始 latent code z。因为分数在编辑后变为<img alt="f(g(\mathop z\nolimits_{edit} )) = f(g(z)) + \lambda \alpha" class="mathcode" src="https://images2.imgbox.com/67/9e/jj6a5YTj_o.png">，当<img alt="\alpha &gt; 0" class="mathcode" src="https://images2.imgbox.com/8c/cf/3T58H7Gt_o.png">时，这将使合成的图像在该语义上更正向。</p> 
<p><strong>条件操作</strong>。当存在多个属性时，编辑一个可能会影响另一个，因为某些语义相互耦合。 为了实现更精确的控制，通过强制等式（5）中的<img alt="\mathop N\nolimits^T N" class="mathcode" src="https://images2.imgbox.com/ab/d7/bbcCyimI_o.png">为对角阵来进行条件操作。 特别是，我们使用投影来正交化不同的向量。</p> 
<p class="img-center"><img alt="" height="225" src="https://images2.imgbox.com/b5/31/grw8UBq0_o.png" width="461"></p> 
<p>如图上图，给定两个法向量为 n_1 和 n_2 的超平面，我们找到一个投影方向<img alt="\mathop n\nolimits_1 - (\mathop n\nolimits_1^T \mathop n\nolimits_2 )" class="mathcode" src="https://images2.imgbox.com/a8/8a/jWo9W2wx_o.png">。沿着这个新方向移动样本可以改变“属性 1”而不影响“属性 2”。我们称此操作为条件操作。 如果要调节的属性不止一个，我们只需要减去从原始方向到由所有条件方向构造的平面上的投影。</p> 
<p><strong>真实图像处理</strong>。 由于我们的方法可以在固定的 GAN 模型的 latent space 进行语义编辑，因此我们需要先将真实图像映射到 latent code，然后再执行操作。为此，现有方法提出直接优化 latent code 以最小化重建损失，或者学习额外的编码器将目标图像转换到 latent space。 还有一些模型涉及编码器以及 GAN 的训练过程，我们可以直接将其用于推理。</p> 
<h2 id="3.%20%E5%AE%9E%E9%AA%8C">3. 实验</h2> 
<p>在本节中，我们使用最先进的 GAN 模型 PGGAN 和 StyleGAN 评估 InterFaceGAN。</p> 
<ul><li>3.1 节、3.2 节和3.3 节中的实验是在 PGGAN 上进行的，以解释传统生成器的 latent space 。</li><li>3.4 节中的实验在 StyleGAN 上进行，以研究基于 style 的生成器，并比较 StyleGAN 中两组 latent representation之间的差异。</li><li>3.5 节中将我们的方法应用于真实图像，以了解如何将 GAN 隐式学习的语义应用于真实面部编辑。</li></ul> 
<h3 id="3.1%C2%A0%20%E9%9A%90%E7%A9%BA%E9%97%B4%E5%88%86%E7%A6%BB">3.1  隐空间分离</h3> 
<p>如2.1 节所述，我们的框架基于这样的假设：对于任何二元属性，在 latent space 中存在一个超平面，使得来自同一侧的所有样本都具有相同的属性。 因此，我们想首先评估这个假设的正确性，以使剩余的分析具有可靠性。</p> 
<p>我们在姿态、微笑、年龄、性别、眼镜方面训练了五个独立的线性 SVM，然后在验证集（6K 个具有高置信度属性分数的样本）以及整个集（480K 个随机样本）上对其进行评估。</p> 
<p class="img-center"><img alt="" height="117" src="https://images2.imgbox.com/e4/04/HlxnyMBk_o.png" width="458"></p> 
<p>结果如上图所示。 我们发现，所有线性边界在验证集上的准确率超过 95%，在整个集合上的准确率超过 75%，这表明对于二元属性，潜在空间中存在一个线性超平面，可以很好地将数据分成两组。</p> 
<p class="img-center"><img alt="" height="297" src="https://images2.imgbox.com/7e/e1/JZiGvFEa_o.png" width="455"></p> 
<p>基于到决策边界的距离，对它们进行排序，从而可视化一些样本（如上图所示）。 请注意，那些极端情况（上图中的第一行和最后一行）不太可能被直接采样，而是通过将 latent code “无限”地沿着法线方向移动来构建。 从上图可以看出，正样本和负样本在对应的属性上是可以区分的。</p> 
<h3 id="3.2%20%E9%9A%90%E7%A9%BA%E9%97%B4%E6%93%8D%E6%8E%A7%C2%A0">3.2 隐空间操控 </h3> 
<p>在本节，我们验证由 InterFaceGAN 生成的语义是否可控。</p> 
<p class="img-center"><img alt="" height="431" src="https://images2.imgbox.com/b0/18/isYx72i6_o.png" width="650"></p> 
<p><strong>操纵单一属性</strong>。上图绘制了五个不同属性的操作结果。 这表明我们的操作方法在正向和负向的所有属性上都表现良好。特别是在姿态属性上。我们观察到，即使是通过解决双分类问题来搜索边界，移动 latent code 也会产生连续变化。 此外，虽然训练集中缺乏足够的极端姿势数据，但 GAN 能够想象侧面脸应该是什么样子。同样的情况也发生在眼镜属性上。尽管训练集中的数据不足，我们还是可以手动创建很多戴眼镜的人脸。这两个观察提供了强有力的证据，证明 GAN 不会随机生成图像，而是从潜在空间中学习一些可解释的语义。</p> 
<p><strong>语义子空间的距离效应</strong>。在操纵 latent code 时，我们观察到一个有趣的距离效应，即如果样本距离边界太远，其外观将发生剧烈变化，最终往往会变成图 3 所示的极端情况。</p> 
<p>图 5 以性别编辑为例说明了这一现象。靠近边界的操作效果很好。然而，当样本超出某个区域（门限值设为5）时，编辑结果不再像原始人脸。</p> 
<p>但这种效应并不影响我们对 latent space 中分离语义的理解。 这是因为这种极端样本不太可能直接从标准正态分布中抽取，这在第 2.1 节的性质 2 中指出。 相反，它们是通过沿特定方向不断移动正常采样的 latent code 来手动构建的。通过这种方式，我们可以更好地解释 GAN 的隐语义。</p> 
<p class="img-center"><img alt="" height="371" src="https://images2.imgbox.com/0c/b7/kqw0RFVK_o.png" width="460"></p> 
<p><strong>错误修正</strong>。我们进一步应用我们的方法来修复有时出现在合成输出中的错误。 我们手动标记 4K 个不好的合成图，然后训练一个线性 SVM 来找到分离超平面。 我们惊奇地发现 GAN 也在 latent space  中编码了这些信息。 基于这一发现，我们能够纠正 GAN 在生成过程中犯的一些错误，如上图所示。</p> 
<h3 id="3.3.%20%E6%9D%A1%E4%BB%B6%E6%93%8D%E4%BD%9C">3.3. 条件操作</h3> 
<p>本节，研究不同属性的分离，并且评估条件操作。</p> 
<p><strong>属性的相关</strong>。不同于引入感知路径长度和线性可分性来衡量 latent space 的分离特性，我们更关注不同隐语义之间的关系，并研究它们如何相互联系。 在这里，使用两个不同的指标来衡量两个属性之间的相关性。</p> 
<ul><li>n_1 和 n_2 两个方向的余弦相似性（cosine similarity）</li><li>把每一个属性分数当做随机变量，然后使用从所有 500K 个合成数据中观察到的属性分布来计算相关系数<img alt="\mathop \rho \nolimits_{\mathop A\nolimits_1 \mathop A\nolimits_2 } = \frac{​{Cov(\mathop A\nolimits_1 ,\mathop A\nolimits_2 )}}{​{\mathop \sigma \nolimits_{\mathop A\nolimits_1 } \mathop \sigma \nolimits_{\mathop A\nolimits_2 } }}" class="mathcode" src="https://images2.imgbox.com/5b/18/OG2SErFM_o.png">，其中，A_1 和 A_2 表示对应于两个属性的随机变量</li></ul> 
<p class="img-center"><img alt="" height="323" src="https://images2.imgbox.com/7e/33/k9IqXjID_o.png" width="457"></p> 
<p>结果如上图所示。我们可以看出属性在这两个指标下的行为相似，表明我们的 InterFaceGAN 能够准确识别隐藏在 latent space 中的语义。我们还发现姿势和微笑几乎与其他属性正交。然而，性别、年龄和眼镜彼此高度相关。这一观察在一定程度上反映了训练数据集（即 CelebA-HQ）中的属性相关性，其中男性老年人更可能戴眼镜。 GAN 在学习产生真实观测时也捕捉到了这一特征。 </p> 
<p><strong>条件操作</strong>。为了对独立面部属性编辑的不同语义进行分离，我们在 2.2 节中提出了条件操作。</p> 
<p class="img-center"><img alt="" height="305" src="https://images2.imgbox.com/c2/3f/Tc2uGKZF_o.png" width="650"></p> 
<p class="img-center"><img alt="" height="304" src="https://images2.imgbox.com/93/22/PSttNEE4_o.png" width="462"></p> 
<p>把一个属性作为条件，对另一个属性作进行操作，结果如上两图所示。眼镜方向向量减去其在年龄和性别方向上的投影，得到一个新向量。当样本在新方向上移动时，年龄和性别属性不会改变。</p> 
<h3 id="3.4%20%E5%9C%A8%20StyleGAN%20%E4%B8%8A%E7%9A%84%E7%BB%93%E6%9E%9C">3.4 在 StyleGAN 上的结果</h3> 
<p>与传统的 GAN 不同，StyleGAN 提出了基于样式的生成器。基本上，StyleGAN 学习将 latent code z 从 latent space Z 映射到另一个高维空间 W，然后再将其输入生成器。W 表现出比 Z 强得多的属性分离特性，因为 W 不限于任何特定分布，并且可以更好地模拟真实数据的潜在特征。</p> 
<p>我们对 StyleGAN 的 Z 空间和 W 空间进行了与 PGGAN 类似的分析，发现 W 空间确实学习了更分离的 representation。 这种分离使 W 空间在属性编辑方面比 Z 空间具有更强的优势。</p> 
<p class="img-center"><img alt="" height="284" src="https://images2.imgbox.com/dd/8b/hztpl1xR_o.png" width="650"></p> 
<p>如上图所示，年龄和眼镜在 StyleGAN 模型中相关。与 Z 空间（第二行）相比，W 空间（第一行）表现更好，尤其是在远距离（超平面）操作中。尽管如此，我们可以使用 2.2 节中描述的条件操作技巧来分离 Z 空间（第三行）中这两个属性，从而产生更好的结果。</p> 
<p>然而，这个技巧不能应用于 W 空间。我们发现 W 空间有时会捕获训练数据的属性相关性，并将它们编码为相关的“样式”。以上图为例，“年龄”和“眼镜”被设定为两个独立的语义，但 StyleGAN 实际上学习了一个包含眼镜的年龄方向，因此这个新方向在某种程度上与眼镜方向本身正交。 这样，减去几乎为零的投影，几乎不会影响最终的结果。</p> 
<h3 id="3.5%20%E7%9C%9F%E5%AE%9E%E5%9B%BE%E5%83%8F%E6%93%8D%E4%BD%9C">3.5 真实图像操作</h3> 
<p>在这一部分中，我们使用 InterFaceGAN 对真实人脸进行操作，以验证 GAN 学习的语义属性是否可以应用于真实人脸。 回想一下，InterFaceGAN 通过沿特定方向移动 latent code 来实现语义人脸编辑。 因此，我们需要首先将给定的真实图像转换为 latent code。 事实证明这是一项困难的任务，因为 GAN 不能完全捕获所有模式以及真实分布的多样性。</p> 
<p>要反转预训练的 GAN 模型，有两种典型的方法。</p> 
<ul><li>一种是基于优化的方法，它使用固定生成器直接优化潜在代码，以最小化像素重建误差。</li><li>另一种是基于编码器的，其中训练了一个额外的编码器网络来学习逆映射。</li></ul> 
<p>我们在 PGGAN 和 StyleGAN 上测试了两种 baseline 方法。</p> 
<p class="img-center"><img alt="" height="283" src="https://images2.imgbox.com/9d/03/dUMZjB73_o.png" width="650"></p> 
<p>结果如上图所示。 我们可以看出，基于优化（第一行）和基于编码器（第二行）的方法在反转（inverting）PGGAN 时表现不佳。 这可以归因于训练和测试数据分布之间的强烈差异。 例如，即使输入是东方人，该模型也倾向于生成西方人（参见上图中的右侧示例）。 然而，即使与输入不同，invert 的图像仍然可以使用 InterFaceGAN 进行语义编辑。与 PGGAN 相比，StyleGAN（第三行）的结果要好得多。 在这里，我们将逐层样式（即所有层的 w）视为优化目标。 编辑实例时，我们将所有样式代码推向同一方向。 如上图所示，我们成功地改变了真实人脸图像的属性而无需重新训练 StyleGAN，这是利用了来自 latent space 的解释语义。 </p> 
<p>我们还在编码器-解码器生成模型上测试了 InterFaceGAN，该模型在训练编码器的同时，训练生成器和鉴别器。 模型收敛后，可以直接使用编码器进行推理，将给定图像映射到 latent space。</p> 
<p class="img-center"><img alt="" height="197" src="https://images2.imgbox.com/1f/25/XgHEEIkP_o.png" width="650"></p> 
<p>我们应用我们的方法来解释最近的编码器-解码器模型 LIA 的 latent space。操作结果如上图所示。我们成功地编辑了具有多种属性的输入面孔，例如年龄和面部姿势。 这表明基于编码器-解码器的生成模型中的 latent code 也支持语义操作。 此外，与图 10（b）中编码器在 GAN 训练好后单独学习相比，与生成器一起训练的编码器提供了更好的重建和操作结果。 </p> 
<h2 id="4.%20%E7%BB%93%E8%AE%BA">4. 结论</h2> 
<p>我们提出 InterFaceGAN 来解释 GAN 的 latent space 中编码的语义。 通过利用解释的语义以及条件操作技术，可使用任何固定的 GAN 模型精确控制面部属性，甚至可以将无条件 GAN 转变为可控 GAN。大量实验表明 InterFaceGAN 也可以应用于真实图像编辑。</p> 
<h2 id="%E4%B8%80%E4%BA%9B%E6%83%B3%E6%B3%95">一些想法</h2> 
<p>如 3.2 节所示，“移动 latent code 会产生连续变化。此外，虽然训练集中缺乏足够的极端姿势数据，但 GAN 能够想象侧面脸应该是什么样子。”那么是否能基于此生成视频？或者进行data augmentation。</p> 
<p>如3.3 节所示，因为属性的相关性，某个属性方向（如眼镜）上的向量会有其他属性方向（如年龄、性别）上的分量，通过投影减去这些方向上的分量之后，得到一个新的向量，在该方向上移动不会对其他属性产生影响。</p> 
<p>如 3.4 节所示</p> 
<ul><li>W 空间是比 latent space Z 更高维的空间，该空间基于对 latent space Z 的映射而来（这让我想到了 normalizing flow，通过映射可以把简单分布变成更复杂的分布，或者把复杂的分布转换为更简单的分布）。</li><li>W 空间具有比 latent space Z 更好的属性解耦（分离）特性。直观地理解就是通过向更高维度的映射来对各种属性进行分离。在更高的维度中，会出现一些新的方向，这些方向就是通过 “减分量” 操作得到的新方向。该方向虽然无法用语言直接描述，但是确实完全独立的一个方向（属性）</li></ul> 
<p></p> 
<h2 id="%E5%8F%82%E8%80%83" style="text-align:justify;">参考</h2> 
<p style="text-align:justify;">Shen, Y., Gu, J., Tang, X., &amp; Zhou, B. (2020). Interpreting the latent space of gans for semantic face editing. In <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em> (pp. 9243-9252).</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6f5dec6d0fc804300dfa522fc5bb2c05/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">pycharm 报错处理 libpng warning: iCCP: cHRM chunk does not match sRGB</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/cfb824c5cf23e5f7f1deb453481e93b8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">matplotlib 同时显示两个figure</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>