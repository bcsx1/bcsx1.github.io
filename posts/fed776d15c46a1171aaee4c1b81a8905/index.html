<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【微服务】springboot整合kafka-stream使用详解 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【微服务】springboot整合kafka-stream使用详解" />
<meta property="og:description" content="目录
一、前言
二、kafka stream概述
2.1 什么是kafka stream
2.2 为什么需要kafka stream
2.2.1 对接成本低
2.2.2 节省资源
2.2.3 使用简单
2.3 kafka stream特点
2.4 kafka stream中的一些概念
2.5 Kafka Stream应用场景
三、环境准备
3.1 搭建zk
3.1.1 自定义docker网络
3.1.2 拉取zk镜像
3.1.3 启动zk容器
3.2 搭建kafka
3.2.1 下载kafka并解压
3.2.2 修改配置文件
3.2.3 启动kafka服务
3.3 kafka测试
3.3.1 创建topic
3.3.2 开启kafka生产端控制台
3.3.3 开启kafka消费端控制台
3.4 java客户端集成kafka测试
四、kafka stream 使用
4.1 前置准备
4.2 kafka stream应用开发步骤
4.2.1 步骤1：创建Kafka Streams 实例
4.2.2 步骤2：指定输入与输出topic
4.2.3 步骤3：启动Kafka Streams 实例" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/fed776d15c46a1171aaee4c1b81a8905/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-24T17:30:09+08:00" />
<meta property="article:modified_time" content="2023-12-24T17:30:09+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【微服务】springboot整合kafka-stream使用详解</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="-toc" style="margin-left:0px;"></p> 
<p id="%E4%B8%80%E3%80%81%E5%89%8D%E8%A8%80-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81%E5%89%8D%E8%A8%80" rel="nofollow">一、前言</a></p> 
<p id="%E4%BA%8C%E3%80%81kafka%20stream%E6%A6%82%E8%BF%B0-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81kafka%20stream%E6%A6%82%E8%BF%B0" rel="nofollow">二、kafka stream概述</a></p> 
<p id="2.1%20%E4%BB%80%E4%B9%88%E6%98%AFkafka%20stream-toc" style="margin-left:40px;"><a href="#2.1%20%E4%BB%80%E4%B9%88%E6%98%AFkafka%20stream" rel="nofollow">2.1 什么是kafka stream</a></p> 
<p id="2.2%20%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81kafka%20stream-toc" style="margin-left:40px;"><a href="#2.2%20%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81kafka%20stream" rel="nofollow">2.2 为什么需要kafka stream</a></p> 
<p id="2.2.1%20%E5%AF%B9%E6%8E%A5%E6%88%90%E6%9C%AC%E4%BD%8E-toc" style="margin-left:80px;"><a href="#2.2.1%20%E5%AF%B9%E6%8E%A5%E6%88%90%E6%9C%AC%E4%BD%8E" rel="nofollow">2.2.1 对接成本低</a></p> 
<p id="2.2.2%20%E8%8A%82%E7%9C%81%E8%B5%84%E6%BA%90-toc" style="margin-left:80px;"><a href="#2.2.2%20%E8%8A%82%E7%9C%81%E8%B5%84%E6%BA%90" rel="nofollow">2.2.2 节省资源</a></p> 
<p id="2.2.3%20%E4%BD%BF%E7%94%A8%E7%AE%80%E5%8D%95-toc" style="margin-left:80px;"><a href="#2.2.3%20%E4%BD%BF%E7%94%A8%E7%AE%80%E5%8D%95" rel="nofollow">2.2.3 使用简单</a></p> 
<p id="2.3%20kafka%20stream%E7%89%B9%E7%82%B9-toc" style="margin-left:40px;"><a href="#2.3%20kafka%20stream%E7%89%B9%E7%82%B9" rel="nofollow">2.3 kafka stream特点</a></p> 
<p id="2.4%20kafka%20stream%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5-toc" style="margin-left:40px;"><a href="#2.4%20kafka%20stream%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5" rel="nofollow">2.4 kafka stream中的一些概念</a></p> 
<p id="2.5%C2%A0Kafka%20Stream%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF-toc" style="margin-left:40px;"><a href="#2.5%C2%A0Kafka%20Stream%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF" rel="nofollow">2.5 Kafka Stream应用场景</a></p> 
<p id="%E4%B8%89%E3%80%81%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87" rel="nofollow">三、环境准备</a></p> 
<p id="3.1%20%E6%90%AD%E5%BB%BAzk-toc" style="margin-left:40px;"><a href="#3.1%20%E6%90%AD%E5%BB%BAzk" rel="nofollow">3.1 搭建zk</a></p> 
<p id="3.1.1%20%E8%87%AA%E5%AE%9A%E4%B9%89docker%E7%BD%91%E7%BB%9C-toc" style="margin-left:80px;"><a href="#3.1.1%20%E8%87%AA%E5%AE%9A%E4%B9%89docker%E7%BD%91%E7%BB%9C" rel="nofollow">3.1.1 自定义docker网络</a></p> 
<p id="3.1.2%C2%A0%E6%8B%89%E5%8F%96zk%E9%95%9C%E5%83%8F-toc" style="margin-left:80px;"><a href="#3.1.2%C2%A0%E6%8B%89%E5%8F%96zk%E9%95%9C%E5%83%8F" rel="nofollow">3.1.2 拉取zk镜像</a></p> 
<p id="3.1.3%C2%A0%E5%90%AF%E5%8A%A8zk%E5%AE%B9%E5%99%A8-toc" style="margin-left:80px;"><a href="#3.1.3%C2%A0%E5%90%AF%E5%8A%A8zk%E5%AE%B9%E5%99%A8" rel="nofollow">3.1.3 启动zk容器</a></p> 
<p id="3.2%20%E6%90%AD%E5%BB%BAkafka-toc" style="margin-left:40px;"><a href="#3.2%20%E6%90%AD%E5%BB%BAkafka" rel="nofollow">3.2 搭建kafka</a></p> 
<p id="3.2.1%20%E4%B8%8B%E8%BD%BDkafka%E5%B9%B6%E8%A7%A3%E5%8E%8B-toc" style="margin-left:80px;"><a href="#3.2.1%20%E4%B8%8B%E8%BD%BDkafka%E5%B9%B6%E8%A7%A3%E5%8E%8B" rel="nofollow">3.2.1 下载kafka并解压</a></p> 
<p id="3.2.2%20%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-toc" style="margin-left:80px;"><a href="#3.2.2%20%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6" rel="nofollow">3.2.2 修改配置文件</a></p> 
<p id="3.2.3%20%E5%90%AF%E5%8A%A8kafka%E6%9C%8D%E5%8A%A1-toc" style="margin-left:80px;"><a href="#3.2.3%20%E5%90%AF%E5%8A%A8kafka%E6%9C%8D%E5%8A%A1" rel="nofollow">3.2.3 启动kafka服务</a></p> 
<p id="3.3%20kafka%E6%B5%8B%E8%AF%95-toc" style="margin-left:40px;"><a href="#3.3%20kafka%E6%B5%8B%E8%AF%95" rel="nofollow">3.3 kafka测试</a></p> 
<p id="3.3.1%20%E5%88%9B%E5%BB%BAtopic-toc" style="margin-left:80px;"><a href="#3.3.1%20%E5%88%9B%E5%BB%BAtopic" rel="nofollow">3.3.1 创建topic</a></p> 
<p id="3.3.2%20%E5%BC%80%E5%90%AFkafka%E7%94%9F%E4%BA%A7%E7%AB%AF%E6%8E%A7%E5%88%B6%E5%8F%B0-toc" style="margin-left:80px;"><a href="#3.3.2%20%E5%BC%80%E5%90%AFkafka%E7%94%9F%E4%BA%A7%E7%AB%AF%E6%8E%A7%E5%88%B6%E5%8F%B0" rel="nofollow">3.3.2 开启kafka生产端控制台</a></p> 
<p id="3.3.3%20%E5%BC%80%E5%90%AFkafka%E6%B6%88%E8%B4%B9%E7%AB%AF%E6%8E%A7%E5%88%B6%E5%8F%B0-toc" style="margin-left:80px;"><a href="#3.3.3%20%E5%BC%80%E5%90%AFkafka%E6%B6%88%E8%B4%B9%E7%AB%AF%E6%8E%A7%E5%88%B6%E5%8F%B0" rel="nofollow">3.3.3 开启kafka消费端控制台</a></p> 
<p id="3.4%20java%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%9B%86%E6%88%90kafka%E6%B5%8B%E8%AF%95-toc" style="margin-left:40px;"><a href="#3.4%20java%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%9B%86%E6%88%90kafka%E6%B5%8B%E8%AF%95" rel="nofollow">3.4 java客户端集成kafka测试</a></p> 
<p id="%E5%9B%9B%E3%80%81kafka%20stream%20%E4%BD%BF%E7%94%A8-toc" style="margin-left:0px;"><a href="#%E5%9B%9B%E3%80%81kafka%20stream%20%E4%BD%BF%E7%94%A8" rel="nofollow">四、kafka stream 使用</a></p> 
<p id="4.1%20%E5%89%8D%E7%BD%AE%E5%87%86%E5%A4%87-toc" style="margin-left:40px;"><a href="#4.1%20%E5%89%8D%E7%BD%AE%E5%87%86%E5%A4%87" rel="nofollow">4.1 前置准备</a></p> 
<p id="4.2%20kafka%20stream%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91%E6%AD%A5%E9%AA%A4-toc" style="margin-left:40px;"><a href="#4.2%20kafka%20stream%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91%E6%AD%A5%E9%AA%A4" rel="nofollow">4.2 kafka stream应用开发步骤</a></p> 
<p id="4.2.1%20%E6%AD%A5%E9%AA%A41%EF%BC%9A%E5%88%9B%E5%BB%BAKafka%20Streams%20%E5%AE%9E%E4%BE%8B-toc" style="margin-left:80px;"><a href="#4.2.1%20%E6%AD%A5%E9%AA%A41%EF%BC%9A%E5%88%9B%E5%BB%BAKafka%20Streams%20%E5%AE%9E%E4%BE%8B" rel="nofollow">4.2.1 步骤1：创建Kafka Streams 实例</a></p> 
<p id="4.2.2%20%E6%AD%A5%E9%AA%A42%EF%BC%9A%E6%8C%87%E5%AE%9A%E8%BE%93%E5%85%A5%E4%B8%8E%E8%BE%93%E5%87%BAtopic-toc" style="margin-left:80px;"><a href="#4.2.2%20%E6%AD%A5%E9%AA%A42%EF%BC%9A%E6%8C%87%E5%AE%9A%E8%BE%93%E5%85%A5%E4%B8%8E%E8%BE%93%E5%87%BAtopic" rel="nofollow">4.2.2 步骤2：指定输入与输出topic</a></p> 
<p id="4.2.3%20%E6%AD%A5%E9%AA%A43%EF%BC%9A%E5%90%AF%E5%8A%A8Kafka%20Streams%20%E5%AE%9E%E4%BE%8B-toc" style="margin-left:80px;"><a href="#4.2.3%20%E6%AD%A5%E9%AA%A43%EF%BC%9A%E5%90%AF%E5%8A%A8Kafka%20Streams%20%E5%AE%9E%E4%BE%8B" rel="nofollow">4.2.3 步骤3：启动Kafka Streams 实例</a></p> 
<p id="4.3%20kafka%20stream%E6%93%8D%E4%BD%9C%E6%A1%88%E4%BE%8B-toc" style="margin-left:40px;"><a href="#4.3%20kafka%20stream%E6%93%8D%E4%BD%9C%E6%A1%88%E4%BE%8B" rel="nofollow">4.3 kafka stream操作案例</a></p> 
<p id="4.3.1%20%E8%BD%AC%E6%8D%A2%E5%8D%95%E8%AF%8D%E5%A4%A7%E5%B0%8F%E5%86%99-toc" style="margin-left:80px;"><a href="#4.3.1%20%E8%BD%AC%E6%8D%A2%E5%8D%95%E8%AF%8D%E5%A4%A7%E5%B0%8F%E5%86%99" rel="nofollow">4.3.1 转换单词大小写</a></p> 
<p id="4.3.2%20%E5%B0%86topic1%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5%E5%88%B0topic2%E4%B8%AD-toc" style="margin-left:80px;"><a href="#4.3.2%20%E5%B0%86topic1%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5%E5%88%B0topic2%E4%B8%AD" rel="nofollow">4.3.2 将topic1数据写入到topic2中</a></p> 
<p id="4.3.3%20%E7%BB%9F%E8%AE%A1wordcount-toc" style="margin-left:80px;"><a href="#4.3.3%20%E7%BB%9F%E8%AE%A1wordcount" rel="nofollow">4.3.3 统计wordcount</a></p> 
<p id="4.4%20kafka%20stream%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0%E4%BD%BF%E7%94%A8-toc" style="margin-left:40px;"><a href="#4.4%20kafka%20stream%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0%E4%BD%BF%E7%94%A8" rel="nofollow">4.4 kafka stream窗口函数使用</a></p> 
<p id="4.4.1%20%E9%9C%80%E6%B1%82%E4%B8%80%EF%BC%8C%E5%9B%BA%E5%AE%9A%E6%97%B6%E9%97%B4%E8%BE%93%E5%87%BA%E7%BB%9F%E8%AE%A1%E7%BB%93%E6%9E%9C%E5%88%B0%E5%8F%A6%E4%B8%80%E4%B8%AAtopic-toc" style="margin-left:80px;"><a href="#4.4.1%20%E9%9C%80%E6%B1%82%E4%B8%80%EF%BC%8C%E5%9B%BA%E5%AE%9A%E6%97%B6%E9%97%B4%E8%BE%93%E5%87%BA%E7%BB%9F%E8%AE%A1%E7%BB%93%E6%9E%9C%E5%88%B0%E5%8F%A6%E4%B8%80%E4%B8%AAtopic" rel="nofollow">4.4.1 需求一，固定时间输出统计结果到另一个topic</a></p> 
<p id="4.4.2%20%E9%9C%80%E6%B1%82%E4%BA%8C%EF%BC%8C%E7%BB%9F%E8%AE%A1topic1%E4%B8%AD10%E7%A7%92%E5%86%85%E7%9A%84wordcount%E5%86%99%E5%88%B0topic2-toc" style="margin-left:80px;"><a href="#4.4.2%20%E9%9C%80%E6%B1%82%E4%BA%8C%EF%BC%8C%E7%BB%9F%E8%AE%A1topic1%E4%B8%AD10%E7%A7%92%E5%86%85%E7%9A%84wordcount%E5%86%99%E5%88%B0topic2" rel="nofollow">4.4.2 需求二，统计topic1中10秒内的wordcount写到topic2</a></p> 
<p id="4.5%20Kafka%20Streams%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%E6%8B%93%E5%B1%95-toc" style="margin-left:40px;"><a href="#4.5%20Kafka%20Streams%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%E6%8B%93%E5%B1%95" rel="nofollow">4.5 Kafka Streams使用场景拓展</a></p> 
<p id="4.5.1%20%E4%BA%8B%E4%BB%B6%E6%97%A5%E5%BF%97%E7%9B%91%E6%8E%A7-toc" style="margin-left:80px;"><a href="#4.5.1%20%E4%BA%8B%E4%BB%B6%E6%97%A5%E5%BF%97%E7%9B%91%E6%8E%A7" rel="nofollow">4.5.1 事件日志监控</a></p> 
<p id="4.5.2%20%E4%BA%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90-toc" style="margin-left:80px;"><a href="#4.5.2%20%E4%BA%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90" rel="nofollow">4.5.2 事用户行为统计分析</a></p> 
<p id="4.5.3%20%E6%95%B0%E6%8D%AE%E8%81%9A%E5%90%88%E4%B8%8E%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97-toc" style="margin-left:80px;"><a href="#4.5.3%20%E6%95%B0%E6%8D%AE%E8%81%9A%E5%90%88%E4%B8%8E%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97" rel="nofollow">4.5.3 数据聚合与实时计算</a></p> 
<p id="4.5.4%20%E5%AE%9E%E6%97%B6%E6%8E%A8%E8%8D%90-toc" style="margin-left:80px;"><a href="#4.5.4%20%E5%AE%9E%E6%97%B6%E6%8E%A8%E8%8D%90" rel="nofollow">4.5.4 实时推荐</a></p> 
<p id="4.5.5%20%E5%AE%9E%E6%97%B6%E5%91%8A%E8%AD%A6-toc" style="margin-left:80px;"><a href="#4.5.5%20%E5%AE%9E%E6%97%B6%E5%91%8A%E8%AD%A6" rel="nofollow">4.5.5 实时告警</a></p> 
<p id="4.5.6%20%E5%BA%94%E7%94%A8%E8%A7%A3%E8%80%A6-toc" style="margin-left:80px;"><a href="#4.5.6%20%E5%BA%94%E7%94%A8%E8%A7%A3%E8%80%A6" rel="nofollow">4.5.6 应用解耦</a></p> 
<p id="%E4%BA%94%E3%80%81kafka%20stream%E6%95%B4%E5%90%88springboot-toc" style="margin-left:0px;"><a href="#%E4%BA%94%E3%80%81kafka%20stream%E6%95%B4%E5%90%88springboot" rel="nofollow">五、kafka stream整合springboot</a></p> 
<p id="5.1%20%E6%95%B4%E5%90%88%E8%BF%87%E7%A8%8B-toc" style="margin-left:40px;"><a href="#5.1%20%E6%95%B4%E5%90%88%E8%BF%87%E7%A8%8B" rel="nofollow">5.1 整合过程</a></p> 
<p id="5.1.1%20%E5%AF%BC%E5%85%A5springboot%E7%9B%B8%E5%85%B3%E4%BE%9D%E8%B5%96-toc" style="margin-left:80px;"><a href="#5.1.1%20%E5%AF%BC%E5%85%A5springboot%E7%9B%B8%E5%85%B3%E4%BE%9D%E8%B5%96" rel="nofollow">5.1.1 导入springboot相关依赖</a></p> 
<p id="5.1.2%20%E9%85%8D%E7%BD%AEkafka%E7%9B%B8%E5%85%B3%E4%BF%A1%E6%81%AF-toc" style="margin-left:80px;"><a href="#5.1.2%20%E9%85%8D%E7%BD%AEkafka%E7%9B%B8%E5%85%B3%E4%BF%A1%E6%81%AF" rel="nofollow">5.1.2 配置kafka相关信息</a></p> 
<p id="5.1.3%C2%A0%E6%B7%BB%E5%8A%A0Kafka%20Stream%E9%85%8D%E7%BD%AE%E7%B1%BB-toc" style="margin-left:80px;"><a href="#5.1.3%C2%A0%E6%B7%BB%E5%8A%A0Kafka%20Stream%E9%85%8D%E7%BD%AE%E7%B1%BB" rel="nofollow">5.1.3 添加Kafka Stream配置类</a></p> 
<p id="5.1.4%C2%A0%E8%87%AA%E5%AE%9A%E4%B9%89Kafka%20Stream%E4%B8%9A%E5%8A%A1%E5%A4%84%E7%90%86%E7%9B%91%E5%90%AC%E5%99%A8-toc" style="margin-left:80px;"><a href="#5.1.4%C2%A0%E8%87%AA%E5%AE%9A%E4%B9%89Kafka%20Stream%E4%B8%9A%E5%8A%A1%E5%A4%84%E7%90%86%E7%9B%91%E5%90%AC%E5%99%A8" rel="nofollow">5.1.4 自定义Kafka Stream业务处理监听器</a></p> 
<p id="5.1.5%20%E6%95%88%E6%9E%9C%E6%B5%8B%E8%AF%95-toc" style="margin-left:80px;"><a href="#5.1.5%20%E6%95%88%E6%9E%9C%E6%B5%8B%E8%AF%95" rel="nofollow">5.1.5 效果测试</a></p> 
<p id="%E5%85%AD%E3%80%81%E5%86%99%E5%9C%A8%E6%96%87%E6%9C%AB-toc" style="margin-left:0px;"><a href="#%E5%85%AD%E3%80%81%E5%86%99%E5%9C%A8%E6%96%87%E6%9C%AB" rel="nofollow">六、写在文末</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="%E4%B8%80%E3%80%81%E5%89%8D%E8%A8%80">一、前言</h2> 
<p>随着大数据技术的发展越来越成熟，大数据涉及的领域也越来越多，从以往的T+1到如今的实时处理，得益于底层技术的强大支撑，尤其是流式计算技术的发展让众多的业务场景价值得以深度挖掘，聊到流式计算，涌入入脑海中的Spark Streaming，Flink等，本文接下来将介绍另一种流式计算技术kafka stream。</p> 
<p></p> 
<h2 id="%E4%BA%8C%E3%80%81kafka%20stream%E6%A6%82%E8%BF%B0">二、kafka stream概述</h2> 
<h4></h4> 
<h3 id="2.1%20%E4%BB%80%E4%B9%88%E6%98%AFkafka%20stream">2.1 什么是kafka stream</h3> 
<p>Kafka Stream是一款开源、<a href="https://so.csdn.net/so/search?q=%E5%88%86%E5%B8%83%E5%BC%8F&amp;spm=1001.2101.3001.7020" title="分布式">分布式</a>和水平扩展的流处理平台，其在Apache Kafka之上进行构建，借助其高性能、可伸缩性和容错性，可以实现高效的流处理应用程序。</p> 
<p></p> 
<h3 id="2.2%20%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81kafka%20stream">2.2 为什么需要kafka stream</h3> 
<p>在处理流式计算的场景中，发展到今天出现了很多成熟的性能高效的技术框架，比如老牌的Apache Storm，大数据处理框架Spark Streaming，Flink等，而且像Spark 与flink都能与SQL紧密结合，集成便捷，功能也很强大，为何还需要kafka stream呢？</p> 
<p></p> 
<h4 id="2.2.1%20%E5%AF%B9%E6%8E%A5%E6%88%90%E6%9C%AC%E4%BD%8E">2.2.1 对接成本低</h4> 
<blockquote> 
 <p>kafka可以说在很多互联网公司都有着广泛的使用，只要维护了kafka的环境，即可集成和使用kafka stream。</p> 
</blockquote> 
<p></p> 
<h4 id="2.2.2%20%E8%8A%82%E7%9C%81%E8%B5%84%E6%BA%90">2.2.2 节省资源</h4> 
<blockquote> 
 <p>相比于部署spark，storm等这样的大数据处理框架需要的计算资源，部署kafka占用的服务器资源更少，而且维护起来也相对节省人力。</p> 
</blockquote> 
<p></p> 
<h4 id="2.2.3%20%E4%BD%BF%E7%94%A8%E7%AE%80%E5%8D%95">2.2.3 使用简单</h4> 
<blockquote> 
 <p>相比与spark和flink这样的大数据框架，kafka在日常的开发中接触和使用会更多，学习和上手成本会低很多。  </p> 
</blockquote> 
<p></p> 
<h3 id="2.3%20kafka%20stream%E7%89%B9%E7%82%B9">2.3 kafka stream特点</h3> 
<p>Kafka Stream是Apache Kafka从0.10版本引入的一个新Feature。它是提供了对存储于Kafka内的数据进行流式处理和分析的功能。具有如下特点：</p> 
<ul><li> <p>Kafka Stream提供了一个非常简单而轻量的Library，可以方便的嵌入任意Java应用中，也可以任意方式打包和部署；</p> </li><li> <p>充分利用Kafka分区机制实现水平扩展和顺序性保证；</p> </li><li> <p>提供记录级的处理能力，从而实现毫秒级的低延迟；</p> </li><li> <p>支持基于事件时间的窗口操作，并且可处理晚到的数据（late arrival of records），这点与spark和flink中的时间窗口处理机制很像；</p> </li><li> <p>提供底层的处理原语Processor（类似于Storm的spout和bolt），以及高层抽象的DSL（类似于Spark的map/group/reduce）；</p> </li><li> <p>通过可容错的state store实现高效的状态操作（如windowed join和aggregation）；</p> </li><li> <p>除了Kafka外，无任何外部依赖，且支持正好一次处理语义；</p> </li></ul> 
<p></p> 
<h3 id="2.4%20kafka%20stream%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5">2.4 kafka stream中的一些概念</h3> 
<p>在kafka stream中，KStream和KTable是理解kafka stream时非常核心的两个概念。</p> 
<p></p> 
<p><strong>KStream</strong></p> 
<blockquote> 
 <p>KStream是一个数据流，是一段顺序的、可以无限长、不断更新的数据集，可以认为所有的记录都通过Insert only的方式插入进这个数据流中。</p> 
</blockquote> 
<p></p> 
<p><strong>KTable</strong></p> 
<blockquote> 
 <p>KTable代表一个完整的数据集，可对照mysql理解为数据库中的表。每条记录都有KV键值对，key可理解为数据库中的主键，是唯一的，而value代表一条记录，记录通常是一段可序列化的字符串。可以认为KTable中的数据时通过Update only的方式进入的。如果是相同的key，会覆盖掉原来那条记录。</p> 
</blockquote> 
<p><strong>综上来说：</strong></p> 
<ul><li> <p>KStream是数据流，即不断传输过来的流式数据记录，以Insert only的方式不断插入；</p> </li><li> <p>KTable是数据集（逻辑概念），相同key的数据只保留最新的记录，也就是Update only；</p> </li></ul> 
<p></p> 
<h3 id="2.5%C2%A0Kafka%20Stream%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF">2.5 Kafka Stream应用场景</h3> 
<p>Kafka Streams主要用于以下应用场景：</p> 
<ul><li>实时数据处理，通过实时流计算，对数据进行快速分析和处理，或者处理之后转交下游应用；</li><li>流式ETL，将数据从一个数据源抽取到另一个数据源，或将数据进行转换、清洗和聚合操作；</li><li>流-表格Join：将一条流数据与一个表进行关联查询，实现实时查询和联合分析；</li><li>行为数据统计分析与推荐，在电商场景中，通过接收用户行为日志数据进行分析计算从而为用户推荐提供数据支撑；</li></ul> 
<p></p> 
<h2 id="%E4%B8%89%E3%80%81%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87">三、环境准备</h2> 
<p>在开始使用kafka stream之前，先快速搭建起kafka的环境，参照下面的步骤快速部署kafka的环境。</p> 
<p></p> 
<h3 id="3.1%20%E6%90%AD%E5%BB%BAzk">3.1 搭建zk</h3> 
<h4 id="3.1.1%20%E8%87%AA%E5%AE%9A%E4%B9%89docker%E7%BD%91%E7%BB%9C">3.1.1 自定义docker网络</h4> 
<pre><code class="language-bash">docker network create zk-kafka --driver bridge</code></pre> 
<h4 id="3.1.2%C2%A0%E6%8B%89%E5%8F%96zk%E9%95%9C%E5%83%8F">3.1.2 拉取zk镜像</h4> 
<pre><code class="language-bash">docker pull zookeeper:3.8.1</code></pre> 
<h4 id="3.1.3%C2%A0%E5%90%AF%E5%8A%A8zk%E5%AE%B9%E5%99%A8">3.1.3 启动zk容器</h4> 
<pre><code class="language-bash">docker run -d --name zk-server -p 2181:2181 --network zk-kafka -e ALLOW_ANONYMOUS_LOGIN=yes zookeeper:3.8.1 </code></pre> 
<p style="text-align:center;"> <img alt="" src="https://images2.imgbox.com/9c/2a/uAZUBDMM_o.png"></p> 
<p></p> 
<h3 id="3.2%20%E6%90%AD%E5%BB%BAkafka">3.2 搭建kafka</h3> 
<h4 id="3.2.1%20%E4%B8%8B%E8%BD%BDkafka%E5%B9%B6%E8%A7%A3%E5%8E%8B">3.2.1 下载kafka并解压</h4> 
<p>下载地址：<a href="https://kafka.apache.org/downloads" rel="nofollow" title="Apache Kafka">Apache Kafka</a>，这里我使用 kafka_2.12-3.1.1.tgz</p> 
<pre><code class="language-bash">tar -zxvf  kafka_2.12-3.1.1.tgz

cd kafka_2.12-3.1.1

mkdir logs</code></pre> 
<p style="text-align:center;"><img alt="" class="left" src="https://images2.imgbox.com/b8/07/DeZUrKpB_o.png"></p> 
<p></p> 
<h4 id="3.2.2%20%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">3.2.2 修改配置文件</h4> 
<p>进到config目录下，找到server.properties配置文件，主要修改下面几个核心配置即可（覆盖原有的默认的配置参数）</p> 
<blockquote> 
 <p>broker.id=0</p> 
 <p>listeners=PLAINTEXT://云服务器内网IP:9092<br> zookeeper.connect=内外网均可，如果不对外暴露使用内网IP:2181<br> log.dirs=/usr/local/kafka/kafka_2.12-3.1.1/logs<br> advertised.listeners=PLAINTEXT://外网IP:9092</p> 
</blockquote> 
<p><strong>参数说明： </strong></p> 
<ul><li>listeners=PLAINTEXT://云服务器内网ip:9092，如果是云服务器，一定要配置成内网IP；</li><li>advertised.listeners=PLAINTEXT://云服务器公网ip:9092，若要远程访问需配置此项为云服务器的公网ip；</li></ul> 
<h4 id="3.2.3%20%E5%90%AF%E5%8A%A8kafka%E6%9C%8D%E5%8A%A1">3.2.3 启动kafka服务</h4> 
<p>在主目录下，使用下面的命令启动kafka服务前台启动</p> 
<pre><code class="language-bash">./bin/kafka-server-start.sh ./config/server.properties</code></pre> 
<p>或者使用下面的命令后台启动</p> 
<pre><code class="language-bash">./bin/kafka-server-start.sh -daemon ./config/server.properties</code></pre> 
<p style="text-align:center;"><img alt="" class="left" src="https://images2.imgbox.com/be/34/bPHJWUrl_o.png"></p> 
<p></p> 
<h3 id="3.3%20kafka%E6%B5%8B%E8%AF%95">3.3 kafka测试</h3> 
<p>kafka服务启动之后，接下来创建一个测试用的topic并测试是否能够正常生产和消费消息</p> 
<h4 id="3.3.1%20%E5%88%9B%E5%BB%BAtopic">3.3.1 创建topic</h4> 
<p>使用下面的命令创建一个名为zcy的topic</p> 
<pre><code class="language-bash">bin/kafka-topics.sh --create --topic zcy --bootstrap-server 公网IP:9092</code></pre> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/e0/b8/761TiofB_o.png"></p> 
<p></p> 
<h4 id="3.3.2%20%E5%BC%80%E5%90%AFkafka%E7%94%9F%E4%BA%A7%E7%AB%AF%E6%8E%A7%E5%88%B6%E5%8F%B0">3.3.2 开启kafka生产端控制台</h4> 
<p>使用下面的命令，开启一个生产者的控制台窗口，并发送一条消息</p> 
<pre><code class="language-bash">bin/kafka-console-producer.sh --broker-list 公网IP:9092 --topic zcy</code></pre> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/b7/c1/bZP3L598_o.png"></p> 
<h4 id="3.3.3%20%E5%BC%80%E5%90%AFkafka%E6%B6%88%E8%B4%B9%E7%AB%AF%E6%8E%A7%E5%88%B6%E5%8F%B0">3.3.3 开启kafka消费端控制台</h4> 
<p>使用下面的命令，开启一个消费端的控制台窗口，检查是否能够正常消费消息</p> 
<pre><code class="language-bash">bin/kafka-console-consumer.sh --bootstrap-server 公网IP:9092 --topic zcy
或者
bin/kafka-console-consumer.sh --bootstrap-server 公网IP:9092 --topic zcy --from-beginning</code></pre> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/96/e4/K9GGVY2M_o.png"></p> 
<h3 id="3.4%20java%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%9B%86%E6%88%90kafka%E6%B5%8B%E8%AF%95">3.4 java客户端集成kafka测试</h3> 
<p>引入kafka的客户端依赖</p> 
<pre><code class="language-java">        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
            &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
        &lt;/dependency&gt;</code></pre> 
<p>编写如下的测试代码，向上述kafka的zcy这个topic中发送一条消息</p> 
<pre><code class="language-java">public static void main(String[] args) throws Exception {

        // 1. 创建 kafka 生产者的配置对象
        Properties properties = new Properties();
        // 2. 给 kafka 配置对象添加配置信息：bootstrap.servers
        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "公网IP:9092");
        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");
        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");

        // 3. 创建 kafka 生产者对象
        KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;String, String&gt;(properties);
        System.out.println("开始发送数据");
        // 4. 调用 send 方法,发送消息
        for (int i = 0; i &lt; 5; i++) {
            kafkaProducer.send(new ProducerRecord&lt;&gt;("zcy","congge " + i));
        }
        // 5. 关闭资源
        kafkaProducer.close();
    }</code></pre> 
<p>运行上面的代码，运行成功后，可以看到上面的kafka的消费端的控制台正确接收到了5条消息</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/e6/44/3YJUz8Ic_o.png"></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/e3/72/6DEGWbLj_o.png"></p> 
<p></p> 
<h2 id="%E5%9B%9B%E3%80%81kafka%20stream%20%E4%BD%BF%E7%94%A8">四、kafka stream 使用</h2> 
<p>介绍了kafka stream的相关概念之后，接下来通过一些案例感受下如何使用</p> 
<h3 id="4.1%20%E5%89%8D%E7%BD%AE%E5%87%86%E5%A4%87">4.1 前置准备</h3> 
<p>创建一个maven工程，引入如下依赖</p> 
<pre><code class="language-java">    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;2.3.4.RELEASE&lt;/version&gt;
    &lt;/parent&gt;

    &lt;dependencies&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;
            &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
                    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
            &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;
            &lt;artifactId&gt;fastjson&lt;/artifactId&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
            &lt;artifactId&gt;kafka-streams&lt;/artifactId&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;connect-json&lt;/artifactId&gt;
                    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
                    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;

    &lt;/dependencies&gt;</code></pre> 
<p></p> 
<p>再创建另一个topic</p> 
<pre><code class="language-bash">bin/kafka-console-consumer.sh --bootstrap-server IP:9092 --topic zcy-out</code></pre> 
<h3 id="4.2%20kafka%20stream%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91%E6%AD%A5%E9%AA%A4">4.2 kafka stream应用开发步骤</h3> 
<p>使用kafka stream进行应用的业务开发，即相关的API使用，按照下面几步操作：</p> 
<h4 id="4.2.1%20%E6%AD%A5%E9%AA%A41%EF%BC%9A%E5%88%9B%E5%BB%BAKafka%20Streams%20%E5%AE%9E%E4%BE%8B">4.2.1 步骤1：创建Kafka Streams 实例</h4> 
<pre><code class="language-java">Properties props = new Properties();
props.put(StreamsConfig.APPLICATION_ID_CONFIG, "定义本次实例名称，保持全局唯一");
props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "kafka连接IP地址:9092");
//... 更多其他的属性可以点击到StreamsConfig配置类进行查看
StreamsBuilder builder = new StreamsBuilder();
KafkaStreams streams = new KafkaStreams(builder.build(), props);</code></pre> 
<p><strong>参数说明：</strong></p> 
<ul><li> <p><code>props.put(StreamsConfig.APPLICATION_ID_CONFIG, "my-stream")</code> 指定本次流处理应用的唯一标识符；</p> </li><li> <p><code>props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092")</code> 指定连接的 Kafka 集群的地址；</p> </li><li> <p><code>StreamsBuilder builder = new StreamsBuilder()</code> 创建 StreamsBuilder 实例，并用其构建 TOPOLOGY；</p> </li></ul> 
<h4 id="4.2.2%20%E6%AD%A5%E9%AA%A42%EF%BC%9A%E6%8C%87%E5%AE%9A%E8%BE%93%E5%85%A5%E4%B8%8E%E8%BE%93%E5%87%BAtopic">4.2.2 步骤2：指定输入与输出topic</h4> 
<pre><code class="language-java">final String inputTopic = "topic-input";
final String outputTopic = "topic-output";
KStream&lt;String, String&gt; inputStream = builder.stream(inputTopic);
//从input-topic中拿到数据进行逻辑处理
KStream&lt;String, String&gt; outputStream = inputStream.mapValues(value -&gt; value.toUpperCase());
//将处理后的数据输出到其他的topic中
outputStream.to(outputTopic);</code></pre> 
<h4 id="4.2.3%20%E6%AD%A5%E9%AA%A43%EF%BC%9A%E5%90%AF%E5%8A%A8Kafka%20Streams%20%E5%AE%9E%E4%BE%8B">4.2.3 步骤3：启动Kafka Streams 实例</h4> 
<pre><code class="language-bash">streams.start();</code></pre> 
<p>以上几步可以说是Kafka Streams编程的一种固定的方法模板，需重点关注。</p> 
<p></p> 
<h3 id="4.3%20kafka%20stream%E6%93%8D%E4%BD%9C%E6%A1%88%E4%BE%8B">4.3 kafka stream操作案例</h3> 
<h4 id="4.3.1%20%E8%BD%AC%E6%8D%A2%E5%8D%95%E8%AF%8D%E5%A4%A7%E5%B0%8F%E5%86%99">4.3.1 转换单词大小写</h4> 
<p>业务场景如下，从topic1中接收到消息，将消息内容转换为大写之后，输出到topic2</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/d2/11/r1LYozAV_o.png"></p> 
<p>完整的代码如下：</p> 
<pre><code class="language-java">public static void main(String[] args) {
        Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "stream-convert-app");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "IP:9092");
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG,Serdes.String().getClass());

        StreamsBuilder builder = new StreamsBuilder();
        KStream&lt;String, String&gt; inputStream = builder.stream("zcy");
        KStream&lt;String, String&gt; outputStream = inputStream
                .mapValues(value -&gt; value.toUpperCase());
        outputStream.to("zcy-out", Produced.with(Serdes.String(), Serdes.String()));
        KafkaStreams streams = new KafkaStreams(builder.build(), props);
        streams.start();
    }</code></pre> 
<p>运行代码之前，我们将zcy-out这个topic的消费端的终端打开，便于看到程序中处理之后的结果</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/2b/a3/YbYEwl3R_o.png"></p> 
<p>运行上面的程序，通过观察控制台日志可以发现当前处于等待接收消息输入的状态</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/65/88/3zx1dSa2_o.png"></p> 
<p>由于之前zcy这个topic中已经有消息了，可以看到，经过程序的处理，窗口中能够获取到之前的消息，并且已经将消息转为大写了</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/fe/bc/lXeCTVei_o.png"></p> 
<p>此时通过生产端的控制台发送一条消息，然后再在zcy-out消息控制台中就能近乎实时看到被转换后的消息了</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/80/87/2FkwoR9x_o.png"></p> 
<p>注意：如果实际业务中想适当节省计算资源，即不需要实时计算，而是间隔计算之后提交结果，可以通过设置下面的这个参数，即3秒提交一次结果</p> 
<pre><code class="language-bash">prop.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG,3000);  //提交时间设置为3秒</code></pre> 
<h4 id="4.3.2%20%E5%B0%86topic1%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5%E5%88%B0topic2%E4%B8%AD">4.3.2 将topic1数据写入到topic2中</h4> 
<p>业务场景如下，topic1接收外部消息，然后转发到topic2中</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/56/43/8GHh81SS_o.png"></p> 
<p>实际开发中，可能需要将原始的消息经过简单的处理之后发到另一个topic中，以供后面的业务使用，可以考虑使用下面这种方式</p> 
<pre><code class="language-java">public class StreamCopy {

    public static void main(String[] args) {
        Properties prop =new Properties();
        prop.put(StreamsConfig.APPLICATION_ID_CONFIG,"copy-stream");
        prop.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG,"IP:9092");
        prop.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        prop.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG,Serdes.String().getClass());

        prop.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG,3000);

        StreamsBuilder builder = new StreamsBuilder();
        KStream&lt;String, String&gt; inputStream = builder.stream("zcy");
        inputStream.to("zcy-out", Produced.with(Serdes.String(), Serdes.String()));
        KafkaStreams streams = new KafkaStreams(builder.build(), prop);
        streams.start();
    }

}</code></pre> 
<p>运行代码之后，仍然采用上面的方式做测试，在zcy的生产者窗口发送一条消息，可以看到zcy-out</p> 
<p>中接收到相同的消息</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/f8/bf/EWksvS9x_o.png"></p> 
<h4 id="4.3.3%20%E7%BB%9F%E8%AE%A1wordcount">4.3.3 统计wordcount</h4> 
<p>需求场景如下，通过kafka stream将第一个topic中接收到的消息经过计算之后输出到topic2中</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/1a/6b/iRmMgMva_o.png"></p> 
<p>完整代码如下</p> 
<pre><code class="language-java">public class KafkaStreamWordCount {

    public static void main(String[] args) {
        //kafka的配置
        Properties prop = new Properties();
        prop.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "IP:9092");
        prop.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        prop.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        prop.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-count");

        StreamsBuilder streamsBuilder = new StreamsBuilder();

        KStream&lt;String, String&gt; stream = streamsBuilder.stream("zcy");
        stream.flatMapValues(new ValueMapper&lt;String, Iterable&lt;String&gt;&gt;() {
            @Override
            public Iterable&lt;String&gt; apply(String value) {
                return Arrays.asList(value.split(" "));
            }
        })
                //按照value进行聚合处理
                .groupBy((key, value) -&gt; value)
                //时间窗口
                .windowedBy(TimeWindows.of(Duration.ofSeconds(10)))
                //统计单词的个数
                .count()
                //转换为kStream
                .toStream()
                .map((key, value) -&gt; {
                    System.out.println("key:" + key + " ,vlaue:" + value);
                    return new KeyValue&lt;&gt;(key.key().toString(), value.toString());
                })
                //发送消息
                .to("zcy-out");

        KafkaStreams kafkaStreams = new KafkaStreams(streamsBuilder.build(), prop);
        kafkaStreams.start();
    }

}</code></pre> 
<h3 id="4.4%20kafka%20stream%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0%E4%BD%BF%E7%94%A8">4.4 kafka stream窗口函数使用</h3> 
<p>窗口函数在很多技术框架中都有着广泛的使用，比如spark,flink,hive，甚至在mysql8也开始支持窗口函数了，利用窗口函数可以对某个时间窗口内的数据进行统计、聚合和计算，接下来通过几个案例展示下在kafka stream中窗口函数的使用。</p> 
<h4></h4> 
<h4 id="4.4.1%20%E9%9C%80%E6%B1%82%E4%B8%80%EF%BC%8C%E5%9B%BA%E5%AE%9A%E6%97%B6%E9%97%B4%E8%BE%93%E5%87%BA%E7%BB%9F%E8%AE%A1%E7%BB%93%E6%9E%9C%E5%88%B0%E5%8F%A6%E4%B8%80%E4%B8%AAtopic" style="background-color:transparent;">4.4.1 需求一，固定时间输出统计结果到另一个topic</h4> 
<p>这里每隔3秒输出一次从topic1中过去10秒的数据到topic2中</p> 
<pre><code class="language-java">import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.*;
import org.apache.kafka.streams.kstream.*;

import java.time.Duration;
import java.util.Arrays;
import java.util.Properties;

public class WindowStream1 {

    public static void main(String[] args) {
        Properties prop = new Properties();
        prop.put(StreamsConfig.APPLICATION_ID_CONFIG, "WindowCountStream");
        prop.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "IP:9092");
        prop.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 3000);
        prop.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        prop.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());

        StreamsBuilder builder = new StreamsBuilder();
        KStream&lt;Object, Object&gt; source = builder.stream("zcy");
        KTable&lt;Windowed&lt;String&gt;, Long&gt; countTable = source
                .flatMapValues(value -&gt; Arrays.asList(value.toString().split("\\s+")))
                .map((x, y) -&gt; {
                    return new KeyValue&lt;String, String&gt;(y, "1");
                }).groupByKey()
                //加10秒窗口,按步长3秒滑动
                .windowedBy(TimeWindows.of(Duration.ofSeconds(10).toMillis()).advanceBy(Duration.ofSeconds(3).toMillis()))
                .count();

        countTable.toStream().foreach((key, val) -&gt; {
            System.out.println("key: " + key + "  val: " + val);
        });

        countTable.toStream().map((key, val) -&gt; {
            return new KeyValue&lt;String, String&gt;(key.toString(), val.toString());
        }).to("zcy-out");

        final Topology topo = builder.build();
        final KafkaStreams streams = new KafkaStreams(topo, prop);
        streams.start();
    }
}</code></pre> 
<p>运行代码，按照上述相同的方式测试，然后再在控制台可以看到统计到的时间窗口内的单词数</p> 
<p style="text-align:center;"><img alt="" class="left" src="https://images2.imgbox.com/89/cd/LN4iJ3cX_o.png"></p> 
<h4></h4> 
<h4 id="4.4.2%20%E9%9C%80%E6%B1%82%E4%BA%8C%EF%BC%8C%E7%BB%9F%E8%AE%A1topic1%E4%B8%AD10%E7%A7%92%E5%86%85%E7%9A%84wordcount%E5%86%99%E5%88%B0topic2">4.4.2 需求二，统计topic1中10秒内的wordcount写到topic2</h4> 
<p>一个典型的场景就是，通过session会话的时间窗口统计用户访问网站的时长，对某个特定的用户来说，用户从登录开始，即该用户的窗口开始，直到发生退出或者会话超时，窗口期结束，可以统计在窗口期间发生的各种动作，比如点击某些按钮，浏览某个页面的时长等行为。</p> 
<pre><code class="language-java">public class WindowStream2 {

    public static void main(String[] args) {
        Properties prop = new Properties();
        prop.put(StreamsConfig.APPLICATION_ID_CONFIG, "WindowCountStream");
        prop.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "IP:9092");
        prop.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 2000);
        prop.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        prop.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());

        StreamsBuilder builder = new StreamsBuilder();
        KStream&lt;Object, Object&gt; source = builder.stream("zcy");
        KTable&lt;Windowed&lt;String&gt;, Long&gt; countTable = source.flatMapValues(value -&gt; Arrays.asList(value.toString().split("\\s+")))
                .map((x, y) -&gt; {
                    return new KeyValue&lt;&gt;(y, "1");
                }).groupByKey()
                .windowedBy(SessionWindows.with(Duration.ofSeconds(15).toMillis()))
                .count();

        countTable.toStream().foreach((key, val) -&gt; {
            System.out.println("key: " + key + "  val: " + val);
        });

        countTable
                .toStream()
                .map((key, val) -&gt; {
                    return new KeyValue&lt;String, String&gt;(key.toString(), val.toString());
                })
                .to("zcy-out");
        KafkaStreams streams = new KafkaStreams(builder.build(), prop);
        streams.start();
    }

}</code></pre> 
<h3 id="4.5%20Kafka%20Streams%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%E6%8B%93%E5%B1%95">4.5 Kafka Streams使用场景拓展</h3> 
<h4 id="4.5.1%20%E4%BA%8B%E4%BB%B6%E6%97%A5%E5%BF%97%E7%9B%91%E6%8E%A7">4.5.1 事件日志监控</h4> 
<p>假如现在需要对某系统中实时上报到topic-1的错误或告警日志进行转换，并输出到下游的topic-2中做大屏监控，如下为原始的从topic-1中获取到的日志数据格式</p> 
<pre><code class="language-java">{
    "timestamp" : "2023-12-11 23:25:13",
    "method": "GET",
    "endpoint": "http://10.1.63.112:9098/fox/message/get",
    "status_code": 500,
    "source_ip":"192.168.9.138",
    "request_params":"type=5&amp;status=3",
    "operation_user":"6613"
}</code></pre> 
<p>假如下游的应用需要实时可视化用户请求日志，需要的数据格式如下：</p> 
<pre><code class="language-bash">{
    "ope_time": "2023-12-11 23:25:13",
    "ope_user": [
        {"user_id": "6613", "source_ip": "192.168.9.138","endpoint":"http://10.1.63.112:9098/fox/message/get"}
    ]
}</code></pre> 
<p>如果使用Kafka Stream来处理，可以考虑下面的思路</p> 
<ul><li> <p>根据业务需求对原始日志进行聚合和转换，重新组装结果的格式，并将结果写到下游的topic中；</p> </li><li> <p>下游应用从topic中获取处理的结果，按照大屏的数据格式再次组装数据，最后展示到大屏；</p> </li></ul> 
<p></p> 
<h4 id="4.5.2%20%E4%BA%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90">4.5.2 事用户行为统计分析</h4> 
<p>比如某电商网站或app的后台需要统计用户某些指标的数据，从而分析用户的消费习惯为后续做促销提供数据决策依据，现在从原始的topic中可以拿到下面几类指标信息</p> 
<pre><code class="language-bash">{ 
    "enter_type": app, 
    "online_time": 16m, 
    "user_type": "level_1" ,
    "buy_time_in_month":2,
    "user_id":1003
}</code></pre> 
<p>有了这些信息，就可以计算某种类型的用户，在过去一年内产生在app或网站来浏览的时长，购买的总次数，如果需要汇聚更多的信息，可以要求上游的topic中传入更详细的参数。</p> 
<p></p> 
<h4 id="4.5.3%20%E6%95%B0%E6%8D%AE%E8%81%9A%E5%90%88%E4%B8%8E%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97">4.5.3 数据聚合与实时计算</h4> 
<blockquote> 
 <p>kafka stream可以作为简单的实时计算框架，对数据进行准实时的聚合统计，快速汇总计算数据按业务维度进行数据分发，承载一部分大数据实时计算的功能。</p> 
</blockquote> 
<p></p> 
<h4 id="4.5.4%20%E5%AE%9E%E6%97%B6%E6%8E%A8%E8%8D%90">4.5.4 实时推荐</h4> 
<blockquote> 
 <p>基于现有的数据模型进行相关的指标计算，预测某些指标的行为，进一步指导业务决策，比如上面统计电商网站中用户的网站浏览动作。</p> 
</blockquote> 
<p></p> 
<h4 id="4.5.5%20%E5%AE%9E%E6%97%B6%E5%91%8A%E8%AD%A6">4.5.5 实时告警</h4> 
<blockquote> 
 <p>检测系统异常指标，通过准实时计算汇聚结果，将异常行为进行上报。</p> 
</blockquote> 
<p></p> 
<h4 id="4.5.6%20%E5%BA%94%E7%94%A8%E8%A7%A3%E8%80%A6">4.5.6 应用解耦</h4> 
<blockquote> 
 <p>这个与消息中间件的作用类似，为了减少源系统的计算压力，可以通过kafka stream进行解耦，所有的计算动作在kafka stream中进行，然后再将计算结果推送到下游的topic进行后续的使用。</p> 
</blockquote> 
<p></p> 
<h2 id="%E4%BA%94%E3%80%81kafka%20stream%E6%95%B4%E5%90%88springboot">五、kafka stream整合springboot</h2> 
<p>有了上面对kafka stream的了解和使用，接下来演示下如何在springboot中整合kafka stream</p> 
<p></p> 
<h3 id="5.1%20%E6%95%B4%E5%90%88%E8%BF%87%E7%A8%8B">5.1 整合过程</h3> 
<h4 id="5.1.1%20%E5%AF%BC%E5%85%A5springboot%E7%9B%B8%E5%85%B3%E4%BE%9D%E8%B5%96">5.1.1 导入springboot相关依赖</h4> 
<p> 在上述已经导入的依赖的基础上补充下面几个依赖</p> 
<pre><code class="language-java">        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-autoconfigure&lt;/artifactId&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;
            &lt;artifactId&gt;lombok&lt;/artifactId&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;/dependency&gt;</code></pre> 
<h4 id="5.1.2%20%E9%85%8D%E7%BD%AEkafka%E7%9B%B8%E5%85%B3%E4%BF%A1%E6%81%AF">5.1.2 配置kafka相关信息</h4> 
<p>在配置文件中添加如下配置信息</p> 
<pre><code class="language-bash">server:
  port: 8088

spring:
  application:
    name: kafka-sream-app
  kafka:
    bootstrap-servers: kafka连接IP:9092
    producer:
      retries: 5
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer

    consumer:
      group-id: ${spring.application.name}-test
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer

kafka:
  hosts: kafka连接IP:9092
  group: ${spring.application.name}</code></pre> 
<h4 id="5.1.3%C2%A0%E6%B7%BB%E5%8A%A0Kafka%20Stream%E9%85%8D%E7%BD%AE%E7%B1%BB">5.1.3 添加Kafka Stream配置类</h4> 
<pre><code class="language-java">
@Setter
@Getter
@Configuration
@EnableKafkaStreams
@ConfigurationProperties(prefix="kafka")
public class KafkaStreamConfig {

    private static final int MAX_MESSAGE_SIZE = 16 * 1024 * 1024;

    private String hosts;

    private String group;

    @Bean(name = KafkaStreamsDefaultConfiguration.DEFAULT_STREAMS_CONFIG_BEAN_NAME)
    public KafkaStreamsConfiguration defaultKafkaStreamsConfig() {
        Map&lt;String, Object&gt; props = new HashMap&lt;&gt;();
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, hosts);
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, this.getGroup()+"_stream_aid");
        props.put(StreamsConfig.CLIENT_ID_CONFIG, this.getGroup()+"_stream_id");
        props.put(StreamsConfig.RETRIES_CONFIG, 5);
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        return new KafkaStreamsConfiguration(props);
    }
}</code></pre> 
<h4 id="5.1.4%C2%A0%E8%87%AA%E5%AE%9A%E4%B9%89Kafka%20Stream%E4%B8%9A%E5%8A%A1%E5%A4%84%E7%90%86%E7%9B%91%E5%90%AC%E5%99%A8">5.1.4 自定义Kafka Stream业务处理监听器</h4> 
<p>还记得在编写消息中间件客户端程序的时候添加的那些监听器吗，原理类似，这里自定义一个监听器处理类，接收上游的topic消息进行处理之后再发送到下一个topic中，相当于是把上面的代码搬过来放到spring的ioc容器中</p> 
<pre><code class="language-java">import org.apache.kafka.streams.KeyValue;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.kstream.KStream;
import org.apache.kafka.streams.kstream.TimeWindows;
import org.apache.kafka.streams.kstream.ValueMapper;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

import java.time.Duration;
import java.util.Arrays;

@Configuration
public class StreamCountListener {

    @Bean
    public KStream&lt;String, String&gt; kStream(StreamsBuilder streamsBuilder) {
        KStream&lt;String, String&gt; stream = streamsBuilder.stream("zcy");
        stream.flatMapValues(new ValueMapper&lt;String, Iterable&lt;String&gt;&gt;() {
            @Override
            public Iterable&lt;String&gt; apply(String value) {
                return Arrays.asList(value.split(" "));
            }
        })
                //按照value进行聚合处理
                .groupBy((key, value) -&gt; value)
                //时间窗口
                .windowedBy(TimeWindows.of(Duration.ofSeconds(10)))
                //统计单词的个数
                .count()
                //转换为kStream
                .toStream()
                .map((key, value) -&gt; {
                    System.out.println("key:" + key + " ,vlaue:" + value);
                    return new KeyValue&lt;&gt;(key.key().toString(), value.toString());
                })
                //发送消息
                .to("zcy-out");

        return stream;
    }

}</code></pre> 
<h4 id="5.1.5%20%E6%95%88%E6%9E%9C%E6%B5%8B%E8%AF%95">5.1.5 效果测试</h4> 
<p>运行项目，运行之后，使用下面的代码，往zcy这个topic中发送一些消息</p> 
<pre><code class="language-java">public static void main(String[] args) throws Exception {

        // 1. 创建 kafka 生产者的配置对象
        Properties properties = new Properties();
        // 2. 给 kafka 配置对象添加配置信息：bootstrap.servers
        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "IP:9092");
        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");
        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");

        // 3. 创建 kafka 生产者对象
        KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;String, String&gt;(properties);
        System.out.println("开始发送数据");
        // 4. 调用 send 方法,发送消息
        for (int i = 0; i &lt; 5; i++) {
            kafkaProducer.send(new ProducerRecord&lt;&gt;("zcy","congge_" + i));
        }
        // 5. 关闭资源
        kafkaProducer.close();
    }</code></pre> 
<p>发送成功后，在控制台中可以看到经过上面的监听类处理得到的结果输出信息</p> 
<p><img alt="" height="171" src="https://images2.imgbox.com/86/64/GAR0eYZ0_o.png" width="825"></p> 
<p></p> 
<h2 id="%E5%85%AD%E3%80%81%E5%86%99%E5%9C%A8%E6%96%87%E6%9C%AB">六、写在文末</h2> 
<p>本篇通过较大得篇幅详细分享了kafka stream的使用，流式计算可以说是当下非常火热的技术之一，对于非大数据场景下的业务处理，kafka stream提供了一种很好的解决思路，希望对看到的同学有所帮助，本篇到此介绍，感谢观看。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/48d26fd67022086113124922071dccc7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【STM32】STM32学习笔记-TIM定时中断(13)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/585e061362623156fa6add0f653724ba/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【Java学习笔记】19 成员方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>