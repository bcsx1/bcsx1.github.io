<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【自监督论文阅读笔记】EVA: Exploring the Limits of Masked Visual Representation Learning at Scale - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【自监督论文阅读笔记】EVA: Exploring the Limits of Masked Visual Representation Learning at Scale" />
<meta property="og:description" content="Abstract： 本文推出了 EVA，这是一个以视觉为中心的基础模型，旨在仅使用可公开访问的数据来探索大规模视觉表示的局限性。EVA 是一种经过预训练的普通 ViT，用于重建 以可见图像块为条件的 屏蔽掉的图像-文本对齐（image-text aligned）的视觉特征。通过这个前置任务，我们可以有效地将 EVA 扩展到 10 亿个参数，并在图像识别、视频动作识别、目标检测、实例分割和语义分割等广泛的代表性视觉下游任务上创造新记录，而无需大量监督训练。
此外，我们观察到 缩放 EVA 的量变导致迁移学习性能的质变，这在其他模型中是不存在的。例如，EVA 在具有挑战性的大词汇量实例分割任务中取得了巨大飞跃：本文的模型在具有超过一千个类别的 LVISv1.0 数据集和只有八十个类别的 COCO 数据集上实现了几乎相同的最先进性能。
除了纯粹的视觉编码器，EVA 还可以作为 以视觉为中心的多模态的支点 来连接图像和文本。我们发现从 EVA 初始化巨型 CLIP 的视觉塔可以 以更少的样本和更少的计算 极大地稳定训练 并优于从头开始的训练，为 扩大 和 加速 多模态基础模型的昂贵训练 提供了新的方向。为了方便未来的研究，本文发布了所有代码和十亿规模的模型。
（Code &amp; Models: https://github.com/baaivision/EVA）
1. Introduction 扩大预训练语言模型 (PLM) [9,63,76] 在过去几年彻底改变了自然语言处理 (NLP)。这一成功的关键在于掩码信号预测 [31、74] 的简单且可扩展的自监督学习任务，利用该任务，Transformer 模型 [101] 可以使用几乎无限的未标记数据扩展到数十亿个参数，并且只需很少的调整就可以很好地泛化到各种下游任务。随着计算、数据和模型规模的进一步扩展，PLM 不仅带来了持续的性能改进 [51、75、76]，而且令人惊讶地出现了上下文学习(in-context learning)能力 [9、25、107、108]。
受 NLP 模型扩展成功的推动，我们还可以将这种成功从语言转化为视觉，即 扩大以视觉为中心的基础模型，该模型有利于视觉和多模态下游任务。最近，掩码图像建模 (MIM) [5, 40, 116] 作为一种可行的视觉模型预训练和缩放方法得到了蓬勃发展。然而，最具竞争力的数十亿级视觉预训练模型 [33、64、71、123] 仍然 严重依赖监督或弱监督训练 以及数亿（通常是公开不可访问的）标记数据。 MIM 在某种程度上仅被用作 严格地监督预训练之前的 初始化阶段 [64]，或者纯 MIM 预训练模型无法在十亿规模的模型大小下实现良好的性能 [117]。我们认为这种差距源于自然图像是原始的且信息稀疏的事实。同时，理想的视觉前置任务 不仅需要 低级几何结构信息的抽象，还需要高级语义的抽象，而像素级恢复任务 很难捕获这些信息[115]。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/b07d15791462cf9eaf1e77e620529ed7/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-02-15T23:23:59+08:00" />
<meta property="article:modified_time" content="2023-02-15T23:23:59+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【自监督论文阅读笔记】EVA: Exploring the Limits of Masked Visual Representation Learning at Scale</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>Abstract：</h2> 
<p>        本文推出了 <span style="color:#be191c;"><strong>EVA</strong></span>，这是一个<strong>以视觉为中心</strong>的基础模型，旨在仅使用可公开访问的数据来<span style="color:#be191c;"><strong>探索大规模</strong>视觉表示的<strong>局限性</strong></span>。EVA 是一种经过预训练的普通 ViT，用于<strong><span style="color:#be191c;">重建</span></strong> 以可见图像块为条件的 屏蔽掉的<strong><span style="color:#be191c;">图像-文本对齐（image-text aligned）的视觉特征</span></strong>。通过这个前置任务，我们可以有效地<strong><span style="color:#be191c;">将 EVA 扩展到 10 亿个参数</span></strong>，并在图像识别、视频动作识别、<u>目标检测</u>、实例分割和语义分割等广泛的代表性视觉下游任务上<strong>创造新记录</strong>，而无需大量监督训练。</p> 
<p>        此外，我们观察到 <strong>缩放 EVA 的量变导致<span style="color:#be191c;">迁移学习性能的质变</span>，这在其他模型中是不存在的</strong>。例如，EVA 在具有挑战性的大词汇量实例分割任务中取得了巨大飞跃：本文的模型在具有超过一千个类别的 LVISv1.0 数据集和只有八十个类别的 COCO 数据集上<strong>实现了几乎相同的最先进性能</strong>。</p> 
<p>        除了纯粹的视觉编码器，EVA 还可以作为 <span style="color:#be191c;">以视觉为中心的多模态的支点 来连接图像和文本</span>。我们发现从 EVA 初始化巨型 CLIP 的视觉塔<strong>可以 以更少的样本和更少的计算 </strong><strong>极大地稳定训练 并优于从头开始的训练</strong>，为 扩大 和 加速 多模态基础模型的昂贵训练 提供了新的方向。为了方便未来的研究，本文发布了所有代码和十亿规模的模型。</p> 
<p>（Code &amp; Models: https://github.com/baaivision/EVA）</p> 
<hr> 
<h2>1. Introduction</h2> 
<p>        扩大预训练语言模型 (PLM) [9,63,76] 在过去几年彻底改变了自然语言处理 (NLP)。这一成功的关键在于掩码信号预测 [31、74] 的简单且可扩展的自监督学习任务，利用该任务，<span style="color:#be191c;">Transformer 模型 [101] 可以使用几乎无限的未标记数据扩展到数十亿个参数</span>，并且<span style="color:#be191c;">只需很少的调整就可以很好地泛化到各种下游任务</span>。随着计算、数据和模型规模的进一步扩展，PLM 不仅带来了持续的性能改进 [51、75、76]，而且令人惊讶地<strong>出现了上下文学习(in-context learning)能力</strong> [9、25、107、108]。</p> 
<p>        受 NLP 模型扩展成功的推动，我们还可以将这种成功<strong>从语言转化为视觉</strong>，即 <span style="color:#be191c;">扩大以视觉为中心的基础模型</span>，该模型有利于视觉和多模态下游任务。最近，掩码图像建模 (MIM) [5, 40, 116] 作为一种可行的视觉模型预训练和缩放方法得到了蓬勃发展。然而，<span style="background-color:#fefcd8;">最具竞争力的数十亿级视觉预训练模型 [33、64、71、123] 仍然 <strong>严重依赖监督或弱监督训练 </strong>以及数亿（通常是公开不可访问的）标记数据</span>。 MIM 在某种程度上<strong>仅被用作 严格地监督预训练之前的 初始化阶段</strong> [64]，或者<span style="color:#be191c;">纯 MIM 预训练模型无法在十亿规模的模型大小下实现良好的性能</span> [117]。我们认为这种差距源于<span style="color:#be191c;"><strong><span style="background-color:#d7d8d9;">自然图像是原始的且信息稀疏的事实</span></strong></span>。同时，<span style="background-color:#d7d8d9;">理想的视觉前置任务 不仅需要 低级几何结构信息的抽象，还<span style="color:#be191c;">需要高级语义的抽象</span>，而</span><span style="color:#be191c;"><strong><span style="background-color:#d7d8d9;">像素级恢复任务 很难捕获这些信息</span></strong></span>[115]。</p> 
<p>        在这项工作中，<span style="background-color:#fefcd8;">本文</span><span style="color:#be191c;"><span style="background-color:#fefcd8;">为</span><strong><span style="background-color:#fefcd8;">大规模</span></strong><span style="background-color:#fefcd8;">视觉表示学习寻找合适的 MIM 前置任务</span></span><span style="background-color:#fefcd8;">，并探索其</span><strong><span style="color:#be191c;"><span style="background-color:#fefcd8;">在十亿参数规模和数千万未标记数据下的极限</span></span></strong>。最近，有一些试验 利用 图像-图像 或 图像-文本 对比学习 [13、22、73] 的语义信息进行 MIM 预训练 [44、109、130]，它们<strong>在视觉下游任务中表现相当好</strong>。然而，关于 (i) <strong>标记化语义特征</strong> 可以为视觉中的掩码建模提供更好的监督信号 [5、70、104] 以及 (ii) 良好的性能也可以通过 没有掩码预测任务的<strong> 简单后蒸馏过程</strong> [110 ]来实现 <span style="background-color:#fefcd8;">仍然存在争论 </span>。通过试点实证研究，本文发现简单地<span style="background-color:#fff5e6;">使用</span><span style="color:#be191c;"><span style="background-color:#fff5e6;">图像-文本对齐</span></span><span style="background-color:#fff5e6;">（即 CLIP [73]）视觉特征作为 MIM 中的预测目标</span>可以很好地扩展 并在广泛的下游基准测试中 取得令人满意的性能。<span style="background-color:#fff5e6;">该预训练任务受益于图像文本对比学习的高级语义抽象 以及 </span><strong><span style="background-color:#fff5e6;">掩码图像建模中几何和结构的良好捕获</span></strong><span style="background-color:#fff5e6;">，这通常涵盖了大多数视觉感知任务所需的信息</span>。</p> 
<p style="text-align:center;"><img alt="" height="347" src="https://images2.imgbox.com/41/ea/LrstdTsI_o.png" width="410"></p> 
<p>        通过这个 MIM 前置任务，我们可以有效地将一个普通的 ViT 编码器 [33]，称为<strong> EVA，扩展到十亿个具有强大视觉表示的参数</strong>，可以很好地传输到广泛的下游任务（图 1）。使用 2960 万张可公开访问的未标记图像进行预训练，EVA 在几个具有代表性的视觉基准测试中创造了新记录，例如 ImageNet1K [30] 上的图像分类（89.7% top-1 准确率）、LVISv1.0  [39] (62.2 APbox &amp; 55.0 APmask on val) 和 COCO [61] (64.5 APbox &amp; 55.0 APmask on val, 64.7 APbox &amp; 55.5 APmask on test-dev)上的 <strong><u>目标检测 </u></strong>和实例分割，COCO-stuff [11]（53.4 mIoUss）和 ADE20K [129] （62.3 mIoUms）上的语义分割，以及 Kinetics-400 [52]（89.7% top-1 准确率）、Kinetics-600 [14]（89.8% top-1 准确率）、Kinetics700 [15]（82.9% top-1 准确率）上的视频动作识别。值得注意的是，不同于其他 <span style="color:#be191c;">需要数千万甚至数十亿标记图像的 </span>最先进的十亿级视觉基础模型，例如使用 ImageNet-21K-ext-70M [64] 的 SwinV2-G 和 ViT- g/G 使用 JFT-3B [123]，<span style="background-color:#fefcd8;">EVA <strong>不需要昂贵的监督训练阶段</strong>，仅利用来自开源数据集的图像来实现学术可重复性</span>。</p> 
<p>        此外，我们观察到 <strong>扩大EVA 的量变导致迁移学习性能的质变</strong>，这<strong>在其他较小规模的模型中没有观察到</strong>，例如，EVA 在具有挑战性的大词汇量 目标级识别任务中 取得了重大突破：我们的模型在 LVISv1.0 [39] （55.0 APmask on val），一个具有超过 1,200 个类别的实例分割基准，上实现了与 COCO [61] （55.0 APmask on val），它几乎与 LVISv1.0 共享相同的图像集<strong>但仅注释了 80 个类别</strong>，几乎相同的性能（55.0 APmask on val）。这种涌现能力很好地符合模型扩展的期望[108]，即<span style="background-color:#fefcd8;"><strong>模型的更大能力</strong> 不仅会 导致 标准基准的可预测<strong>性能改进</strong>，而且还会产生不可预测的现象和解决更具挑战性任务的能力</span>。</p> 
<p>        除了纯粹的视觉编码器，<span style="color:#be191c;">EVA 还可以充当以视觉为中心的多模态支点</span>，在视觉和语言之间架起一座桥梁。本文表明，在 11 亿个参数的 CLIP 模型中通过 预训练的 EVA 初始化图像编码器 可以 在广泛的零样本图像/视频分类基准测试中 <strong>胜过从头开始的训练</strong>，而且<span style="color:#be191c;">样本和计算量要少得多</span>。此外，<span style="color:#be191c;">EVA 可以极大地<strong>稳定巨型 CLIP 的训练和优化过程</strong></span>。由于大型 CLIP 模型通常会遇到训练不稳定和效率低下的问题 [2, 48]，本文希望我们的解决方案为 <span style="color:#be191c;">扩大和加速多模态基础模型的昂贵训练</span>开辟新的方向。</p> 
<p>        通过使用 MIM 预训练 扩展以视觉为中心的基础模型 以在广泛的下游任务上实现强大的性能，本文希望<strong> <span style="background-color:#fefcd8;">EVA 能够通过 掩码信号建模 </span><span style="color:#be191c;"><span style="background-color:#fefcd8;">弥合视觉和语言之间的差距</span></span></strong><span style="background-color:#fefcd8;">，并</span><span style="color:#be191c;"><span style="background-color:#fefcd8;">为不同模式的大融合</span></span><span style="background-color:#fefcd8;">做出贡献</span>。</p> 
<hr> 
<h2>2. Fly EVA to the Moon</h2> 
<p>        本文首先在 §2.1 中进行了一系列试点实验 以 选择理想的视觉前置任务，然后通过在 §2.2 中选择的预训练目标 扩大了 EVA 预训练。最后，在 §2.3 中评估了各种下游任务的预训练表示。详细的实验设置和配置在附录 A 中。</p> 
<hr> 
<h3>2.1. The Feature Instrumentality Project 特征工具项目</h3> 
<p>        在本节中，我们寻求具有令人信服的<strong>迁移性能</strong>的 MIM 视觉前置任务。基于先前关于视觉预训练的文献，本文研究了<strong>两个有前途的候选者</strong>：（i）<span style="color:#be191c;">恢复被屏蔽的标记化语义视觉特征</span> [5、70、104]，以及（ii）<span style="color:#be191c;">从强大的预训练表示中特征蒸馏</span>，如 [110]。它们都利用预训练的图像-文本对齐视觉特征（即 CLIP [73] 视觉特征）。通过表 2 中所示的一系列试点实验，我们发现：（i）（附加的）<strong>CLIP 特征标记化（tokenization）过程</strong> 对于 实现良好的下游性能 <span style="color:#be191c;"><strong>是不必要的</strong></span>（ii）<strong>特征蒸馏</strong><span style="color:#be191c;">无法提供持续的性能增益</span>，因为预训练变得更长。相反，<span style="background-color:#f9eda6;">我们发现 </span><strong><span style="color:#be191c;"><span style="background-color:#f9eda6;">简单地重建 以可见图像块为条件的 掩码的 CLIP 视觉特征是高性能的</span></span></strong><span style="background-color:#f9eda6;">，它被选择用于扩展 EVA</span>。</p> 
<p style="text-align:center;"><img alt="" height="443" src="https://images2.imgbox.com/c7/c7/2d7bV9gj_o.png" width="391"></p> 
<p>        澄清一下，这个 MIM 前置任务最初并不是本文提出的。 MVP [109] 研究了 MIM 预训练的掩码图像-文本对齐视觉特征的回归，最近<span style="background-color:#fefcd8;"> MILAN</span> [44] 重新讨论了这一问题。在这项工作中，<span style="background-color:#f9eda6;">本文表明</span><strong><span style="background-color:#f9eda6;">这个前置任务可以扩展到十亿级参数和数千万个未标记图像，用于以视觉为中心的表示学习</span></strong><span style="background-color:#f9eda6;">，</span><span style="color:#be191c;"><span style="background-color:#f9eda6;"><strong>无需</strong> (i) <strong>语义特征量化/标记化</strong></span></span><span style="background-color:#f9eda6;"> [5、70]，和 (ii) </span><span style="color:#be191c;"><span style="background-color:#f9eda6;">明确使用<strong>图像-文本配对的预训练数据</strong>和大型语料库</span></span>，如 BEiT-3 [104]。</p> 
<hr> 
<h3>2.2. Pre-training</h3> 
<h4>架构：</h4> 
<p>        EVA 的架构配置如表 3a 所示。 EVA 是具有 1.0B 参数的普通ViT [33]。她的形状遵循 ViT giant [123] 和 BEiT-3 [104] 的视觉编码器。本文在预训练期间<strong>不使用相对位置嵌入</strong> [89] 和 layer-scale [99]。</p> 
<p style="text-align:center;"><img alt="" height="50" src="https://images2.imgbox.com/52/0b/WxZj3f6G_o.png" width="468"></p> 
<hr> 
<h4>预训练目标：</h4> 
<p>        EVA 经过预训练，<span style="color:#be191c;"><span style="background-color:#fefcd8;">可以重建以可见图像块为条件的屏蔽掉的图像-文本对齐视觉特征</span></span>。我们用 [MASK] 标记破坏了输入patches，并且我们遵循 [5, 70, 104] 使用屏蔽率为 40% 的<span style="color:#be191c;">块级屏蔽</span>。 MIM 预训练的目标来自公开可用的OpenAI CLIP-L/14 视觉塔，在 224×224 像素图像上训练 [73]。 EVA 的输出特征首先被归一化 [3]，然后通过<strong>线性层投影到与 CLIP 特征相同的维度</strong>。我们使用<strong><span style="color:#be191c;">负余弦相似度作为损失函数</span></strong>。</p> 
<hr> 
<h4>预训练数据：</h4> 
<p>        表 3b 总结了本文用于预训练 EVA 的数据。对于 CC12M [16] 和 CC3M [88] 数据集，我们只使用没有说明文字（caption）的图像数据。对于 COCO [61] 和 ADE20K [129] 数据集，我们只使用训练集数据。还使用了 ImageNet-21K [30] 和 Object365 [87] 图像数据。所有这些数据都可以公开访问。用于预训练的合并数据集<strong>共有 2960 万张图像</strong>。 </p> 
<p style="text-align:center;"><img alt="" height="69" src="https://images2.imgbox.com/1b/16/FULQUlNc_o.png" width="460"></p> 
<p>        我们<strong>用作 MIM 预测目标的 CLIP 特征</strong>以自监督的方式在 4 亿图像-文本数据集上进行训练。因此，在预训练期间，EVA 也在某种程度上隐式地利用了来自该数据集的知识。同时，这些 CLIP 特征也广泛用于其他最先进的表示学习和预训练工作，例如 BEiT 家族 [70、104]、AI 生成的内容 [78、83、84] 和大规模数据集过滤 [10、85、86]。</p> 
<hr> 
<h4>预训练设置和超参数。</h4> 
<p>        如表 3c 所示，EVA 通过 Adam [53] 进行了优化，解耦权重衰减 [67] 为 0.05。峰值学习率为 1e-3 并根据余弦学习率计划衰减。本文采用随机深度 [46]，正则化速率为 0.1和 RandResizeCrop (0.2, 1) 进行数据增强。不使用颜色抖动。</p> 
<p style="text-align:center;"><img alt="" height="80" src="https://images2.imgbox.com/8e/bf/8vJV1R5l_o.png" width="501"></p> 
<hr> 
<h4> 预训练基础设施和统计数据。</h4> 
<p>        表 3d 中提供了一些基本的预训练统计数据。我们使用的 GPU 是 NVIDIA A100-SXM4-40GB。<strong><span style="background-color:#fefcd8;">预训练代码基于用 PyTorch [69] 编写的 BEiT</span></strong> [5]。我们还采用带有 ZeRO stage-1 优化器 [77] 的 DeepSpeed 优化库 [80] 来节省内存。我们发现在整个预训练过程中使用具有动态损失缩放的 fp16 格式足够稳定，而无需使用 bfloat16 格式。由于我们使用 fp16 精度，EVA 也可以使用 16× NVIDIA 24GB (32GB) GPU 进行预训练，带（不带）梯度检查点 [20]。 </p> 
<p style="text-align:center;"><img alt="" height="78" src="https://images2.imgbox.com/68/2a/J2DiqG8A_o.png" width="515"></p> 
<hr> 
<h3>2.3.下游任务评估</h3> 
<p>        在本节中，在几个代表性基准上广泛评估预训练的 EVA，例如图像分类（§2.3.1）、视频动作识别（§2.3.2）、目标检测和实例分割（§2.3.3）、语义分割（§2.3.4），以及具有零样本评估的对比图像文本预训练（§2.3.5）。 <strong>EVA 在广泛的下游任务上实现了最先进的性能</strong>。</p> 
<hr> 
<p>（图像分类和视频动作识别省略）</p> 
<h4>2.3.3 目标检测&amp;实例分割</h4> 
<p><strong>数据集</strong>。</p> 
<p>        我们评估了 EVA 在 COCO [61] 和 LVISv1.0 [39] 基准上的目标检测和实例分割性能。 COCO 是一种广泛使用的目标级识别基准，分别由 118k train、5k val 和 20k test-dev 图像组成，具有 80 个常见目标类别。 LVISv1.0 是一个新兴的大词汇目标级识别基准，它有超过 1,200 个对象类别以及超过 200 万个高质量实例分割掩码（接近 COCO 实例掩码的 2 倍）。</p> 
<p>        值得注意的是，<span style="color:#be191c;">COCO 和 LVISv1.0 几乎使用同一组图像</span>，LVISv1.0 的 train 和 val split 与 COCO train 和 val split 有很大的重叠。同时，COCO 的对象类别比 L VISv1.0 少得多（即 80 对 1,200+）。因此，评估模型在 COCO 和 LVIS 上的性能是有趣且有意义的。</p> 
<hr> 
<p><strong>指标</strong>。</p> 
<p>        对于 COCO，本文在 val 和 test-dev split上报告了标准框 AP (APbox) 和掩码 AP (APmask)。对于 LVISv1.0，我们使用 [39] 中定义的 APbox、APmask 和 APmaskrare 在 v1.0 验证集上评估 EVA。我们还报告了 LVIS 2021 challenge3 中使用的实验性“边界，固定 AP”（表 8a 中的 APboundary）以供参考。</p> 
<hr> 
<p><strong>训练和评估设置</strong>。</p> 
<p>        <span style="color:#be191c;">EVA 使用 Cascade Mask R-CNN [12] 作为检测器</span>，并采用 ViTDet [59] 的训练设置（例如，大规模抖动数据增强 [37]）和架构配置（例如，交错窗口和全局注意力）。按照惯例 [64, 104, 126]，我们首先使用分辨率为 1024*1024 的 Objects365 [87] 数据集对整个检测器进行中间微调，然后我们分别用1280*1280的输入在 COCO 和 LVISv1.0 train split上微调检测器。</p> 
<p>        我们报告了 EVA 的单尺度评估和多尺度评估/测试时间增强 (tta) 结果以供比较。对于 COCO，还应用了 Soft-NMS [8]。例如分割任务，分类分数是通过掩码 [105] 校准的 [47]。</p> 
<p>        模型架构以及 COCO 和 LVISv1.0 的超参数几乎相同（即，超参数几乎是从 COCO 到 LVISv1.0 的“零样本”转移），期望我们使用联合损失 [131] 和在 LVISv1.0 上遵循 ViTDet 重复因子采样 [39]。</p> 
<hr> 
<p><strong>COCO结果</strong>。</p> 
<p>        或许COCO才是最猛的视觉标杆。表 7 将 EVA 与 COCO 上的一些先前领先方法进行了比较。<strong><span style="background-color:#fff5e6;">本文的模型在目标检测和实例分割任务上创造了新的最先进的结果</span></strong>。</p> 
<p><img alt="" height="567" src="https://images2.imgbox.com/98/6d/uVfvu2rp_o.png" width="1200"></p> 
<p>        与同样使用 Cascade Mask R-CNN [12] 的 ViTDet-H [59] 相比，<span style="background-color:#fefcd8;">EVA 表明，使用<span style="color:#be191c;">更大的模型</span>和更好的编码器&amp;检测器预训练，在使用相同的检测器时可以大大提高性能</span>。</p> 
<p>        与选择更成熟和高度优化的 DINO 检测器 [126] 的 FocalNet [119] 和 Group DETRv2 [19] 相比，<strong>EVA 表明，如果有足够的模型大小、数据和预训练，也可以通过经典的 R-CNN 框架 [38] 实现更好的性能</strong>。另一方面，由于使用 DINO，FocalNet 和 Group DETRv2 无法进行实例分割。</p> 
<p>        与 SwinV2-Giant [64] 和 FD-SwinV2Giant [110] 相比，它们也采用了来自 R-CNN 家族的（更强的 HTC++ [17]）检测器，但具有 ~3× 模型大小的 EVA，本文的方法简化了预训练过程并通过更好的表示来完成“Giant-killing”行为。</p> 
<p>        与 BEiT-3 相比，<strong>EVA 表明可以在 在预训练期间 不利用 (i) 语义特征量化/标记化 [5、70] 和 (ii) 图文配对预训练数据和大型语料库的情况下 构建最先进的目标级识别系统</strong>。</p> 
<hr> 
<p><strong>LVIS 结果</strong>。</p> 
<p>        表 8a 总结了 LVISv1.0 验证集的结果。 <strong>EVA 通过单尺度评估在所有指标下实现了最先进的性能，大大优于以前的最佳方法</strong>。</p> 
<p><img alt="" height="365" src="https://images2.imgbox.com/18/3e/j5uKOvjo_o.png" width="1200"></p> 
<hr> 
<p><strong>分析LVIS-COCO差距：more is different</strong>。</p> 
<p>         <strong><span style="background-color:#f9eda6;">在 COCO 和 LVISv1.0 基准上评估和分析模型是有趣且有价值的，因为它们共享几乎相同的图像集，但具有不同数量的带标记的目标类别</span></strong>。与只有 80 个标注类别的 COCO 相比，LVIS 标注了 1200 多个对象类别，因此自然<strong>具有长尾分布</strong>，<span style="color:#be191c;">更接近具有挑战性的现实世界场景 </span>[39]。一般来说，<span style="background-color:#fefcd8;">LVIS 被认为是目标级识别中比 COCO 更难的基准</span>，与 COCO 相比，<strong>传统方法在 LVIS 上的性能通常会大幅下降</strong>。</p> 
<p>        在表 8b 中，本文分析了 EVA 的 LVISv1.0 和 COCO 基准与其他最先进方法之间的性能差距。对于之前领先的方法，如 ViTDet，APbox 之间的性能差距在 8 左右，APmask 之间的差距在 5 左右。但是，使用相同的检测器（Cascade Mask R-CNN）和与 ViTDet 预训练的几乎相同的设置MAE-Huge (ViTDet-H)，<strong>EVA 不仅在 LVIS 和 COCO 基准测试上同时取得了最先进的结果，而且在很大程度上缩小了它们之间的性能差距</strong>，特别是对于实例分割任务，EVA通过单尺度评估在 LVIS 和 COCO 上实现相同的性能。与 ViTDet-H 相比，本文展示了一个更大的模型和更强的表示可以在具有挑战性的大词汇量实例分割基准测试中取得巨大飞跃——下面将描述一个警告。</p> 
<p>        请注意，说 EVA 基于“零 APmask 间隙”“解决”LVIS 大词汇量实例分割任务是不准确的，并且有可能在 LVIS 上实现比 COCO 更高的 APmask（例如，LVIS 具有更高质量的掩码注释训练和评估）。尽管 LVIS 上 EVA 的 APmaskrare 比以前的方法好得多，但稀有类别和常见/频繁类别之间仍然存在很大差距（即 APmaskrare = 48.3 vs APmaskcomm = 55.5 / APmaskfreq = 57.4）。从另一个角度来看，LVIS 和 COCO 之间的 APbox 差距（EVA 为 1.9）可能<strong>更好地反映了大词汇和普通词汇对象级识别之间的整体差距</strong>，因为该指标消除了实例掩码注释质量的混杂因素。</p> 
<p>        尽管如此，EVA 在具有挑战性的大词汇量对象级识别任务中取得了重大突破。我们相信其他大型视觉基础模型，如 SwinV2-G 和 BEiT-3 在 COCO 和 LVISv1.0 基准测试中也具有类似的特性。我们希望我们的努力可以鼓励社区在扩展模型和在每个任务中追求最先进的同时<strong>更多地关注不同任务之间的关系</strong>。</p> 
<hr> 
<h4>2.3.5 具有零样本分类评估的对比语言-图像预训练</h4> 
<p>        <span style="color:#be191c;"><strong><span style="background-color:#d7d8d9;">CLIP（对比语言-图像预训练）</span></strong></span><span style="background-color:#d7d8d9;">[48、50、72、73] 是一种多模态基础模型，通过对比图像-文本预训练连接视觉和语言。 CLIP 可以应用于任何图像分类基准，</span><strong><span style="background-color:#d7d8d9;">只需提供要识别的视觉类别的名称</span></strong><span style="background-color:#d7d8d9;"> [1]。因此，CLIP 的引入实质上重塑了视觉识别的格局。同时，CLIP 特征还在表示学习 [70、104]、AI 生成的内容 [78、83、84] 和大数据集过滤 [10、85、86] 等方面发挥着核心作用</span>。</p> 
<p>        在本节和表 10 中，本文展示了 EVA 不仅是广泛的视觉下游任务的强大编码器，而且还是在视觉和语言之间架起桥梁的多模式支点。为了证明这一点，我们在各种零样本图像/视频分类基准中将 EVA 作为十亿级 CLIP 的视觉塔进行训练和评估。</p> 
<hr> 
<p><strong>CLIP 模型放大的基线和主要挑战</strong>。</p> 
<p>        本文将本文的 CLIP（称为 <span style="color:#be191c;"><strong>EVA CLIP</strong></span>）与其他仅利用可公开访问的数据/学术资源的开源 CLIP 竞争对手进行比较。模型配置和统计数据详见表 10a。</p> 
<p>        <strong><span style="background-color:#d7d8d9;">CLIP 模型训练和扩展有两个众所周知的主要挑战</span></strong>：(i) 大规模 Open CLIP 模型（例如，Open CLIP-H 和 Open CLIP-g [2, 48]）通常会遇到<span style="color:#be191c;">严重的训练不稳定问题 </span>[ 2] 并且必须使用 bfloat16 格式进行优化。 (ii) <span style="color:#be191c;">训练效率低</span>，这可能会阻碍模型扩大和下游性能。例如，Open CLIP-g 由于其大量的计算需求而严重缺乏训练，其性能甚至比模型尺寸较小的充分训练的 Open CLIP-H 更差。</p> 
<p>        与本文的 CLIP 模型相比，Open CLIP-H &amp; -g 是从头开始训练的，使用更多的图像-文本对（本文的～2.9× 和～1.1×）在∼3 倍的 GPU上 从更大的数据集（我们的～5×）中采样。通过利用 EVA，可以通过改进的零样本分类性能加速十亿级 CLIP 模型训练，如下所述。</p> 
<hr> 
<p><strong>训练设置</strong>。</p> 
<p>        对于本文的 CLIP 模型，本文通过 <span style="color:#be191c;">预训练的 EVA 初始化视觉编码器</span> 和 来自 OpenAI CLIP-L 的语言编码器。预训练实现基于 Open CLIP [48]。本文还采用带有 ZeRO stage-1 优化器 [77] 的 DeepSpeed 优化库 [80] 来节省内存。我们发现在整个训练过程中使用具有动态损失缩放的 fp16 格式足够稳定，而无需使用 bfloat16 格式。这些修改使我们能够在 256× NVIDIA A100 40GB GPU 上训练批处理大小为 41k 的 1.1B CLIP。</p> 
<hr> 
<p><strong>评估设置</strong>。</p> 
<p>        本文在 12 个基准上评估每个 CLIP 模型的零样本图像/视频分类性能，并报告 top-1 准确性以进行比较。</p> 
<p>        对于<strong>零样本图像分类任务</strong>，本文选择了 8 个基准，即 ImageNet-1K [30]、ImageNet-V2 [81]、ImageNet-Adversarial (ImageNet-Adv.) [43]、ImageNetRendition (ImageNet-Adv.) [ 42]、ImageNet-Sketch (ImageNetSke.) [102]、ObjectNet [6]、CIFAR-10 和 CIFAR-100 [54]。我们还对 CLIP 模型的鲁棒性感兴趣，通过具有自然分布变化的 ImageNet-{1K, V2, Adv., Ren., Ske.} 和 ObjectNet 的平均性能与原始 ImageNet-1K 之间的性能差距来评估验证准确性。</p> 
<p>        对于零样本视频分类任务，我们选择了 4 个基准，即 UCF-101 [92]、Kinetics-400 [52]、Kinetics600 [14] 和 Kinetics-700 [15]。</p> 
<hr> 
<p><strong>结果</strong>。</p> 
<p>        表 10b 显示了比较。本文的<strong> EVA CLIP 实现了最高的平均精度</strong>，并且在 12 个零样本分类基准中的 10 个中表现最好。值得注意的是，在不使用任何训练集标签的情况下，ImageNet-1K 验证零样本 top-1 的准确率为 78.2%，与原始 ResNet-101 [41] 相匹配。此外，<strong>本文的模型非常稳健</strong>，并且在面对 ImageNet 中的<span style="color:#be191c;">自然分布变化时性能下降最小</span>。</p> 
<p>        最后，在表 11 中，本文提供了 EVA-CLIP 在 ImageNet-1K 验证集上的零样本、线性探测和端到端微调 top-1 精度，以供参考。<span style="background-color:#fefcd8;">本文的方法在所有现有的自监督学习方法中创造了新的最先进的结果</span>。</p> 
<p>        请注意，EVA CLIP 的<strong>视觉分支从 OpenAI CLIP-L 学习</strong>，而语言分支从相同的 CLIP-L 模型初始化。因此，从只有 430M 参数的 CLIP-L 开始，本文逐步扩展到 1.1B EVA CLIP-g，性能有了很大改进。</p> 
<p>        这意味着 <span style="background-color:#f9eda6;"><span style="color:#be191c;"><strong>交错的 MIM 和图像文本对比预训练</strong></span> 可能是一种高效且可扩展的 CLIP 训练方法</span>。据我们所知，EVA CLIP-g 是通过可公开访问的数据和资源训练的最大的高性能 CLIP 模型。本文希望本文在扩展和改进 CLIP 方面的实践也能启发和转移到其他大规模多模态基础模型的研究。</p> 
<hr> 
<h2>3. Related Work</h2> 
<h4>掩码图像建模 (MIM)</h4> 
<p>        通过预测以可见上下文为条件的掩码视觉内容来学习丰富的视觉表示。 ViT [33] 和 iGPT [18] 报告了第一个有意义的 MIM 预训练结果。 BEiT 家族 [5, 70, 104] 通过掩码视觉标记预测极大地提高了 MIM 的性能。<span style="background-color:#d7d8d9;">最近的工作 [4、21、32、34、40、106、116、130]（重新）探索 MIM 中的像素/特征回归，但仅限于<span style="color:#be191c;">相对较小的模型和数据规模</span></span>。在这项工作中，本文通过掩码图像-文本对齐特征预测 [44、109] 探索了大规模 MIM 预训练的局限性。</p> 
<hr> 
<h4>视觉基础模型。</h4> 
<p>         ConvNets [56] 长期以来一直是事实上的标准视觉架构。自 AlexNet [55] 以来，ConvNets 迅速发展并变得更深、更广、更大 [41,45,65,91,93,96,114]。然而，<span style="background-color:#d7d8d9;">在足够大的模型和数据规模下，由于缺乏可扩展的预训练任务和内置的归纳偏差，ConvNets 落后于 ViTs [33]。</span>进入 2020 年代，大型预训练 ViT [33,123]，例如具有分层架构的 SwinV2-G [64] 以及具有多模态表示的 BEiT-3 [104] 开始展示各种视觉基准。在这项工作中，本文展示了通过利用未标记的图像，普通的ViT 可以有效地扩展到十亿级参数，并在各种下游任务中脱颖而出。</p> 
<hr> 
<h2>4. 结论</h2> 
<p>        在这项工作中，本文推出了 EVA，这是一个十亿参数的普通的 ViT 编码器，用于<strong>探索掩码视觉表示学习的极限</strong>。本文展示了 简单的掩蔽特征建模 作为视觉学习前置任务 在具有最小视觉先验的体系结构上很好地扩展，并在具有代表性和多样化的下游任务集中取得了优异的结果。</p> 
<p>        本文希望 EVA 能够通过掩码建模弥合视觉和语言研究之间的差距，并为视觉研究的霓虹灯起源做出贡献。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/02b7ccba5f276587ee3d8047a4a185af/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Go语言之Gin框架引用静态资源CSS的方法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e3beaf7210a04cb77297f3dd8f78ff46/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【Android学习】kotlin语言基础知识学习</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>