<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度学习领域专业词汇_深度学习时代的人文领域专业知识 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度学习领域专业词汇_深度学习时代的人文领域专业知识" />
<meta property="og:description" content="深度学习领域专业词汇
It’s a bit of an understatement to say that Deep Learning has recently become a hot topic. Within a decade alone, the field has made significant strides on problems once thought to be impossible, including facial recognition, generating text that mimics human writing, making art, and playing games involving strategy and intuition.
我T的有点轻描淡写地说，深学，最近成为热门话题。 仅在十年内，该领域就在人们认为不可能的问题上取得了长足的进步，包括面部识别，生成模仿人类文字的文字，制作艺术品以及玩涉及策略和直觉的游戏。
Given the buzz surrounding the (seemingly unreasonable) effectiveness of these algorithms, it’s easy to get lost in the extremes of speculation and skepticism towards Deep Learning." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/8001b8cacd3787b4320281998c7b90ba/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-10-09T23:30:14+08:00" />
<meta property="article:modified_time" content="2020-10-09T23:30:14+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习领域专业词汇_深度学习时代的人文领域专业知识</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <article style="font-size: 16px;"> 
 <p>深度学习领域专业词汇</p> 
 <div> 
  <section> 
   <div> 
    <div> 
     <p>It’s a bit of an understatement to say that Deep Learning has recently become a hot topic. Within a decade alone, the field has made significant strides on problems once thought to be impossible, including <a href="https://www.sciencedirect.com/science/article/abs/pii/S1077314219301183" rel="noopener nofollow">facial recognition</a>, <a href="https://openai.com/blog/better-language-models/" rel="noopener nofollow">generating text that mimics human writing</a>, <a href="https://medium.com/r?url=https%3A%2F%2Faiartists.org%2Fai-generated-art-tools" rel="noopener">making art</a>, <a href="https://www.alphagomovie.com/" rel="noopener nofollow">and playing games involving strategy and intuition</a>.</p> 
     <p>我T的有点轻描淡写地说，深学，最近成为热门话题。 仅在十年内，该领域就在人们认为不可能的问题上取得了长足的进步，包括<a href="https://www.sciencedirect.com/science/article/abs/pii/S1077314219301183" rel="noopener nofollow">面部识别</a>，<a href="https://openai.com/blog/better-language-models/" rel="noopener nofollow">生成模仿人类文字的文字</a>，<a href="https://medium.com/r?url=https%3A%2F%2Faiartists.org%2Fai-generated-art-tools" rel="noopener">制作艺术品</a><a href="https://www.alphagomovie.com/" rel="noopener nofollow">以及玩涉及策略和直觉的游戏</a>。</p> 
     <p>Given the buzz surrounding the (seemingly unreasonable) effectiveness of these algorithms, it’s easy to get lost in the extremes of speculation and skepticism towards Deep Learning. Instead, I’d like to focus on the following issues.</p> 
     <p> 鉴于围绕这些算法的有效性(看似不合理)的嗡嗡声，很容易在对深度学习的猜测和怀疑的极端中迷失了方向。 相反，我想重点关注以下问题。</p> 
     <p>First, what is the role of human expertise in a world where Deep Learning is becoming more prevalent as a problem-solving tool? Second, where is the field of Deep Learning headed, and how can (human) expertise help move it forward?</p> 
     <p> 首先，在深度学习日益成为解决问题的工具的世界中，人类专业知识的作用是什么？ 其次，深度学习领域的发展方向如何，(人类)专业知识如何帮助其向前发展？</p> 
     <p>As we will soon see, these two questions are intimately related. But first, let’s start with the basics.</p> 
     <p> 我们将很快看到，这两个问题密切相关。 但是首先，让我们从基础开始。</p> 
     <h2> 深度学习与人类学习 <span style="font-weight: bold;">(</span>Deep vs. human learning<span style="font-weight: bold;">)</span></h2> 
     <p>To get a better scope of the picture, it helps to get a common understanding of what we mean when we refer to Deep Learning.</p> 
     <p>为了更好地了解图片，这有助于我们对深度学习的理解。</p> 
     <blockquote> 
      <p>Simply put, Deep Learning refers to a broad range of algorithms, very loosely inspired by the human brain.</p> 
      <p> 简而言之，深度学习涉及广泛的算法，受到人脑的启发。</p> 
     </blockquote> 
     <p><a href="https://en.wikipedia.org/wiki/Deep_learning" rel="noopener nofollow">These algorithms take the form of networks, known as Deep Neural Networks (DNNs), which iteratively improve as they encounter new examples</a>. It’s important to note that the kind of learning we refer to when we talk about Deep Learning is a very narrow subset of what we as humans might consider learning.</p> 
     <p> <a href="https://en.wikipedia.org/wiki/Deep_learning" rel="noopener nofollow">这些算法采用称为深度神经网络(DNN)的网络形式，在遇到新示例时会不断改进</a>。 重要的是要注意，当我们谈论深度学习时，我们所指的那种学习只是我们人类可能认为学习的一小部分。</p> 
     <p>Human learning often involves the ability to explain, generalize, and even teach concepts that we have come to understand, all of which encompass different forms and levels of learning. It turns out that a lot of the questions surrounding these variants of learning are still open problems in Deep Learning, ones where domain expertise may play a large role.</p> 
     <p> 人类学习通常包含解释，概括甚至教授我们已经理解的概念的能力，所有这些概念都包含不同形式和水平的学习。 事实证明，围绕这些学习变体的许多问题仍然是深度学习中的开放问题，在这些领域中领域专业知识可能起着很大的作用。</p> 
     <h2> 可解释性 <span style="font-weight: bold;">(</span>Explainability<span style="font-weight: bold;">)</span></h2> 
     <p>While Deep Learning algorithms outperform other machine learning methods at a wide variety of tasks, they are often notoriously difficult to interpret. Often, these networks are so complex that it’s difficult to know how they made their decisions. Like your pesky third-grade math teacher, we want these networks to “show their work”.</p> 
     <p>虽然深度学习算法在各种各样的任务上胜过其他机器学习方法，但众所周知，它们通常难以解释。 通常，这些网络是如此复杂，以至于很难知道它们是如何做出决定的。 就像您讨厌的三年级数学老师一样，我们希望这些网络“展示他们的工作”。</p> 
     <p><a href="https://arxiv.org/abs/2004.14545" rel="noopener nofollow">This problem of understanding how and why a network arrives at its decisions is known as <strong>explainability</strong></a>. <a href="https://towardsdatascience.com/an-overview-of-model-explainability-in-modern-machine-learning-fc0f22c8c29a" rel="noopener">In machine learning literature, this term is often used in conjunction with concepts of <em>interpretability, understandability</em>, and <em>trust.</em></a><em> </em>Explainability is important because it gives us a better reason to trust that these algorithms are doing what we want, and enables us to troubleshoot them when problems arise.</p> 
     <p> <a href="https://arxiv.org/abs/2004.14545" rel="noopener nofollow">理解网络如何以及为何做出决策的问题称为可<strong>解释性</strong></a>。 <a href="https://towardsdatascience.com/an-overview-of-model-explainability-in-modern-machine-learning-fc0f22c8c29a" rel="noopener">在机器学习文献中，该术语通常与可<em>解释性，可理解性</em>和<em>信任度</em>概念结合使用<em>。</em></a><em> </em>可解释性很重要，因为它使我们有更好的理由相信这些算法可以实现我们想要的功能，并可以在出现问题时对它们进行故障排除。</p> 
     <p>It’s important to note that explainability isn’t a one-way street. In the vast majority of cases, you want to explain your decisions <em>to someone</em>. That’s where expertise comes in. Just as a doctor knows what information should be included in a medical report or diagram to reach a diagnosis, domain experts know what components are necessary to make sound decisions in their domains.</p> 
     <p> 重要的是要注意，可解释性不是一条单向的街道。 在大多数情况下，您想<em>向某人</em>解释您的决定。 这就是专业知识的源泉。正如医生知道应该在医疗报告或图表中包含哪些信息以进行诊断一样，领域专家也知道在他们的领域做出正确决策所必需的组件。</p> 
     <p>These experts are not simply peripheral consultants; they are necessary players in designing and implementing good problem-solving algorithms. Experts know what information is relevant and irrelevant to making a decision, and this domain expertise is incredibly valuable in constraining the kinds of algorithms that are worth considering. At the end of the day, it will be the experts who will be able to see if a decision is made correctly, and what steps need to be taken to fix it.</p> 
     <p> 这些专家不仅仅是简单的外围顾问。 他们是设计和实施好的问题解决算法的必要参与者。 专家知道哪些信息与决策有关且无关，并且该领域的专业知识在限制值得考虑的算法种类方面具有不可估量的价值。 归根结底，专家们将能够查看决策是否正确以及需要采取哪些步骤来解决该问题。</p> 
     <p>Without experts, we cannot design good algorithms, period. It’s telling that <a href="https://www.alphagomovie.com/" rel="noopener nofollow">when DeepMind decided to develop AlphaGo to defeat one of the reigning world champions at the game of Go, they regularly consulted with Go experts to design their algorithm⁵.</a> This idea is so important it’s worth repeating again: domain expertise isn’t peripheral. It’s the whole damn picture.</p> 
     <p> 没有专家，我们就无法设计好的算法。 这说明<a href="https://www.alphagomovie.com/" rel="noopener nofollow">DeepMind决定开发AlphaGo在Go游戏中击败一位统治的世界冠军时，他们会定期与Go专家协商以设计算法⁵。</a> 这个想法是如此重要，值得再次重复：领域专业知识不是外围的。 这是整个该死的图片。</p> 
     <h2> 概括 <span style="font-weight: bold;">(</span>Generalization<span style="font-weight: bold;">)</span></h2> 
     <p>Another key aspect of learning is the ability to <a href="https://developers.google.com/machine-learning/crash-course/generalization/video-lecture" rel="noopener nofollow">generalize. In the Machine Learning community, generalization refers to the ability to learn beyond just the examples you’ve already seen.</a> Any college undergrad knows how to brute-force memorize sections of the textbook, but how well do they really <em>know </em>the ideas they’ve crammed into their heads the night before? The test lies in the ability to answer questions that test similar, but slightly different information to the concepts they’ve studied.</p> 
     <p>学习的另一个关键方面是<a href="https://developers.google.com/machine-learning/crash-course/generalization/video-lecture" rel="noopener nofollow">泛化</a>能力<a href="https://developers.google.com/machine-learning/crash-course/generalization/video-lecture" rel="noopener nofollow">。</a> <a href="https://developers.google.com/machine-learning/crash-course/generalization/video-lecture" rel="noopener nofollow">在机器学习社区中，泛化指的是超越您已经看到的示例的学习能力。</a> 任何一所大学的本科生都知道如何强行记住教科书的各个部分，但是他们到底有多<em>了解</em>前一天晚上塞入他们脑海的想法呢？ 测试在于回答与所研究概念相似但略有不同信息的问题的能力。</p> 
     <p>It’s no different for Deep Neural Networks (incidentally, there are a lot of similarities that can be drawn between DNNs and college undergrads, particularly in terms of decision-making efficacy, but I’m writing a Medium article and not a novel so I’ll leave it at that).</p> 
     <p> 深度神经网络没有什么不同(顺便说一下，DNN和大学本科生之间可以得出很多相似之处，特别是在决策效力方面，但是我写的是中级文章，而不是小说，所以我我会留在那)。</p> 
     <p>There are two main kinds of generalization: generalization within distribution, and out-of-distribution. The first refers to examples where the examples you use to test your network are similar to the ones you used to train it. If I train a network on, say, thousands of pictures of apples, it should be able to identify new apple even if it hasn’t seen that specific image before. DNNs are actually fairly good at this.</p> 
     <p> 归纳主要有两种：分布内的归纳和分布外的归纳。 第一个参考示例，其中用于测试网络的示例与用于训练网络的示例相似。 如果我在数千张苹果图片上训练网络，则即使以前没有看到该特定图片，它也应该能够识别新苹果。 DNN实际上对此非常擅长。</p> 
     <p>But while DNNs require tens of thousands of examples to identify an apple, humans and other intelligent animals can learn after just a handful of examples. <a href="https://arxiv.org/abs/1904.05046" rel="noopener nofollow">This is known as few-shot learning (sometimes used in relation to zero-shot or one-shot learning), referring to the ability to learn or generalize over just a few examples.</a></p> 
     <p> 但是，尽管DNN需要数以万计的示例来识别一个苹果，但人类和其他智能动物只需几个示例就可以学习。 <a href="https://arxiv.org/abs/1904.05046" rel="noopener nofollow">这被称为少拍学习(有时用于零拍或单拍学习)，指的是仅通过几个示例进行学习或概括的能力。</a></p> 
     <p>The second kind of generalization is out-of-distribution learning. Ever notice how your athletic friend seems to be good at not only running but also football and swimming? Or how the math whiz in your class is also great at physics and chemistry? That’s because those concepts, while distinct, are related. <a href="https://ruder.io/transfer-learning/" rel="noopener nofollow">The ability to take one skill and apply it to another domain is known in Machine Learning circles as transfer learning.</a></p> 
     <p> 第二种概括是分布外学习。 是否曾经注意到您的运动朋友似乎不仅擅长跑步，还擅长足球和游泳？ 或者您的班级中的数学天才在物理和化学方面也很出色？ 这是因为这些概念虽然不同，但却是相关的。 <a href="https://ruder.io/transfer-learning/" rel="noopener nofollow">掌握一项技能并将其应用于另一领域的能力在机器学习界称为转移学习。</a></p> 
     <p>Like few-shot learning, the idea of transfer learning centers around the idea that it shouldn’t take thousands of examples to learn concepts, especially if you’ve already learned a similar task before. Importantly, learning a new task shouldn’t require that you <em>unlearn</em> the old one.</p> 
     <p> 像几次学习一样，转移学习的思想围绕这样一个思想，即不应该使用成千上万的示例来学习概念，特别是如果您之前已经学习过类似的任务。 重要的是，学习一项新任务不应要求您<em>取消</em>旧任务。</p> 
     <p>Again, human expertise is necessary to understand how to approach the design of these algorithms as well. When you talk to experts, they often tie distinct examples together, while also describing their differences: “Playing guitar is <em>kind of like</em> playing piano, <em>except</em>…” This is exactly the kind of thing that transfer learning seeks to mimic. Seeking expert opinions is the key to understanding if transfer learning is effective across the domains it seeks to transfer to.</p> 
     <p> 同样，必须有专门的人员才能理解如何进行这些算法的设计。 当您与专家交谈时，他们通常将不同的例子联系在一起，同时也描述了他们的差异：“弹吉他<em>有点像</em>弹钢琴，<em>除了</em>……”这正是转移学习试图模仿的东西。 寻求专家意见是了解转让学习是否在其试图转移到的各个领域有效的关键。</p> 
     <h2> 元学习 <span style="font-weight: bold;">(</span>Meta-learning<span style="font-weight: bold;">)</span></h2> 
     <p>A deep (pardon the pun) question in the Deep Learning community is how to develop algorithms not only to learn but to <em>learn how to learn</em>. This idea of learning how to learn is known as <a href="https://arxiv.org/abs/2004.05439" rel="noopener nofollow">meta-learning</a>, a term just as confusing as it is mysterious.</p> 
     <p>深度学习社区中一个深层的问题是如何开发算法，不仅要学习，而且要<em>学习如何学习</em>。 这种学习如何学习的想法被称为<a href="https://arxiv.org/abs/2004.05439" rel="noopener nofollow">元学习</a>，这个术语既神秘又令人困惑。</p> 
     <p>The key behind meta-learning is this: often what separates an expert from a simpleton is the <em>way that they practice.</em> The best violinists in the world practice for hours on end, but they do so smartly — that’s what makes them the best. If we can develop effective algorithms that can practice <em>smarter</em>, then we can make significant strides towards more-intelligent agents. It’s not hard to see where the value of expertise comes here too. Different domains require different approaches toward a variety of definitions of success. To practice well as a pianist may look very different from practicing well as a swimmer, for example.</p> 
     <p> 元学习背后的关键是这样的：通常，使专家与简单者分开的是<em>他们的实践方式。</em> 世界上最好的小提琴家连续练习了几个小时，但他们做得很聪明-这就是使他们成为最好的小提琴家的原因。 如果我们能够开发出可以实践得更<em>聪明的</em>有效算法，那么我们就可以朝着更智能的代理迈进一大步。 不难看出专业知识的价值在这里也能体现出来。 不同的领域要求对成功的各种定义采取不同的方法。 例如，练好钢琴家看起来与练好运动员完全不同。</p> 
     <p>What “smart learning” looks like may vary from discipline to discipline, ultimately requiring the informed design with human experts at the lead.</p> 
     <p> “智能学习”的外观可能因学科而异，最终需要在人类专家的带领下进行明智的设计。</p> 
     <h2> 期待 <span style="font-weight: bold;">(</span>Looking forward<span style="font-weight: bold;">)</span></h2> 
     <p>As impressive as the accomplishments of Deep Learning are, it’s hard to imagine that any of these networks would have achieved their level of prowess without the experts that were consulted to design and structure them. Human experts, not AI agents, facilitated the curation of datasets, the design of reward functions, and the deployment of these algorithms. On top of that, it’s up to human experts to translate the outputs of these algorithms to decisions and insights in the real world. For now, humans are the agents.</p> 
     <p>尽管深度学习取得了令人印象深刻的成就，但很难想象如果没有经过咨询的专家来设计和构造这些网络，它们中的任何一个都将达到自己的水平。 人类专家而不是AI代理促进了数据集的管理，奖励功能的设计以及这些算法的部署。 最重要的是，由人类专家将这些算法的输出转换为现实世界中的决策和见解。 目前，人类是主体。</p> 
     <p>In the future, as Deep Learning becomes more prevalent in industry applications, people outside the field will be faced with adapting their roles to accommodate these new and unfamiliar statistical tools. As domain experts, they can contribute meaningfully by considering the role they play in facilitating the ability to interpret, transfer, and teach these algorithms to learn and perform better within their domain of interest. They can interface with ML researchers and engineers by helping to narrow the search space of possible algorithms.</p> 
     <p> 将来，随着深度学习在行业应用中变得越来越普遍，该领域以外的人们将面临着调整其角色以适应这些新的和不熟悉的统计工具的挑战。 作为领域专家，他们可以通过考虑他们在促进解释，转移和教导这些算法的能力方面做出有意义的贡献，以在自己感兴趣的领域内学习和更好地发挥作用。 他们可以帮助缩小可能算法的搜索空间，从而与ML研究人员和工程师建立联系。</p> 
     <p>I like to think of the role of expertise as constraining the set of possible algorithms from <em>infinity </em>to<em> finite</em>, and the role of ML researchers as narrowing from <em>finite</em> to <em>a few</em>. I leave it as an exercise to the reader to determine which of the two is harder.</p> 
     <p> 我喜欢认为专业知识的作用是将可能的算法从<em>无穷限制</em>到<em>有限</em>，而ML研究人员的作用则从<em>有限限制</em>到<em>少数</em>。 我将其留给读者练习，以确定两者中哪个更难。</p> 
     <p>Human expertise isn’t dead, not by a long shot. In fact, in the age of Deep Learning, it may be more important than ever.</p> 
     <p> 人类的专业知识不会死，也不会长远。 实际上，在深度学习时代，它可能比以往任何时候都更加重要。</p> 
    </div> 
   </div> 
  </section> 
 </div> 
 <blockquote> 
  <p>翻译自: <a href="https://medium.com/swlh/human-domain-expertise-in-the-age-of-deep-learning-89b3381c5cba" rel="nofollow">https://medium.com/swlh/human-domain-expertise-in-the-age-of-deep-learning-89b3381c5cba</a></p> 
 </blockquote> 
 <p>深度学习领域专业词汇</p> 
</article>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/406a2c54414eecb80ba5e6e36cfd6619/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">聊天产生器</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2dbdb2622d5f1642ae80e5cd0ca29039/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">ECS_FML——小议高斯分布</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>