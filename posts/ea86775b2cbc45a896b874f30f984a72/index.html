<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>clickhouse 在货拉拉的应用实践，千亿级别数据实现秒级查询 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="clickhouse 在货拉拉的应用实践，千亿级别数据实现秒级查询" />
<meta property="og:description" content="作者：扬大平仔
前携程、网易高级工程师，现为货拉拉高级工程师。热爱技术，敢于将新技术用于项目实践。
前言
为了解决线上问题定位慢，相应不及时等问题。所以我们决定开发一套智能问题定位系统。对于我们的一些核心系统，每个订单会对应推送多个司机（线上最多达到上千个司机，平均也有几百个司机）。如果要每个司机都记录一条埋点信息那么数据量将会非常庞大，目前埋点数据日均60&#43;亿，一个月接近2000亿数据。这种以司机维度存储数据会存在大量的数据冗余，耗费大量存储等机器资源。并且传统的关系型数据库在动则几十亿到上千亿级别的表上查询，几乎做不到秒级响应。前期我们调研的时候考虑过使用hbase 、druid、kudu等，但这些要么不太适合我们的业务要求，要么就是公司还不支持。后来调研了clickhouse 后，如沐春风。使用clickhouse 能够绝佳的满足我们的需求。首先，clickhouse 不但查询速度快，而且能够像mysql 那样支持多维度查询；其次，clickhouse 的嵌套类型 能够很好的满足我们订单和司机一对多的存储要求。调研后决定存储复合结构数据，即订单维度存储，一个订单对应多个司机。
一、什么是clickhouse？
ClickHouse是一个用于联机分析(OLAP)的列式数据库管理系统(DBMS)。ClickHouse是Yandex(俄罗斯最大的搜索引擎)开源的一个用于实时数据分析的基于列存储的数据库，其处理数据的速度比传统方法快100-1000倍。ClickHouse的性能超过了目前市场上可比的面向列的DBMS，每秒钟每台服务器每秒处理数亿至十亿多行和数十千兆字节的数据。
详细可以移步官方文档：什么是ClickHouse？ | ClickHouse文档
二、ClickHouse在具体业务的实践
1.系统基本设计
为了更好的收集机构化的数据，我们自己开发了一款埋点sdk用于收集各业务放的埋点日志，日志统一异步发送Kafka，然后我们在大数据平台使用flink消费Kafka数据，经过一定处理后写入click house。然后我们也开发了一个专门用于在线查询的系统对外提供查询服务。
2.具体实现
flink 摄入 因为写clickhouse底层都是使用httpclient的方式写入的，所以对于clickhouse来说单条频繁写入效率很低，适合批量写入。官网建议没批次写入100000&#43;条（要视flink TM 内存大小调整，防止批量过大出息oom）。我们自定义了sink 用于摄入clickhouse，达到一定批次或者执行checkpoint时就写入一次。
clickhouse 部署 为了实现高可用，在具体部署上我们采用的是多副本，写本地表查分布式表。ck的表分为两种:
分布式表：
一个逻辑上的表, 可以理解为数据库中的视图, 一般查询都查询分布式表. 分布式表引擎会将我们的查询请求路由本地表进行查询, 然后进行汇总最终返回给用户。
本地表:
实际存储数据的表
1. 不写分布式表的原因
分布式表接收到数据后会将数据拆分成多个parts, 并转发数据到其它服务器, 会引起服务器间网络流量增加、服务器merge的工作量增加, 导致写入速度变慢, 并且增加了Too many parts的可能性.
数据的一致性问题, 先在分布式表所在的机器进行落盘, 然后异步的发送到本地表所在机器进行存储，中间没有一致性的校验, 而且在分布式表所在机器时如果机器出现down机, 会存在数据丢失风险.
数据写入默认是异步的，短时间内可能造成不一致.
2. Replication &amp; Sharding
ClickHouse依靠ReplicatedMergeTree引擎族与ZooKeeper实现了复制表机制, 成为其高可用的基础。ClickHouse像ElasticSearch一样具有数据分片(shard)的概念, 这也是分布式存储的特点之一, 即通过并行读写提高效率. ClickHouse依靠Distributed引擎实现了分布式表机制, 在所有分片（本地表）上建立视图进行分布式查询。
三、线上问题以及解决方案 3.1 线上问题 问题1： 司机明细相关业务查询极其缓慢，一个查询往往要耗时15s， 订单明细相关查询却很快。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/ea86775b2cbc45a896b874f30f984a72/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-09-29T15:48:12+08:00" />
<meta property="article:modified_time" content="2021-09-29T15:48:12+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">clickhouse 在货拉拉的应用实践，千亿级别数据实现秒级查询</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>作者：扬大平仔</p> 
<p>前携程、网易高级工程师，现为货拉拉高级工程师。热爱技术，敢于将新技术用于项目实践。</p> 
<p id="main-toc"><br><strong><strong>前言</strong></strong></p> 
<p style="margin-left:.0001pt;text-align:justify;">        为了解决线上问题定位慢，相应不及时等问题。所以我们决定开发一套智能问题定位系统。对于我们的一些核心系统，每个订单会对应推送多个司机（线上最多达到上千个司机，平均也有几百个司机）。如果要每个司机都记录一条埋点信息那么数据量将会非常庞大，目前埋点数据日均60+亿，一个月接近2000亿数据。这种以司机维度存储数据会存在大量的数据冗余，耗费大量存储等机器资源。并且传统的关系型数据库在动则几十亿到上千亿级别的表上查询，几乎做不到秒级响应。前期我们调研的时候考虑过使用hbase 、druid、kudu等，但这些要么不太适合我们的业务要求，要么就是公司还不支持。后来调研了clickhouse 后，如沐春风。使用clickhouse 能够绝佳的满足我们的需求。首先，clickhouse 不但查询速度快，而且能够像mysql 那样支持多维度查询；其次，clickhouse 的<a href="https://clickhouse.tech/docs/zh/sql-reference/data-types/nested-data-structures/nested/" rel="nofollow" title="嵌套类型">嵌套类型</a> 能够很好的满足我们订单和司机一对多的存储要求。调研后决定存储复合结构数据，即订单维度存储，一个订单对应多个司机。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><br><strong><strong>一、什么是clickhouse？</strong></strong></p> 
<p style="margin-left:.0001pt;text-align:justify;">        ClickHouse是一个用于联机分析(OLAP)的列式数据库管理系统(DBMS)。ClickHouse是Yandex(俄罗斯最大的搜索引擎)开源的一个用于实时数据分析的基于列存储的数据库，其处理数据的速度比传统方法快100-1000倍。ClickHouse的性能超过了目前市场上可比的面向列的DBMS，每秒钟每台服务器每秒处理数亿至十亿多行和数十千兆字节的数据。</p> 
<p style="margin-left:.0001pt;text-align:justify;">详细可以移步官方文档：<a href="https://clickhouse.tech/docs/zh/" rel="nofollow" title="什么是ClickHouse？ | ClickHouse文档">什么是ClickHouse？ | ClickHouse文档</a></p> 
<p style="margin-left:.0001pt;text-align:justify;"><br><strong><strong>二、ClickHouse在具体业务的实践</strong></strong></p> 
<p style="margin-left:.0001pt;text-align:justify;"><br><strong><strong>1.系统基本设计</strong></strong></p> 
<p style="margin-left:.0001pt;text-align:justify;">为了更好的收集机构化的数据，我们自己开发了一款埋点sdk用于收集各业务放的埋点日志，日志统一异步发送Kafka，然后我们在大数据平台使用flink消费Kafka数据，经过一定处理后写入click house。然后我们也开发了一个专门用于在线查询的系统对外提供查询服务。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="284" src="https://images2.imgbox.com/99/56/K29wdRtK_o.png" width="1200"></p> 
<p></p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><strong>2.具体实现</strong></strong></p> 
<ul><li style="text-align:justify;"> flink 摄入</li></ul> 
<p style="margin-left:.0001pt;text-align:justify;">        因为写clickhouse底层都是使用httpclient的方式写入的，所以对于clickhouse来说单条频繁写入效率很低，适合批量写入。官网建议没批次写入100000+条（要视flink TM 内存大小调整，防止批量过大出息oom）。我们自定义了sink 用于摄入clickhouse，达到一定批次或者执行checkpoint时就写入一次。</p> 
<ul><li style="text-align:justify;">clickhouse 部署</li></ul> 
<p style="margin-left:.0001pt;text-align:justify;">        为了实现高可用，在具体部署上我们采用的是多副本，写本地表查分布式表。ck的表分为两种:</p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><strong>分布式表</strong></strong><strong><strong>：</strong></strong></p> 
<p style="margin-left:.0001pt;text-align:justify;">一个逻辑上的表, 可以理解为数据库中的视图, 一般查询都查询分布式表. 分布式表引擎会将我们的查询请求路由本地表进行查询, 然后进行汇总最终返回给用户。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><strong>本地表:</strong></strong></p> 
<p style="margin-left:.0001pt;text-align:justify;">实际存储数据的表</p> 
<p style="margin-left:.0001pt;text-align:justify;">1. 不写分布式表的原因</p> 
<p style="margin-left:.0001pt;text-align:justify;">分布式表接收到数据后会将数据拆分成多个parts, 并转发数据到其它服务器, 会引起服务器间网络流量增加、服务器merge的工作量增加, 导致写入速度变慢, 并且增加了Too many parts的可能性.</p> 
<p style="margin-left:.0001pt;text-align:justify;">数据的一致性问题, 先在分布式表所在的机器进行落盘, 然后异步的发送到本地表所在机器进行存储，中间没有一致性的校验, 而且在分布式表所在机器时如果机器出现down机, 会存在数据丢失风险.</p> 
<p style="margin-left:.0001pt;text-align:justify;">数据写入默认是异步的，短时间内可能造成不一致.</p> 
<p style="margin-left:.0001pt;text-align:justify;">2. Replication &amp; Sharding</p> 
<p style="margin-left:.0001pt;text-align:justify;">ClickHouse依靠ReplicatedMergeTree引擎族与ZooKeeper实现了复制表机制, 成为其高可用的基础。ClickHouse像ElasticSearch一样具有数据分片(shard)的概念, 这也是分布式存储的特点之一, 即通过并行读写提高效率. ClickHouse依靠Distributed引擎实现了分布式表机制, 在所有分片（本地表）上建立视图进行分布式查询。</p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<h3 id="%E4%B8%89%E3%80%81%E7%BA%BF%E4%B8%8A%E9%97%AE%E9%A2%98%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88" style="text-align:left;"><strong><strong><strong>三、线上问题以及解决方案</strong></strong></strong></h3> 
<h4 id="3.1%20%E7%BA%BF%E4%B8%8A%E9%97%AE%E9%A2%98" style="text-align:left;"><strong><strong><strong>3.1 </strong></strong><strong><strong>线上问题</strong></strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">问题1： 司机明细相关业务查询极其缓慢，一个查询往往要耗时15s， 订单明细相关查询却很快。</p> 
<p style="margin-left:.0001pt;text-align:justify;">问题2： 汇总相关指标查询特别慢，一个查询接近20S</p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong> </strong><strong><strong>3</strong></strong><strong><strong>.</strong></strong><strong><strong>2</strong></strong><strong><strong>尝试解决方案  </strong></strong>   </p> 
<p style="margin-left:.0001pt;text-align:justify;">         通过分析发现，查询缓慢的都是针对司机维度查询的。虽然我们针对司机ID 这个维度建了一个二级的索引 （INDEX driverId_idx drivers.driverId TYPE bloom_filter(0.01) GRANULARITY 5），也分析了查询sql 的执行计划，重执行日志上看是使用到了我们建的二级索引的。但查询还是十分的慢，这完全超过了我们的预期。</p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<ul><li style="text-align:justify;"><strong><strong>尝试绕过嵌套类型作为查询条件</strong></strong></li></ul> 
<p style="margin-left:.0001pt;text-align:left;">        一开始分析发现，查询慢的都是因为在查询条件中使用了嵌套类型的司机ID作为查询条件，这导致了查询十分缓慢。顾我们尝试使用一个额外的字段来绕过嵌套类型查询。新增一个driverIds 字段，里面包含所有司机ID（多个司机ID用，拼接），在这个字段上建索引。查询司机场景时通过 driverIds like %%查询；使用driverIds作为条件like查询，2600万数据查询耗时3+s左右，性能能提升一倍，但还是不够理想，随着数据量上升，耗时将越来越大。</p> 
<pre><code class="language-sql">Select appId,arrayElement(driversDriverId, index) as driversDriverId 
,pushId,orderId,uuid,orderPushType,orderVehicleId,eventTime 
,arrayElement(driversAppStatus, index)as driversAppStatus,eventLogEvent, arrayElement(isWinner, index) as isWinner 
from (Select arrayFirstIndex(id -&gt; id = '9047485', drivers.driverId) as index,appId, 
drivers.driverId as driversDriverId,pushId,orderId,uuid,orderPushType,orderVehicleId,eventTime 
,drivers.appStatus as driversAppStatus,eventLogEvent,drivers.isWinner as isWinner 
from ai_XX_log_db.ai_XXXX_all prewhere driverIds like '%9047485%' 
AND toUnixTimestamp(eventTime) &gt;= 1629907200 AND toUnixTimestamp(eventTime) &lt;= 1629993600 
AND appId = 'XXX' AND scene = 'XXXX' ) 
ORDER BY eventTime DESC LIMIT 1,20;</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<ul><li style="text-align:left;"><strong><strong><strong>通过sq执行计划进行分析</strong></strong></strong></li></ul> 
<p style="margin-left:.0001pt;text-align:justify;">        尝试绕过嵌套类型作为查询条件带来的性能提升并不够理想，于是我们决定深入了解clickhouse的一些底层机制。尝试通过SQL执行计划来确定一个sql 的查询瓶颈。目前查看sql 执行计划有两种方法，方法一（20.6之前版本）</p> 
<p style="margin-left:.0001pt;text-align:justify;">clickhouse-client -u xxxx --password xxxxxx --send_logs_level=trace &lt;&lt;&lt; 'your query sql' &gt; /dev/null</p> 
<p style="margin-left:.0001pt;text-align:justify;"> 方法二（20.6与20.6之后版本）</p> 
<p style="margin-left:.0001pt;text-align:justify;">explain SQL</p> 
<p style="margin-left:.0001pt;text-align:justify;">方法一是指定clickhouse 执行日志级别为trace，这样可以打印出来sql 各个阶段执行的日志，通过日志型来分析SQL执行情况，能够详细的了解到SQL执行情况。方法二有点像mysql那样，但这个只能打印部分SQL执行情况，不够详细。所以我们最终使用了方法一。</p> 
<p style="margin-left:.0001pt;text-align:justify;">部分问题SQL的执行计划日志：</p> 
<p style="margin-left:.0001pt;text-align:center;"><br><img alt="" src="https://images2.imgbox.com/95/a3/n9JhmRUV_o.png"></p> 
<p></p> 
<p></p> 
<p style="margin-left:.0001pt;text-align:left;">测试环境2600W 查询耗时大概7.3S+， 生产环境2+亿数据查询耗时15+s。</p> 
<p style="margin-left:.0001pt;text-align:left;">从上面的执行计划看，driverId_idx 是使用到了的，但主键索引和分区所以都没有用到。没有用到主键索引是导致查询慢的主要原因。至于为什么，这个要从clickhouse的底层存储结构说了，这里不详细说明，想了解的可以去看看这两个帖子：</p> 
<p style="margin-left:.0001pt;text-align:left;">1、<a href="https://developer.aliyun.com/article/780402?utm_content=g_1000223389" rel="nofollow" title="云数据库ClickHouse二级索引-最佳实践">云数据库ClickHouse二级索引-最佳实践</a> </p> 
<p style="margin-left:.0001pt;text-align:left;">2、<a href="https://developer.aliyun.com/article/762092" rel="nofollow" title="ClickHouse内核分析-MergeTree的存储结构和查询加速">ClickHouse内核分析-MergeTree的存储结构和查询加速</a></p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<ul><li style="text-align:justify;"><strong><strong>提出解决方案</strong></strong></li></ul> 
<p style="margin-left:.0001pt;text-align:justify;">        通过上面的一系列分析，我们知道导致click house查询慢的根本原因是没有使用到clickhouse主键索引（一级索引）。click house 的存储结构决定对于大数据量查询时，使用主键索引能够精确的找到所需的数据块，减少不必要的数据块扫描，这样更够极大的提高查询效率。顾为了使用司机ID查询的时候也能够用到主键索引，所以我们尝试自己建立一个索引映射机制，类似于hbase 做二级索引。首先建立一张司机和订单的映射表，将查询司机的场景也映射到通过订单查询的场景。如果存储司机维度的数据，那么每天需要存储60亿数据，一个月就是1800亿，这是相当庞大的数据量。为此我们考虑到了分表策略，取司机ID最后一位作为表后缀，即分十张表存储数据，将数据控制在百亿级别。在数据这块我们是使用flink摄入数据的，使用分表了策略后我们又自定义了分表摄入的策略。</p> 
<p style="margin-left:.0001pt;text-align:justify;">1、建立司机和订单映射表：</p> 
<p style="margin-left:.0001pt;text-align:justify;">其中一张表：</p> 
<pre><code class="language-sql">CREATE TABLE ai_XX_log_db.ai_XXX_log_driver_local_0 ON CLUSTER ck_cluster (
orderId String,
driverId String,
scene String,
eventTime DateTime 
) ENGINE ReplicatedMergeTree('/clickhouse_ai/tables/{shard}/ai_XXX_log_db/ai_XXX_log_driver_local_0', '{replica}')
PARTITION BY toYYYYMMDD(eventTime) TTL eventTime + INTERVAL 14 DAY
ORDER BY(driverId, eventTime, scene)
SETTINGS index_granularity = 8192;</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p style="margin-left:.0001pt;text-align:justify;">在测试环境中单表数据在800+亿，通过司机ID、时间和scene 查询订单ID耗时在200ms 以内。然后通过订单ID到主表中查询埋点记录，耗时在300ms以内。</p> 
<p style="margin-left:.0001pt;text-align:justify;">完整查询示例：</p> 
<pre><code class="language-sql">select orderId,orderTime,eventTime,drivers.driverId as driversDriverId , 
drivers.isWinner as isWinner,drivers.pkRule as pkRule, 
eventLogLogicType,eventLogRefProp, eventLogEvent 
from ai_XX_log_db.ai_XXXevent_log_all prewhere scene = 'pk_driver_result' 
AND toUnixTimestamp(eventTime) &gt;=1630857600 AND toUnixTimestamp(eventTime) &lt;= 1630944000 
AND appId = 'ai-pk-api' 
AND has(drivers.driverId, '1899302024') 
AND orderId IN ( 
select DISTINCT orderId from ai_order_log_db.ai_XXX_log_driver_all_4 
WHERE driverId = '1899302024' 
AND toUnixTimestamp(eventTime) &gt;=1630857600 AND toUnixTimestamp(eventTime) &lt;= 1630944000 
AND scene = 'pk_driver_result' 
) 
ORDER BY eventTime DESC</code></pre> 
<p style="margin-left:.0001pt;text-align:left;"></p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p style="margin-left:.0001pt;text-align:justify;">通过这样优化后我们绝大部分司机维度查询场景耗时在500ms 以内，有些查询耗时在200ms以内，性能较原来的最高提高了近百倍。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/55cd07983b05d80e04414bf1e498eb43/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">android项目内导入zxing库</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2d857deae6fff5aaad97cea170020996/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">安装 ubuntu18.04 出现 Device/Credential Guard 不兼容、启动蓝屏等问题的解决思路</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>