<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【论文解读】用于概念标定的逻辑强化大模型LEFT（NeurIPS 2023） - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【论文解读】用于概念标定的逻辑强化大模型LEFT（NeurIPS 2023）" />
<meta property="og:description" content="来源：投稿 作者：橡皮
编辑：学姐
论文链接：https://arxiv.org/abs/2310.16035
开源代码：https://github.com/joyhsu0504/LEFT
摘要： VisProg 和 ViperGPT 等最新研究成果巧妙地组成了视觉推理的基础模型--利用大型语言模型（LLM）生成可由预先训练的视觉语言模型执行的程序。然而，它们只能在有限的领域（如二维图像）中运行，无法充分发挥语言的通用性：像 &#34;左 &#34;这样的抽象概念也可以基于三维、时间和动作数据，如向左移动。这种有限的泛化能力源于这些纯推理方法无法学习或调整预先训练好的模型以适应新的领域。我们提出了逻辑增强基础模型（LEFT），这是一个统一的框架，可以通过一个可区分的、与领域无关的、基于一阶逻辑的程序执行器，学习跨领域的概念基础和推理。LEFT 有一个 LLM 解释器，可输出以通用逻辑推理语言表示的程序，该语言可在所有领域和任务中共享。然后，LEFT 的执行器通过可训练的特定领域基础模块执行程序。我们展示了 LEFT 在四个领域中灵活学习概念的能力： 二维图像、三维场景、人体动作和机器人操作。它在各种任务中都表现出强大的推理能力，包括那些复杂的、在训练过程中未见过的任务，而且可以轻松地应用到新的领域。
图 1：LEFT 是一个统一的概念学习和推理框架，可跨领域地使用模块化概念，并通过基础模型灵活地在不同任务中使用概念进行推理。
1.引言 语言的力量在于其基于抽象的概括能力。一个单一的概念，如 &#34;左&#34;，可以跨领域使用：在感知中，我们可以从椅子的三维点云中识别出左腿；在导航中，我们可以绕过街区尽头的咖啡馆左转回家；在操作中，我们可以在盘子的左边挑选并放置一个杯子。左图在每个领域都有特定的基础，但概念符号本身作为一种抽象概念，是跨领域复杂和多步骤推理的基础。
用于视觉推理的机器系统也受益于概念学习与推理的分离。最近，VisProg和 ViperGPT等模型利用大型语言模型（LLM）来生成基于语言查询的程序。例如，要回答 &#34;猫的颜色是什么？&#34;，程序首先会定位猫的位置，然后查询猫的颜色。这些程序使用预先定义和训练好的视觉语言模型（如开放式词汇分割模型）进行定位和问题解答。这些作品在基于图像的推理方面表现出色，但令人惊讶的是，尽管语言似乎免费提供了通用性，但在其他领域的成功案例却不多。
要让可视化推理系统跨领域发挥作用，还剩下什么？我们发现现有方法存在两个关键缺陷。首先，这些方法仅限于推理，完全依靠预先训练好的模型来解释程序执行过程中的 &#34;左 &#34;等概念。因此，在数据稀缺的领域（如三维、人类运动或机器人动作），预训练模型（如果有的话）的表现并不理想。因此，我们需要可训练的系统来学习特定领域的基础概念，以便跨领域执行。不幸的是，这些可视化推理模型也无法做到可训练，因为它们存在第二个缺陷：它们依赖于一个不可分的程序执行器来组成这些基础模块。因此，尽管三维场景和机器人动作的基础可能不同于二维图像，但这些模块无法通过在这些领域的额外训练进行调整。
为了解决这些问题，我们提出了逻辑增强基础模型（LEFT），这是一种学习跨领域概念基础和推理的模型。LEFT 还利用 LLMs 进行语言解析；但与之前的工作不同的是，LEFT 具有可训练的概念基础模块，这些模块可从领域数据中学习，并通过一个可微分的、与领域无关的、基于一阶逻辑的程序执行器来实现。受之前通过可微分执行将符号程序与深度模型相结合的研究成果的启发，我们的设计不再需要人工设计特定领域的语言，而且有利于跨领域泛化。具体来说，LEFT 的 LLM 解释器将语言查询作为输入，解决文本歧义，并输出以一阶逻辑通用推理语言表示的非歧义程序，该程序可跨领域和任务共享。然后，LEFT 的执行器利用可学习的特定领域基础模块执行逻辑程序，这些模块由 LLM 生成的概念自动初始化。
LEFT 采用模块化结构，具有跨领域的高性能和数据效率优势。在单个领域内，LEFT 利用 LLM 解释器的一般推理能力，并通过通用一阶逻辑执行器有效地重新组合所学到的基础概念，从而将 &#34;0-shot &#34;泛化为未见过的复杂任务。LEFT 可被视为 VisProg 和 ViperGPT 的通用框架；在可获得预训练模型且不需要训练的领域（如二维图像），LEFT 同样可以仅用于推理。
我们在四个不同领域和七项任务上验证了 LEFT 的性能，包括二维问题解答、三维指代表达理解、时序推理和机器人操作。在性能和数据效率方面，通用 LEFT 模型明显优于之前针对特定任务的单一方法，其性能可与之前的神经符号方法相媲美，而且无需针对每个领域预先定义程序实现。重要的是，统一的 LEFT 框架可以进行跨领域的概念学习，并能在三个具有挑战性且从未见过的推理任务中实现零转移。相比之下，仅基于 LLM 的推理方法和一般视觉语言模型则无法实现泛化。
2.相关工作 基于LLM的分解框架。我们的框架集成了神经网络、逻辑推理和用于常识推理的大型语言模型。第一组相关文献研究基于LLM的方法。受 LLM 成功的启发 ，最近的许多工作提出了利用 LLM 将基于文本的任务分解为对现有模型的 API 调用序列的框架 。与 LEFT 相比，这些方法仅使用 LLM 和 API 模型进行推理，并且仅限于语言领域，无需学习模态基础。例如，LLM可能能够推理从语言推断出的对象类别，但它无法识别当前场景中的候选对象，或生成机器人动作来移动对象。 Gupta 和 Kembhavi以及 Surís 等人在图像上执行程序，但假设 API 可以访问一组可用模块，无需进一步培训。相比之下，LEFT 通过训练每种模态的概念嵌入来学习跨不同领域的模块化概念，而不需要任何预定义和预训练的模块。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/92fcefc791282dafa590a801f1771154/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-29T16:09:36+08:00" />
<meta property="article:modified_time" content="2023-12-29T16:09:36+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【论文解读】用于概念标定的逻辑强化大模型LEFT（NeurIPS 2023）</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <blockquote> 
 <p>来源：投稿 作者：橡皮<br> 编辑：学姐</p> 
</blockquote> 
<p></p> 
<p class="img-center"><img alt="" height="302" src="https://images2.imgbox.com/ef/e9/kEiEvbP9_o.png" width="936"></p> 
<p>论文链接：https://arxiv.org/abs/2310.16035</p> 
<p>开源代码：https://github.com/joyhsu0504/LEFT</p> 
<h3>摘要：</h3> 
<p>VisProg 和 ViperGPT 等最新研究成果巧妙地组成了视觉推理的基础模型--利用大型语言模型（LLM）生成可由预先训练的视觉语言模型执行的程序。然而，它们只能在有限的领域（如二维图像）中运行，无法充分发挥语言的通用性：像 "左 "这样的抽象概念也可以基于三维、时间和动作数据，如向左移动。这种有限的泛化能力源于这些纯推理方法无法学习或调整预先训练好的模型以适应新的领域。我们提出了逻辑增强基础模型（LEFT），这是一个统一的框架，可以通过一个可区分的、与领域无关的、基于一阶逻辑的程序执行器，学习跨领域的概念基础和推理。LEFT 有一个 LLM 解释器，可输出以通用逻辑推理语言表示的程序，该语言可在所有领域和任务中共享。然后，LEFT 的执行器通过可训练的特定领域基础模块执行程序。我们展示了 LEFT 在四个领域中灵活学习概念的能力： 二维图像、三维场景、人体动作和机器人操作。它在各种任务中都表现出强大的推理能力，包括那些复杂的、在训练过程中未见过的任务，而且可以轻松地应用到新的领域。</p> 
<p></p> 
<p class="img-center"><img alt="" height="336" src="https://images2.imgbox.com/aa/20/xdb1pbdb_o.png" width="1066"></p> 
<p>图 1：LEFT 是一个统一的概念学习和推理框架，可跨领域地使用模块化概念，并通过基础模型灵活地在不同任务中使用概念进行推理。</p> 
<h3>1.引言</h3> 
<p>语言的力量在于其基于抽象的概括能力。一个单一的概念，如 "左"，可以跨领域使用：在感知中，我们可以从椅子的三维点云中识别出左腿；在导航中，我们可以绕过街区尽头的咖啡馆左转回家；在操作中，我们可以在盘子的左边挑选并放置一个杯子。左图在每个领域都有特定的基础，但概念符号本身作为一种抽象概念，是跨领域复杂和多步骤推理的基础。</p> 
<p>用于视觉推理的机器系统也受益于概念学习与推理的分离。最近，VisProg和 ViperGPT等模型利用大型语言模型（LLM）来生成基于语言查询的程序。例如，要回答 "猫的颜色是什么？"，程序首先会定位猫的位置，然后查询猫的颜色。这些程序使用预先定义和训练好的视觉语言模型（如开放式词汇分割模型）进行定位和问题解答。这些作品在基于图像的推理方面表现出色，但令人惊讶的是，尽管语言似乎免费提供了通用性，但在其他领域的成功案例却不多。</p> 
<p>要让可视化推理系统跨领域发挥作用，还剩下什么？我们发现现有方法存在两个关键缺陷。首先，这些方法仅限于推理，完全依靠预先训练好的模型来解释程序执行过程中的 "左 "等概念。因此，在数据稀缺的领域（如三维、人类运动或机器人动作），预训练模型（如果有的话）的表现并不理想。因此，我们需要可训练的系统来学习特定领域的基础概念，以便跨领域执行。不幸的是，这些可视化推理模型也无法做到可训练，因为它们存在第二个缺陷：它们依赖于一个不可分的程序执行器来组成这些基础模块。因此，尽管三维场景和机器人动作的基础可能不同于二维图像，但这些模块无法通过在这些领域的额外训练进行调整。</p> 
<p>为了解决这些问题，我们提出了逻辑增强基础模型（LEFT），这是一种学习跨领域概念基础和推理的模型。LEFT 还利用 LLMs 进行语言解析；但与之前的工作不同的是，LEFT 具有可训练的概念基础模块，这些模块可从领域数据中学习，并通过一个可微分的、与领域无关的、基于一阶逻辑的程序执行器来实现。受之前通过可微分执行将符号程序与深度模型相结合的研究成果的启发，我们的设计不再需要人工设计特定领域的语言，而且有利于跨领域泛化。具体来说，LEFT 的 LLM 解释器将语言查询作为输入，解决文本歧义，并输出以一阶逻辑通用推理语言表示的非歧义程序，该程序可跨领域和任务共享。然后，LEFT 的执行器利用可学习的特定领域基础模块执行逻辑程序，这些模块由 LLM 生成的概念自动初始化。</p> 
<p>LEFT 采用模块化结构，具有跨领域的高性能和数据效率优势。在单个领域内，LEFT 利用 LLM 解释器的一般推理能力，并通过通用一阶逻辑执行器有效地重新组合所学到的基础概念，从而将 "0-shot "泛化为未见过的复杂任务。LEFT 可被视为 VisProg 和 ViperGPT 的通用框架；在可获得预训练模型且不需要训练的领域（如二维图像），LEFT 同样可以仅用于推理。</p> 
<p>我们在四个不同领域和七项任务上验证了 LEFT 的性能，包括二维问题解答、三维指代表达理解、时序推理和机器人操作。在性能和数据效率方面，通用 LEFT 模型明显优于之前针对特定任务的单一方法，其性能可与之前的神经符号方法相媲美，而且无需针对每个领域预先定义程序实现。重要的是，统一的 LEFT 框架可以进行跨领域的概念学习，并能在三个具有挑战性且从未见过的推理任务中实现零转移。相比之下，仅基于 LLM 的推理方法和一般视觉语言模型则无法实现泛化。</p> 
<h3>2.相关工作</h3> 
<p>基于LLM的分解框架。我们的框架集成了神经网络、逻辑推理和用于常识推理的大型语言模型。第一组相关文献研究基于LLM的方法。受 LLM 成功的启发 ，最近的许多工作提出了利用 LLM 将基于文本的任务分解为对现有模型的 API 调用序列的框架 。与 LEFT 相比，这些方法仅使用 LLM 和 API 模型进行推理，并且仅限于语言领域，无需学习模态基础。例如，LLM可能能够推理从语言推断出的对象类别，但它无法识别当前场景中的候选对象，或生成机器人动作来移动对象。 Gupta 和 Kembhavi以及 Surís 等人在图像上执行程序，但假设 API 可以访问一组可用模块，无需进一步培训。相比之下，LEFT 通过训练每种模态的概念嵌入来学习跨不同领域的模块化概念，而不需要任何预定义和预训练的模块。</p> 
<p>通用视觉语言模型。近年来，统一视觉语言模型（VLM）在多模态领域的学习中取得了成功。作为一个代表性的例子，Flamingo利用强大的预训练视觉和语言模型来实现语言生成任务的小样本学习，交错图像、视频和文本，然后输出相关文本。然而，这些统一的 VLM 和广泛的端到端方法在不同领域的应用仍然受到限制。此外，在实践中，他们还要努力应对更复杂的看不见的任务。相比之下，LEFT 在跨领域表现良好，并且可以推广到从未见过的新的具有挑战性的任务。</p> 
<p>神经符号方法。将程序与神经网络相结合的神经符号方法已在各种视觉推理领域取得了成功，其模块化设计具有强大的数据效率和泛化能力。神经符号 VQA提出了用于问答任务的符号程序执行，以及神经符号概念学习器通过取消密集监督的要求，进一步改进了训练范式。受到二维图像领域成功的启发，神经符号方法已被引入 3D 场景中的基础，时间序列推理，以及机器人操作。然而，这些神经符号工作需要预定义的特定领域语言，以便将任务指令解析为一组受约束的程序，然后用代码手动实现每个程序。因此，将常识推理纳入其中或推广到不同领域是具有挑战性的。相比之下，LEFT 保留了神经符号学习的所有优点，同时提出了适用于所有领域的通用语言。 LEFT 既不需要任何特定于域的定义，也不需要特定于域的程序示例。它仅利用通过 LLM 的一阶逻辑用法的最小提示示例编码的结构；因此，它通常适用于跨域。</p> 
<h3>3.逻辑强化基础模型 (LEFT)</h3> 
<p>我们的逻辑增强基础模型（LEFT）是一个用于不同领域和任务的概念学习与推理的统一框架。它将大型语言模型与可微分逻辑模块以及模块化神经网络整合在一起，为每种模式中的概念提供基础。如图 2 所示，我们的系统由三个主要部分组成：</p> 
<ol><li> <p>第一个是与领域无关的 LLM 语言解释器，它生成一阶逻辑查询，并将其发送给 FOL 执行引擎（第 3.1 节）。生成的符号程序用分层一阶逻辑结构表示。</p> </li><li> <p>第二个是与领域无关的 FOL 执行器，它根据领域中的实体特征（对象、关系、动作等）有区别地执行逻辑程序（第 3.2 节）。(第 3.2 节）。LEFT 的执行器是完全可微分的，因此可以进行反向传播。</p> </li><li> <p>第三个模块是特定领域的基础模块，由每种模态的编码器（用于提取以实体为中心的特征和关系特征）以及相应的概念嵌入（以模块化神经网络的形式实现）组成（第 3.3 节）。概念基础模块由 LLM 解释器生成的概念初始化。</p> </li></ol> 
<p>LEFT 是一个由独立于领域的推理模块（由 LLM 解释器和 FOL 执行器组成）和特定领域的基础模块组成的框架。通过这些组件，LEFT 可以进行跨领域和跨任务的概念学习和推理。</p> 
<h4>3.1 独立于领域的 LLM 解释器</h4> 
<p></p> 
<p class="img-center"><img alt="" height="620" src="https://images2.imgbox.com/24/d3/NfAe1KM5_o.png" width="801"></p> 
<p>图 2：LEFT 的核心是一个由基础模型驱动的语言解释器（可从语言查询生成一阶逻辑（FOL）程序）、一个独立于领域的可微分 FOL 执行器（可对所有类型的实体进行操作）以及特定领域的接地模块（可将一阶逻辑语言中的符号与特定领域的特征连接起来）。</p> 
<p>我们利用预先训练好的大型语言模型作为独立于领域的语言解释器。它将语言查询作为输入，并为下游模块生成一阶逻辑查询。我们用来提示 LLM 的示例是一组与领域无关的通用示例和 FOL 语言语法规则的系统级描述。LLM 同时处理自然语言理解和常识推理。例如，它需要解决核心参照和模棱两可的修饰语附加问题： "看书架上的书，蓝色的那本，选择它左边的物体"。此外，它还会执行常识推理，例如 "找到一个能放入一排蓝色球体中的物体"。它为下游 FOL 推理模块生成的查询将是非歧义程序：ιx.blue(x) ∧ sphere(x)。这种策略最大限度地利用了 LLM 跨任务的推理能力，同时保留了强大的泛化能力。我们在实验中证明，LEFT 解释器可以进行类比推理、解决核心参照问题，并解决复杂的难题。</p> 
<p>从形式上看，给定一个语言输入任务查询 Q，我们的 LLM 解释器就会输出程序 P，这些程序是用一阶逻辑编写的，将被传递给下游的 FOL 执行器。每个程序都由函数名和参数组成，并可分层串联，这样一个函数的输出就可以成为另一个函数的输入参数。LLMinterpreted 程序包含与领域无关的 FOL 函数，这些函数可以将特定领域函数的输出作为参数。FOL 函数是一组具有内置实现的预定义程序，表 1 列出了全部函数。这些函数要么是逻辑和数字运算（如计数、forall、存在），要么是处理输入和输出的函数（如返回对象类别的文本描述，或执行其他模块输出的操作）。我们将在下一节介绍 FOL 语言的语法和内置组件。</p> 
<p>LEFT 利用 GPT-3.5作为其 LLM 骨干。重要的是，LEFT 提示与领域无关，包括简单的语法规则和用一般概念编写的示例，如猫和苹果，这些概念在我们的任何评估领域中都不会出现。我们提示中的示例也具有最小的复杂性，例如 "蛋糕旁边是否有一个苹果 "的一阶逻辑句子，以及简短的 FOL 使用说明，如 "要对两个对象是否具有某种属性进行分类，请使用'on(x, y)'"。此外，我们还使用了一种逐步提示范式，要求 LLM 首先对查询进行推理，并以语言形式简化文本，然后生成 FOL 程序。</p> 
<p></p> 
<p class="img-center"><img alt="" height="524" src="https://images2.imgbox.com/ea/a3/wGj7AF9Z_o.png" width="1057"></p> 
<p>表 1： LEFT 的一阶逻辑语言是一种跨领域的通用推理语言。</p> 
<h4>3.2 与领域无关的一阶逻辑执行器</h4> 
<p>独立于领域的 LEFT 执行器在一阶逻辑上运行，一阶逻辑是不同任务共享的通用推理语言。根据 LLM 解释器解析的 FOL 程序 P，执行器通过接地模块对 P 进行操作，返回最终答案。程序以递归方式执行，以模拟 FOL 推理过程的层次结构。LEFT 程序是作为 Python 代码中的可微分一阶逻辑模块实现的。</p> 
<p>我们的 FOL 语言包含三个部分：第一，基本的一阶逻辑运算，如布尔运算和逻辑量词；第二，内置函数，如 describe（为一个实体或一对实体生成自然语言回复）和 do（执行机器人动作）；第三，特定领域的概念名称，包括对象属性（如猫）、对象关系（如左）和动作（如打包）。这些概念谓词是由 LLM 根据语言查询自动生成的，而不是从一组预定义的函数中挑选出来的；因此，LEFT 的表达能力不受存在和计数等给定基元的限制。它可以学习执行 LLM 提出的任何新函数。</p> 
<p>对于给定的实体宇宙，LEFT 执行器使用基于张量的真值表示法来执行所有表达式。例如，给定一个有 N 个对象的场景，表达式 sphere(x) 的执行结果（其中 x 是一个未量化的变量）可以表示为一个长度为 N 的向量，其中每个条目 i 表示对象 i 是否是一个球体。关系表达式（如 left(x,y)）可以表示为大小为 N × N 的矩阵。同样，对象是否被选中也可以表示为大小为 N 的向量，其中每个条目 i 表示对象 i 被选中的概率。在实际应用中，sphere(x) 和 left(x, y) 等值将由特定领域的接地神经网络预测。在表 1 中，我们详细介绍了所有操作的执行策略。对于我们建立通用神经符号框架的目标来说，LEFT 只需要这些列出的 FOL 函数，就能在众多领域进行推理，包括二维、三维、时间和机器人操作。</p> 
<p>我们的 LLM 解释器生成的程序包括 FOL 程序和特定领域概念，这些程序由我们的接地模块执行。LEFT 执行器根据一阶逻辑程序的复杂程度（如函数中的参数数）来实现这些程序。对于输入的每个实体，一元级函数使用大小为 N 的以实体为中心的特征来实现，二元级函数使用大小为 N × N 的关系特征来实现，等等。值得注意的是，每个特定领域的概念都是由 LLM 初始化的，函数的实现是通过模块化神经网络执行的，不需要在代码中手动定义。</p> 
<p>执行。下面我们将介绍 FOL 程序的实现。详见附录。</p> 
<p><code>exists(var,expr)</code>： 我们首先递归执行 expr，得到一个张量，其中 var 是张量的维数之一。我们在该维度上执行最大池化。例如，执行 sphere(x) 得到一个长度为 N 的向量，对于 exists(x,sphere(x))，我们取返回张量的最大值。</p> 
<p><code>iota(var,expr)</code>： 我们递归执行 expr，得到一个向量，其中 var 对应于向量的维数。然后我们对该向量执行软最大值运算。</p> 
<p><code>and(expr1 , expr2 )</code>： 递归执行 expr1 和 expr2 将得到两个张量。两个张量的连接是通过对两个张量进行元素最小运算完成的。同样的执行策略也用于或运算（取元素最大值）和逻辑否定（取元素否定值）。</p> 
<p><code>count(var,expr)</code>： 我们递归执行 expr，然后执行一个 sigmoid 运算，接着对向量求和，计算预期对象数。</p> 
<p><code>eq(expr1 , expr2 )</code>：expr1 和 expr2 的递归执行将产生两个标量值 s1 和 s2。我们将 s1 = s2 的概率定义为 σ (α · (γ − |s1 − s2|))，其中 α = 8 和 γ = 0.25 是固定超参数。小于和大于操作的实现方式类似。</p> 
<p><code>left(x, y)</code>：作为特定于域的概念的示例，left 函数将两个变量作为输入，并生成大小为 N × N 的得分矩阵。这是通过对由特征提取器。请注意，在整个执行过程中，我们保留预测值的“logit”，而不是使用 sigmoidal 激活将它们标准化为 [0, 1]。这提高了执行的数值稳定性。</p> 
<h4>3.3 特定领域的标定模块</h4> 
<p>LEFT 的最后一个组件是特定领域的基础模块，它将一阶逻辑语言中的概念与特定模态的表示连接起来。 LEFT 不依赖于预定义的词汇概念集，而是依赖 LLM 自动从数据中的语言中提取概念名称。然后，我们的框架相应地初始化特定领域的基础模块，以从训练数据中学习这些概念的基础。换句话说，标定模块不需要手动定义，可以支持任何给定查询的执行，并且用下面描述的通用机制来实现。基础模块是通过模块化神经网络实现的，由特定于模态的编码器和概念嵌入组成。我们首先使用特定于模态的编码器 ε 提取以实体为中心的特征和关系特征。对于 2D 图像，ε 是 Faster R-CNN，用于编码以对象为中心的特征，而 DenseCLIP用于生成对象注意掩模。对于 3D 点云，ε 是 PointNet++；对于运动序列，ε 是双流自适应图卷积网络。一元特征表示为每个实体的一组特征，二元特征表示为每对实体之间关系的 2D 矩阵，三元关系表示为 3D 矩阵等。</p> 
<p>给定以实体为中心的表示，然后我们执行概念函数，例如 sphere 和 left(x, y)。一般来说，我们可以拥有任意神经网络，它采用实体的关系表示并输出 [0, 1] 中的（软）布尔值。在本文中，我们为大语言模型提到的每个概念初始化一个多层感知器（MLP），以表示概念嵌入。例如，在我们的 2D 图像域中，每个对象 i 都有一个由 Faster R-CNN 编码器提取的向量嵌入 obji。我们将 MLPsphere 应用于每个对象表示 obji 并生成长度为 N 的向量，这是表达式 sphere(x) 的执行结果。</p> 
<p>标定模块根据LLM提出的概念名称和数量自动初始化。例如，如果一个概念是一元的（例如，red(x)），则基础模块将是将对象表示映射到上述软布尔值标量的 MLP 层。对于旁边的概念，LLM 指定该概念采用两个实体的特征，因此 LEFT 创建了一个对二进制特征进行操作的 MLP。特定领域的特征提取器和概念 MLP 通过反向传播进行训练，因为我们的执行器是完全可微分的。</p> 
<h3>4.实验</h3> 
<p></p> 
<p class="img-center"><img alt="" height="310" src="https://images2.imgbox.com/d0/84/P66Ub9tn_o.png" width="1200"></p> 
<p>图 3：LEFT 在四大类任务上进行评估，包括 2D 图像中的概念学习、3D 点云、时间运动序列和机器人操作。</p> 
<p>我们在四个领域和七个任务上评估 LEFT，并在多种设置中展示其有效性。在第 4.1 节中，我们将 LEFT 与先前最先进的神经符号和整体端到端方法在四个领域的概念学习上进行比较。我们的模型在任务性能和数据效率方面表现出色，无需任何特定领域的知识。接下来，在第 4.2 节中，我们介绍了 LEFT 对三个新颖且具有挑战性的视觉推理任务的零样本泛化性能，并将其推理能力与基于 LLM 的分解框架和通用视觉语言模型的推理能力进行比较。 LEFT 的实施和培训详细信息可以在附录中找到。</p> 
<h4>4.1 跨领域的概念学习</h4> 
<p>我们在六个不同的数据集上训练和验证 LEFT，这些数据集分为四个领域（图 3）。</p> 
<p><strong>2D 视觉概念学习。</strong> 在第一个领域，我们专注于 2D 图像中的视觉问答任务：给定图像和语言查询（例如，“有多少个小圆柱体被部分隐藏？”），目标是从一组中预测答案词汇。准确度计算为与真实答案的精确匹配。我们在 CLEVR 和 CLEVR-Humans 数据集上验证 LEFT，其中包含具有不同形状、大小、材质和颜色渲染的对象的图像。 CLEVR 包含综合生成的查询，而 CLEVR-Humans 是包含各种自然语言问题的人类注释数据集。</p> 
<p><strong>3D视觉概念学习。</strong> 第二个域评估 3D 引用表达理解。输入是场景中的一组对象点云和一个查询（例如“您将分类为其他两个中间的监视器”）。通过选择正确目标对象的准确性来评估模型。在 3D 领域，我们需要参考系来消除像左这样的关系的歧义，这些概念的标定将通过不需要特定实现的通用高数量函数来考虑语言查询中指定的视点。我们使用来自 ScanNet（室内场景集合）的 3D 点云，在 ReferIt3D 数据集上评估 LEFT。 ReferIt3D 由两个子集组成，SR3D 专注于具有综合生成查询的空间参考，NR3D 具有人工注释的查询。我们使用 NS3D 中的 NR3D 子集，其中包含与 SR3D 中相同的概念集。</p> 
<p><strong>动作捕捉概念学习。</strong> 第三个领域包含人体动作捕捉。特别是，我们关注时间运动序列中的问答任务。给定一系列由关节参数化的人体运动和一个问题（例如“人在向后移动之前和踢腿之后会做什么动作？”），目标是预测精确匹配的答案。我们在 HumanMotionQA 数据集上训练和评估 LEFT，该数据集询问有关时间关系和各种属性的问题，包括动作、方向和身体部位。</p> 
<p><strong>机器人操纵。</strong> 最后，我们证明 LEFT 可以在机器人操纵的背景下学习与机器人运动相对应的动作概念。该任务的输入是代表桌面环境的图像和语言指令（例如，“将 yoshi 人物放入棕色盒子中。”）目标是生成一系列动作，使该机器人的平均总奖励最大化操纵任务。我们在 CLIPort 数据集上验证 LEFT，重点关注顺序打包 Google 扫描对象的任务。</p> 
<h5>4.1.1 准确性</h5> 
<p>我们将 LEFT 与所有六个数据集中的两类表现最佳的模型进行比较。第一类包含神经符号模型，它需要预定义的特定领域语言和每个程序的实现。第二类包含整体端到端方法，通常需要大量数据进行训练。值得注意的是，我们的统一 LEFT 框架能够在所有领域上执行，而不需要为每个设置提供特定的推理模块。</p> 
<p></p> 
<p class="img-center"><img alt="" height="160" src="https://images2.imgbox.com/1c/db/tIhNvWRO_o.png" width="991"></p> 
<p>表 2：LEFT 与之前表现最佳的神经符号和端到端作品的比较结果。我们统一的 LEFT 框架在所有设置下都能产生强大的性能，无需任何预定义的程序。</p> 
<p>表 2 总结了涉及综合生成语言的四个数据集上不同模型的结果，因此神经符号方法和我们的方法存在真实程序。我们看到，在 CLEVR、SR3D、HumanMotionQA 和 CLIPort 中，LEFT 实现了与之前的神经符号工作相当的性能，并且优于 CLEVR、HumanMotionQA 和 CLIPort 中的端到端方法。重要的是，我们的 LEFT 模型针对所有领域和任务应用了统一的语言解释和程序执行框架，而先前的神经符号学习工作需要针对每个数据集的特定领域语言和推理模块。这表明一阶逻辑语言可以作为表示语言查询的有效且通用的语言。</p> 
<p></p> 
<p class="img-center"><img alt="" height="166" src="https://images2.imgbox.com/92/29/JIhyS9GT_o.png" width="589"></p> 
<p>表 3：自然语言任务的比较。</p> 
<p>在表 3 中，我们使用人工注释的自然语言查询来检查 LEFT 任务。 LEFT 的第一个数字表示由 LLM 生成的程序可执行的所有查询的准确性，而后一个数字表示所有查询的准确性，考虑到不可执行的 LLM 输出的百分比。我们注意到 MDETR在 CLEVR-Humans 上的表现优于 LEFT，可能是因为它可以更好地捕获相似概念（例如 front 和 in-front-of）之间的相似性。相反，LEFT 中不同概念的表示是独立的。我们与之前的神经符号方法在 NR3D 上的表现优于 LEFT，因为它还利用 LLM 进行语言解释，但在手工制作的提示中使用了一组特定于领域的示例。</p> 
<h5>4.1.2 数据效率</h5> 
<p>我们在 CLEVR和 SR3D的数据效率设置上评估 LEFT。 LEFT 在测试的每个数据有效百分比点上都显着优于所有端到端方法，并且与性能最佳的神经符号方法相当（见表 4 和表 5）。百分比表示用于训练的训练数据与完整数据集相比的百分比。</p> 
<p></p> 
<p class="img-center"><img alt="" height="201" src="https://images2.imgbox.com/77/c5/rFs86m7s_o.png" width="786"></p> 
<p>表 4：数据高效设置下 SR3D 的结果。 LEFT 的表现与之前的神经符号工作相当，并且显着优于端到端工作。</p> 
<p>将 CLEVR 作为一项消融研究，我们注意到 NSCL 在 10% 设置上优于 LEFT。我们将这种差距归因于 NSCL 利用的额外的特定领域归纳偏差。特别是，NSCL 假设存在四个属性空间（颜色、形状、材料和大小），并且每个概念恰好位于一个属性空间中。此外，同一属性空间内的所有概念都是互斥的。因此，NSCL 编码的归纳偏差可以被视为基于分层程序的推理机制和概念上附加的特定领域先验的组合。相比之下，LEFT 仅利用基于程序的推理机制，该机制在许多领域和任务中通用。将我们的模型与其他基线和 NSCL 进行比较，我们表明这种基于程序的通用推理机制足以解释大部分数据效率的改进。</p> 
<h4>4.2 跨任务的推理泛化</h4> 
<p>最后，我们展示了 LEFT 零样本泛化到未见过的任务的能力。</p> 
<p><strong>CLEVR 推理任务。</strong> 我们基于 CLEVR 数据集生成三个零样本迁移任务，每个任务有 100 个评估示例。有关每个任务的示例，请参见图 4。 CLEVRRef 任务由 CLEVR 图像和语言查询组成，需要 LEFT 来定位准确的目标对象。 CLEVR-Puzzle 任务由与视觉谜题描述配对的图像组成，这需要变量分配谜题解决。 CLEVR-RPM 任务基于 Raven 的渐进矩阵，它由图像和由语言描述的 3x3 网格组成。此任务需要 LLM 解释器按行和列推理网格中的模式，以确定目标对象的属性，并向 FOL 执行器生成查询以检查场景中是否存在此类对象。我们在附录中包含数据集详细信息。</p> 
<p></p> 
<p class="img-center"><img alt="" height="177" src="https://images2.imgbox.com/b5/8d/w0VC8EJq_o.png" width="571"></p> 
<p>表 5：数据高效设置中 CLEVR 的结果。</p> 
<p></p> 
<p class="img-center"><img alt="" height="777" src="https://images2.imgbox.com/36/f5/lKzXuKMG_o.png" width="1200"></p> 
<p>图 4：三个零样本泛化任务。正如 CLEVR-Ref 示例中所示，LEFT 的领域独立模块将语言解析为一阶逻辑并生成答案。</p> 
<h5>4.2.1 准确性</h5> 
<p>我们在 CLEVR 问答任务上训练 LEFT，并报告三个未见过的推理任务的零样本迁移性能。在表 6 中，我们看到 LEFT 在所有数据集上都取得了出色的性能。我们将 LEFT 性能与真实程序（GT 程序）以及来自我们的 LLM 解释器的 FOL 程序（LLM 程序）进行比较。这种消融表明，与所有三个复杂推理任务中的真实程序相比，LLM 程序的性能仅略有下降。 LEFT 可以通过 LLM 生成的一阶逻辑零样本迁移到新颖的视觉推理任务，并有效地重用概念嵌入，从而实现灵活的泛化。重要的是，我们看到 LEFT 在 CLEVR-RPM 上表现良好，这是一项具有挑战性的任务，其中包含复杂的 Raven 渐进矩阵描述，需要 LLM 解释器对语言描述的模式进行推理。虽然LLM的提示很简单，但我们的解释器可以解决复杂的任务。</p> 
<p>作为基线，我们将 LEFT 与基于 LLM 的分解框架和通用视觉语言模型进行比较。 VisProg和 ViperGPT是仅推理、与 API 框架集成的LLM，已在各种视觉推理任务上取得了成功。在表 6 中，我们看到 LEFT 在 CLEVR 传输任务上显着优于 VisProg 和 ViperGPT。基于LLM的分解框架通常无法输出复杂查询的可执行代码；另外，API执行的准确性较低。例如，在 CLEVR-RPM 任务中，ViperGPT 仅成功执行了 0.40 个查询，可执行查询的准确率为 0.10 个。由于这些方法使用一组受约束的预定义函数执行程序，因此执行未定义关系（例如在 LEFT 中学习到的右侧关系）的准确性很低。此外，对于 3D 和时间人体运动序列等复杂领域，尚不清楚这些仅推理框架应使用哪些预定义模型。</p> 
<p></p> 
<p class="img-center"><img alt="" height="304" src="https://images2.imgbox.com/b0/14/f1949GES_o.png" width="1200"></p> 
<p>表 6：LEFT 的零样本泛化到三个未见过的任务； LEFT 使用 LLM 解释器重新组合与领域无关的程序和学习的概念，以实现灵活的迁移。</p> 
<p>为了与一般视觉语言模型进行比较，我们评估了 OpenFlamingo，它是 Flamingo的开源版本。 OpenFlamingo 是一个由 CLIP 视觉编码器和 LLAMA 语言编码器支持的 VL 模型。在 4 样本和 8 样本变体中，LEFT 的性能均显着优于 OpenFlamingo。值得注意的是，OpenFlamingo 无法预测 CLEVR-Ref 的答案，它需要对象索引作为输出。</p> 
<h3>5.讨论</h3> 
<p>我们提出了逻辑增强基础模型（LEFT），这是一个统一的概念学习和推理框架，可以灵活地学习不同领域的概念，并将推理推广到看不见的复杂任务。我们的框架将基础模型和一阶逻辑执行器组合为与领域无关的推理模块，以及特定领域的可学习基础模块。LEFT 代表以正确的方式对一般推理能力进行建模的一步。</p> 
<p>LEFT 利用预先训练的 LLM 进行与领域无关的推理。因此，我们系统的结果受到LLM质量的限制，我们无法对其进行微调。对于LLM生成的程序中的语法错误，我们可以轻松检测到它们并重新采样。然而，我们目前无法从程序中的语义错误中恢复。未来的方向是将执行跟踪或失败作为 LLM 的附加输入，以便他们可以修复所生成的查询。</p> 
<p>如果 LEFT 不能有效地训练特定领域的接地模块，那么它也可能会因领域数据不足而失败。我们注意到，对于大多数仅在一个给定数据集上训练的学习系统来说，这是一个限制。为了部分缓解这个问题，LEFT 指示LLM在合理的情况下规范化概念，这有助于学习某些罕见的概念。通过直接将预训练的视觉模型集成为特定领域的基础模块，可以直接合并外部训练的模块。例如，如果我们有先验任务知识，我们可以插入 Grounding DINO 进行对象检测。然而，对于某些领域，没有现成的视觉识别模型（例如关系和人体动作）；因此，LEFT 可以通过从数据中学习来训练其概念基础。值得注意的是，由于其模块化结构，与端到端方法相比，LEFT 的数据效率更高，因为它可以将复杂的概念分解为原始概念。</p> 
<p>我们的框架目前没有对语言和感知之间的交互进行建模。相反，LEFT 依赖于上下文相关的基础模块，该模块从数据中学习来解决模糊的语言。 LEFT 的未来方向包括构建实用推理框架、对说话者意图建模以及利用感知执行的反馈。</p> 
<p><strong>关注下方《学姐带你玩AI》🚀🚀🚀</strong></p> 
<p><strong>回复“NeurIPS获奖”获取全部获奖论文和代码</strong></p> 
<p><strong>码字不易，欢迎大家点赞评论收藏</strong></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f2dc5d5bef33b9f962c48f6b0943fa45/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">前端通过iframe进行传值的方式：</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1da23584609efe675de845d64ffd0a2f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">SQL面试题挑战14：每年的在校人数</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>