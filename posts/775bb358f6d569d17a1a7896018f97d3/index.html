<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【具身智能评估9】Open X-Embodiment: Robotic Learning Datasets and RT-X Models - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【具身智能评估9】Open X-Embodiment: Robotic Learning Datasets and RT-X Models" />
<meta property="og:description" content="论文标题：Open X-Embodiment: Robotic Learning Datasets and RT-X Models
论文作者：–
论文原文：https://arxiv.org/abs/2310.08864
论文出处：–
论文被引：–（12/18/2023）
论文代码：https://github.com/google-deepmind/open_x_embodiment
项目主页：https://robotics-transformer-x.github.io/
Abstract 在不同数据集上训练的大型高容量模型在高效处理下游应用方面取得了显著的成功。在从 NLP 到计算机视觉等领域，这导致了预训练模型的整合，通用预训练骨干成为许多应用的起点。机器人技术能否实现这种整合？传统的机器人学习方法是为每种应用、每个机器人甚至每个环境训练一个单独的模型。我们能否训练出 “通用型” X-robot 策略，使其能有效地适应新的机器人、任务和环境？在本文中，我们提供了标准化数据格式和模型的数据集，以便在机器人操纵的背景下探索这种可能性，同时还提供了有效 X-robot 策略的实验结果。我们汇集了 21 家机构合作收集的 22 种不同机器人的数据集，展示了 527 种技能（160266 项任务）。我们的研究表明，在这些数据基础上训练出来的大容量模型（我们称之为 RT-X）可以利用来自其他平台的经验，实现正迁移并提高多个机器人的能力。更多详细信息，请访问项目网站 https://robotics-transformer-x.github.io/。
I. INTRODUCTION 从机器学习和人工智能的最新进展中得到的一个重要启示是，通过提供通用的预训练模型，从广泛多样的数据集中进行大规模学习，可以使人工智能系统具备能力。事实上，通常在大型、多样化数据集上训练的大规模通用模型，其性能往往优于在较小但更具任务针对性的数据集上训练的狭义目标模型。例如，在从网络上抓取的大型数据集上训练的开放词汇图像分类器（如 CLIP [1]）往往优于在更有限的数据集上训练的固定词汇模型，而在海量文本语料库上训练的大型语言模型 [2, 3] 往往优于只在狭窄的特定任务数据集上训练的系统。越来越多的情况是，处理特定狭窄任务（如视觉或 NLP）的最有效方法是调整通用模型。然而，这些经验很难应用到机器人领域：任何单一的机器人领域都可能过于狭窄，虽然计算机视觉和 NLP 可以利用从网络上获取的大型数据集，但机器人交互领域却很难获得类似的大型、广泛数据集。即使是最大规模的数据收集工作，最终得到的数据集在规模和多样性上仍然只是视觉（5-18M）[4, 5]和 NLP（1.5B-4.5B）[6, 7]基准数据集的一小部分。也许更重要的是，这些数据集在某些变化轴上仍然很狭窄，要么只关注单一环境、单一物体集，要么只关注狭窄的任务范围。我们如何才能克服机器人学中的这些挑战，将机器人学习领域推向在其他领域取得巨大成功的那种大数据体系？
受在不同数据上对大型视觉或语言模型进行预训练可实现通用化的启发，我们认为， 训练可通用的机器人策略这一目标需要进行 X-embodiment 训练，即使用来自多个机器人平台的数据。虽然每个单独的机器人学习数据集可能过于狭窄，但所有这些数据集的联合能更好地覆盖环境和机器人的变化。学习可通用的机器人策略需要开发能够利用 X-embodiment 数据的方法，挖掘来自许多实验室、机器人和环境的数据集。即使这些数据集目前的规模和覆盖范围不足以达到大型语言模型所展示的令人印象深刻的泛化结果，但在未来，这些数据的联合有可能提供这种覆盖范围。正因为如此，我们认为，在当前的关键时刻，对 X-embodiment 机器人学习的研究至关重要。
根据这一原理，我们的工作有两个主要目标：
1）证明根据来自多个不同机器人和环境的数据训练的策略可以实现正向增益，比仅根据来自每个评估设置的数据训练的策略获得更好的性能。2）为机器人界提供数据集、数据格式和模型，以促进未来对 X-embodiment 模型的研究。 我们的工作重点是 机器人操纵（robotic manipulation）。
针对目标（1），我们的经验性贡献是证明了最近的几种机器人学习方法，只需极少的修改，就能利用 X-embodiment 数据并实现正迁移。具体来说，我们在 9 个不同的机器人操纵器上训练 RT-1 [8] 和 RT-2 [9] 模型。结果表明，我们称之为 RT-X 的模型可以改进仅在评估域数据上训练的策略，表现出更好的泛化能力和新功能。针对目标（2），我们提供了 Open X-embodiment（OXE）资源库，其中包括一个数据集，包含来自 21 个不同机构的 22 种不同的机器人装置，可以帮助机器人社区进一步研究 X-embodiment 模型，并提供开源工具以促进此类研究。我们的目标不是在特定架构和算法方面进行创新，而是提供我们训练的模型以及数据和工具，为围绕 X-embodiment 机器人学习的研究注入活力。 II." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/775bb358f6d569d17a1a7896018f97d3/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-19T04:45:00+08:00" />
<meta property="article:modified_time" content="2023-12-19T04:45:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【具身智能评估9】Open X-Embodiment: Robotic Learning Datasets and RT-X Models</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><img src="https://images2.imgbox.com/75/94/CF6HlD4d_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/df/01/tWKqn6GF_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>论文标题：Open X-Embodiment: Robotic Learning Datasets and RT-X Models<br> 论文作者：–<br> 论文原文：https://arxiv.org/abs/2310.08864<br> 论文出处：–<br> 论文被引：–（12/18/2023）<br> 论文代码：https://github.com/google-deepmind/open_x_embodiment<br> 项目主页：https://robotics-transformer-x.github.io/</p> 
</blockquote> 
<h2><a id="Abstract_17"></a>Abstract</h2> 
<p>在不同数据集上训练的大型高容量模型在高效处理下游应用方面取得了显著的成功。在从 NLP 到计算机视觉等领域，这导致了预训练模型的整合，通用预训练骨干成为许多应用的起点。机器人技术能否实现这种整合？传统的机器人学习方法是为每种应用、每个机器人甚至每个环境训练一个单独的模型。<strong>我们能否训练出 “通用型” X-robot 策略，使其能有效地适应新的机器人、任务和环境</strong>？在本文中，我们提供了标准化数据格式和模型的数据集，以便在机器人操纵的背景下探索这种可能性，同时还提供了有效 X-robot 策略的实验结果。<strong>我们汇集了 21 家机构合作收集的 22 种不同机器人的数据集，展示了 527 种技能（160266 项任务）</strong>。我们的研究表明，在这些数据基础上训练出来的大容量模型（我们称之为 RT-X）可以利用来自其他平台的经验，实现正迁移并提高多个机器人的能力。更多详细信息，请访问项目网站 https://robotics-transformer-x.github.io/。</p> 
<h2><a id="I_INTRODUCTION_21"></a>I. INTRODUCTION</h2> 
<p>从机器学习和人工智能的最新进展中得到的一个重要启示是，<strong>通过提供通用的预训练模型，从广泛多样的数据集中进行大规模学习，可以使人工智能系统具备能力</strong>。事实上，通常在大型、多样化数据集上训练的大规模通用模型，其性能往往优于在较小但更具任务针对性的数据集上训练的狭义目标模型。例如，在从网络上抓取的大型数据集上训练的开放词汇图像分类器（如 CLIP [1]）往往优于在更有限的数据集上训练的固定词汇模型，而在海量文本语料库上训练的大型语言模型 [2, 3] 往往优于只在狭窄的特定任务数据集上训练的系统。越来越多的情况是，处理特定狭窄任务（如视觉或 NLP）的最有效方法是调整通用模型。然而，这些经验很难应用到机器人领域：任何单一的机器人领域都可能过于狭窄，虽然计算机视觉和 NLP 可以利用从网络上获取的大型数据集，但机器人交互领域却很难获得类似的大型、广泛数据集。即使是最大规模的数据收集工作，最终得到的数据集在规模和多样性上仍然只是视觉（5-18M）[4, 5]和 NLP（1.5B-4.5B）[6, 7]基准数据集的一小部分。也许更重要的是，这些数据集在某些变化轴上仍然很狭窄，要么只关注单一环境、单一物体集，要么只关注狭窄的任务范围。我们如何才能克服机器人学中的这些挑战，将机器人学习领域推向在其他领域取得巨大成功的那种大数据体系？</p> 
<p>受在不同数据上对大型视觉或语言模型进行预训练可实现通用化的启发，我们认为，<strong><font color="blue"> 训练可通用的机器人策略这一目标需要进行 X-embodiment 训练，即使用来自多个机器人平台的数据</font></strong>。虽然每个单独的机器人学习数据集可能过于狭窄，但所有这些数据集的联合能更好地覆盖环境和机器人的变化。学习可通用的机器人策略需要开发能够利用 X-embodiment 数据的方法，挖掘来自许多实验室、机器人和环境的数据集。即使这些数据集目前的规模和覆盖范围不足以达到大型语言模型所展示的令人印象深刻的泛化结果，但在未来，这些数据的联合有可能提供这种覆盖范围。正因为如此，我们认为，在当前的关键时刻，对 X-embodiment 机器人学习的研究至关重要。</p> 
<p>根据这一原理，我们的工作有两个主要目标：</p> 
<ul><li>1）证明根据来自多个不同机器人和环境的数据训练的策略可以实现正向增益，比仅根据来自每个评估设置的数据训练的策略获得更好的性能。</li><li>2）为机器人界提供数据集、数据格式和模型，以促进未来对 X-embodiment 模型的研究。</li></ul> 
<p>我们的工作重点是 <strong>机器人操纵（robotic manipulation）</strong>。</p> 
<ul><li>针对目标（1），我们的经验性贡献是<strong>证明了最近的几种机器人学习方法，只需极少的修改，就能利用 X-embodiment 数据并实现正迁移</strong>。具体来说，我们在 9 个不同的机器人操纵器上训练 RT-1 [8] 和 RT-2 [9] 模型。结果表明，我们称之为 RT-X 的模型可以改进仅在评估域数据上训练的策略，表现出更好的泛化能力和新功能。</li><li>针对目标（2），我们提供了 Open X-embodiment（OXE）资源库，其中包括一个数据集，包含来自 21 个不同机构的 22 种不同的机器人装置，可以帮助机器人社区进一步研究 X-embodiment 模型，并提供开源工具以促进此类研究。<strong>我们的目标不是在特定架构和算法方面进行创新，而是提供我们训练的模型以及数据和工具，为围绕 X-embodiment 机器人学习的研究注入活力</strong>。</li></ul> 
<p><img src="https://images2.imgbox.com/46/0f/119nuCCZ_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="II_RELATED_WORK_40"></a>II. RELATED WORK</h2> 
<h3><a id="Transfer_across_embodiments_42"></a>Transfer across embodiments</h3> 
<p>之前有许多研究都对模拟机器人[10-22]和真实机器人[23-29]的跨机器人体现迁移方法进行了研究。这些方法通常引入了专门用于解决不同机器人之间体现差距的机制，例如：</p> 
<ul><li>共享动作表示[14, 30]</li><li>纳入表示学习目标[17, 26]</li><li>根据具身信息调整学习策略[11, 15, 18, 30, 31]</li><li>解耦机器人和环境表示[24]</li></ul> 
<p>之前的工作已经初步展示了 X-embodiment 训练 [27] 和 Transformer 模型的迁移 [25, 29, 32]。我们研究了互补的架构，提供了互补的分析，特别是<strong>研究了 X-embodiment 迁移与网络规模预训练之间的相互作用</strong>。</p> 
<ul><li>同样，用于在人类和具身机器人之间迁移的方法也经常采用减少具身差距（embodiment gap）的技术，即<strong>通过领域间转换或学习可迁移表征[33-43]</strong>。</li><li>另外，一些研究还关注问题的子方面，如从人类视频数据中学习可迁移的奖励函数 [17, 44-48]，目标 [49, 50]，动力学模型 [51] 或视觉表征 [52-59]。<strong>与之前的大多数研究不同，我们直接在 X-embodiment 数据上训练策略，而不使用任何机制来减少具身差距，并通过利用这些数据观察到正向的迁移</strong>。</li></ul> 
<h3><a id="Largescale_robot_learning_datasets_56"></a>Large-scale robot learning datasets</h3> 
<p>机器人学习社区已经创建了开源机器人学习数据集，涵盖</p> 
<ul><li>抓取[60-71]</li><li>推动交互[23, 72-74]</li><li>物体和模型集[75-85]</li><li>远程操作演示[8, 86-95]</li></ul> 
<p>除了 RoboNet [23]，这些数据集都包含同一类型机器人的数据，而我们的重点则是<strong>跨多个机器人的数据</strong>。我们数据资源库的目标是与这些努力相辅相成的：<strong><font color="blue">我们将大量先前的数据集进行处理并汇总到一个单一、标准化的资源库中，名为 Open X-Embodiment，它展示了机器人学习数据集如何以有意义、有用的方式进行共享</font></strong>。</p> 
<h3><a id="Languageconditioned_robot_learning_67"></a>Language-conditioned robot learning</h3> 
<p>先前的研究旨在赋予机器人和其他智能体理解并遵循语言指令的能力[96-101]，通常是通过学习语言条件策略[8, 40, 45, 102-106]。我们<strong>通过模仿学习来训练语言条件策略，这与之前的许多研究一样，但我们使用的是大规模多实验演示数据</strong>。在机器人模仿学习中，先前的研究利用了预训练的语言嵌入[8, 40, 45, 103, 107-112]和预训练的视觉语言模型[9, 113-115]，我们在实验中研究了这两种形式的预训练，特别是 RT-1 [8] 和 RT-2 [9]。</p> 
<h2><a id="III_THE_OPEN_XEMBODIMENT_REPOSITORY_71"></a>III. THE OPEN X-EMBODIMENT REPOSITORY</h2> 
<p>我们推出了 Open X-Embodiment 资源库——一个开源资源库，其中<strong>包括用于 X-embodied 机器人学习研究的大规模数据和预训练模型检查点</strong>。更具体地说，我们为更广泛的社区提供并维护以下开源资源：</p> 
<ul><li>Open X-Embodiment Dataset：机器人学习数据集，包含来自 22 个机器人实验的 100 多万条机器人轨迹。</li><li>Pre-Trained Checkpoints：精选的 RT-X 模型检查点，可用于推理和微调。</li></ul> 
<p>我们希望这些资源能为机器人学习领域的 X-embodiment 研究奠定基础，但这仅仅是个开始。Open X-Embodiment 是一项由社区推动的工作，目前有来自世界各地的 21 家机构参与其中，我们希望随着时间的推移，进一步扩大参与范围，发展最初的 Open X-Embodiment 数据集。在本节中，我们将总结数据集和 X-embodiment 学习框架，然后讨论我们用来评估数据集和实验结果的具体模型。</p> 
<h3><a id="A_The_Open_XEmbodiment_Dataset_80"></a>A. The Open X-Embodiment Dataset</h3> 
<p>Open X-Embodiment 实验数据集包含 100 多万条真实机器人轨迹，涵盖 22 种机器人实验，从单臂机器人到双臂机器人和四足机器人。该数据集是通过汇集来自全球 34 个机器人研究实验室的 60 个现有机器人数据集而构建的，并将其转换为统一的数据格式，以方便下载和使用。<strong>我们使用的是 RLDS 数据格式 [119]，它以序列化的 tf-record 文件保存数据，可适应不同机器人设置的各种动作空间和输入模式，如不同数量的 RGB 摄像头，深度摄像头和点云</strong>。它还支持所有主要深度学习框架的高效并行数据加载。有关数据存储格式和全部 60 个数据集的详细信息，请参见 robotics-transformer-x.github.io。</p> 
<h3><a id="B_Dataset_Analysis_84"></a>B. Dataset Analysis</h3> 
<p>图 2 分析了开放式 X 实施数据集。图 2(a)显示了按机器人实施例分列的数据集，其中 Franka 机器人最为常见。这反映在每个实施方案的独特场景数量上（基于数据集元数据）（图 2(b)），其中弗兰卡机器人占主导地位。图 2©显示了每个实施方案的轨迹分类。为了进一步分析多样性，我们使用了数据中的语言注释。我们使用 PaLM 语言模型 [3] 从指令中提取物体和行为。图 2(d,e) 显示了技能和物体的多样性。虽然大多数技能属于拾取-放置系列，但数据集的长尾部分包含了 “擦拭” 或 “组装” 等技能。此外，数据还涵盖了从家用电器到食品和器皿等一系列家用物品。</p> 
<h2><a id="IV_RTX_DESIGN_88"></a>IV. RT-X DESIGN</h2> 
<p>为了评估 X-embodiment 训练能在多大程度上提高所学策略在单个机器人上的性能，我们需要模型有足够的能力来有效利用这种大型异构数据集。为此，我们的实验将以最近提出的两种基于 Transformer 的机器人策略为基础：RT1 [8] 和 RT-2 [9]。在本节中，我们将简要总结这两个模型的设计，并讨论我们在实验中如何将它们应用到 X-embodiment 环境中。</p> 
<h3><a id="A_Data_format_consolidation_92"></a>A. Data format consolidation</h3> 
<p><strong>创建 X-embodiment 模型的一个挑战是，不同机器人的观察和行动空间差异很大。我们在不同的数据集中使用粗略对齐的动作和观察空间。该模型接收近期图像和语言指令的历史记录作为观察结果，并预测一个控制末端执行器的 7 维动作向量（x，y，z，roll，pitch，yaw，gripper opening or the rates of these quantities）</strong>。我们从每个数据集中选择一个典型的相机视图作为输入图像，将其调整为通用分辨率，然后将原始动作集转换为 7 DoF 末端执行器动作。在离散化之前，我们对每个数据集的动作进行归一化处理。这样一来，模型的输出结果就可以根据所使用的实施方式进行不同的解释（去规范化）。值得注意的是，尽管进行了粗对齐，但不同数据集的摄像头观测结果仍有很大差异，例如，由于摄像头相对于机器人的位置不同，或者摄像头的属性不同，见图 3。同样，对于动作空间，我们不会在控制末端执行器的数据集之间对坐标框架进行对齐，而是允许动作值代表绝对或相对位置或速度，正如每个机器人最初选择的控制方案一样。因此，相同的动作矢量对不同的机器人可能会产生截然不同的运动。</p> 
<p><img src="https://images2.imgbox.com/8a/80/dQxgQbt8_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="B_Policy_architectures_99"></a>B. Policy architectures</h3> 
<p>我们在实验中考虑了两种模型架构：</p> 
<ul><li>1）RT-1 [8]，一种基于 Transformer 的高效架构，专为机器人控制而设计；</li><li>2）RT-2 [9]，一种大型视觉语言模型，经过共同微调，可将机器人动作输出为自然语言标记。</li></ul> 
<p>这两个模型都接收视觉输入和描述任务的自然语言指令，并输出标记化的动作。对于每种模型，动作都被标记为 256 个分区，沿八个维度均匀分布；一个维度用于终止情节，七个维度用于末端执行器运动。尽管这两种架构在其原始论文[8, 9]中都有详细描述，但我们在下文中将对每种架构进行简要概述：</p> 
<ul><li>RT-1 [8] 是一个基于 Transformer 架构 [118] 的 35M 参数网络，设计用于机器人控制，如图 3 所示。<strong>它接收 15 幅图像和自然语言的历史记录</strong>。每幅图像都会通过经过 ImageNet 训练的 EfficientNet [117] 进行处理，自然语言指令则会转换为 USE [120] 嵌入。然后，视觉和语言表征通过 FiLM [116] 层交织在一起，产生 81 个视觉语言标记。这些标记被送入解码器 Transformer，输出标记化的动作。</li><li>RT-2 [9]是一个大型视觉-语言-动作模型（VLA）系列，它是在互联网规模的视觉和语言数据以及机器人控制数据的基础上训练而成的。RT-2 将标记化动作（tokenized actions）转换为文本标记（text tokens），例如，一个可能的动作可能是 “1 128 91 241 5 101 127”。因此，任何经过预训练的视觉语言模型（VLM [121-123]）都可以针对机器人控制进行微调，从而利用视觉语言模型的骨干并迁移其部分泛化特性。在这项工作中，<strong>我们重点研究 RT-2-PaLI-X 变体[121]，它建立在视觉模型 ViT [124] 和语言模型 UL2 [125] 的基础上，主要在 WebLI [121] 数据集上进行预训练</strong>。</li></ul> 
<h3><a id="C_Training_and_inference_details_111"></a>C. Training and inference details</h3> 
<p>这两个模型在其输出空间（RT-1 为离散桶，RT-2 为所有可能的语言标记）上都<strong>使用了标准的分类交叉熵目标</strong>。</p> 
<p>我们将所有实验中使用的机器人混合数据定义为来自 9 个机械手的数据，这些数据来自：</p> 
<ul><li>RT-1 [8]</li><li>QT-Opt [66]</li><li>Bridge [95]</li><li>Task Agnostic Robot Play [126, 127]</li><li>Jaco Play [128]</li><li>Cable Routing [129]</li><li>RoboTurk [86]</li><li>NYU VINN [130]</li><li>Austin VIOLA [131]</li><li>Berkeley Autolab UR5 [132]</li><li>TOTO [133]</li><li>Language Table [91]</li></ul> 
<p>RT-1-X 仅根据上述定义的机器人混合数据进行训练，而 RT-2-X 则通过联合精细调整（类似于原始 RT-2 [9]）进行训练，原始 VLM 数据和机器人混合数据的比例大致为一比一。请注意，我们实验中使用的机器人数据混合物包括 9 个实例，少于整个 Open X-Embodiment 数据集（22 个）——造成这种差异的实际原因是，我们随着时间的推移不断扩展数据集，而在实验时，上述数据集代表了所有数据。今后，我们计划继续在扩展版本的数据集上训练策略，并与机器人学习社区一起不断扩大数据集。</p> 
<p><strong>推理时，每个模型以机器人所需的速率（3-10 Hz）运行，RT-1 在本地运行，RT-2 托管在云服务上并通过网络查询</strong>。</p> 
<h2><a id="V_EXPERIMENTAL_RESULTS_134"></a>V. EXPERIMENTAL RESULTS</h2> 
<p>我们的实验回答了有关 X-embodiment 训练效果的三个问题：</p> 
<ul><li>1）在我们的 X-embodiment 数据集上训练的策略能否有效地实现正迁移，从而在多个机器人上收集的数据上进行联合训练，提高训练任务的性能？</li><li>2）对来自多个平台和任务的数据进行联合训练是否能提高模型对新的、未见过的任务的泛化能力？</li><li>3）不同的设计维度（如模型大小、模型架构或数据集组成）对所产生策略的性能和泛化能力有何影响？</li></ul> 
<p>为了回答这些问题，我们对 6 种不同的机器人进行了总计 3600 次评估试验。</p> 
<h3><a id="A_Indistribution_performance_across_different_embodiments_144"></a>A. In-distribution performance across different embodiments</h3> 
<p><img src="https://images2.imgbox.com/58/da/VXA4ujBv_o.png" alt="在这里插入图片描述"></p> 
<p>为了评估我们的 RT-X 模型变体从 X-embodiment 数据中学习的能力，我们评估了它们在分布任务中的性能。我们将评估分为两类用例：</p> 
<ul><li>一类是对只有小规模数据集的领域进行评估（图 4），我们期望从更大规模数据集迁移数据能显著提高性能；</li><li>另一类是对拥有大规模数据集的领域进行评估（表 I），我们期望进一步提高性能更具挑战性。</li></ul> 
<p>请注意，我们在本节介绍的所有评估中使用了相同的机器人数据训练混合物（定义见第四章 C 节）。</p> 
<ul><li>在小规模数据集实验中，我们考虑了 Kitchen Manipulation [128]、Cable Routing [129]、NYU Door Opening [130]、AUTOLab UR5 [132] 和 Robot Play [134]。我们使用了与相关出版物中相同的评估和具身机器人方式。</li><li>在大规模数据集实验中，我们考虑使用 Bridge [95] 和 RT-1 [8] 进行分布式评估，并使用它们各自的机器人：WidowX 和 Google Robot。</li></ul> 
<p>对于每个小型数据集域，我们比较 RT-1-X 模型的性能；对于每个大型数据集，我们同时考虑 RT-1-X 和 RT-2-X 模型。在所有实验中，模型都是在完整的 X-embodiment 数据集上共同训练的。在整个评估过程中，我们与两个基准模型进行了比较：</p> 
<ul><li>1）数据集创建者开发的模型，仅在该数据集上进行了训练。这构成了一个合理的基准，因为我们可以预期该模型已经过优化，可以很好地处理相关数据；我们将该基准模型称为原始方法模型。</li><li>2）在数据集上单独训练的 RT-1 模型；这一基线允许我们评估 RT-X 模型架构是否有足够能力同时代表多个不同机器人平台的策略，以及在多实验数据上进行联合训练是否会带来更高的性能。</li></ul> 
<p>Small-scale dataset domains (Fig. 4).</p> 
<p>在 5 个数据集中的 4 个数据集上，RT-1-X 的表现优于在每个机器人特定数据集上训练的原始方法，平均改进幅度较大，这表明数据有限的领域从 X-embodiment 数据的联合训练中获益匪浅。</p> 
<p>Large-scale dataset domains (Table I).</p> 
<p>在大规模数据集设置中，RT-1-X 模型的表现并不优于仅在特定化身数据集上训练的 RT-1 基线模型，这表明该模型类别拟合不足。不过，更大的 RT-2-X 模型的表现优于原始方法和 RT-1，这表明 X-robot训练可以提高数据丰富领域的性能，但前提是必须使用足够大容量的架构。</p> 
<p><img src="https://images2.imgbox.com/c8/88/UdZCP2ZH_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="B_Improved_generalization_to_outofdistribution_settings_177"></a>B. Improved generalization to out-of-distribution settings</h3> 
<p>现在，我们将研究 X-embodiment 训练如何能够更好地推广到分布外环境以及更复杂、更新颖的指令。这些实验侧重于高数据域，并使用 RT-2-X 模型。</p> 
<p>Unseen objects, backgrounds and environments.</p> 
<p>我们首先进行了与 [9] 中提出的相同的泛化特性评估，测试了在未知环境和未知背景下操作未知物体的能力。我们发现 RT-2 和 RT-2-X 的表现大致相当（表 II，第（1）和（2）行，最后一列）。这并不出乎意料，因为 RT-2 在这些维度上的通用性已经很好（见 [9]），这得益于其 VLM 骨干。</p> 
<p><img src="https://images2.imgbox.com/7d/aa/JGwN0SCs_o.png" alt="在这里插入图片描述"></p> 
<p>Emergent skills evaluation.</p> 
<p>为了研究<strong>机器人之间的知识迁移</strong>，我们使用谷歌机器人进行了实验，评估其在图 5 所示任务中的表现。这些任务涉及的物体和技能在 RT-2 数据集中并不存在，但在不同机器人（WidowX-robot）的 Bridge 数据集中出现过[95]。结果如表二新兴技能评估一栏所示。比较第(1)行和第(2)行，我们发现 RT-2-X 比 RT-2 高出 ∼ 3 倍，这表明，将其他机器人的数据纳入训练，即使机器人已经拥有大量可用数据，也能提高可执行任务的范围。我们的结果表明，使用其他平台的数据进行联合训练为 RT-2-X 控制器注入了该平台的额外技能，而这些技能在该平台的原始数据集中并不存在。</p> 
<p>我们的下一步工作是将 Bridge 数据集从 RT-2-X 训练中移除：第(3)行显示的是 RT-2-X 的结果，其中包括 RT-2-X 使用的所有数据，但 Bridge 数据集除外。这种变化大大降低了在保持任务上的性能，表明 WidowX 数据的迁移可能确实是 RT-2-X 与谷歌机器人一起执行额外技能的原因。</p> 
<h3><a id="C_Design_decisions_194"></a>C. Design decisions</h3> 
<p>最后，我们进行了消融，以衡量不同设计决策对性能最好的 RT-2-X 模型的泛化能力的影响，结果如表 II 所示。我们注意到，包含短历史图像可显著提高泛化性能（第（4）行与第（5）行）。与 RT-2 论文[9]中的结论类似，基于网络的模型预训练对于大型模型实现高性能至关重要（第（4）行与第（6）行）。我们还注意到，与 5B 模型相比（第（2）行与第（4）行），55B 模型在 “新兴技能” 项目中的成功率明显更高，这表明模型容量越大，跨机器人数据集的迁移程度就越高。与以前的 RT-2 研究结果相反，共同微调和微调在新兴技能和泛化评估中的表现相似（第（4）行与第（7）行），我们将其归因于 RT-2-X 中使用的机器人数据比以前使用的机器人数据集更加多样化。</p> 
<p><img src="https://images2.imgbox.com/cc/c9/GkBZwUSy_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="VI_DISCUSSION_FUTURE_WORK_AND_OPEN_PROBLEMS_201"></a>VI. DISCUSSION, FUTURE WORK, AND OPEN PROBLEMS</h2> 
<p>我们展示了一个综合数据集，该数据集综合了 21 家机构合作收集的 22 种机器人的数据，展示了 527 种技能（160266 项任务）。我们还通过实验证明，在这些数据上训练出来的基于 Transformer 的策略可以在数据集中的不同机器人之间实现显著的正迁移。我们的结果表明，RT-1-X 策略的成功率比不同合作机构提供的最先进的原始方法高出 50%，而更大的基于视觉语言模型的版本（RT-2-X）与仅在评估体现数据上训练的模型相比，通用性提高了 ∼ 3 倍。此外，我们还为机器人社区提供了探索 X-embodiment 机器人学习研究的多种资源，包括：统一的 X-robot 和 X-institution 数据集、展示如何使用这些数据的示例代码，以及作为未来探索基础的 RT-1-X 模型。</p> 
<p>虽然 RT-X 展示了向 X-robot 通才迈出的一步，但要将这一未来变为现实，还需要采取更多措施。我们的实验有许多局限性：</p> 
<ul><li>没有考虑具有截然不同的感知和执行模态的机器人</li><li>没有研究对新机器人的泛化</li><li>没有提供何时发生或不发生正向迁移的判定标准</li></ul> 
<p>研究这些问题是未来工作的一个重要方向。我们希望这项工作不仅能作为 X-robot 学习可行且实用的范例，还能为今后推进该方向的研究提供工具。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c38556283168c0390b60352f9ff10250/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【具身智能评估8】BEHAVIOR-1K: A Benchmark for Embodied AI with 1,000 Everyday Activities and ...</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6da4bda609b308826f35687355042988/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">学习使用DDP: DistributedDataParallel</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>