<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>语音增强数据集总结【持续更新】 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="语音增强数据集总结【持续更新】" />
<meta property="og:description" content="目录 语音增强数据集总结纯净语音AVSpeechCommon Voice(2019首次发布，至今一直在更新)Multi Lingual Speech(MLS) (2020）DidiSpeech（2021）VCTK（2019）（又名Voice Bank）AiShellLibriSpeechTIMIT 噪声AudioSetWHAM!(2019)DEMANDFSD50K（2021）Noise-92AudioSet 已经合好的公开语音增强数据集VoiceBank&#43;DemandNISQANOISEX-92 真实带噪语音REAL-M(2021)DAPS（2014） 其他MUSAN（2015）BABEL（2011-）语音分离LibriMix(2020)AMI(2005) 语音增强数据集总结 语音增强的第一步一般是准备数据，包含带噪语音和纯净语音。一般这些数据都是人工合成的，以纯净语音和噪声数据为基础即可合出带噪语音信号。以下介绍一些语音增强领域常用的数据集。
纯净语音 AVSpeech 16kHz，由谷歌发布
干净语音，用于语音分离任务。音视频数据集(Audio-Visual Dataset)。
每条数据长度在3~10s，共计4700h，包括不同人种、不同语言和不同表情姿态的150,000个说话人。每条数据 &gt; 只出现一个说话人的面部和声音
数据分布如下图：
原文：Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation
网页：https://looking-to-listen.github.io/avspeech/
github：http://looking-to-listen.github.io/
Common Voice(2019首次发布，至今一直在更新) 48kHz，16bit，MP3格式 截至2023/7/21,
该数据集包括112种语言的28118小时语音，可用于训练的数据共18652小时。具体分布可看官网。
论文：https://arxiv.org/abs/1912.06670
官网链接：https://commonvoice.mozilla.org/en/languages
因为数据集一直在更新，所以建议直接去官网看。
Multi Lingual Speech(MLS) (2020） 源自LibriVox有声读物的数据集 16kHz，8种语言。
44.6k小时的英语数据，以及总计6k左右的其他7种语言的数据
具体分布如下：
论文：https://arxiv.org/abs/2012.03411
链接：http://www.openslr.org/94/
DidiSpeech（2021） 普通话，由滴滴出行发布 共800小时，6000个说话人，采样率48kHz，位深度16 bit。
性别、地域分布和录音设备（IOS/Andorid）比例基本接近1:1。年龄分布为adults(&gt;=20): youths(13-19): children(&lt;13) = 5:3:2。
包括两个子集DidiSpeech-1和DidiSpeech-2。DidiSpeech-1对标VC（Voice Conversion）任务，包括4500个说话人，共572h，每个说话人的录音有50 parallel sentences（平行语句，每个说话人都说的相同的句子）和50 non-parallel sentences（不被重复的、每个人不同的内容）。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/49ec3ec9c4ce07b21c387cac83bbcd17/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-24T00:45:14+08:00" />
<meta property="article:modified_time" content="2023-12-24T00:45:14+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">语音增强数据集总结【持续更新】</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#_1" rel="nofollow">语音增强数据集总结</a></li><li><ul><li><a href="#_4" rel="nofollow">纯净语音</a></li><li><ul><li><a href="#AVSpeech_5" rel="nofollow">AVSpeech</a></li><li><a href="#Common_Voice2019_17" rel="nofollow">Common Voice(2019首次发布，至今一直在更新)</a></li><li><a href="#Multi_Lingual_SpeechMLS_2020_26" rel="nofollow">Multi Lingual Speech(MLS) (2020）</a></li><li><a href="#DidiSpeech2021_37" rel="nofollow">DidiSpeech（2021）</a></li><li><a href="#VCTK2019Voice_Bank_47" rel="nofollow">VCTK（2019）（又名Voice Bank）</a></li><li><a href="#AiShell_54" rel="nofollow">AiShell</a></li><li><a href="#LibriSpeech_62" rel="nofollow">LibriSpeech</a></li><li><a href="#TIMIT_74" rel="nofollow">TIMIT</a></li></ul> 
   </li><li><a href="#_87" rel="nofollow">噪声</a></li><li><ul><li><a href="#AudioSet_88" rel="nofollow">AudioSet</a></li><li><a href="#WHAM2019_89" rel="nofollow">WHAM!(2019)</a></li><li><a href="#DEMAND_100" rel="nofollow">DEMAND</a></li><li><a href="#FSD50K2021_110" rel="nofollow">FSD50K（2021）</a></li><li><a href="#Noise92_122" rel="nofollow">Noise-92</a></li><li><a href="#AudioSet_129" rel="nofollow">AudioSet</a></li></ul> 
   </li><li><a href="#_137" rel="nofollow">已经合好的公开语音增强数据集</a></li><li><ul><li><a href="#VoiceBankDemand_138" rel="nofollow">VoiceBank+Demand</a></li><li><a href="#NISQA_159" rel="nofollow">NISQA</a></li><li><a href="#NOISEX92_170" rel="nofollow">NOISEX-92</a></li></ul> 
   </li><li><a href="#_182" rel="nofollow">真实带噪语音</a></li><li><ul><li><a href="#REALM2021_183" rel="nofollow">REAL-M(2021)</a></li><li><a href="#DAPS2014_192" rel="nofollow">DAPS（2014）</a></li></ul> 
   </li><li><a href="#_199" rel="nofollow">其他</a></li><li><ul><li><a href="#MUSAN2015_200" rel="nofollow">MUSAN（2015）</a></li><li><a href="#BABEL2011_211" rel="nofollow">BABEL（2011-）</a></li><li><a href="#_219" rel="nofollow">语音分离</a></li><li><a href="#LibriMix2020_229" rel="nofollow">LibriMix(2020)</a></li><li><a href="#AMI2005_237" rel="nofollow">AMI(2005)</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="_1"></a>语音增强数据集总结</h2> 
<p>语音增强的第一步一般是准备数据，包含带噪语音和纯净语音。一般这些数据都是人工合成的，以纯净语音和噪声数据为基础即可合出带噪语音信号。以下介绍一些语音增强领域常用的数据集。</p> 
<h3><a id="_4"></a>纯净语音</h3> 
<h4><a id="AVSpeech_5"></a>AVSpeech</h4> 
<blockquote> 
 <p>16kHz，由谷歌发布<br> 干净语音，用于语音分离任务。音视频数据集(Audio-Visual Dataset)。<br> 每条数据长度在3~10s，共计4700h，包括不同人种、不同语言和不同表情姿态的150,000个说话人。每条数据 &gt; 只出现一个说话人的面部和声音</p> 
</blockquote> 
<p>数据分布如下图：<br> <img src="https://images2.imgbox.com/b4/02/bZpjpMUM_o.png" alt="在这里插入图片描述" width="600"></p> 
<p>原文：<a href="https://paperswithcode.com/paper/looking-to-listen-at-the-cocktail-party-a" rel="nofollow">Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation</a><br> 网页：<a href="https://looking-to-listen.github.io/avspeech/" rel="nofollow">https://looking-to-listen.github.io/avspeech/</a><br> github：<a href="http://looking-to-listen.github.io/" rel="nofollow">http://looking-to-listen.github.io/</a></p> 
<h4><a id="Common_Voice2019_17"></a>Common Voice(2019首次发布，至今一直在更新)</h4> 
<blockquote> 
 <p>48kHz，16bit，MP3格式 截至2023/7/21,<br> 该数据集包括112种语言的28118小时语音，可用于训练的数据共18652小时。具体分布可看官网。</p> 
</blockquote> 
<p>论文：<a href="https://arxiv.org/abs/1912.06670" rel="nofollow">https://arxiv.org/abs/1912.06670</a><br> 官网链接：<a href="https://commonvoice.mozilla.org/en/languages" rel="nofollow">https://commonvoice.mozilla.org/en/languages</a><br> 因为数据集一直在更新，所以建议直接去官网看。</p> 
<h4><a id="Multi_Lingual_SpeechMLS_2020_26"></a>Multi Lingual Speech(MLS) (2020）</h4> 
<blockquote> 
 <p>源自LibriVox有声读物的数据集 16kHz，8种语言。<br> 44.6k小时的英语数据，以及总计6k左右的其他7种语言的数据</p> 
</blockquote> 
<p>具体分布如下：</p> 
<p><img src="https://images2.imgbox.com/3e/64/IhCOiubB_o.png" alt="在这里插入图片描述" width="600"><br> 论文：<a href="https://arxiv.org/abs/2012.03411" rel="nofollow">https://arxiv.org/abs/2012.03411</a><br> 链接：<a href="http://www.openslr.org/94/" rel="nofollow">http://www.openslr.org/94/</a></p> 
<h4><a id="DidiSpeech2021_37"></a>DidiSpeech（2021）</h4> 
<blockquote> 
 <p>普通话，由滴滴出行发布 共800小时，6000个说话人，采样率48kHz，位深度16 bit。<br> 性别、地域分布和录音设备（IOS/Andorid）比例基本接近1:1。年龄分布为adults(&gt;=20): youths(13-19): children(&lt;13) = 5:3:2。<br> 包括两个子集DidiSpeech-1和DidiSpeech-2。DidiSpeech-1对标VC（Voice Conversion）任务，包括4500个说话人，共572h，每个说话人的录音有50 parallel sentences（平行语句，每个说话人都说的相同的句子）和50 non-parallel sentences（不被重复的、每个人不同的内容）。<br> DidiSpeech-2对标multi-speaker speech synthesis 和 ASR任务，共227h，1500个说话人，没个说话人读&gt;100句 non-parallel sentences。</p> 
</blockquote> 
<p>原文链接：<a href="https://ieeexplore.ieee.org/document/9414423" rel="nofollow">https://ieeexplore.ieee.org/document/9414423</a><br> 数据地址：<a href="https://outreach.didichuxing.com/research/opendata/" rel="nofollow">https://outreach.didichuxing.com/research/opendata/</a></p> 
<h4><a id="VCTK2019Voice_Bank_47"></a>VCTK（2019）（又名Voice Bank）</h4> 
<blockquote> 
 <p>英语，由爱丁堡大学发布 共44小时(数据来源网络),<br> 共110个英语说话人，包含不同的口音，每个说话人读约400个句子，采样率48kHz，位深度16bit。</p> 
</blockquote> 
<p>官方文档及数据下载地址：<a href="https://datashare.ed.ac.uk/handle/10283/3443" rel="nofollow">https://datashare.ed.ac.uk/handle/10283/3443</a></p> 
<h4><a id="AiShell_54"></a>AiShell</h4> 
<blockquote> 
 <p>中文，北京希尔贝壳科技 AiShell-1(2017) 共178小时，400个来自中国不同地区的说话人，采样率为16kHz，位深度16bit。<br> AiShell-3（2020）<br> 共85小时，218个说话人,共88035条语音，采样率为44.1kHz，位深度16bit。说话人分布偏向女性，年龄分布集中在20岁左右。</p> 
</blockquote> 
<p>详情见官方介绍：<a href="https://www.aishelltech.com/kysjcp" rel="nofollow">https://www.aishelltech.com/kysjcp</a></p> 
<h4><a id="LibriSpeech_62"></a>LibriSpeech</h4> 
<blockquote> 
 <p>英语 2015 共1000小时，16kHz采样率，接近US English，derive from LibriVox</p> 
</blockquote> 
<p>具体分为如下子集：<br> <img src="https://images2.imgbox.com/5d/9e/VED5ISzD_o.png" alt="在这里插入图片描述" width="400"></p> 
<blockquote> 
 <p>基于WSJ语料库训练出的ASR模型评测每个说话人的WER，WER更低的 speaker被分为clean，WER更高的分为other。</p> 
</blockquote> 
<p>论文：<a href="https://ieeexplore.ieee.org/document/7178964" rel="nofollow">Librispeech: An ASR corpus based on public domain audio books</a><br> 链接：<a href="http://www.openslr.org/12/" rel="nofollow">http://www.openslr.org/12/</a></p> 
<h4><a id="TIMIT_74"></a>TIMIT</h4> 
<blockquote> 
 <p>英语，由TI（Texas Instruments）、MIT（Massachusetts Institute of Technology）和SRI (Stanford Research Institute)共同收集语料库，也是其名字的由来。<br> 16kHz，共6300个句子，由来自美国8个主要方言区的630人每人读10个句子，10个句子中包括2个方言句、5个phonetically-compact sentences（发音紧凑的句子）和3个phonetically-diverse sentences （语音多样的句子），</p> 
</blockquote> 
<p>具体分布如下表：<br> <img src="https://images2.imgbox.com/59/f6/XrEjls0V_o.png" alt="在这里插入图片描述" width="600"></p> 
<blockquote> 
 <p>其完整测试集占据整个数据材料的27%，共168个说话人、1344条语句。</p> 
</blockquote> 
<p>论文：<a href="https://www.researchgate.net/publication/243787812_TIMIT_Acoustic-phonetic_Continuous_Speech_Corpus" rel="nofollow">https://www.researchgate.net/publication/243787812_TIMIT_Acoustic-phonetic_Continuous_Speech_Corpus</a><br> 链接：<a href="https://www.kaggle.com/datasets/mfekadu/darpa-timit-acousticphonetic-continuous-speech" rel="nofollow">https://www.kaggle.com/datasets/mfekadu/darpa-timit-acousticphonetic-continuous-speech</a></p> 
<h3><a id="_87"></a>噪声</h3> 
<h4><a id="AudioSet_88"></a>AudioSet</h4> 
<h4><a id="WHAM2019_89"></a>WHAM!(2019)</h4> 
<blockquote> 
 <p>16kHz，32bit，双通道、平均时长10s，最短3.5秒，最长47.7秒。<br> 数据集全称为WSJ0 Hipster Ambient Mixtures。 这些噪声录制于旧金山湾区的咖啡馆、餐厅、酒吧、办公楼、公园等城市环境中</p> 
</blockquote> 
<p>分布如下：<br> <img src="https://images2.imgbox.com/02/ab/ijKu7daX_o.png" alt="在这里插入图片描述" width="700"></p> 
<p>原文：<a href="https://arxiv.org/abs/1907.01160#:~:text=WHAM!:%20Extending%20Speech%20Separation%20to%20Noisy%20Environments%20Recent,us%20closer%20to%20solving%20the%20cocktail%20party%20problem." rel="nofollow">WHAM!: Extending Speech Separation to Noisy Environments</a><br> 链接：<a href="http://www.shujujishi.com/dataset/ae73e948-46d8-4e19-aef3-47a7baa044ab.html" rel="nofollow">http://www.shujujishi.com/dataset/ae73e948-46d8-4e19-aef3-47a7baa044ab.html</a></p> 
<h4><a id="DEMAND_100"></a>DEMAND</h4> 
<blockquote> 
 <p>16通道 48kHz 六中大环境下的真实噪声</p> 
</blockquote> 
<p>具体分布如下</p> 
<p><img src="https://images2.imgbox.com/f1/34/5O3BAe8U_o.png" alt="在这里插入图片描述" width="500"></p> 
<p>链接：<a href="https://inria.hal.science/hal-00796707/en#:~:text=DEMAND%20%28Diverse%20Environments%20Multi-channel%20Acoustic%20Noise%20Database%29%20addresses,in%20a%20variety%20of%20indoor%20and%20outdoor%20settings." rel="nofollow">The Diverse Environments Multi-channel Acoustic Noise Database (DEMAND): A database of multichannel environmental noise recordings</a></p> 
<h4><a id="FSD50K2021_110"></a>FSD50K（2021）</h4> 
<blockquote> 
 <p>44.1kHz, 16it<br> 用于声音事件检测任务，有200种噪声种类，51197条音频文件，主要包括人声、动物声音、自然界的声音、音乐和事物的声音，共108小时。</p> 
</blockquote> 
<p>分布如下：<br> <img src="https://images2.imgbox.com/60/8e/ClWKpuvF_o.png" alt="在这里插入图片描述" width="500"></p> 
<p>论文：<a href="https://ieeexplore.ieee.org/document/9645159" rel="nofollow">FSD50K: An Open Dataset of Human-Labeled Sound Events</a><br> 链接：<a href="https://annotator.freesound.org/fsd/release/FSD50K/" rel="nofollow">https://annotator.freesound.org/fsd/release/FSD50K/</a><br> <a href="https://zenodo.org/record/4060432" rel="nofollow">https://zenodo.org/record/4060432</a></p> 
<h4><a id="Noise92_122"></a>Noise-92</h4> 
<blockquote> 
 <p>19.98kHz，16bit，<br> 每条音频时长为235秒 包含15种噪声类型：白噪声、粉红噪声、餐厅内嘈杂说话声、2种工厂噪声、3种驾驶舱噪声、机舱噪声、控制室噪声、两种军车噪声、枪声、车辆内部噪声和高频通道噪声</p> 
</blockquote> 
<p>链接：<a href="http://spib.linse.ufsc.br/noise.html" rel="nofollow">http://spib.linse.ufsc.br/noise.html</a></p> 
<h4><a id="AudioSet_129"></a>AudioSet</h4> 
<blockquote> 
 <p>用于音频事件识别的数据集，取自Youtube video。包含1789621条（178万）10s音频，共计4971小时。其中训练集为1771873条，评估集为17748条。包含632种声音事件，其中有485类声音事件包含至少100个实例。<br> 包含六大类声音事件：Human Sounds, Animal Sounds, Natural Sounds, Music, Sound of Things, Source-ambiguous Sounds, Channel environment and background。</p> 
</blockquote> 
<p>本文偏向语音增强领域的应用，因此具体细节不再赘述，请参考原论文。</p> 
<p>链接：<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7952261" rel="nofollow">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7952261</a></p> 
<h3><a id="_137"></a>已经合好的公开语音增强数据集</h3> 
<h4><a id="VoiceBankDemand_138"></a>VoiceBank+Demand</h4> 
<blockquote> 
 <p>英语，爱丁堡大学，48kHz；语音数据来自VCTK，有两个子集：28说话人和56说话人</p> 
</blockquote> 
<p><strong>28说话人</strong></p> 
<blockquote> 
 <p><strong>train set：</strong> 14female 14male (all England accent)<br> 10种噪声数据，2种人为生成、8种来自Demand，4种信噪比：15,10,5,0dB;因此有10*4=40种不同的噪声。共11572条，9.4h。</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/2e/18/Ittos1Sn_o.png" alt="" width="400"></p> 
<blockquote> 
 <p><strong>test set：</strong> 1 female1male (England); 5种噪声数据，全都来自Demand，4种信噪比：17.5, 12.5, 7.5 以及2.5dB，有5*4=20种噪声。共824条，0.6h。</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/93/47/A8SNhRbH_o.png" alt=""><br> <strong>56说话人</strong></p> 
<blockquote> 
 <p>28female 28male (Scotland and United States)</p> 
</blockquote> 
<p>论文：<a href="https://www.isca-speech.org/archive/interspeech_2016/valentinibotinhao16_interspeech.html" rel="nofollow">Speech Enhancement for a Noise-Robust Text-to-Speech Synthesis System<br> using Deep Recurrent Neural Networks</a><br> 官方链接：<a href="https://datashare.ed.ac.uk/handle/10283/2791" rel="nofollow">Noisy speech database for training speech enhancement algorithms and TTS models</a></p> 
<h4><a id="NISQA_159"></a>NISQA</h4> 
<blockquote> 
 <p>发布于Quality and Usability Lab, Technische Universit ̈at Berlin、Deutsches Forschungszentrum f ̈ur K ̈unstliche Intelligenz (DFKI)，Berlin, Germany<br> 同时拥有合成的和真实的带噪语音 train和validation set都有人工标注的MOS评分<br> <strong>2个training dataset</strong>: NISQA_TRAIN_SIM 10,000 samples from 2,322 speakers; NISQA TRAIN LIVE 1,020 samples from 486 speaker speaker;<br> <strong>2个validation dataset</strong>: NISQA VAL SIM (2,500 samples from 938 speaker）；NISQA VAL LIVE (200 samples from 102 speakers)<br> <strong>4个test set</strong>: NISQA TEST P501(240 samples from 4 speakers 男女各半), NISQA TEST FOR（240 samples from 8 speakers,男女各半）, NISQA TEST NSC（240 samples from 240 speakers），NISQA TEST LIVETALK wit（232 samples from 8 speakers,男女各半）</p> 
</blockquote> 
<p>论文：<a href="https://arxiv.org/abs/2104.09494" rel="nofollow">https://arxiv.org/abs/2104.09494</a><br> 链接：<a href="https://github.com/gabrielmittag/NISQA/wiki/NISQA-Corpus">https://github.com/gabrielmittag/NISQA/wiki/NISQA-Corpus</a></p> 
<h4><a id="NOISEX92_170"></a>NOISEX-92</h4> 
<blockquote> 
 <p>16kHz，16bit，数据需要空间1.4G；<br> 语音部分来自ESPRIT SAM EUROM_0，共一男一女两个说话人，每人读两张一百个独立数字组成的表，一张用于训练一张用于测试，每人还有两张50个三位数组成的表，同样分别作为训练和测试，噪声部分来自RSG.10 NOISE-ROM-0，从中选择了8种噪声。</p> 
</blockquote> 
<p>具体分布如下：<br> <img src="https://images2.imgbox.com/1e/af/1mZZJROO_o.png" alt="在这里插入图片描述" width="300"></p> 
<blockquote> 
 <p>最终音频信噪比为18,12,6,0,-6dB.</p> 
</blockquote> 
<p>论文：<a href="https://sci-hub.se/10.1016/0167-6393%2893%2990095-3#:~:text=Assessment%20for%20automatic%20speech%20recognition:%20II.%20NOISEX-92:%20A,Communication,%2012%20%283%29,%20247%E2%80%93251.%20doi:10.1016/0167-6393%20%2893%2990095-3%2010.1016/0167-6393%20%2893%2990095-3" rel="nofollow">Assessment for automatic speech recognition: II. NOISEX-92: A database and an experiment to study the effect of additive noise on speech recognition systems </a></p> 
<h3><a id="_182"></a>真实带噪语音</h3> 
<h4><a id="REALM2021_183"></a>REAL-M(2021)</h4> 
<blockquote> 
 <p>8kHz<br> 文本来自LibriSpeech，提供给每一对说话者，房间里的说话人同时阅读句子，共收集了1436个mixture；另外有144个mixture的其中一个说话人语音是通过视频会议软件录制的；这增加了数据集的多样性和应用场景。</p> 
</blockquote> 
<p>论文：<a href="https://arxiv.org/abs/2110.10812" rel="nofollow">https://arxiv.org/abs/2110.10812</a><br> 链接：<a href="https://sourceseparationresearch.com/static/REAL-M-v0.1.0.tar.gz" rel="nofollow">https://sourceseparationresearch.com/static/REAL-M-v0.1.0.tar.gz</a></p> 
<h4><a id="DAPS2014_192"></a>DAPS（2014）</h4> 
<blockquote> 
 <p>该数据集有真实的带噪语音，并且提供了答案。收集方法为：现在安静环境下录干净语音，然后在特定噪声环境下经这段语音用扬声器放出来，和噪声一起录下来，作为带噪语音。<br> 20 speakers, 每个说话人有5段脚本大概14 minutes的语音，采样率44.1kHz</p> 
</blockquote> 
<p>论文：<a href="https://ieeexplore.ieee.org/document/6981922/" rel="nofollow">Can we Automatically Transform Speech Recorded on Common Consumer Devices in Real-World Environments into Professional Production Quality Speech?—A Dataset, Insights, and Challenges</a><br> 链接：<a href="https://ccrma.stanford.edu/~gautham/Site/daps.html" rel="nofollow">https://ccrma.stanford.edu/~gautham/Site/daps.html</a></p> 
<h3><a id="_199"></a>其他</h3> 
<h4><a id="MUSAN2015_200"></a>MUSAN（2015）</h4> 
<blockquote> 
 <p>约翰斯霍普金斯大学语言语音处理中心<br> 包括语音、音乐和噪声三种数据集，109小时，16kHz<br> <strong>Speech</strong>：约60h，其中20h21m来自Librivox, 40h1m来自美国政府公开的听证会、委员会和辩论等<br> <strong>Music</strong>：42h31m，分为popular genres和Western art music<br> <strong>Noise</strong>：约6小时，共929条音频文件，来自<a href="https://www.freesound.org/" rel="nofollow">Free Sound</a>和<a href="http://soundbible.com/" rel="nofollow">Sound Bible</a></p> 
</blockquote> 
<p>论文：<a href="https://arxiv.org/abs/1510.08484" rel="nofollow">MUSAN: A Music, Speech, and Noise Corpus</a><br> 链接：<a href="http://www.openslr.org/17/" rel="nofollow">http://www.openslr.org/17/</a></p> 
<h4><a id="BABEL2011_211"></a>BABEL（2011-）</h4> 
<blockquote> 
 <p>BABEL计划收集的数据集，该计划的目标是开发可应用与任何人类语言的语音技术，目前包含爪哇语、粤语、蒙古语、Dholuo、阿姆哈拉语、瓜拉尼、Igbo Language, Lithuanian Language, Cebuano Language, Kazakh Language, TokPisin Language, Telugu Language, Haitian Creole Language, Kurmanji Kurdish Language, Lao Language, Swahili Language, Tamil Language, Vietnamese Language, Zulu Language, Assamese Language, Bengali Language, Georgian Language, Pashto Language, Tagalog Language, Turkish Language共25种语言<br> 8kHz，通话录音</p> 
</blockquote> 
<p>链接：<a href="https://catalog.ldc.upenn.edu/byyear" rel="nofollow">https://catalog.ldc.upenn.edu/byyear</a><br> <a href="https://www.iarpa.gov/index.php/research-programs/babel" rel="nofollow">https://www.iarpa.gov/index.php/research-programs/babel</a></p> 
<h4><a id="_219"></a>语音分离</h4> 
<p><strong>LibriCSS（2020）</strong></p> 
<blockquote> 
 <p>16kHz、源自LibriSpeech，面向CSS任务(Continuous Speech Separation)<br> 包含10 sessions，每个session 1小时，LibriCSS共计10小时。<br> 每个session包含6 段10分钟的mini session，每个段mini session 有8个说话人（从LibriSpeech development set的40个说话人中随机选择），OVR (overlap ratio)为0-40%，重叠率为0时，句子间silence为0.1-0.5s的被称为short silence version, 句间silence为2.9-3.0s的则被称为long silence; mini session中的uttrance为52-125不等。</p> 
</blockquote> 
<p>原文：<a href="https://arxiv.org/abs/2001.11482v1#:~:text=Title:Continuous%20speech%20separation:%20dataset%20and%20analysis%20Authors:Zhuo%20Chen,,algorithms.%20Most%20prior%20studies%20on%20speech%20separation%20use" rel="nofollow">Continuous speech separation: Dataset and analysis</a><br> 数据：<a href="https://github.com/chenzhuo1011/libri_css">https://github.com/chenzhuo1011/libri_css</a></p> 
<h4><a id="LibriMix2020_229"></a>LibriMix(2020)</h4> 
<blockquote> 
 <p>16kHz，语音源自LibriSpeech，噪声来自WHAM！ 在训练集中，每个句子只使用一次。</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/98/b7/UxnGSjfX_o.png" alt="在这里插入图片描述" width="400"><br> 原文：<a href="https://arxiv.org/abs/2005.11262" rel="nofollow">LibriMix: An Open-Source Dataset for Generalizable Speech Separation</a><br> 参考链接：<a href="https://blog.csdn.net/FonFon27/article/details/113834692">https://blog.csdn.net/FonFon27/article/details/113834692</a></p> 
<h4><a id="AMI2005_237"></a>AMI(2005)</h4> 
<blockquote> 
 <p>爱丁堡大学<br> 包含100小时的会议录音，英语。大部分说话人为非母语人士</p> 
</blockquote> 
<p>原文：<a href="https://www.researchgate.net/publication/228341280_The_AMI_meeting_corpus" rel="nofollow">The AMI meeting corpus</a><br> 链接：<a href="https://groups.inf.ed.ac.uk/ami/corpus/" rel="nofollow">https://groups.inf.ed.ac.uk/ami/corpus/</a><br> <a href="https://groups.inf.ed.ac.uk/ami/corpus/overview.shtml" rel="nofollow">https://groups.inf.ed.ac.uk/ami/corpus/overview.shtml</a></p> 
<p>个性化语音增强<br> DNS4-Track2<br> WSJ0-2mix<br> AiShell2Mix<br> Common Voice</p> 
<p>最后，这是网上一些其他的数据集整理链接<br> <a href="https://www.cnblogs.com/LXP-Never/p/15474948.html" rel="nofollow">https://www.cnblogs.com/LXP-Never/p/15474948.html</a>（凌逆战）<br> <a href="https://blog.csdn.net/qq_34637672/article/details/117925485">https://blog.csdn.net/qq_34637672/article/details/117925485</a><br> <a href="https://github.com/nanahou/Awesome-Speech-Enhancement">https://github.com/nanahou/Awesome-Speech-Enhancement</a><br> <a href="https://zhuanlan.zhihu.com/p/267372288" rel="nofollow">https://zhuanlan.zhihu.com/p/267372288</a></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7540c45b800713e2cc063300e823ee49/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">如何在linux安装软件</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/29fd28466db41000f3d72e1cf83d9b75/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">第十二章集合类总结</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>