<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Unet相关知识及网络解析 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Unet相关知识及网络解析" />
<meta property="og:description" content="这里写目录标题 Unet相关算法SoftmaxLogSoftmaxBatchNorm2dUpsampleConvTranspose2dPad 网络概述网络详解下采样上采样输出 完整网络代码注意点，Sigmoid VS softmax Unet 相关算法 这一部分涉及到的Conv2d二维卷积,，ReLU线性激活，MaxPool2d二维池化，之前文章已提及，这里不在赘述，详情可见上一篇博客
Softmax 归一化到范围0~1，总和为1，常用于将分类网络输出转为对应类别的非线性激活函数，
LogSoftmax 从数学本质上看，就是对Softmax做了log运算。这样产生的就是log probability 而不是 standard probability；
那为什么要log probability呢？
log probability计算更快，结果更稳定，虽然对于人的视角来看，计算结果不直观，但对于计算机在计算方面，效率更高。那又为什么效率高，详情可以看，下面几张图片都来自于链接
Softmax VS LogSoftmax
Why are log probabilities useful?
快：Log(a/b)=loga - logb
稳定：
BatchNorm2d 比较特殊，输入必须是4维，其中一定有batch
主要输入参数
作用：
Var是 torch.var
见论文
训练深度神经网络是复杂的，因为在训练过程中，随着前一层的参数变化，每一层输入的分布都会发生变化。这需要更低的学习率和仔细的参数初始化，从而减慢了训练速度，并使训练具有饱和非线性的模型变得非常困难。我们将这种现象称为 internal covariate shift，并通过normalizing layer inputs来解决这个问题。我们的方法通过将规范化作为模型架构的一部分并对每个训练小批量执行规范化来发挥其优势。批量规范化允许我们使用更高的学习率，并且在初始化时不需要那么小心。在某些情况下，它还起到了正则化因子的作用，消除了Dropout(不知道干嘛的看上篇)的必要性。应用于最先进的图像分类模型，Batch Normalization以减少14倍的训练步骤实现了相同的精度，并以显著的优势击败了原始模型。使用批量归一化网络的集合，我们改进了ImageNet分类的最佳发布结果：达到4.9%的前五名验证误差（和4.8%的测试误差），超过了人类评分者的准确性。
Upsample 可以理解为：插值填充
主要步骤
上采样插值
计算要进行拼接的两个输入参数维度偏差，比如(128,128)，(64,64),diff等于64和64
将维度小的周围填0(nn.functional.pad)，使得输入input1和input2维度一样，然后concat
然后双卷积，注意，此处由于拼接了，所以卷积的输入通道，是函数是两个输入的通道之和
ConvTranspose2d 也是卷积，但是会改变输出shape，也是填充，但不是单单Upsample；等同于Conv2d的计算方式达到Upsample的效果
在Unet网络中，主要与Upsample做区分，具体区别如下
UpSampling2D is just a simple scaling up of the image by using nearest neighbour or bilinear upsampling, so nothing smart." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/1012d9f60102da49c46d84c1f0932df1/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-09-04T11:52:53+08:00" />
<meta property="article:modified_time" content="2023-09-04T11:52:53+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Unet相关知识及网络解析</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>这里写目录标题</h4> 
 <ul><li><a href="#Unet_2" rel="nofollow">Unet</a></li><li><ul><li><a href="#_3" rel="nofollow">相关算法</a></li><li><ul><li><a href="#Softmax_6" rel="nofollow">Softmax</a></li><li><a href="#LogSoftmax_11" rel="nofollow">LogSoftmax</a></li><li><a href="#BatchNorm2d_23" rel="nofollow">BatchNorm2d</a></li><li><a href="#Upsample_32" rel="nofollow">Upsample</a></li><li><a href="#ConvTranspose2d_46" rel="nofollow">ConvTranspose2d</a></li><li><a href="#Pad_55" rel="nofollow">Pad</a></li></ul> 
   </li><li><a href="#_60" rel="nofollow">网络概述</a></li><li><a href="#_70" rel="nofollow">网络详解</a></li><li><ul><li><a href="#_72" rel="nofollow">下采样</a></li><li><a href="#_116" rel="nofollow">上采样</a></li><li><a href="#_155" rel="nofollow">输出</a></li></ul> 
   </li><li><a href="#_167" rel="nofollow">完整网络代码</a></li><li><ul><li><a href="#Sigmoid_VS_softmax_201" rel="nofollow">注意点，Sigmoid VS softmax</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="Unet_2"></a>Unet</h2> 
<h3><a id="_3"></a>相关算法</h3> 
<p>这一部分涉及到的Conv2d二维卷积,，ReLU线性激活，MaxPool2d二维池化，之前文章已提及，这里不在赘述，<a href="https://blog.csdn.net/weixin_45277117/article/details/132396647">详情可见上一篇博客</a></p> 
<h4><a id="Softmax_6"></a>Softmax</h4> 
<p>归一化到范围0~1，总和为1，常用于将分类网络输出转为对应类别的非线性激活函数，</p> 
<p><img src="https://images2.imgbox.com/5f/63/ZYeanMC1_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/e1/2d/QoFU6KPA_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="LogSoftmax_11"></a>LogSoftmax</h4> 
<p>从数学本质上看，就是对Softmax做了log运算。这样产生的就是log probability 而不是 standard probability；<br> 那为什么要log probability呢？<br> log probability计算更快，结果更稳定，虽然对于人的视角来看，计算结果不直观，但对于计算机在计算方面，效率更高。那又为什么效率高，详情可以看，下面几张图片都来自于链接<br> <a href="https://medium.com/@AbhiramiVS/softmax-vs-logsoftmax-eb94254445a2" rel="nofollow">Softmax VS LogSoftmax</a><br> <a href="https://stats.stackexchange.com/questions/483927/why-are-log-probabilities-useful" rel="nofollow">Why are log probabilities useful?</a><br> 快：Log(a/b)=loga - logb<br> <img src="https://images2.imgbox.com/f2/4e/oIu52vKf_o.png" alt="在这里插入图片描述"><br> 稳定：<img src="https://images2.imgbox.com/f3/69/53B4K8bc_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/45/c3/tBDgrfBn_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ca/24/IbiuoFX1_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="BatchNorm2d_23"></a>BatchNorm2d</h4> 
<p>比较特殊，输入必须是4维，其中一定有batch<br> 主要输入参数<br> <img src="https://images2.imgbox.com/22/73/cXvrkEy5_o.png" alt="在这里插入图片描述"><br> 作用：<br> <img src="https://images2.imgbox.com/44/9d/2V65tDQQ_o.png" alt="在这里插入图片描述"><br> Var是 torch.var<br> <a href="https://arxiv.org/abs/1502.03167" rel="nofollow">见论文</a><br> 训练深度神经网络是复杂的，因为在训练过程中，随着前一层的参数变化，每一层输入的分布都会发生变化。这需要更低的学习率和仔细的参数初始化，从而减慢了训练速度，并使训练具有饱和非线性的模型变得非常困难。我们将这种现象称为 internal covariate shift，并通过normalizing layer inputs来解决这个问题。我们的方法通过将规范化作为模型架构的一部分并对每个训练小批量执行规范化来发挥其优势。批量规范化允许我们使用更高的学习率，并且在初始化时不需要那么小心。在某些情况下，它还起到了正则化因子的作用，消除了Dropout(<a href="https://blog.csdn.net/weixin_45277117/article/details/132396647">不知道干嘛的看上篇</a>)的必要性。应用于最先进的图像分类模型，Batch Normalization以减少14倍的训练步骤实现了相同的精度，并以显著的优势击败了原始模型。使用批量归一化网络的集合，我们改进了ImageNet分类的最佳发布结果：达到4.9%的前五名验证误差（和4.8%的测试误差），超过了人类评分者的准确性。</p> 
<h4><a id="Upsample_32"></a>Upsample</h4> 
<p>可以理解为：插值填充</p> 
<p>主要步骤</p> 
<ul><li> <p>上采样插值</p> </li><li> <p>计算要进行拼接的两个输入参数维度偏差，比如(128,128)，(64,64),diff等于64和64</p> </li><li> <p>将维度小的周围填0(nn.functional.pad)，使得输入input1和input2维度一样，然后concat</p> </li><li> <p>然后双卷积，注意，此处由于拼接了，所以卷积的输入通道，是函数是两个输入的通道之和</p> </li><li> <p><img src="https://images2.imgbox.com/87/bd/4jkC5R1n_o.png" alt="在这里插入图片描述"></p> </li></ul> 
<h4><a id="ConvTranspose2d_46"></a>ConvTranspose2d</h4> 
<p>也是卷积，但是会改变输出shape，也是填充，但不是单单Upsample；等同于Conv2d的计算方式达到Upsample的效果</p> 
<p>在Unet网络中，主要与Upsample做区分，具体区别如下<br> UpSampling2D is just a simple scaling up of the image by using nearest neighbour or bilinear upsampling, so nothing smart. Advantage is it’s cheap.</p> 
<p>Conv2DTranspose is a convolution operation whose kernel is learnt (just like normal conv2d operation) while training your model. Using Conv2DTranspose will also upsample its input but the key difference is the model should learn what is the best upsampling for the job.<br> <img src="https://images2.imgbox.com/16/1a/K0cROsj4_o.png" alt="在这里插入图片描述"><br> 与Conv的区别见<a href="https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d" rel="nofollow">链接</a></p> 
<h4><a id="Pad_55"></a>Pad</h4> 
<p>补足周围元素使维度一致，即填充、膨胀<br> <img src="https://images2.imgbox.com/83/c5/BSVwHg9d_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_60"></a>网络概述</h3> 
<p><img src="https://images2.imgbox.com/1a/1e/lNHmo1Su_o.png" alt="在这里插入图片描述"><br> Unet经典网络，无更改；</p> 
<p>2015年提出的UNet模型是我们学习语义分割必学的一个优秀模型，它兼具轻量化与高性能，因此通常作为语义分割任务的基线测试模型，至今仍是如此，其优秀程度可见一斑。</p> 
<p>UNet从本质上来说也属于一种全卷积神经网络模型，它的取名来源于其架构形状：模型整体呈现"U"形。它的出生是为了解决医疗影像语义分割问题的，但之后几年的发展，也证实了它是语义分割任务中的全能选手，或许这就是优秀网络架构的优异之处。</p> 
<p>图像分类有ResNet，语义分割有UNet，目标检测有YOLO，NLP有Transformer，生成式AI有Diffusion Model。</p> 
<h3><a id="_70"></a>网络详解</h3> 
<p>由于这里我们进行图像缺陷检测，为了方便数据提取，需要对中间层做一些改动</p> 
<h4><a id="_72"></a>下采样</h4> 
<p>主要是起到提取特征的作用</p> 
<p>步骤：先进行最大池化，ks=2，然后进行两次卷积单元</p> 
<p>下采样由一次池化何两次卷积单元组成<br> 两次卷积单元也叫DoubleConv，由Conv2d，BatchNorm2d，ReLU组成</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">DoubleConv</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""(convolution =&gt; [BN] =&gt; ReLU) * 2"""</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>double_conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>double_conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> 
<p>完整下采样单元</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Down</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Downscaling with maxpool then double conv"""</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>maxpool_conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            DoubleConv<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>maxpool_conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> 
<p>下采样代表网络中的前四个单元<br> <img src="https://images2.imgbox.com/a9/57/G2aezNoH_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="_116"></a>上采样</h4> 
<p>主要是起到拼接特征的作用</p> 
<p>步骤：</p> 
<ul><li> <p>上采样插值</p> </li><li> <p>计算要进行拼接的两个输入参数维度偏差，比如(128,128)，(64,64),diff等于64和64</p> </li><li> <p>将维度小的周围填0(nn.functional.pad)，使得输入x1和x2维度一样，然后concat</p> </li><li> <p>然后双卷积，注意，此处由于拼接了，所以卷积的输入通道，是函数是两个输入的通道之和</p> </li></ul> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Up</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Upscaling then double conv"""</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> bilinear<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># if bilinear, use the normal convolutions to reduce the number of channels</span>
        <span class="token keyword">if</span> bilinear<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>up <span class="token operator">=</span> nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">,</span> align_corners<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>up <span class="token operator">=</span> nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span>in_channels <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">,</span> in_channels <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> DoubleConv<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x1<span class="token punctuation">,</span> x2<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x1 <span class="token operator">=</span> self<span class="token punctuation">.</span>up<span class="token punctuation">(</span>x1<span class="token punctuation">)</span>
        <span class="token comment"># input is CHW</span>
        diffY <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>x2<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">-</span> x1<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        diffX <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>x2<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">-</span> x1<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        x1 <span class="token operator">=</span> F<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>x1<span class="token punctuation">,</span> <span class="token punctuation">[</span>diffX <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">,</span> diffX <span class="token operator">-</span> diffX <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">,</span>
                        diffY <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">,</span> diffY <span class="token operator">-</span> diffY <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>x2<span class="token punctuation">,</span> x1<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> 
<h4><a id="_155"></a>输出</h4> 
<p>最简单的一个卷积，输出通道数是类别</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">OutConv</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>OutConv<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="_167"></a>完整网络代码</h3> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">UNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_channels<span class="token punctuation">,</span> n_classes<span class="token punctuation">,</span> bilinear<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>UNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>n_channels <span class="token operator">=</span> n_channels
        self<span class="token punctuation">.</span>n_classes <span class="token operator">=</span> n_classes
        self<span class="token punctuation">.</span>bilinear <span class="token operator">=</span> bilinear

        self<span class="token punctuation">.</span>inc <span class="token operator">=</span> DoubleConv<span class="token punctuation">(</span>n_channels<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>down1 <span class="token operator">=</span> Down<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>down2 <span class="token operator">=</span> Down<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>down3 <span class="token operator">=</span> Down<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>down4 <span class="token operator">=</span> Down<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>up1 <span class="token operator">=</span> Up<span class="token punctuation">(</span><span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> bilinear<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>up2 <span class="token operator">=</span> Up<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> bilinear<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>up3 <span class="token operator">=</span> Up<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> bilinear<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>up4 <span class="token operator">=</span> Up<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> bilinear<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>outc <span class="token operator">=</span> OutConv<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> n_classes<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x1 <span class="token operator">=</span> self<span class="token punctuation">.</span>inc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x2 <span class="token operator">=</span> self<span class="token punctuation">.</span>down1<span class="token punctuation">(</span>x1<span class="token punctuation">)</span>
        x3 <span class="token operator">=</span> self<span class="token punctuation">.</span>down2<span class="token punctuation">(</span>x2<span class="token punctuation">)</span>
        x4 <span class="token operator">=</span> self<span class="token punctuation">.</span>down3<span class="token punctuation">(</span>x3<span class="token punctuation">)</span>
        x5 <span class="token operator">=</span> self<span class="token punctuation">.</span>down4<span class="token punctuation">(</span>x4<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>up1<span class="token punctuation">(</span>x5<span class="token punctuation">,</span> x4<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>up2<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x3<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>up3<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x2<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>up4<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x1<span class="token punctuation">)</span>
        logits <span class="token operator">=</span> self<span class="token punctuation">.</span>outc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> logits
</code></pre> 
<h4><a id="Sigmoid_VS_softmax_201"></a>注意点，Sigmoid VS softmax</h4> 
<p>在进行本质是二分类的语义分割时，计算probs的归一化函数请用sigmoid；<br> 在进行本质是多分类的语义分割时，请用softmax<br> 原因：<br> sigmoid何softmax都是非线性激活函数，本质在于<br> <img src="https://images2.imgbox.com/66/37/ZihTmyvE_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/dd/81/OcOENIja_o.png" alt="在这里插入图片描述"><br> softmax相较于sigmoid，会计算j个类别的百分比，而sigmoid计算的只是1-的百分比</p> 
<p>在两类逻辑回归中，使用sigmoid函数预测的概率如下：<br> <img src="https://images2.imgbox.com/c0/dc/bqmbaSYL_o.png" alt="在这里插入图片描述"><br> 在多类逻辑回归中，K类，预测概率如下，使用softmax函数：<br> <img src="https://images2.imgbox.com/3c/04/TwBowv1E_o.png" alt="在这里插入图片描述"><br> <a href="https://stats.stackexchange.com/questions/233658/softmax-vs-sigmoid-function-in-logistic-classifier" rel="nofollow">Softmax vs Sigmoid function in Logistic classifier?</a></p> 
<p>简单点说，sigmoid只能表示“是不是这个的概率”，而softmax可以表示每一个可能性的概率<br> <img src="https://images2.imgbox.com/d7/f5/Hgl4GFvH_o.png" alt="在这里插入图片描述"><br> <strong>另外</strong><br> 如果在train时，突然把sigmoid换成softmax，或者反过来，在进行推理的时候也要注意，要同步修改，并且你对应标注的label也要修改，因为我们生成label的格式一般都是背景灰度是0，类别1灰度是1，一次类推来生成mask图。shape是【1，h，w】，对应到unet输出，维度并不一样；那么对应loss对比的维度也不一样，所以要做postprocess，同时对应选择sigmoid或者softmax</p> 
<p>在网络输出中，【b，c，h，w】，b是batch，c是类别通道(不是图片通道)，而你的标注是【b，1，h，w】，在进行普通的softmax后，要对loss进行分析格式化，不然得到的loss是假loss</p> 
<p><a href="https://glassboxmedicine.com/2019/05/26/classification-sigmoid-vs-softmax/" rel="nofollow">Multi-label vs. Multi-class Classification: Sigmoid vs. Softmax</a></p> 
<p>具体看下一篇博客</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/92d7d13c0647bba332969f670ddc1835/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Kotlin教程 扩展函数</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/30db5898015dbb3957b1238d18804ed7/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Kotlin教程 内联函数</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>