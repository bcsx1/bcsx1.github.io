<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Weakly Supervised Action Localization by Sparse Temporal Pooling Network - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Weakly Supervised Action Localization by Sparse Temporal Pooling Network" />
<meta property="og:description" content="摘要
我们提出了一种使用卷积的未修剪视频的弱监督时间动作定位算法 神经网络。我们的算法预测给定视频级别标签的人类动作的时间间隔，而不需要动作的时间定位信息。 这个目标是通过提出一种新颖的深度神经网络来实现的，该网络通过视频片段的自适应时间汇集来识别动作并识别与动作相关联的稀疏关键片段集合。我们设计网络的损失函数包含两个术语 - 一个用于分类错误，另一个用于选定片段的稀疏性。在识别出针对关键段的稀疏关注权重后，我们使用时间类激活映射来提取时间提议，以估计本地化目标操作的时间间隔。所提出的算法在THUMOS14数据集上达到了最先进的精度，并且即使在监控能力较弱的情况下也能在ActivityNet1.3上表现出色。 1.介绍
视频中的动作识别是诸如事件检测，视频摘要，视频中的视觉问题回答等任务所必需的高级视频理解的关键问题。许多研究人员在过去的几十年中一直在广泛研究这个问题。行动认可的主要挑战是缺乏适当的视频表示方法。与卷积神经网络（CNNs）在与图像相关的许多视觉识别任务中几乎立竿见影的成功相反，由于数据的固有复杂结构，高计算量，缺乏建模知识 时间信息等等。这个问题意味着基于深度学习的表示方法[15,25,31,36]并不比依靠手工制作视觉特征的方法好得多[18,32,33]。因此，许多现有的算法试图通过结合手工和学习功能来实现最高水平的性能。 这个问题的另一个问题是缺乏视频理解所需的注释。大多数现有技术都假设视频级分类的修剪视频，或者依赖动作间隔的注释来进行时间定位。由于未修剪的视频通常包含与其类别标签有关的大量不相关的帧，因此视频表示学习和动作分类可能会因从原始视频中提取显着信息而面临挑战而失败。另一方面，注释大规模数据集以进行动作检测的代价非常昂贵且耗时，使得开发具有这种标签运行的竞争性算法更加实用。 我们的目标是暂时将未修剪的视频中的操作本地化。为此，我们提出了一种新的深层神经网络，它能够选择对动作识别有用的帧稀疏子集，其中损失函数测量每个视频中帧选择的分类错误和稀疏性。对于本地化，采用时间类激活映射（THE-CAM）来生成一维时间动作提议并计算目标动作在时间域中的定位。请注意，我们不会在训练过程中利用目标数据集中的任何时间信息，并仅根据视频级的动作类标签来学习模型。 我们的算法概述如图1所示。 图1： 我们的算法针对视频采用双流输入 - RGB和光流 - 并行执行动作分类和定位。 为了进行本地化，从两个流计算时间类激活映射（T-CAM）并用于生成一个从时域定位目标动作的时间动作提议。 这篇文章的主要贡献:
•我们引入了原理性深层神经网络架构，用于对未修剪的视频进行弱监督动作识别和定位，其中从网络识别的稀疏子帧中检测动作。
•我们提出一种技术来计算时间类激活映射，然后使用学习的注意力权重对时间动作建议进行本地化目标动作。
•所提出的弱监督动作定位技术在THUMOS14 [14]上实现了最新的准确性，并在ActivityNet1.3 [12]的首次公开评估中表现出色。
本文的其余部分安排如下。 我们在第2节中讨论相关工作，并在第3节中描述我们的动作局部化算法。第4节介绍了我们的实验的细节，第5节总结本文
2相关工工作 (略)
3 Proposed 算法
我们仅基于视频级动作标签描述了我们的弱监督时间动作定位算法。这个目标是通过设计一个基于稀疏子段的视频分类的深度神经网络和识别与目标类别相关的时间间隔来实现的。
3.1大致想法
我们声称可以通过识别一系列呈现重要动作组件的关键片段来从视频中识别动作。我们的算法提出了一种新颖的深度神经网络，使用一组具有代表性和独特的片段来预测每个视频的类别标签，以针对从输入视频自动选择的动作。请注意，所提出的深层神经网络是为分类而设计的，但能够测量每个分段在预测分类标签中的重要性。在每个视频中查找相关类别之后，我们通过计算各个片段的时间关注度，生成时间行为建议，以及汇总相关建议来估计与所标识的动作相对应的时间间隔。我们的方法仅依赖于视频级别标签来执行时间动作本地化，并提供了一种方法来提取关键段并确定与目标动作相对应的适当时间间隔。使用我们的框架，可以在单个视频中识别和本地化多个操作。图2说明了我们的弱监督动作识别组件的深层神经网络架构。我们描述了我们的算法的每个步骤如下。 图2：我们的弱监督时间动作定位的神经网络架构。 我们首先使用预训练网络从一组均匀采样的视频片段中提取特征表示。 注意模块生成对应于各个特征的注意力权重，这些特征被用于通过时间加权平均池合计算视频级表示。 该表示被赋予分类模块，并且在这个注意权重向量上施加l1损失来强制执行稀疏约束。
3.2. Action Classification
为了预测每个视频中的类别标签，我们首先从输入视频中采样一组视频片段，并使用预训练的卷积神经网络从每个片段提取特征表示。 然后将这些表示中的每一个呈现给由两个全连接（FC）层和位于两个FC层之间的ReLU层组成的注意模块。 第二个FC层的输出被赋予一个S形函数，迫使生成的注意力权重在0和1之间归一化。然后使用这些注意力权重调整时间平均池 - 特征向量的加权总和 - 创建视频级别表示。 我们通过FC和S形图层传递这个表示来获得分类分数。 形式上，Xt∈Rm是从时间t中心的视频片段提取的m维特征表示，λt是相应的关注权值。 视频等级表示（由表示）对应于注意加权时间平均池，这是由公式：
其中λ = (λ1, . . . , λT )T是来自sigmoid函数的标量输出的矢量以标准化激活范围，并且T是为分类而配置的视频段的总数。 注意力权重向量λ以类不可知的方式用稀疏性约束来学习。这有助于识别与任何感兴趣的动作相关的时间片段并估计动作候选者的时间间隔。
该网络中的损失函数由分类损失和稀疏损失两个项组成。
其中Lclass表示在视频级别上计算的分类损失，稀疏性是稀疏损失，β是控制这两个项之间的权衡的常数。分类损失是基于ground-truth和之间标准的多标签交叉熵损失（经过如图2所示的几个层次之后），而稀疏损失则由l1注意力损失权重为||λ|| 1。由于我们对每个注意权重λt都应用了一个Sigmoid函数，所有的注意权值都可能有接近0-1的二进制值，这是由于“l1损失”造成的。请注意，集成稀疏损失与我们声称可以通过视频中关键段的稀疏子集识别动作是一致的。
3.3. Temporal Class Activation Mapping 为了确定与目标事件相对应的时间间隔，我们首先提取一些行动间隔候选。 基于[46]中的想法，我们推导出时间域中的一维类激活映射，称为时间类激活映射（T-CAM）。 通过Wc(k)表示分类模型参数w中的第k个元素，对应于类c。类c的最后sigmoid层的输入是" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/10f311914919a636e36b1d8ea8cc2807/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-03-20T15:36:59+08:00" />
<meta property="article:modified_time" content="2018-03-20T15:36:59+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Weakly Supervised Action Localization by Sparse Temporal Pooling Network</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><span style="color:rgb(34,34,34);font-family:arial, sans-serif;text-align:left;">摘要</span></p> 
<div> 
 <span style="color:rgb(34,34,34);font-family:arial, sans-serif;font-weight:400;min-height:0px;">    我们提出了一种使用卷积的未修剪视频的弱监督时间动作定位算法</span> 
 <span style="background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;min-height:0px;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;">神经网络。我们的算法<strong>预测</strong><u>给定视频级别标签的人类动作的时间间隔，而不需要动作的时间定位信息</u>。 这个目标是通过提出一种新颖的深度神经网络来实现的，该网络<u>通过视频片段的自适应时间汇集来识别动作并识别与动作相关联的稀疏关键片段集合</u>。我们设计网络的<strong>损失函数包含两个术语</strong> - <u>一个用于分类错误，另一个用于选定片段的稀疏性</u>。在<strong>识别出针对关键段的稀疏关注权重后，我们使用时间类激活映射来提取时间提议，以估计本地化目标操作的时间间隔</strong>。所提出的算法在THUMOS14数据集上达到了最先进的精度，并且即使在监控能力较弱的情况下也能在ActivityNet1.3上表现出色。</span> 
</div> 
<span style="text-align:left;color:rgb(34,34,34);text-transform:none;text-indent:0px;letter-spacing:normal;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;text-decoration:none;word-spacing:0px;white-space:normal;min-height:0px;background-color:transparent;"></span> 
<p style="margin-top:0px;margin-bottom:0px;margin-left:0px;margin-right:0px;text-indent:0px;"><span style="font-weight:bold;">1.介绍</span></p> 
<div> 
 <span lang="zh-cn" style="color:rgb(34,34,34);font-family:arial, sans-serif;font-weight:400;min-height:89px;"><span style="color:rgb(34,34,34);font-size:16px;min-height:0px;">      视频中的动作识别是诸如事件检测，视频摘要，视频中的视觉问题回答等任务所必需的高级视频理解的关键问题。<span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;">许多研究人员在过去的几十年中一直在广泛研究这个问题。<span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;">行动认可的主要挑战是缺乏适当的视频表示方法。<span style="background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;min-height:0px;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;">与卷积神经网络（CNNs）在与图像相关的许多视觉识别任务中几乎立竿见影的成功相反，由于数据的固有复杂结构，高计算量，缺乏建模知识</span><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"> </span><span style="background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;min-height:0px;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;">时间信息等等。<span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;">这个问题意味着基于深度学习的表示方法[15,25,31,36]并不比依靠手工制作视觉特征的方法好得多[18,32,33]。<span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;">因此，许多现有的算法试图通过结合手工和学习功能来实现最高水平的性能。</span></span></span></span></span></span></span> 
</div> 
<div> 
 <span lang="zh-cn" style="background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;min-height:89px;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="color:rgb(34,34,34);font-size:16px;min-height:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;min-height:0px;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"></span></span></span></span></span></span></span> 
 <span lang="zh-cn" style="background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;min-height:89px;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="color:rgb(34,34,34);font-size:16px;min-height:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;min-height:0px;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span lang="zh-cn" style="background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;min-height:89px;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="color:rgb(34,34,34);font-size:16px;min-height:0px;"><span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;min-height:0px;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;">     这个问题的另一个问题是缺乏视频理解所需的注释。<span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;">大多数现有技术都假设视频级分类的修剪视频，或者依赖动作间隔的注释来进行时间定位。<span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;">由于未修剪的视频通常包含与其类别标签有关的大量不相关的帧，因此视频表示学习和动作分类可能会因从原始视频中提取显着信息而面临挑战而失败。<span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;">另一方面，注释大规模数据集以进行动作检测的代价非常昂贵且耗时，使得开发具有这种标签运行的竞争性算法更加实用。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> 
</div> 
<div> 
 <span lang="zh-cn" style="background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;min-height:89px;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="color:rgb(34,34,34);font-size:16px;min-height:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;min-height:0px;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span lang="zh-cn" style="background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;min-height:89px;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="color:rgb(34,34,34);font-size:16px;min-height:0px;"><span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;min-height:0px;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;">        <span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;">我们的目标是暂时将未修剪的视频中的操作本地化。<span style="color:rgb(34,34,34);font-family:arial, sans-serif;text-align:left;">为此，我们提出了一种新的深层神经网络，它能够选择对动作识别有用的帧稀疏子集，其中损失函数测量每个视频中帧选择的分类错误和稀疏性。<span style="color:rgb(34,34,34);font-family:arial, sans-serif;text-align:left;">对于本地化，采用时间类激活映射（THE-CAM）来生成一维时间动作提议并计算目标动作在时间域中的定位。<span style="color:rgb(34,34,34);font-family:arial, sans-serif;min-height:0px;text-align:left;">请注意，我们不会在训练过程中利用目标数据集中的任何时间信息，并仅根据视频级的动作类标签来学习模型。</span><span style="color:rgb(34,34,34);font-family:arial, sans-serif;text-align:left;"> </span><span style="color:rgb(34,34,34);font-family:arial, sans-serif;min-height:0px;text-align:left;">我们的算法概述如图1所示。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> 
</div> 
<div> 
 <span lang="zh-cn" style="background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;min-height:89px;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="color:rgb(34,34,34);font-size:16px;min-height:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;min-height:0px;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span lang="zh-cn" style="background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;min-height:89px;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="color:rgb(34,34,34);font-size:16px;min-height:0px;"><span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;min-height:0px;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="color:rgb(34,34,34);font-family:arial, sans-serif;text-align:left;"><span style="color:rgb(34,34,34);font-family:arial, sans-serif;text-align:left;"><span style="color:rgb(34,34,34);font-family:arial, sans-serif;min-height:0px;text-align:left;"><img src="https://images2.imgbox.com/c9/9c/0biAJKOd_o.png" alt=""><br></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> 
</div> 
<div> 
 <span lang="zh-cn" style="background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;min-height:89px;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="color:rgb(34,34,34);font-size:16px;min-height:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;min-height:0px;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span lang="zh-cn" style="background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;min-height:89px;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="color:rgb(34,34,34);font-size:16px;min-height:0px;"><span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;min-height:0px;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="background-color:transparent;color:rgb(34,34,34);float:none;font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="float:none;background-color:transparent;color:rgb(34,34,34);font-family:arial, sans-serif;font-size:16px;font-style:normal;font-variant:normal;font-weight:400;letter-spacing:normal;text-align:left;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;"><span style="color:rgb(34,34,34);font-family:arial, sans-serif;text-align:left;"><span style="color:rgb(34,34,34);font-family:arial, sans-serif;text-align:left;"><span style="color:rgb(34,34,34);font-family:arial, sans-serif;min-height:0px;text-align:left;">图1：<span style="color:rgb(34,34,34);font-family:arial, sans-serif;text-align:left;"> </span><span style="color:rgb(34,34,34);font-family:arial, sans-serif;min-height:0px;text-align:left;">我们的算法针对视频采用双流输入 - RGB和光流 - 并行执行动作分类和定位。</span><span style="color:rgb(34,34,34);font-family:arial, sans-serif;text-align:left;"> </span><span style="color:rgb(34,34,34);font-family:arial, sans-serif;min-height:0px;text-align:left;">为了进行本地化，从两个流计算时间类激活映射（T-CAM）并用于生成一个从时域定位目标动作的时间动作提议。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> 
</div> 
<div style="text-align:left;"> 
 <br> 
</div> 
<p><strong>这篇文章的主要贡献:</strong></p> 
<p><span style="font-family:'Microsoft YaHei';"><span style="color:rgb(34,34,34);text-align:left;">•我们引入了原理性深层神经网络架构，用于对未修剪的视频进行弱监督动作识别和定位，其中从网络识别的稀疏子帧中检测动作。</span><br></span></p> 
<p><span style="color:rgb(34,34,34);text-align:left;"><span style="font-family:'Microsoft YaHei';"><span style="color:rgb(34,34,34);text-align:left;">•我们提出一种技术来计算时间类激活映射，然后使用学习的注意力权重对时间动作建议进行本地化目标动作。</span><br></span></span></p> 
<p><span style="color:rgb(34,34,34);text-align:left;"><span style="font-family:'Microsoft YaHei';"><span style="color:rgb(34,34,34);text-align:left;">•</span>所提出的弱监督动作定位技术在THUMOS14 [14]上实现了最新的准确性，并在ActivityNet1.3 [12]的首次公开评估中表现出色。</span></span></p> 
<p><span style="color:rgb(34,34,34);text-align:left;"><span style="font-family:'Microsoft YaHei';"><span style="color:rgb(34,34,34);font-family:arial, sans-serif;min-height:0px;text-align:left;">本文的其余部分安排如下。</span><span style="color:rgb(34,34,34);font-family:arial, sans-serif;text-align:left;"> </span><span style="color:rgb(34,34,34);font-family:arial, sans-serif;min-height:0px;text-align:left;">我们在第2节中讨论相关工作，并在第3节中描述我们的动作局部化算法。第4节介绍了我们的实验的细节，第5节总结本文</span><br></span></span></p> 
<p><span style="color:rgb(34,34,34);text-align:left;"><span style="font-family:'Microsoft YaHei';"><strong>2相关工工作   (略)</strong></span></span></p> 
<p><span style="color:rgb(34,34,34);text-align:left;"><span style="font-family:'Microsoft YaHei';"><strong>3 Proposed 算法</strong></span></span></p> 
<p><span style="color:rgb(34,34,34);text-align:left;"><span style="font-family:'Microsoft YaHei';"><span style="color:rgb(34,34,34);font-family:arial, sans-serif;text-align:left;">        我们仅基于视频级动作标签描述了我们的弱监督时间动作定位算法。<span style="color:rgb(34,34,34);font-family:arial, sans-serif;text-align:left;">这个目标是通过设计一个基于稀疏子段的视频分类的深度神经网络和识别与目标类别相关的时间间隔来实现的。</span></span><br></span></span></p> 
<p><span style="color:rgb(34,34,34);text-align:left;"><span style="font-family:'Microsoft YaHei';"><strong>3.1大致想法</strong></span></span></p> 
<p><span style="color:rgb(34,34,34);text-align:left;"><span style="font-family:'Microsoft YaHei';"><span title="We claim that an action can be recognized from a video by identifying a series of key segments presenting important action components." style="color:rgb(34,34,34);font-family:arial, sans-serif;min-height:0px;text-align:left;"><strong>       </strong>我们声称可以通过识别一系列呈现重要动作组件的关键片段来从视频中识别动作。</span><span title="Our algorithm proposes a novel deep neural network to predict class labels per video using a sub?set of representative and unique segments to target actions, which are selected automatically from an input video." style="color:rgb(34,34,34);font-family:arial, sans-serif;min-height:0px;text-align:left;">我们的算法提出了一种新颖的深度神经网络，使用一组具有代表性和独特的片段来预测每个视频的类别标签，以针对从输入视频自动选择的动作。</span><span title="Note that the proposed deep neural network is designed for classification but has the capability to measure the importance of each segment in predicting classification labels." style="color:rgb(34,34,34);font-family:arial, sans-serif;min-height:0px;text-align:left;">请注意，所提出的深层神经网络是为分类而设计的，但能够测量每个分段在预测分类标签中的重要性。</span><span title="After find?ing the relevant classes in each video, we estimate temporal intervals corresponding to the identified actions by comput?ing temporal attention of individual segments, generating temporal action proposals, and aggregating relevant propos?als." style="color:rgb(34,34,34);font-family:arial, sans-serif;min-height:0px;text-align:left;">在每个视频中查找相关类别之后，我们通过计算各个片段的时间关注度，生成时间行为建议，以及汇总相关建议来估计与所标识的动作相对应的时间间隔。</span><span title="Our approach relies only on video-level class labels to perform temporal action localization and presents a princi?pled way to extract key segments and determine appropriate time intervals corresponding to target actions." style="color:rgb(34,34,34);font-family:arial, sans-serif;min-height:0px;text-align:left;">我们的方法仅依赖于视频级别标签来执行时间动作本地化，并提供了一种方法来提取关键段并确定与目标动作相对应的适当时间间隔。</span><span title="It is possible to recognize and localize multiple actions in a single video using our framework." style="color:rgb(34,34,34);font-family:arial, sans-serif;min-height:0px;text-align:left;">使用我们的框架，可以在单个视频中识别和本地化多个操作。</span><span title="The deep neural network architecture for our weakly supervised action recognition component is illustrated in Figure 2. We describe each step of our algo?rithm as follows." style="color:rgb(34,34,34);font-family:arial, sans-serif;min-height:0px;text-align:left;">图2说明了我们的弱监督动作识别组件的深层神经网络架构。我们描述了我们的算法的每个步骤如下。</span><span title="" style="color:rgb(34,34,34);font-family:arial, sans-serif;min-height:0px;text-align:left;"></span> </span></span></p> 
<p><span style="color:rgb(34,34,34);text-align:left;"><span style="font-family:'Microsoft YaHei';"><strong><img src="https://images2.imgbox.com/d1/73/RUGbEqRM_o.png" alt=""><br></strong></span></span></p> 
<p><span style="color:rgb(34,34,34);text-align:left;"><strong><span style="font-family:arial, sans-serif;color:rgb(34,34,34);min-height:0px;text-align:left;">图2：我们的弱监督时间动作定位的神经网络架构。</span><span style="font-family:arial, sans-serif;color:rgb(34,34,34);text-align:left;"> </span><span style="font-family:arial, sans-serif;color:rgb(34,34,34);min-height:0px;text-align:left;">我们首先使用预训练网络从一组均匀采样的视频片段中提取特征表示。</span><span style="font-family:arial, sans-serif;color:rgb(34,34,34);text-align:left;"> </span><span style="font-family:arial, sans-serif;color:rgb(34,34,34);min-height:0px;text-align:left;">注意模块生成对应于各个特征的注意力权重，这些特征被用于通过时间加权平均池合计算视频级表示。</span><span style="font-family:arial, sans-serif;color:rgb(34,34,34);text-align:left;"> </span><span style="font-family:arial, sans-serif;color:rgb(34,34,34);min-height:0px;text-align:left;">该表示被赋予分类模块，并且在这个注意权重向量上施加</span><span style="color:rgb(34,34,34);min-height:0px;text-align:left;"><span style="font-family:'Comic Sans MS';">l1</span></span><span style="font-family:arial, sans-serif;color:rgb(34,34,34);min-height:0px;text-align:left;">损失来强制执行稀疏约束。</span><br></strong></span></p> 
<p><span style="color:rgb(34,34,34);text-align:left;"><span style="font-family:'Microsoft YaHei';"><strong>3.2. Action Classification<br></strong></span></span></p> 
<p><span style="font-family:SimHei;"><span style="text-align:left;"><span style="text-align:left;background-color:rgb(255,255,255);"><span style="color:rgb(0,0,0);">    为了预测每个视频中的类别标签，我们首先从输入视频中采样一组视频片段，并使用预训练的卷积神经网络从每个片段提取特征表示。 然后将这些表示中的每一个呈现给由两个全连接（FC）层和位于两个FC层之间的ReLU层组成的注意模块。 第二个FC层的输出被赋予一个S形函数，迫使生成的注意力权重在0和1之间归一化。然后使用这些注意力权重调整时间平均池 - 特征向量的加权总和 - 创建视频级别表示。 我们通过FC和S形图层传递这个表示来获得分类分数。</span></span></span><span style="color:rgb(34,34,34);text-align:left;">  </span></span></p> 
<p></p> 
<p style="color:rgb(34,34,34);"><span style="font-family:SimHei;">    形式上，X<sub>t</sub>∈R<sup>m</sup>是从时间t中心的视频片段提取的m维特征表示，λ<sub>t</sub>是相应的关注权值。 视频等级表示（由<img src="https://images2.imgbox.com/82/b7/DZqJQLJ5_o.png" alt="">表示）对应于注意加权时间平均池，这是由公式：</span></p> 
<p style="color:rgb(34,34,34);"><span style="font-family:SimHei;"><img src="https://images2.imgbox.com/10/bc/RnJvjTVf_o.png" alt=""></span></p> 
<p style="color:rgb(34,34,34);"><span style="font-family:SimHei;font-size:16px;">其中λ = (λ<sub>1</sub>, . . . , λ<sub>T</sub> )</span><sup><span style="font-family:SimHei;font-size:16px;">T是来自sigmoid函数的标量输出的矢量以标准化激活范围，并且T是为分类而配置的视频段的总数。 注意力权重向量λ以类不可知的方式用稀疏性约束来学习。这有助于识别与任何感兴趣的动作相关的时间片段并估计动作候选者的时间间隔。</span></sup></p> 
<p style="color:rgb(34,34,34);"><span style="font-family:SimHei;">    <span style="font-size:16px;">该网络中的损失函数由分类损失和稀疏损失两个项组成。</span><img src="https://images2.imgbox.com/17/01/rTc8gUHu_o.png" alt=""><br></span></p> 
<p><span style="font-family:SimHei;"></span></p> 
<p><span style="font-size:16px;"><span style="color:#000000;">其中L<sub>class</sub>表示在视频级别上计算的分类损失，稀疏性是稀疏损失，β是控制这两个项之间的权衡的常数。分类损失是基于ground-truth和<img src="https://images2.imgbox.com/cf/64/4XRjL2B8_o.png" alt="">之间标准的多标签交叉熵损失（经过如图2所示的几个层次之后），而稀疏损失则由l<sub>1</sub>注意力损失权重为||λ||<sub> 1</sub>。由于我们对每个注意权重λ<sub>t</sub>都应用了一个Sigmoid函数，所有的注意权值都可能有接近0-1的二进制值，这是由于“l<sub>1</sub>损失”造成的。请注意，集成稀疏损失与我们声称可以通过视频中关键段的稀疏子集识别动作是一致的。</span></span></p> 
<strong>3.3. Temporal Class Activation Mapping</strong> 
<br> 
<p style="color:rgb(34,34,34);"><span style="font-family:SimHei;">    为了确定与目标事件相对应的时间间隔，我们首先提取一些行动间隔候选。 基于[46]中的想法，我们推导出时间域中的一维类激活映射，称为时间类激活映射（T-CAM）。 通过</span><span style="color:rgb(79,79,79);font-family:SimHei;">W</span><sup style="color:rgb(79,79,79);font-family:SimHei;">c</sup><span style="color:rgb(79,79,79);font-family:SimHei;">(k)</span><span style="font-family:SimHei;">表示分类模型参数w中的第k个元素，对应于类c。类c的最后sigmoid层的输入是</span></p> 
<p><span style="color:rgb(34,34,34);"><span style="font-family:'Microsoft YaHei';"><strong><img src="https://images2.imgbox.com/51/8e/5Um6DU0s_o.png" alt=""><br></strong></span></span></p> 
<p><span style="color:rgb(34,34,34);text-align:left;"><span style="font-family:'Microsoft YaHei';">T-CAM,记为<img src="https://images2.imgbox.com/69/9d/V7UIJzkE_o.png" alt="">，表示在时间步骤t该表示与各个类别的相关性，其中给出用于类别c（c = 1，...，C）的每个元素<img src="https://images2.imgbox.com/c5/fe/6PFpkgvj_o.png" alt=""></span></span></p> 
<p><span style="color:rgb(34,34,34);text-align:left;"><span style="font-family:'Microsoft YaHei';"><strong><img src="https://images2.imgbox.com/0e/03/WLznXpI0_o.png" alt=""><br></strong></span></span></p> 
<p><span style="color:rgb(34,34,34);text-align:left;"><span style="font-family:'Microsoft YaHei';">图3给出了由所提出的算法给出的视频中的attention weights和T-CAM输出的示例。 我们可以观察到，attention weights和T-CAM有效地突出了区分性时间区域。 请注意，一些具有较大定位权重的时间间隔不对应于较大的T-CAM值，因为这些间隔可能代表了其他干扰行为。 attention weights测量时间视频片段的泛化动作，而T-CAM呈现类别特定信息。<br></span></span></p> 
<p><span style="color:rgb(34,34,34);text-align:left;"><span style="font-family:'Microsoft YaHei';"><strong><img src="https://images2.imgbox.com/a6/0d/dkWpChep_o.png" alt=""><br></strong></span></span></p> 
<p><span style="color:rgb(34,34,34);text-align:left;"><span style="font-family:'Microsoft YaHei';"><span style="font-size:14px;">图3：THUMOS14 [14]数据集中ThrowDiscus类的真实状况时间间隔，时间关注和T-CAM示例视频的示意图。 图中的横轴表示时间索引。 在这个例子中，ThrowDiscus的T-CAM值提供了准确的动作定位信息。 请注意， temporal attention weights在几个不符合真实状况注释的地点很大。 这是因为时间关注权重是以类别不可知的方式训练的。</span><br></span></span></p> 
<p><span style="color:rgb(34,34,34);text-align:left;"><span style="font-family:'Microsoft YaHei';"><span style="font-size:16px;"><strong>3.5. Temporal Action Localization</strong></span><br></span></span></p> 
<p><span style="color:rgb(34,34,34);text-align:left;"><span style="font-family:'Microsoft YaHei';">       对于输入视频，我们首先根据3.2节描述的深度神经网络的视频水平分类评分来确定相关的类标签。 对于每个相关动作，我们生成时间提议，即一个可能由多个片段组成的时间间隔及其class labels 和confidence scores。提议对应于潜在包含目标动作的视频片段，并使用T -CAM在我们的算法中。<br></span></span></p> 
<p><span style="color:rgb(34,34,34);text-align:left;"><span style="font-family:'Microsoft YaHei';">      为了产生 temporal proposals，我们首先使用公式(4)作为<img src="https://images2.imgbox.com/2c/54/i8DUSKUZ_o.png" alt="">，从RGB和 flow streams计算TCAMs，并利用它们导出加权<img src="https://images2.imgbox.com/53/20/io6wHSh0_o.png" alt=""> as:<br></span></span></p> 
<p><span style="color:rgb(34,34,34);text-align:left;"><span style="font-family:'Microsoft YaHei';"><img src="https://images2.imgbox.com/f3/62/RII2mvPV_o.png" alt=""><br></span></span></p> 
<p><span style="color:rgb(34,34,34);text-align:left;"><span style="font-family:'Microsoft YaHei';">     注意，λt是稀疏向量λ的元素，乘以λt可以解释为从以下S形函数中对值进行的软选择。与[46]类似，我们将阈值应用于加权<img src="https://images2.imgbox.com/30/b5/vqAz8NOI_o.png" style="margin-bottom:0px;color:rgb(34,34,34);font-family:'Microsoft YaHei';text-align:left;" alt="">以分割这些信号。然后，temporal proposals是独立地从每个流中提取的一维连通分量。使用加权T-CAM生成action proposals，而不是直接从attention weights中产生，因为每个建议都应包含一种行为。 任意的，我们在阈值化之前对采样段之间的加权T-CAM信号进行线性内插，以改进提案的临时解决方案。<br></span></span></p> 
<p>       与原始的只保留最大的边框的CAM-based bounding box proposals [46]不同，我们保留所有通过预定义阈值的连通组件。由[t<sub>start</sub><span style="font-family:'PingFang SC';">，</span>t<sub>end</sub>]定义的每个 proposal 被分配一个分数作为该提案中所有框架的加权平均T-CAM：</p> 
<p><img src="https://images2.imgbox.com/97/29/H0pzARP6_o.png" alt=""><br></p> 
<p><br></p> 
<p>其中*∈{RGB，FLOW}。 该值对应于类c的每个流中的temporal proposal分数。 最后，我们独立地执行每个类的时间提议中的非最大抑制，以去除高度重叠的检测。<br></p> 
<p><span style="font-size:16px;"><strong>3.6. Discussion</strong></span><br></p> 
<p><span style="font-size:16px;">      我们的算法试图通过分别估计稀疏注attention weights和T-CAM来针对通用和特定行为临时地定位未修剪视频中的动作。 我们认为，与现有的UntrimmedNet [35]算法相比，该方法具有原理性和新颖性，因为它具有独特的深度神经网络结构，具有分类和稀疏损失，其动作定位过程基于完全不同 利用T-CAM利用针对具体课题的行动建议。 请注意[35]遵循[2]中使用的类似框架，其中softmax函数用于两个操作类和提议; 它在处理单个视频中的多个操作类和实例时具有关键限制。<br></span></p> 
<p><strong>4 实验和测试不做翻译</strong></p> 
<p><strong>5.结论</strong><br>我们提出了一种新的弱监督时间交流定位技术，该技术基于具有分类和稀疏性损失的深度神经网络。 分类是通过评估由分段级特征的稀疏加权平均值给出的视频级表示来执行的，其中稀疏系数通过我们的深度神经网络中的稀疏性损失来学习。 对于弱超时的时间动作定位，提取一维行动建议，从中选择与目标类相关的建议以呈现事件的时间间隔。 所提出的方法实现了THUMOS14数据集中最新的结果，并且据我们所知，我们首先报告了ActivityNet1.3数据集上的弱监督时间动作定位结果。<br></p> 
<p><br></p> 
<p><img src="https://images2.imgbox.com/e3/39/8o1eof4I_o.png" alt=""><br></p> 
<p><img src="https://images2.imgbox.com/83/37/gb051CBQ_o.png" alt=""><br></p> 
<p><img src="https://images2.imgbox.com/59/69/kwVn7ZDD_o.png" alt=""><br></p> 
<p><img src="https://images2.imgbox.com/10/4f/bDOBCu2Q_o.png" alt=""><br></p> 
<p><img src="https://images2.imgbox.com/29/6d/BPaldhYO_o.png" alt=""><br></p> 
<p><br></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/0fea2b7f3511542e502bbf017ee3b97c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">正则表达式详解</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/112579e55e553e53199dc342526b1f97/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Ftp上传常见错误和解决方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>