<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>faceswap介绍 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="faceswap介绍" />
<meta property="og:description" content="官网介绍地址：https://forum.faceswap.dev/viewtopic.php?t=146
介绍
很多人在开始交换脸时会不知所措，而且犯了很多错误。错误是好的。这是我们学习的方法，但有时在潜入之前对所涉及的过程有一点了解会有所帮助。
在这篇文章中，我将详细阐述我们如何训练模型。有几个型号，有许多选项。我不会覆盖一切， 但希望这会给你足够的， 作出明智的决定， 你自己。如果您尚未生成用于训练的面孔集，请立即停止，然后前往提取指南现在生成它们。
本指南中有很多背景
信息。我建议你熟悉一切。机器学习是一个复杂的概念，但我试图把它分解为尽可能简单易懂。对神经网络的工作原理有一个基本的了解，以及它从查看中获得的数据类型，将大大提高您成功交换的机会。
我将为本指南使用 GUI，但 cli 的前提完全相同（GUI 中存在的所有选项在 cli 中
可用）。
什么是培训？ 概述
在高层次上，培训正在教我们的神经网络 （NN） 如何重新创建面部。大多数型号主要由两部分组成： 编码器- 这具有将人脸负载作为输入，并&#34;编码&#34;它们为&#34;矢量&#34;形式的表示。需要注意的是，它不是学习您输入到它的每一面的精确表示，而是尝试创建一种算法，该算法可用于以后尽可能紧密地重建面与输入图像。解码器- 这具有将编码器创建的矢量并尝试将其表示回面的工作，尽可能与输入图像匹配。 为什么 1. png （7.17 Kib） 查看 92560 次
有些模型的构造略有不同，但基本前提保持不变。
NN 需要知道它如何做编码和解码
面。它使用 2 个主要工具来做到这一点： 损耗- 对于输入模型的每批面，NN 将查看它试图通过当前编码和解码算法重新创建的面，并将其与输入的实际面进行比较。根据它认为它做得如何，它将给自己一个分数（损失值），并将相应地更新其权重。权重- 模型评估其重新创建面的绩效后，将更新其权重。这些馈入编码器/解码器算法。如果它在一个方向上调整了重量，但觉得它在重建面部方面做得比以前差，那么它知道权重正向错误方向移动，因此它会以相反的方式调整它们。如果它感觉它有所改善，那么它知道不断调整的重量的方向，它去。 why2.png (22.21 KiB) Viewed 92560 times
然后，模型会多次重复此操作，多次根据其损失值不断更新其权重，理论上会随着时间的推移而提高，直到它到达一个点，你觉得它已经学会足够的有效重新创建面，或者损失值停止下降。
现在，我们拥有了神经网络做什么以及如何学会创建人脸的基础知识，这如何适用于人脸
交换？您可能已经注意到，在上述细分中，此 NN 学习如何拍摄一个人的面部负载，然后重建这些面孔。这不是我们想要的...我们想拿一大堆脸， 重建别人的脸。为此，我们的 NN 做几件事： 共享编码器- 当我们训练我们的模型时，我们喂它两组脸。A 集（我们要替换的原始面）和 B 集（我们希望在场景中放置的交换面）。实现此目的的第一步是共享 A 和 B 集的编码器。这样，我们的编码器正在为 2 个不同的人学习单个算法。这一点非常重要，因为我们最终将告诉我们的神经网络，采取一张脸的编码，并解码到另一张脸。因此，编码器需要查看并了解交换所需的两组面。切换解码器- 训练模型时，我们训练 2 个解码器。解码器 A 正在拍摄编码向量并尝试重新创建面 A。 解码器 B 正在拍摄编码向量并尝试重新创建面 B。当涉及到最终交换面时，我们切换解码器，因此我们馈送模型面 A，但通过解码器 B 传递它。由于编码器已对两组面进行训练，因此模型将编码输入的 A 面，但随后尝试从解码器 B 重建它，从而导致交换的面从我们的模型输出。 why3." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/43ba32d321bfe768e98f443a85ca4bc4/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-08-10T11:59:09+08:00" />
<meta property="article:modified_time" content="2020-08-10T11:59:09+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">faceswap介绍</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div> 
 <p>官网介绍地址：<a href="https://forum.faceswap.dev/viewtopic.php?t=146" rel="nofollow">https://forum.faceswap.dev/viewtopic.php?t=146</a></p> 
 <p><span style="color:#000000;"><strong><a name="intro">介绍</a></strong></span></p> 
 <hr> 
 <span style="color:#000000;">很多人在开始交换脸时会不知所措，而且犯了很多错误。错误是好的。这是我们学习的方法，但有时在潜入之前对所涉及的过程有一点了解会有所帮助。<br><br> 在这篇文章中，我将详细阐述我们如何训练模型。有几个型号，有许多选项。我不会覆盖一切， 但希望这会给你足够的， 作出明智的决定， 你自己。如果您尚未生成用于训练的面孔集，请立即停止，然后前往<a class="postlink" href="https://faceswap.dev/forum/viewtopic.php?f=5&amp;t=27" rel="nofollow">提取指南</a>现在生成它们。<br> 本指南中有很多背景<br> 信息。我建议你熟悉一切。机器学习是一个复杂的概念，但我试图把它分解为尽可能简单易懂。对神经网络的工作原理有一个基本的了解，以及它从查看中获得的数据类型，将大大提高您成功交换的机会。<br> 我将为本指南使用 GUI，但 cli 的前提完全相同（GUI 中存在的所有选项在 cli 中<br> 可用）。<br><br><strong><a name="what">什么是培训？</a></strong></span> 
 <hr> 
 <span style="color:#000000;"><strong><a name="overview">概述</a></strong><br> 在高层次上，培训正在教我们的神经网络 （NN） 如何重新创建面部。大多数型号主要由两部分组成：</span> 
 <ol><li><span style="color:#000000;"><strong>编码器</strong>- 这具有将人脸负载作为输入，并"编码"它们为"矢量"形式的表示。需要注意的是，它不是学习您输入到它的每一面的精确表示，而是尝试创建一种算法，该算法可用于以后尽可能紧密地重建面与输入图像。</span></li><li><span style="color:#000000;"><strong>解码器</strong>- 这具有将编码器创建的矢量并尝试将其表示回面的工作，尽可能与输入图像匹配。</span></li></ol> 
 <div> 
  <p> </p> 
  <p><span style="color:#000000;">为什么 1. png （7.17 Kib） 查看 92560 次</span></p> 
 </div> 
 <span style="color:#000000;">有些模型的构造略有不同，但基本前提保持不变。<br> NN 需要知道它如何做编码和解码<br> 面。它使用 2 个主要工具来做到这一点：</span> 
 <ol><li><span style="color:#000000;"><strong>损耗</strong>- 对于输入模型的每批面，NN 将查看它试图通过当前编码和解码算法重新创建的面，并将其与输入的实际面进行比较。根据它认为它做得如何，它将给自己一个分数（损失值），并将相应地更新其权重。</span></li><li><span style="color:#000000;"><strong>权重</strong>- 模型评估其重新创建面的绩效后，将更新其权重。这些馈入编码器/解码器算法。如果它在一个方向上调整了重量，但觉得它在重建面部方面做得比以前差，那么它知道权重正向错误方向移动，因此它会以相反的方式调整它们。如果它感觉它有所改善，那么它知道不断调整的重量的方向，它去。</span></li></ol> 
 <div> 
  <p> </p> 
  <p><span style="color:#000000;">why2.png (22.21 KiB) Viewed 92560 times</span></p> 
 </div> 
 <span style="color:#000000;">然后，模型会多次重复此操作，多次根据其损失值不断更新其权重，理论上会随着时间的推移而提高，直到它到达一个点，你觉得它已经学会足够的有效重新创建面，或者损失值停止下降。<br> 现在，我们拥有了神经网络做什么以及如何学会创建人脸的基础知识，这如何适用于人脸<br> 交换？您可能已经注意到，在上述细分中，此 NN 学习如何拍摄一个人的面部负载，然后重建这些面孔。这不是我们想要的...我们想拿一大堆脸， 重建别人的脸。为此，我们的 NN 做几件事：</span> 
 <ul><li><span style="color:#000000;"><strong>共享编码器</strong>- 当我们训练我们的模型时，我们喂它两组脸。A 集（我们要替换的原始面）和 B 集（我们希望在场景中放置的交换面）。实现此目的的第一步是共享 A 和 B 集的编码器。这样，我们的编码器正在为 2 个不同的人学习单个算法。这一点非常重要，因为我们最终将告诉我们的神经网络，采取一张脸的编码，并解码到另一张脸。因此，编码器需要查看并了解交换所需的两组面。</span></li><li><span style="color:#000000;"><strong>切换解码器</strong>- 训练模型时，我们训练 2 个解码器。解码器 A 正在拍摄编码向量并尝试重新创建面 A。 解码器 B 正在拍摄编码向量并尝试重新创建面 B。当涉及到最终交换面时，我们切换解码器，因此我们馈送模型面 A，但通过解码器 B 传递它。由于编码器已对两组面进行训练，因此模型将编码输入的 A 面，但随后尝试从解码器 B 重建它，从而导致交换的面从我们的模型输出。</span></li></ul> 
 <div> 
  <p> </p> 
  <p><span style="color:#000000;">why3.png (38.63 KiB) Viewed 92558 times</span></p> 
 </div> 
 <span style="color:#000000;"><strong><a name="term">术语</a></strong><br> 使用 Faceswap 时，会看到一些常见的机器学习术语。为了简单得多，此处显示了术语表：</span> 
 <ul><li><span style="color:#000000;"><strong>批</strong>处理 - 批处理是一组同时通过神经网络输入的面。</span></li><li><span style="color:#000000;"><strong>批处理</strong>大小 - 批处理大小是同时通过神经网络输入的批处理的大小。批处理大小为 64 表示一次通过神经网络输入 64 面，然后为此批次图像计算损耗和权重更新。较高的批号将训练得更快，但会导致更高的泛化。较低的批号将训练速度较慢，但会更好地区分面之间的差异。在培训的各个阶段调整批次大小会有所帮助。</span></li><li><span style="color:#000000;"><strong>纪</strong>元 - 纪元是通过神经网络eg提供的数据的完整表示形式：如果您有一个包含 5000 个面的文件夹，则 1 个纪元将是模型看到所有 5000 个面时。2 纪元将是当模型已经看到所有 5000 面两次，等等。就 Faceswap 而言，Epoch 实际上并不是一个有用的衡量标准。由于模型在 2 个数据集（侧 A 和侧 B）上进行训练，除非这些数据集的大小完全相同（不太可能），否则无法计算一个 Epoch，因为它对于每一侧都不同。</span></li><li><span style="color:#000000;"><strong>示例</strong>- 一个示例（以 Faceswap）为例，它是"Face"的另一个名称。它基本上是通过神经网络传递的单面。如果模型看到 10 个示例，则看到 10 面。</span></li><li><span style="color:#000000;"><strong>EG/s</strong> - 这是神经网络每天看到的示例数，或者就 Faceswap 而言，模型正在处理的面数。</span></li><li><span style="color:#000000;"><strong>迭代</strong>- 迭代是通过神经网络处理的一个完整的批处理。因此，在批处理大小 64 的 10 次迭代意味着模型已经看到 640 （64 * 10） 面。</span></li><li><span style="color:#000000;"><strong>NN</strong> - 神经网络的缩写。</span></li></ul> 
 <br> 
 <span style="color:#000000;"><strong><a name="data">培训数据</a></strong></span> 
 <hr> 
 <span style="color:#000000;">数据的质量对模型的重要一点，怎么强调也不为过。较小的模型在使用像样的数据时可以很好地执行，同样，对于较差的数据，没有一个模型会执行良好。在绝对最低应有500不同的图像在你的模型的每一侧，但是更多的数据，越多样化，越好...到一点。要使用的图像数量在 1，000 到 10，000 之间。添加更多的图像实际上会损害训练。<br> 太多的类似图像不会帮助您的<br> 模型。您需要尽可能多的不同角度、表情和照明条件。一种常见的误解是，模型是为特定场景训练。这是"记忆"，并不是你试图实现的目标。您尝试训练模型以了解所有角度的面，所有条件下的所有表达式，并在所有角度将脸与另一个面交换，所有条件下的所有表达式。因此，您希望为 A 和 B 集从尽可能多的不同源构建训练集。<br> 每边的不同角度<br> 非常重要。一个 NN 只能了解它所看到的。如果 95% 的面直视摄像机，5% 的面是侧面的，则模型需要很长时间才能了解如何在面上创建侧面。它可能无法创建它们，因为它很少看到脸上的一面。理想情况下，您希望尽可能均匀地分布面部角度、表情和照明条件。<br> 同样，在 A 和 B 两侧之间具有尽可能多的匹配角度/表达式/照明条件<br> 也很重要。如果您有很多 A 端的配置文件图像，并且 B 端没有配置文件图像，则模型将永远无法在配置文件中执行交换，因为解码器 B 将缺少创建配置文件拍摄所需的信息。培训数据的质量一般不应被掩盖，而且应具有高质量（尖锐和详细）。<br><br> 但是，训练集中某些图像模糊/部分模糊是没问题的。最终，在最后的交换<strong>中，</strong>一些面会模糊/低分辨率/模糊，所以NN看到这些类型的图像也很重要，这样它就可以做忠实的娱乐。<br> 有关创建训练集的更多详细信息<br> ，请参阅<a class="postlink" href="https://faceswap.dev/forum/viewtopic.php?f=5&amp;t=27" rel="nofollow">在提取指南中</a>.<br><br><br><strong><a name="choose">选择模型</a></strong></span> 
 <hr> 
 <span style="color:#000000;">在 Faceswap 中有多种型号可供选择，随着时间的推移，将添加更多型号。每个质量可以是高度主观的，因此这将提供每个（当前）可用的简要概述。最终，最适合您的模型可以归结为许多因素，因此没有明确的答案。各有各的优缺点，但如上所述，最重要的一个因素是数据的质量。没有模型可以修复数据问题。<br> 下面将提到输入和输出大小（例如 64px 输入、64px<br> 输出）。这是输入到模型（输入）的面图像的大小和从模型生成的面的大小（输出），所有输入到模型的面都是方形的，因此 64px 图像将宽 64 像素 x 64 像素高。一种常见的误解是，更高分辨率的输入将导致更好的交换。虽然它可以提供帮助，但情况并非总是如此。NN 正在学习如何将面编码为算法，然后再次解码该算法。它只需要足够的数据，能够创建一个坚实的算法。输入分辨率和输出质量不直接链接。<br> 值得注意的是，模型越大，训练时间越<br> 长。原始型号可能需要 12-48 小时才能在 Nvidia GTX 1080 上训练。Villain 可以在相同的硬件上接受一周多的更换。通常认为，输入大小加倍的模型需要两倍的时间。这是不正确的。它至少需要4倍的时间，而且可能更长。这是因为 64px 图像具有 4，096 像素。但是，128px 图像具有 16，384 像素。这是 4 倍的数量，再加上模型需要缩放，以处理这个增加的数据量和培训时间可以快速堆积起来。</span> 
 <ul><li><span style="color:#000000;"><strong>轻量级</strong>（<em>64px 输入， 64px 输出</em>） - 这是一个非常剥离的模型， 旨在运行与 &lt;=2GB 的 VRAM 的 GPU.这不是您所说的"生产就绪"，但它使具有较低端硬件的用户能够训练模型。在高端 GPU 上，它会快速进行训练，因此在迁移到更征税的模型之前，可以快速了解交换工作得如何。</span></li><li><span style="color:#000000;"><strong>原始</strong>（<em>64px 输入，64px 输出</em>） - 启动这一切的模型。仍然可以提供出色的结果，并非常有用，以了解数据集质量如何真正成为交换质量的最大驱动因素之一。</span></li><li><span style="color:#000000;"><strong>IAE</strong> （<em>64px 输入， 64px</em>输出 ） - 结构与其他型号略有不同的模型。它有一个共享编码器和一个共享解码器，但3个中间层（一个用于 A，一个用于 B，一个用于共享），它们位于编码器和解码器之间。它的结构是这样做，试图更好地分离标识。有关此模型，请在此处了解有关此模型的更多内容：<a class="postlink" href="https://github.com/deepfakes/faceswap/pull/251">https://github.com/deepfakes/faceswap/pull/251</a></span></li><li><span style="color:#000000;"><strong>Dfaker</strong> （<em>64px 输入， 128px</em>输出 ） - 此模型利用了与原始模型不同的技术，并侧重于将输入向上缩放到更高分辨率的输出。尽管存在了一段时间，这个模型仍然取得了巨大的效果，而它缺乏自定义选项，使它很容易'火和忘记'模型。</span></li><li><span style="color:#000000;"><strong>不平衡</strong>（<em>64-512px 输入， 64-512px 输出</em>） - 这是一个强大的模型，有很多方法来自定义和改进模型，但需要更多的专业知识和专业知识，以获得良好的结果。可以说是被"真脸"取代。值得注意的是，该模型更强调 B 解码器，因此反转交换（即交换 B&gt;A 而不是 A&gt;B）将导致不太令人满意的结果。</span></li><li><span style="color:#000000;"><strong>DFL-H128</strong> （<em>128px 输入， 128px 输出</em>） - 此模型实际上使用完全相同的编码器和解码器作为原始，但随后使用 128px 输入，而不是 64px，然后尝试压缩图像到面的表示半大原始。较小的"潜在空间"在质量上与原始空间相比有一些缺点，这否定了较大的输入大小。</span></li><li><span style="color:#000000;"><strong>DFL-SAE</strong> （<em>64-256px 输入，64-256px 输出</em>） - 此模型包含两个不同的网络结构，一个基于原始共享编码器/拆分解码器模型，另一个基于 IAE 模型（共享中间层）。具有许多自定义选项。提供良好的细节，但可能导致某些标识出血（也就是说，A 的一些功能在 B 中可能仍然可见）。</span></li><li><span style="color:#000000;"><strong>Villain</strong> （<em>128px 输入， 128px 输出</em>） - Villain 可能是最详细的模型， 但非常 VRAM 密集， 可以给出低于标准的颜色匹配时， 训练有限的来源.是病毒来源史蒂夫·布塞米/珍妮弗·劳伦斯·迪普法克。由于此模型没有任何自定义选项（除了内存不足的变体），因此，如果您想要更高分辨率的模型，而无需调整任何设置，这是一个不错的选择。</span></li><li><span style="color:#000000;"><strong>实面</strong>（<em>64-128px 输入，64-256px 输出</em>） - 不平衡模型的后继。学习该模型和 Dfaker， 同时希望进一步发展他们。此模型是高度可自定义的，但最好调整选项时，你有一些想法，你在做什么，什么影响设置将有。与不平衡模型一样，该模型更强调 B 解码器，因此反转交换（即交换 B&gt;A 而不是 A&gt;B）将导致结果不太令人满意。</span></li><li><span style="color:#000000;"><strong>Dlight</strong> （<em>128px 输入， 128-384px 输出</em>） - 基于 dfaker 变体的高分辨率模型，侧重于升级面，使用自定义上流器。这是最新的模型，很容易配置。</span></li></ul> 
 <br> 
 <span style="color:#000000;"><strong><a name="config">模型配置设置</a></strong></span> 
 <hr> 
 <span style="color:#000000;">好了，你选择了你的模型，让我们来训练吧！好了，坚持住我钦佩你的渴望， 但你可能会想先设置一些模型特定的选项。我将为此使用 GUI，但配置文件（如果使用命令行）可以在您的人脸交换文件夹中找到位置<em>人脸交换/配置/train.ini</em>。<br> 我不会进入每个模型的选项，因为这些是多种多样的，并保持它更新的新模型将是艰难的，但我会给一些更常见的选项<br> 概述。我们将更专注于适用于所有模型的全球选项。所有选项都有工具提示，因此将鼠标悬停在选项上，以了解有关其功能的详细信息。要访问模型配置面板，请转到<strong>"设置</strong>&gt;<strong>配置训练插件"...</strong></span> 
 <br>   
 <div> 
  <p> </p> 
  <p><span style="color:#000000;">火车 1. png （4.95 Kib） 查看 92553 次</span></p> 
 </div> 
 <ul><li><span style="color:#000000;"><strong>全局</strong><br> 这些选项适用于所有模型：</span> 
   <div> 
    <p> </p> 
    <p><span style="color:#000000;">配置. png （20.05 KiB） 查看 76342 次</span></p> 
   </div> <span style="color:#000000;">此页面上的所有选项（学习率除外<strong>）</strong>仅在创建新模型时生效。开始训练模型后，此处选择的设置将"锁定"到该模型，并且无论此处设置什么，只要您恢复训练，都将重新加载。</span> 
   <ul><li><span style="color:#000000;"><strong>应用于</strong><br> 要输入模型的面的面选项</span> 
     <div> 
      <p> </p> 
      <p><span style="color:#000000;">Face0.jpg （7.27 KiB） 浏览 92549 次</span></p> 
     </div> 
     <ul><li><span style="color:#000000;"><strong>覆盖范围</strong>- 这是将输入模型的源映像量。图像的百分比按给定量从中心裁剪。覆盖率越高，脸部的输入率越高。裁剪的图像量的图示如下所示：</span> 
       <div> 
        <p> </p> 
        <p><span style="color:#000000;">覆盖.jpg （76.39 KiB） 查看 92553 次</span></p> 
       </div> <span style="color:#000000;">虽然，从直觉上看，似乎更高的覆盖率总是更好，但实际上并非如此，这是一个权衡。虽然较高的覆盖范围将意味着更多的面被交换，模型的输入大小始终保持不变，因此生成的交换可能会不太详细，因为需要将更多信息打包到相同大小的图像中。为了说明，下面是同一图像的一个极端示例，其覆盖率为 62.5%，覆盖范围为 100%，大小为 32px。如您所看到的，100% 覆盖图像包含的细节远远小于 62.5% 版本。最终，此选项的正确选择由您决定：</span> 
       <div> 
        <p> </p> 
        <p><span style="color:#000000;">覆盖 2.jpg （33.86 KiB） 查看 92553 次</span></p> 
       </div> </li></ul></li></ul> 
   <ul><li><span style="color:#000000;"><strong>适用于</strong><br> 使用蒙版训练的蒙版选项。</span> 
     <div> 
      <p> </p> 
      <p><span style="color:#000000;">config_mask.png （5.16 KiB） 浏览 76339 次</span></p> 
     </div> <span style="color:#000000;">设置蒙版是指示图像的哪个区域重要的一种方式。在下面的示例中，红色区域被"遮盖"，（第二：它被认为是不重要的），而清除区域是"掩蔽"（它是面，所以我们感兴趣的区域）：</span> 
     <div> 
      <p> </p> 
      <p><span style="color:#000000;">掩码0.png （30.03 KiB） 查看 92549 次</span></p> 
     </div> <span style="color:#000000;">使用面罩进行训练有两个目的：</span> 
     <ol><li><span style="color:#000000;">它侧重于面部区域的训练，迫使模型对背景的重要性降低。这可以帮助模型更快地学习，同时确保它不占用空间学习背景细节并不重要。</span></li><li><span style="color:#000000;">学习的掩码可用于转换阶段。在当前实现中，学习的掩码是否比在转换时使用标准掩码具有任何好处，这是有争议的，但使用掩码进行训练可以确保您可以选择使用它。</span></li></ol><span style="color:#000000;"><strong>NB：</strong>如果使用蒙版进行训练，则必须在每个输入文件夹中为 A 和 B 中的所有面提供对齐文件。</span> 
     <ul><li><span style="color:#000000;"><strong>掩码</strong>类型 - 用于训练的掩码类型。若要使用蒙版<strong>，</strong>您必须将需要的蒙版添加到对齐文件中。您可以使用蒙版工具添加/更新蒙版。看到<a class="postlink" href="https://forum.faceswap.dev/viewtopic.php?f=5&amp;t=27#extract" rel="nofollow">viewtopic.php？f=5&amp;t=27#提取</a>用于对每个掩码进行透彻的描述。</span></li><li><span style="color:#000000;"><strong>蒙版模糊</strong>内核 - 这会对蒙版的边缘应用轻微的模糊。实际上，它可去除蒙版的硬边，并更逐渐地从面混合到背景。这可以帮助处理计算不当的掩码。是否要启用此功能以及使用的值由您决定。默认值应该没问题，但您可以使用掩码工具进行试验。</span></li><li><span style="color:#000000;"><strong>掩码</strong>阈值 - 此选项不会影响基于对齐的掩码（扩展的组件），因为它们是二进制的（IE 掩码是"开"或"关"）。对于基于 NN 的掩码，掩码不是二进制的，并且具有不同的不透明度级别。在某些情况下，这可能导致掩码被斑点化。提高阈值会使蒙版中接近透明、完全透明的部分和接近实心、完全实心的蒙版部分。同样，这种情况会因情况而异。</span></li><li><span style="color:#000000;"><strong>学习面具</strong>- 如前所述，学习面具是否有任何好处是有争议的。启用此选项将使用更多的 VRAM，因此我倾向于将其关闭，但如果您希望预测的掩码在转换中可用，那么您应该启用此选项。</span></li></ul></li></ul> 
   <ul><li><span style="color:#000000;"><strong>应用于初始</strong><br> 化模型的初始化选项。</span> 
     <div> 
      <p> </p> 
      <p><span style="color:#000000;">Init0.jpg （8.86 KiB） 浏览 92549 次</span></p> 
     </div> <span style="color:#000000;">如<a href="https://forum.faceswap.dev/viewtopic.php?t=146#overview" rel="nofollow">培训概述</a>模型具有在每次迭代结束时更新的权重。初始化是第一次设置这些权重的过程。您可以认为这为模型提供帮助。正如我们知道我们的 NN 将用于什么的，我们可以将这些权重设置为值，这将有助于模型在人生中快速启动。<br> 初始化的默认方法是"he_uniform"。<br> 这将从统一分布中抽取样本。本指南的目标不是进入不同的初始化方法及其意义，但此默认值可以由本节中公开的选项覆盖。<br> 应该注意的是，某些模型在内部为模型的一些层设置了初始化方法，因此这些图层不会受到此设置<br> 的影响。但是，对于未显式设置此设置的图层和模型，初始化器将更改为所选选项。<br> 两个现有的初始化器可以一起使用（即，它们都可以启用，没有不良<br> 影响）。我倾向于让他们两个。</span> 
     <ul><li><span style="color:#000000;"><strong>ICNR Init</strong> - 此初始化器仅适用于高档图层。当输出图像在 NN 内进行升级时，标准初始化可能导致输出图像中的"棋盘"伪像。此初始化器旨在防止这些人工制品。有关此方法的更多信息，请参阅本文：<a class="postlink" href="https://arxiv.org/abs/1707.02937" rel="nofollow">https://arxiv.org/abs/1707.02937</a></span></li><li><span style="color:#000000;"><strong>Conv 感知初始</strong>化 - 卷积感知初始化应用于模型中的所有卷积层。此初始化器背后的前提是，它考虑到卷积网络的目的，并相应地初始化权重。从理论上讲，这会导致更高的精度、更低的损耗和更快的收敛。有关此初始化器的有关更多信息，请阅读以下文件：<a class="postlink" href="https://arxiv.org/abs/1702.06295" rel="nofollow">https://arxiv.org/abs/1702.06295</a><br><br><strong>NB：</strong>此初始化器在启动时可以采取更多的 VRAM，因此建议从较低的批处理大小开始，启动模型，然后使用所需的批处理大小重新启动模型。<br><strong>NB：</strong>此初始化器不会在启用多 gpu 模式时运行，因此，如果使用多个 GPU 进行训练，您应该在 1 GPU 上开始训练，停止模型，然后继续启用多 gpu。</span></li></ul></li></ul> 
   <ul><li><span style="color:#000000;"><strong>应用于</strong><br> 模型中图层的网络选项</span> 
     <div> 
      <p> </p> 
      <p><span style="color:#000000;">Network0.jpg （10.54 KiB） 浏览 92549 次</span></p> 
     </div> <span style="color:#000000;">此处的选项适用于模型中使用的所有图层。</span> 
     <ul><li><span style="color:#000000;"><strong>子像素上缩放</strong>- 这是神经网络中升级图像的替代方法。它实际上做完全相同的工作， 因为他们默认像素洗牌层， 只是使用不同的 TensorFlow 操作。我建议只是离开这个， 因为它没有区别 （并可能在未来删除）</span></li><li><span style="color:#000000;"><strong>反射填充</strong>- 某些型号，特别是比利亚因，并在较小程度上 DFL-SAE，有一个显着的"灰色框"周围的交换区域的边缘，在最后的交换。此选项更改卷积层中使用的填充类型，以帮助缓解此人工制品。我只建议为这两种型号启用它， 否则我会离开这个。</span></li></ul></li></ul> 
   <ul><li><span style="color:#000000;"><strong>丢失</strong><br> 要使用的丢失功能。</span> 
     <div> 
      <p> </p> 
      <p><span style="color:#000000;">Loss0.jpg （12.77 KiB） 查看 92549 次</span></p> 
     </div> <span style="color:#000000;">有各种不同的方法来计算损失，或者让 NN 辨别它在训练模型时做得如何。我不会详细讨论每个可用的功能，因为这将是一个有点漫长的过程，并且在互联网上有很多关于每个功能的信息。</span> 
     <ul><li><span style="color:#000000;"><strong>损耗函数</strong>- 最流行的损耗方法是 MAE（平均绝对误差）和 SSIM（结构相似性）。我个人更喜欢使用 SSIM 。</span></li><li><span style="color:#000000;"><strong>受惩罚的</strong>蒙版丢失 - 此选项指示是否应给予图像位于面区域以外的区域比位于面区域内的区域的重要性。应始终启用此选项</span></li></ul></li></ul> 
   <ul><li><span style="color:#000000;"><strong>与优化</strong><br> 器相关的优化器选项。</span> 
     <div> 
      <p> </p> 
      <p><span style="color:#000000;">优化器0.jpg （9.6 KiB） 查看 92549 次</span></p> 
     </div> <span style="color:#000000;">优化器控制神经网络的学习速率。</span> 
     <ul><li><span style="color:#000000;"><strong>学习速率</strong>- 这通常应单独使用，除非模型正在折叠（所有图像更改为纯色块，并且损失峰值达到高水平，并且永远不会恢复）。与此页面上的其他参数不同，可以对现有模型调整此值。<br> 学习速率指示每次迭代时权重可以向上或向下调整多<br> 远。直觉会说更高的学习率更好，但情况并非如此。模型正在尝试学习达到尽可能低的损失值。设置过高的学习率会不断在最低值上方和下方摆动，并且永远不会学到任何东西。设置学习率设置过低，模型可能会达到低谷，并认为它已达到最低点，并将停止改进。<br> 把它想像走<br> 山。你想到底部，所以你应该总是下降。然而，下山的路上并不总是下坡，有较小的山丘和山谷在路上。学习率需要足够高，以便能够走出这些较小的山谷，但没那么高，你最终到下一座山的顶部。</span></li></ul></li></ul></li></ul> 
 <ul><li><span style="color:#000000;"><strong>模型</strong><br> 以下是特定于每个模型插件的设置：</span> 
   <div> 
    <p> </p> 
    <p><span style="color:#000000;">型号 1.png （3.14 KiB） 查看 92544 次</span></p> 
   </div> <span style="color:#000000;">如前所述，我不会详细介绍模型特定设置。这些因插件而异。但是，我将介绍一些常见的选项，你可能会看到在每个插件。与一如既往，每个选项将有一个工具提示，可以给你更多信息。</span> 
   <ul><li><span style="color:#000000;"><strong>低mem</strong> - 某些插件具有"低mem"模式。这使您能够运行模型的精简版本，占用更少的 VRAM，但代价是保真度更低。</span></li><li><span style="color:#000000;"><strong>输入</strong>大小 - 某些插件允许您调整输入到模型中的大小。输入始终为方形，因此这是输入到模型中的图像的宽度和高度的大小（以像素为单位）。不要相信较大的输入总是等于更好的质量。情况并非总是如此。有许多其他因素决定了模型的质量是否不错。更高的输入大小需要指数级更高的 VRAM 来处理。</span></li><li><span style="color:#000000;"><strong>输出</strong>大小 - 某些插件允许您调整模型生成的图像的大小。输入大小和输出大小不必相同，因此某些模型包含的升级器返回的输出图像大于输入图像。</span></li></ul></li></ul> 
 <ul><li><span style="color:#000000;"><strong>培训师</strong><br> 配置设置页面中的最后一个选项卡是"培训师"或"数据增强"选项：</span> 
   <div> 
    <p> </p> 
    <p><span style="color:#000000;">培训师 1.jpg （123.87 KiB） 浏览 92543 次</span></p> 
   </div> <span style="color:#000000;">NN 需要看到许多不同的图像。为了更好地学习面部，它执行各种操作的输入图像。这称为"数据增强"。如注释中所述，标准设置对于 99% 的用例是没问题的，因此，如果您知道这些用例将产生什么影响，则仅更改这些设置。</span> 
   <ul><li><span style="color:#000000;"><strong>评估</strong>- 评估培训状态的选项。</span> 
     <div> 
      <p> </p> 
      <p><span style="color:#000000;">Trainer2.jpg (8.28 KiB) Viewed 92543 times</span></p> 
     </div> 
     <ul><li><span style="color:#000000;"><strong>预览</strong>图像 - 这是交换的每个 A 面和 B 面在预览窗口中显示的面数。</span></li></ul></li></ul> 
   <ul><li><span style="color:#000000;"><strong>图像增强</strong>- 这些是对被送入模型的面执行的操作。</span> 
     <div> 
      <p> </p> 
      <p><span style="color:#000000;">Trainer3.jpg (24.57 KiB) Viewed 92543 times</span></p> 
     </div> 
     <ul><li><span style="color:#000000;"><strong>缩放量</strong>- 在输入到 NN 之前，将面放大或缩小的百分比量。帮助模型处理不对齐。</span></li><li><span style="color:#000000;"><strong>旋转范围</strong>- 在输入到 NN 之前，面顺时针或逆时针旋转的百分比量。帮助模型处理不对齐。</span></li><li><span style="color:#000000;"><strong>移位</strong>范围 - 在输入到 NN 之前，将面向上/向下、向左/向右移动的百分比量。帮助模型处理不对齐。</span></li><li><span style="color:#000000;"><strong>翻转机会</strong>- 水平翻转面部的机会。有助于为 NN 创建更多角度供您学习。</span></li></ul></li></ul> 
   <ul><li><span style="color:#000000;"><strong>颜色增强</strong>- 这些增强操作被输入模型的面的颜色/对比度，使 NN 对颜色差异更加坚固。</span> 
     <div> 
      <p> </p> 
      <p><span style="color:#000000;">Trainer4.jpg (24.47 KiB) Viewed 92543 times</span></p> 
     </div> <span style="color:#000000;">这是一个图示，说明颜色增强在引擎盖下有什么作用（在预览/最终输出中看不到这一点，它只是为了演示目的）：</span> 
     <ul><li><span style="color:#000000;"><strong>颜色浅色</strong>- 上下调整输入图像的光度百分比。有助于处理不同的照明条件。</span></li><li><span style="color:#000000;"><strong>颜色 AB</strong> - 颜色在<em>L*a*b 颜色空间的 A/B 刻度上调整颜色的百分比</em>量。帮助 NN 处理不同的颜色条件。</span></li><li><span style="color:#000000;"><strong>颜色 CLAHE 机会</strong>- 图像具有对比度有限自适应直方图均衡的百分比。CLAHE 是一种尝试本地化对比度更改的对比方法。这有助于 NN 处理不同的对比度量。</span></li><li><span style="color:#000000;"><strong>颜色 CLAHE 最大大小</strong>- CLAHE 算法的最大"网格大小"。这将缩放到输入图像。较高的值将导致更高的对比度应用程序。这有助于 NN 处理不同的对比度量。</span></li></ul></li></ul></li></ul> 
 <span style="color:#000000;">获得模型设置后，请点击<strong>"确定"</strong>以保存配置并关闭窗口。NB：点击确定将保存所有<strong>选项卡</strong>上的选项，因此请务必仔细查看它们。<br><br> 您可以点击"<strong>取消"</strong>以取消任何更改，也可以<strong>重置</strong>以将所有值还原到默认设置。<br><br><br><strong><a name="settings">建立</a></strong></span> 
 <hr> 
 <span style="color:#000000;">现在，你已经把脸放在位了，已经配置了你的模型，是时候开始事情了！<br> 前往 GUI 中的"列车"<br> 选项卡：</span> 
 <div> 
  <p> </p> 
  <p><span style="color:#000000;">Setup1.jpg （47.77 KiB） 查看 92541 次</span></p> 
 </div> 
 <span style="color:#000000;">我们将告诉 Faceswap 所有内容都存储在哪里、我们想使用什么，然后实际开始培训。</span> 
 <ul><li><span style="color:#000000;"><strong>面</strong><br> 这是我们将告诉 Faceswap 面的存储位置，以及它们各自的对齐文件的位置（如果需要）</span> 
   <div> 
    <p> </p> 
    <p><span style="color:#000000;">setup_faces.jpg （21.02 KiB） 浏览 92541 次</span></p> 
   </div> 
   <ul><li><span style="color:#000000;"><strong>输入 A</strong> - 这是包含作为<a class="postlink" href="https://faceswap.dev/forum/viewtopic.php?f=5&amp;t=27" rel="nofollow">提取过程</a>.这些面将从原始场景中删除，以便由交换面替换。此文件夹中应包含大约 1，000 到 10，000 个面。</span></li><li><span style="color:#000000;"><strong>对齐 A</strong> - 如果您使用蒙版进行训练，或使用"向地标扭曲"，则您需要为您的面提供对齐文件。这将作为提取过程的一部分生成。如果文件存在于您的面文件夹中，并且被命名为<strong>对齐.json，</strong>则会自动拾取该文件。人脸中的每个面 A 文件夹必须具有对齐文件中的条目，否则训练将失败。您可能需要合并多个对齐文件。您可以找到有关准备对齐文件以在<a class="postlink" href="https://faceswap.dev/forum/viewtopic.php?f=5&amp;t=27" rel="nofollow">提取指南</a>.</span></li><li><span style="color:#000000;"><strong>输入 B</strong> - 这是文件夹的位置，其中包含您作为<a class="postlink" href="https://faceswap.dev/forum/viewtopic.php?f=5&amp;t=27" rel="nofollow">提取过程</a>.这些是将交换到场景中的面。此文件夹中应包含大约 1，000 到 10，000 个面。</span></li><li><span style="color:#000000;"><strong>对齐 B</strong> - 如果您使用蒙版进行训练，或使用"向地标扭曲"，则您需要为您的面提供对齐文件。这将作为提取过程的一部分生成。如果文件存在于您的面文件夹中，并且被命名为<strong>对齐.json，</strong>则会自动拾取该文件。人脸 B 文件夹中的每一个面必须具有对齐文件中的条目，否则训练将失败。您可能需要合并多个对齐文件。您可以找到有关准备对齐文件以在<a class="postlink" href="https://faceswap.dev/forum/viewtopic.php?f=5&amp;t=27" rel="nofollow">提取指南</a>.</span></li></ul></li><li><span style="color:#000000;"><strong>与</strong><br> 要培训的模型相关的模型选项：</span> 
   <div> 
    <p> </p> 
    <p><span style="color:#000000;">setup_model.jpg （34.47 KiB） 浏览 92541 次</span></p> 
   </div> 
   <ul><li><span style="color:#000000;"><strong>型号 Dir</strong> - 这是模型文件将保存的位置。如果要启动新模型，请选择空文件夹;如果正在从已启动的模型恢复训练，则应选择包含模型文件的现有文件夹。</span></li><li><span style="color:#000000;"><strong>培训师</strong>- 这是您将为交换而培训的模型。不同型号的概述是<a href="https://forum.faceswap.dev/viewtopic.php?t=146#choose" rel="nofollow">上面提供</a>.</span></li><li><span style="color:#000000;"><strong>允许增长</strong>- [仅 NVIDIA]。启用 TensorFlow GPU"allow_growth"配置选项。此选项可防止 TensorFlow 在启动时分配所有 GPU VRAM，但可能导致更高的 VRAM 碎片化和较慢的性能。只有在训练出现问题时才应启用它（具体来说，您遇到了 cuDNN 错误）。</span></li></ul></li><li><span style="color:#000000;"><strong>培训</strong><br> 特定设置：</span> 
   <div> 
    <p> </p> 
    <p><span style="color:#000000;">setup_training.jpg （28.48 KiB） 浏览 92541 次</span></p> 
   </div> 
   <ul><li><span style="color:#000000;"><strong>批处理</strong>大小 - 如上所述，批处理大小是通过模型一次输入的图像量。增加这个数字将增加 VRAM 的使用率。增加批次大小将加速培训到一定程度。小批量提供一种法规形式，有助于模型泛化。而大批量训练更快，批量尺寸在8至16范围内可能会产生更好的质量。对于其他形式的监管能否取代或消除这种需要，仍然是一个悬而未决的问题。</span></li><li><span style="color:#000000;"><strong>迭代</strong>- 自动停止训练之前要执行的迭代次数。这真的只是自动化的，或者确保训练在一定时间后停止。通常，当您对预览的质量满意时，您将手动停止训练。</span></li><li><span style="color:#000000;"><strong>GPU</strong> - [仅 NVIDIA] - 要培训的 GPU 数量。如果您的系统中有多个 GPU，则最多可以使用其中 8 个来加快培训速度。请注意，此加速不是线性的，添加的 GPU 越多，回报的递减将越小。最终，它允许您通过将更大的批处理大小拆分为多个 GPU 来训练更大的批处理大小。您始终会受到最弱 GPU 的速度和 VRAM 的瓶颈，因此在相同的 GPU 上进行训练时，效果最佳。您可以阅读更多关于 Keras 多 gpu 的信息<a class="postlink" href="https://keras.io/utils/#multi_gpu_model" rel="nofollow">这里</a></span></li><li><span style="color:#000000;"><strong>无日志</strong>- 提供损耗和模型日志记录，以便能够分析<a class="postlink" href="https://www.tensorflow.org/tensorboard/r1/summaries" rel="nofollow">张力板</a>和 Gui 。关闭此选项将意味着您无法访问此数据。实际上，没有理由禁用日志记录，因此通常不应检查这一点。</span></li><li><span style="color:#000000;"><strong>扭曲到地标</strong>- 如前所述，数据是扭曲的，以便 NN 可以学习如何重新创建面。向地标扭曲是一种不同的扭曲方法，它尝试将面随机扭曲到另一侧的类似面（即对于 A 集，它会从 B 集找到一些类似的面，并应用一个扭曲，并随机化）。陪审团不知道这是否提供了任何好处/差异比标准随机扭曲。</span></li><li><span style="color:#000000;"><strong>无翻转</strong>- 图像随机翻转，以帮助增加 NN 将看到的数据量。在大多数情况下，这很好，但面不是对称的，因此对于某些目标，这可能不可取（例如，面部一侧的摩尔）。一般来说，这应该不受检查，当然，在开始培训时，应不加检查。稍后在会话期间，您可能希望禁用此功能以进行某些交换。</span></li><li><span style="color:#000000;"><strong>无增强颜色</strong>- Faceswap 执行颜色增强（前面详细）。这真的有助于匹配 A 和 B 之间的颜色/照明/对比度，但有时可能不可取，因此可以在这里禁用。颜色增强的影响如下所示：</span> 
     <div> 
      <p> </p> 
      <p><span style="color:#000000;">颜色 aug. jpg （79.9 KiB） 查看 92534 次</span></p> 
     </div> </li></ul></li><li><span style="color:#000000;"><strong>用于保存 VRAM</strong><br> 的 VRAM 节省优化设置：</span> 
   <div> 
    <p> </p> 
    <p><span style="color:#000000;">setup_vram.jpg （15.01 KiB） 浏览 92541 次</span></p> 
   </div> <span style="color:#000000;">Faceswap 提供了许多 VRAM 保存优化，使用户能够训练他们可能无法训练的模型。不幸的是，这些选项目前仅适用于 Nvidia 用户。这些应该是最后一个呼叫端口。如果可以训练至少 6-8 的批处理大小而不启用这些选项，那么您应该先进行，因为这些都配有速度惩罚。所有这些选项都可以相互启用，以堆叠节省。</span> 
   <ul><li><span style="color:#000000;"><strong>内存节省梯度</strong>- [仅 NVIDIA] - MSG 是一种优化方法，可以计算成本节省 VRAM。在最好的情况下，它可以在培训时间增加 20% 时将 VRAM 要求减半。这是您应该尝试的第一个选项。您可以阅读有关内存保存渐变的更多内容<a class="postlink" href="https://github.com/cybertronai/gradient-checkpointing">这里</a>.</span></li><li><span style="color:#000000;"><strong>优化器节省</strong>- [仅 NVIDIA] - 通过对 CPU 而不是 GPU 执行一些优化计算，这可以节省相当数量的 VRAM。它确实以增加系统 RAM 使用率和降低训练速度为成本。这应该是您尝试的第二个选项。</span></li><li><span style="color:#000000;"><strong>乒乓球 -</strong> [仅 NVIDIA] - 阿卡 "最后的手段"。这是迄今为止最差的 VRAM 保存选项，但它可能足以获得您需要的。这基本上将模型一分为二，一次将训练模型的一半。这可以节省高达 40% 的 VRAM，但需要两倍的时间来训练模型。这应该是您尝试的最后一个选项。<br><strong>NB：</strong>启用此选项后，将不可用的张力记录/图形。<br><strong>NB：</strong>在模型的两侧完成训练周期之前，预览不会显示。</span></li></ul></li><li><span style="color:#000000;"><strong>保存</strong><br> 选项以计划保存模型文件：</span> 
   <div> 
    <p> </p> 
    <p><span style="color:#000000;">setup_saving.jpg （14.62 KiB） 浏览 92541 次</span></p> 
   </div> 
   <ul><li><span style="color:#000000;"><strong>保存间隔</strong>- 应将模型保存到磁盘的时间。保存模型时，未对模型进行训练，因此可以提高此值，以获得训练速度的轻微提升（即，它并不等待模型经常写入磁盘）。你可能不想提高这个太高，因为它基本上是你的"故障安全"。如果模型在训练期间崩溃，则只能从上次保存开始继续。<br><strong>NB：</strong>如果使用乒乓球内存保存选项，则不应增加此容量超过 100，因为它可能会损害最终质量。</span></li><li><span style="color:#000000;"><strong>快照</strong>间隔 - 快照是模型在时间点的副本。这样，如果您对模型的进度不满意，可以回滚到较早的快照;如果保存文件已损坏且没有可用的备份，则回滚。通常，这应该是一个高数字（在大多数情况下，默认值应该没问题），因为创建快照可能需要一段时间，并且在此过程完成时，您的模型不会进行训练。</span></li></ul></li><li><span style="color:#000000;"><strong>用于</strong><br> 显示训练进度预览窗口的预览选项：</span> 
   <div> 
    <p> </p> 
    <p><span style="color:#000000;">setup_preview.jpg （13.17 KiB） 浏览 92541 次</span></p> 
   </div> <span style="color:#000000;">如果您使用的是 GUI，则通常您不需要使用这些选项。预览是一个窗口，弹出显示训练进度。GUI 将此信息嵌入到"显示"面板中，因此弹出的窗口将只显示完全相同的信息，并且是冗余的。每次保存迭代时预览更新。</span> 
   <ul><li><span style="color:#000000;"><strong>预览比例</strong>- 弹出的预览大小与训练图像的大小。如果您的训练图像为 256px，则完整的预览窗口将为 3072x1792。对于大多数显示器来说，这太大，因此此选项会按给定的百分比缩小预览。</span></li><li><span style="color:#000000;"><strong>预览</strong>- 启用弹出预览窗口，禁用以不弹出预览窗口。对于 GUI，通常不选中此选项。</span></li><li><span style="color:#000000;"><strong>写入图像</strong>- 这会将预览图像写入人脸交换文件夹。如果在无头系统上训练，很有用。</span></li></ul></li><li><span style="color:#000000;"><strong>用于生成</strong><br> 可选的延时图像集的延时选项：</span> 
   <div> 
    <p> </p> 
    <p><span style="color:#000000;">setup_timelapse.jpg （21.11 KiB） 浏览 92541 次</span></p> 
   </div> <span style="color:#000000;">"时间延迟"选项是一项可选功能，使您能够查看一组固定面上训练在一定时间进行中的进度。每次保存迭代时，将保存一个图像，显示该时间点所选面的训练进度。请注意，Time Lapse 映像占用的磁盘上的空间量可能会随着时间的推移而堆积起来。</span> 
   <ul><li><span style="color:#000000;"><strong>延时输入 A</strong> - 包含要用于生成 A（原始）边的时间推移的面的文件夹。将只使用找到前 14 个面。如果您希望从训练集中选择前 14 个面，只需在"输入 A"文件夹上点这个。</span></li><li><span style="color:#000000;"><strong>延时输入 B</strong> - 包含要用于生成 B（交换）端的时间推移的面的文件夹。将只使用找到前 14 个面。如果您希望从训练集中选择前 14 个面，只需在"输入 B"文件夹上点这个。</span></li><li><span style="color:#000000;"><strong>延时输出</strong>- 希望保存生成的延时图像的位置。如果为 A 和 B 提供了源，但为此留空，则默认为所选模型文件夹。</span></li></ul></li><li><span style="color:#000000;"><strong>全球</strong><br> 面交换选项：</span> 
   <div> 
    <p> </p> 
    <p><span style="color:#000000;">setup_global.jpg （18.17 KiB） 浏览 92541 次</span></p> 
   </div> <span style="color:#000000;">这些选项对于 Faceswap 的每个部分都是全球性的，而不仅仅是培训。</span> 
   <ul><li><span style="color:#000000;"><strong>配置文件</strong>- 您可以指定自定义训练器.ini 文件，而不是使用存储在<em>人脸交换/配置文件夹中</em>的文件。如果您有几个已知良好的配置，您可以在其中切换，这非常有用。</span></li><li><span style="color:#000000;"><strong>日志级别</strong>- Faceswap 将登录的级别。通常，应始终将这设置为<strong>INFO</strong>。只有在开发人员要求您这样做时，才应将此选项设置为<strong>TRACE，</strong>因为这将显著降低培训速度，并生成大量日志文件。<br><strong>NB：</strong>控制台将只登录到级别<strong>VEROSEOSE。</strong>调试和<strong>TRACE</strong>的<strong>日志</strong>级别仅写入日志文件。</span></li><li><span style="color:#000000;"><strong>日志文件</strong>- 默认情况下，日志文件存储在<em>faceswap/faceswap.log 。</em>如果需要，可以在此处指定其他位置。</span></li></ul><span style="color:#000000;">锁定所有设置后，查看这些设置以确保您满意，然后点击<strong>"火车"</strong>按钮开始培训。</span></li></ul> 
 <br> 
 <span style="color:#000000;"><strong><a name="monitor">监控培训</a></strong></span> 
 <hr> 
 <span style="color:#000000;">开始训练后，该过程需要一两分钟来构建模型、预加载数据并开始训练。启动后，GUI 将进入训练模式，在底部放置一个状态栏，并在右侧打开一些选项卡：</span> 
 <ul><li><span style="color:#000000;"><strong>状态</strong><br> 栏 显示在右下角，并给出当前培训课程的概述。它更新每次迭代：</span> 
   <div> 
    <p> </p> 
    <p><span style="color:#000000;">显示器 4.jpg （21.46 KiB） 查看 92526 次</span></p> 
   </div> <span style="color:#000000;">您不需要太密切地注意此处的损失数字。对于人脸交换，它们实际上毫无意义。这些数字使人了解 NN 认为它重新创建面 A 以及重新创建面 B 的得天其来。但是，我们对模型从面 A 的编码创建面 B 的算法感兴趣。这是不可能得到一个损失值，因为没有实际的例子，交换的脸为NN比较。</span> 
   <ul><li><span style="color:#000000;"><strong>已</strong>结束 - 此培训课程已经过的时间量。</span></li><li><span style="color:#000000;"><strong>会话迭代</strong>- 此培训课程期间已处理的迭代数。</span></li><li><span style="color:#000000;"><strong>Total Iterations</strong> - The total number of iterations that have been processed for all sessions for this model.</span></li><li><span style="color:#000000;"><strong>Loss A/Loss B</strong> - The loss for the current iteration. NB: There may be multiple loss values (e.g. for face, mask, multi-outputs etc). This value is the sum of all the losses, so the figures here can vary quite widely.</span></li></ul></li><li><span style="color:#000000;"><strong>Preview Tab</strong><br> Visualizes the current state of the model. This is a representation of the model's ability to recreate and swap faces. It updates every time the model saves:</span> 
   <div> 
    <p> </p> 
    <p><span style="color:#000000;">monitor1.jpg (111.1 KiB) Viewed 92526 times</span></p> 
   </div> <span style="color:#000000;">The best way to know if a model has finished training is to watch the previews. Ultimately these show what the actual swap will look like. When you are happy with the previews then it is time to stop training. Fine details like eye-glare and teeth will be the last things to come through. Once these are defined, it is normally a good indication that training is nearing completion.</span> 
   <ul><li><span style="color:#000000;">The preview will show 12 columns. The first 6 are the "A" (Original face) side, the second 6 are the "B" (Swap face) side. Each group of 6 columns is split into 2 groups of 3 columns. For each of these columns:</span> 
     <div> 
      <p> </p> 
      <p><span style="color:#000000;">prev1.png (74.99 KiB) Viewed 92515 times</span></p> 
     </div> 
     <ul><li><span style="color:#000000;">Column 1 is the unchanged face that is fed into the model</span></li><li><span style="color:#000000;">Column 2 is the model attempting to recreate that face</span></li><li><span style="color:#000000;">Column 3 is the model attempting to swap the face</span></li></ul></li><li><span style="color:#000000;">These will start out as a solid color, or very blurry, but will improve over time as the NN learns how to recreate and swap faces.</span></li><li><span style="color:#000000;">The opaque red area indicates the area of the face that is masked out (if training with a mask).</span></li><li><span style="color:#000000;">If training with coverage less than 100% you will see the edges of a red box. This indicates the "swap area" or the area that the NN is training on.</span></li><li><span style="color:#000000;">You can save a copy of the current preview image with the save button at the bottom right.</span></li><li><span style="color:#000000;">The preview can be disabled by unchecking the "Enable Preview" box at the bottom right.</span></li></ul></li><li><span style="color:#000000;"><strong>Graph Tab</strong> - This tab contains a graph that shows loss over time. It updates every time the model saves, but can be refreshed by hitting the "Refresh" button:</span> 
   <div> 
    <p> </p> 
    <p><span style="color:#000000;">monitor2.jpg (85 KiB) Viewed 92526 times</span></p> 
   </div> <span style="color:#000000;">You do not need to pay too close attention to the numbers here. For faceswapping they are effectively meaningless. The numbers give an idea of how well the NN thinks it is recreating Face A and how well it is recreating Face B. However we are interested in how well the model is creating Face B from the encodings of Face A. It is impossible to get a loss value for this as there are no real-world examples of a swapped face for the NN to compare against.<br><br> The loss graph is still a useful tool. Ultimately as long as loss is dropping, then the model is still learning. The rate that the model learns will decrease over time, so towards the end it may be hard to discern if it is still learning at all. See the Analysis tab in these instances.</span> 
   <ul><li><span style="color:#000000;">Depending on the number of outputs, there may be several graphs available (e.g. total loss, mask loss, face loss etc). Each graph shows loss for that particular output.</span></li><li><span style="color:#000000;">You can save a copy of the current graph with the save button at the bottom right.</span></li><li><span style="color:#000000;">The graph can be disabled by unchecking the "Enable Preview" box at the bottom right.</span></li></ul></li><li><span style="color:#000000;"><strong>Analysis Tab</strong> - This tab shows some statistics for the currently running and previous training sessions:</span> 
   <div> 
    <p> </p> 
    <p><span style="color:#000000;">monitor3.jpg (64.32 KiB) Viewed 92526 times</span></p> 
   </div> 
   <ul><li><span style="color:#000000;">The columns are as follows:</span> 
     <ul><li><span style="color:#000000;"><strong>Graphs</strong> - Click the blue graph icon to open up a graph for the selected session.</span></li><li><span style="color:#000000;"><strong>Start/End/Elapsed</strong> - The start time, end time and total training time for each session respectively.</span></li><li><span style="color:#000000;"><strong>Batch</strong> - The batch size for each session</span></li><li><span style="color:#000000;"><strong>Iterations</strong> - The total number of iterations processed for each session.</span></li><li><span style="color:#000000;"><strong>EGs/sec</strong> - The number of faces processed through the model per second.</span></li></ul></li><li><span style="color:#000000;">When a model is not training, you can open up stats for previously trained models by hitting the open button at the bottom right and selecting a model's <strong>state.json</strong> file inside the model folder.</span></li><li><span style="color:#000000;">You can save the contents of the analysis tab to a <em>csv</em> file with the save icon at the bottom right.</span></li></ul><span style="color:#000000;">As stated above, the loss graph is useful for seeing if loss is dropping, but it can be hard to discern when the model has been training for a long time. The analysis tab can give you a more granular view.</span> 
   <ul><li><span style="color:#000000;">Clicking the blue graph icon next to your latest training session will bring up the training graph for the selected session.</span> 
     <div> 
      <p> </p> 
      <p><span style="color:#000000;">analysis1.jpg (54.3 KiB) Viewed 92529 times</span></p> 
     </div> </li><li><span style="color:#000000;">Select "Show Smoothed", raise the smoothing amount to 0.99, hit the refresh button and then zoom in on last 5,000 - 10,000 iterations or so:</span> 
     <div> 
      <p> </p> 
      <p><span style="color:#000000;">analysis2.jpg (101.26 KiB) Viewed 92529 times</span></p> 
     </div> </li><li><span style="color:#000000;">Now that the graph is zoomed in, you should be able to tell if the loss is still dropping or whether it has "converged". Convergence is when the model is not learning anything any more. In this example you can see that, whilst at first look it may seem the model has converged, on closer inspection, loss is still dropping:</span> 
     <div> 
      <p> </p> 
      <p><span style="color:#000000;">analysis3.jpg (183.57 KiB) Viewed 92529 times</span></p> 
     </div> </li></ul></li></ul> 
 <br> 
 <span style="color:#000000;"><strong><a name="save">Stopping and Resuming</a></strong></span> 
 <hr> 
 <span style="color:#000000;">It is possible to stop training at any point just by pressing the <strong>Terminate</strong> button at the bottom left of the GUI. The model will save it's current state and exit.<br><br> Models can be resumed by selecting the same settings and pointing the "model dir" folder at the same location as the saved folder. This can be made much easier by saving your Faceswap config, either from the GUI File Menu or from the save icon below the options panel:</span> 
 <div> 
  <p> </p> 
  <p><span style="color:#000000;">save1.jpg (24.47 KiB) Viewed 92524 times</span></p> 
 </div> 
 <div> 
  <p> </p> 
  <p><span style="color:#000000;">save2.jpg (5.99 KiB) Viewed 92524 times</span></p> 
 </div> 
 <span style="color:#000000;">You can then just reload your config and continue training.<br><br> Faces can be added and removed from your Training folders, but make sure that you stop training before making any changes, and then resume again. If you are using Warp to Landmarks or training with a mask, you will need to make sure your alignments file is updated with the new and removed faces.<br><br><br><strong><a name="restore">Recovering a Corrupted Model</a></strong></span> 
 <hr> 
 <span style="color:#000000;">Occasionally models corrupt. This can be for any number of reasons, but is evidenced by all of the faces in the preview turning to a solid/garbled color and the loss values spiking to a high number and not recovering.<br><br> Faceswap offers a tool for easily recovering a model. Backups are saved every save iteration that the loss values fall overall. These backups can be recovered in the following way:</span> 
 <ul><li><span style="color:#000000;">Go to <strong>Tools</strong> &gt; <strong>Restore</strong>:</span> 
   <div> 
    <p> </p> 
    <p><span style="color:#000000;">restore0.png (4.61 KiB) Viewed 91703 times</span></p> 
   </div> </li><li><span style="color:#000000;"><strong>Model Dir</strong> - The folder containing your corrupted model should be entered here:</span> 
   <div> 
    <p> </p> 
    <p><span style="color:#000000;">restore1.png (1.34 KiB) Viewed 91703 times</span></p> 
   </div> </li></ul> 
 <span style="color:#000000;">Hit the <strong>Restore</strong> button. Once restored you should be able to carry on training from your last backup.</span> 
</div> 
<p>Attachments</p> 
<p> </p> 
<p>Mask2.png (30.68 KiB) Viewed 92549 times</p> 
<p> </p> 
<p>Mask1.png (29.67 KiB) Viewed 92549 times</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3cc7820baa6d336b6bb5c09995c7cbac/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">音频分离Spleeter的安装</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7c0ce39fef797bba475ddc7ca26eb734/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">严重 [http-nio-8080-exec-1] org.apache.catalina.core.ApplicationDispatcher.invoke Servlet.service() fo</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>