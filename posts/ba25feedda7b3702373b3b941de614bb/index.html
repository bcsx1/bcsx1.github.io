<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【图像分割 之 开山之作】 2015-FCN CVPR - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【图像分割 之 开山之作】 2015-FCN CVPR" />
<meta property="og:description" content="文章目录 【图像分割 之 开山之作】 2015-FCN CVPR1. 简介1.1 什么是语义分割？？？1.2 FCN 结果 2. 全卷积网络(3个创新点)2.0 传统的CNN在做语义分割存在的问题2.1 全卷积网络2.1.1 FCN结构2.1.2 全卷积怎么替换全连接 2.2 上采样Upsampling2.2.1 常见的上采样2.2.2 转置卷积 2.3 跳跃连接(原文在第4章)使用VGG16作为backboneFCN-16sFCN-8S 3. 分割网络3.1 训练过程3.2 评价指标和损失3.3 实验尝试 4. 优缺点5. 代码6. 常见问题简述什么是上采样？ 7. 不重要部分讲解7.1 patchwise(原本第3章)7.2 shift and stitch 【图像分割 之 开山之作】 2015-FCN CVPR 首个端对端的针对像素级预测的全卷积网络
FCN是一个end-to-end的网络，实现像素级别（pixel-wise）的分类，即对每一个像素点都可以进行分类，在语义分割任务上取得了很不错的效果！
FCN论文地址：https://arxiv.org/abs/1411.4038
FCN原作代码：https://github.com/shelhamer/fcn.berkeleyvision.org
本文链接
论文共分为6个部分
介绍相关工作全卷积网络分割网络结果结论 本次讲解具体讲介绍，全卷积网络，分割网络3个部分
1. 简介 首先介绍一下FCN的背景和工作成果
1.1 什么是语义分割？？？ 语义分割作为计算机视觉三大任务（图像分类、目标检测、图像分割）之一，图像分割已经在近些年里有了长足的发展。这项技术也被广泛地应用于无人驾驶领域中，比如用来识别可通行区域、车道线等。
全卷积网络（Fully Convolutional Networks，FCN）是UC Berkeley的Jonathan Long等人于2015年在Fully Convolutional Networks for Semantic Segmentation一文中提出的用于图像语义分割的一种框架。
看一下语义分割具体的效果
图像语义分割的意思就是机器自动分割并识别出图像中的内容，比如给出一个人骑摩托车的照片，机器判断后应当能够生成右侧图，红色标注为人，绿色是车（黑色表示back ground）。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/ba25feedda7b3702373b3b941de614bb/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-01T11:01:27+08:00" />
<meta property="article:modified_time" content="2022-07-01T11:01:27+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【图像分割 之 开山之作】 2015-FCN CVPR</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#___2015FCN_CVPR_4" rel="nofollow">【图像分割 之 开山之作】 2015-FCN CVPR</a></li><li><ul><li><a href="#1__27" rel="nofollow">1. 简介</a></li><li><ul><li><a href="#11__32" rel="nofollow">1.1 什么是语义分割？？？</a></li><li><a href="#12_FCN__48" rel="nofollow">1.2 FCN 结果</a></li></ul> 
   </li><li><a href="#2_3_66" rel="nofollow">2. 全卷积网络(3个创新点)</a></li><li><ul><li><a href="#20_CNN_75" rel="nofollow">2.0 传统的CNN在做语义分割存在的问题</a></li><li><a href="#21__99" rel="nofollow">2.1 全卷积网络</a></li><li><ul><li><a href="#211_FCN_108" rel="nofollow">2.1.1 FCN结构</a></li><li><a href="#212__132" rel="nofollow">2.1.2 全卷积怎么替换全连接</a></li></ul> 
    </li><li><a href="#22_Upsampling_142" rel="nofollow">2.2 上采样Upsampling</a></li><li><ul><li><a href="#221__156" rel="nofollow">2.2.1 常见的上采样</a></li><li><a href="#222__176" rel="nofollow">2.2.2 转置卷积</a></li></ul> 
    </li><li><a href="#23_4_210" rel="nofollow">2.3 跳跃连接(原文在第4章)</a></li><li><ul><li><a href="#VGG16backbone_218" rel="nofollow">使用VGG16作为backbone</a></li><li><a href="#FCN16s_242" rel="nofollow">FCN-16s</a></li><li><a href="#FCN8S_250" rel="nofollow">FCN-8S</a></li></ul> 
   </li></ul> 
   </li><li><a href="#3__264" rel="nofollow">3. 分割网络</a></li><li><ul><li><a href="#31__266" rel="nofollow">3.1 训练过程</a></li><li><a href="#32__317" rel="nofollow">3.2 评价指标和损失</a></li><li><a href="#33__331" rel="nofollow">3.3 实验尝试</a></li></ul> 
   </li><li><a href="#4__351" rel="nofollow">4. 优缺点</a></li><li><a href="#5__367" rel="nofollow">5. 代码</a></li><li><a href="#6__380" rel="nofollow">6. 常见问题简述</a></li><li><ul><li><a href="#_384" rel="nofollow">什么是上采样？</a></li></ul> 
   </li><li><a href="#7__454" rel="nofollow">7. 不重要部分讲解</a></li><li><ul><li><a href="#71_patchwise3_456" rel="nofollow">7.1 patchwise(原本第3章)</a></li><li><a href="#72_shift_and_stitch_462" rel="nofollow">7.2 shift and stitch</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="___2015FCN_CVPR_4"></a>【图像分割 之 开山之作】 2015-FCN CVPR</h2> 
<p><strong>首个端对端的针对像素级预测的全卷积网络</strong></p> 
<p>FCN是一个<code>end-to-end</code>的网络，实现<code>像素</code>级别（pixel-wise）的分类，即对每一个像素点都可以进行分类，在语义分割任务上取得了很不错的效果！</p> 
<blockquote> 
 <p>FCN论文地址：<a href="https://arxiv.org/abs/1411.4038" rel="nofollow">https://arxiv.org/abs/1411.4038</a></p> 
 <p>FCN原作代码：<a href="https://github.com/shelhamer/fcn.berkeleyvision.org">https://github.com/shelhamer/fcn.berkeleyvision.org</a></p> 
 <p><a href="https://gitee.com/fakerlove/cv" rel="nofollow">本文链接</a></p> 
</blockquote> 
<p>论文共分为6个部分</p> 
<ul><li>介绍</li><li>相关工作</li><li>全卷积网络</li><li>分割网络</li><li>结果</li><li>结论</li></ul> 
<p>本次讲解具体讲<code>介绍</code>，<code>全卷积网络</code>，<code>分割网络</code>3个部分</p> 
<h3><a id="1__27"></a>1. 简介</h3> 
<p>首先介绍一下FCN的背景和工作成果</p> 
<h4><a id="11__32"></a>1.1 什么是语义分割？？？</h4> 
<p>语义分割作为计算机视觉三大任务（图像分类、目标检测、图像分割）之一，图像分割已经在近些年里有了长足的发展。这项技术也被广泛地应用于无人驾驶领域中，比如用来识别可通行区域、车道线等。</p> 
<p>全卷积网络（Fully Convolutional Networks，FCN）是UC Berkeley的Jonathan Long等人于2015年在Fully Convolutional Networks for Semantic Segmentation一文中提出的用于图像语义分割的一种框架。</p> 
<p><strong>看一下语义分割具体的效果</strong></p> 
<p>图像语义分割的意思就是机器自动分割并识别出图像中的内容，比如给出一个人骑摩托车的照片，机器判断后应当能够生成右侧图，红色标注为人，绿色是车（黑色表示back ground）。</p> 
<p><img src="https://images2.imgbox.com/d2/c4/mHPywDbf_o.png" alt="img"></p> 
<h4><a id="12_FCN__48"></a>1.2 FCN 结果</h4> 
<p>接下来，我们直接看一下FCN的结果</p> 
<p>FCN虽然是一个简单的框架，但是结果已经十分接近真实值。效果十分的不错</p> 
<p><img src="https://images2.imgbox.com/94/81/OZBpyXSD_o.png" alt="img"></p> 
<blockquote> 
 <p>这个是在数据集上的一些成功。比当时的其他框架普遍高出10%，这是非常恐怖的事情</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/95/8a/708zhlUg_o.png" alt="image-20220407082851587"></p> 
<p><strong>相关工作就不看，直接看第3章</strong></p> 
<h3><a id="2_3_66"></a>2. 全卷积网络(3个创新点)</h3> 
<ul><li>全卷积网络,使用了卷积模块替换了全连接模块</li><li>上采样模块</li><li>跳跃链接</li><li>任意的input size，输出相应size的output</li></ul> 
<p>首先讲一下之间<code>传统CNN存在的问题</code></p> 
<h4><a id="20_CNN_75"></a>2.0 传统的CNN在做语义分割存在的问题</h4> 
<ul><li> <p>CNN做图像分类甚至做目标检测的效果已经被证明并广泛应用，图像语义分割本质上也可以认为是稠密的目标识别（需要预测每个像素点的类别）。</p> <p>对于一般的分类CNN网络，如VGG和Resnet，都会在网络的最后加入一些全连接层，经过softmax后就可以获得类别概率信息。但是这个概率信息是1维的，即只能标识整个图片的类别，不能标识每个像素点的类别，所以这种全连接方法不适用于图像分割。</p> </li><li> <p>传统的基于CNN的语义分割方法是：将像素周围一个小区域（如25*25）作为CNN输入，做训练和预测。这样做有3个问题</p> <p>这种方法有几个缺点：</p> 
  <ul><li>一是存储开销很大。例如对每个像素使用的图像块的大小为25x25，然后不断滑动窗口，每次滑动的窗口给CNN进行判别分类，因此则所需的存储空间根据滑动窗口的次数和大小急剧上升。</li><li>二是计算效率低下。相邻的像素块基本上是重复的，针对每个像素块逐个计算卷积，这种计算也有很大程度上的重复。</li><li>三是像素块大小的限制了感知区域的大小。通常像素块的大小比整幅图像的大小小很多，只能提取一些局部的特征，从而导致分类的性能受到限制。</li></ul> 
  <blockquote> 
   <p>总结一下就是以下几点</p> 
   <ul><li> <p>像素区域的大小如何确定</p> </li><li> <p>存储及计算量非常大</p> </li><li> <p>像素区域的大小限制了感受野的大小，从而只能提取一些局部特征</p> </li></ul> 
  </blockquote> </li></ul> 
<p>针对上面的问题，文章做出了<code>3个方面的创新</code></p> 
<h4><a id="21__99"></a>2.1 全卷积网络</h4> 
<p>第一步我们大致看一下 FCN的模型架构图</p> 
<p><img src="https://images2.imgbox.com/d7/8b/MT3eAnLc_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>一张猫和狗的图片，经过普通的卷积网络进行下采样，然后通道转置卷积进行上采样。最后输出分割图片</p> 
</blockquote> 
<h5><a id="211_FCN_108"></a>2.1.1 FCN结构</h5> 
<p>首先看一下普通的CNN图像分类网络</p> 
<p><img src="https://images2.imgbox.com/0d/5e/U6jQFccT_o.png" alt=""></p> 
<p>在CNN中, 猫的图片输入到AlexNet, 得到一个长为1000的输出向量, 表示输入图像属于每一类的概率, 其中在“tabby cat”这一类统计概率最高, 用来做分类任务。</p> 
<p>换句话讲为什么需要FCN？</p> 
<blockquote> 
 <p>我们分类使用的网络通常会在最后连接几层全连接层，它会将原来二维的矩阵（图片）压扁成一维的，从而<strong>丢失了空间信息</strong>，最后训练输出一个标量，这就是我们的分类标签。</p> 
 <p>而图像语义分割的输出需要是个分割图，且不论尺寸大小，但是至少是二维的。所以，我们需要丢弃全连接层，换上全卷积层，而这就是全卷积网络了。</p> 
</blockquote> 
<p>总结以下,全连接层丢失了空间信息，所以得使用卷积层代替了全连接层</p> 
<p><img src="https://images2.imgbox.com/20/8e/h5YoGN6E_o.png" alt=""></p> 
<p>总结以下就是以下这幅图</p> 
<p><img src="https://images2.imgbox.com/db/db/sMKlPyYp_o.png" alt="img"></p> 
<h5><a id="212__132"></a>2.1.2 全卷积怎么替换全连接</h5> 
<p>让我们来看一下怎么替换。上面是为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         7 
        
       
         ∗ 
        
       
         7 
        
       
         ∗ 
        
       
         512 
        
       
      
        7*7*512 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">7</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">7</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">5</span><span class="mord">1</span><span class="mord">2</span></span></span></span></span>的feature map 。通过平铺成25088的一维向量。然后和全连接进行运算。</p> 
<p>卷积是如何替换的呢？我们可以通过卷积核为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         7 
        
       
         ∗ 
        
       
         7 
        
       
      
        7*7 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">7</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">7</span></span></span></span></span>，步长为1,卷积核数目为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         4096 
        
       
      
        4096 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">4</span><span class="mord">0</span><span class="mord">9</span><span class="mord">6</span></span></span></span></span>，实现全连接层的效果。</p> 
<p>我们可以看到参数量是一样的。卷积完成后的数据也是一样的</p> 
<p><img src="https://images2.imgbox.com/22/92/1DDA5ZcU_o.png" alt="image-20220407095042213"></p> 
<h4><a id="22_Upsampling_142"></a>2.2 上采样Upsampling</h4> 
<p><img src="https://images2.imgbox.com/03/e1/SrfEC4HD_o.png" alt="img"></p> 
<p><strong>什么叫做上采样</strong></p> 
<p>看一下效果</p> 
<p><img src="https://images2.imgbox.com/66/64/d1dNgbIM_o.png" alt="image-20220403114904836"></p> 
<p>我们可以看到第一章图片的宽高为<code>200*200</code>。通过上采样图片的宽高变成<code>25*250</code>。通过上采样。图片维度变大，但是呢。图片还是差不多</p> 
<h5><a id="221__156"></a>2.2.1 常见的上采样</h5> 
<p>现在我们知道上采样操作，有几种</p> 
<ul><li> <p>转置卷积</p> <p>feature maps补0，然后做卷积操作</p> </li><li> <p>线性差值</p> <p>插值法不需要学习任何的参数，只是根据已知的像素点对未知的点进行预测估计，从而可以扩大图像的尺寸，达到上采样的效果</p> </li><li> <p>反池化unpooling</p> <p>在空隙中填充 0</p> </li><li> <p>unsampling</p> <p>在空隙中填充同样的值</p> </li></ul> 
<h5><a id="222__176"></a>2.2.2 转置卷积</h5> 
<p>这里解释一下转置卷积的内容，它是如何对图片进行上采样的.两张图解释一下</p> 
<p><img src="https://images2.imgbox.com/4f/bf/mBZDsoJV_o.gif" alt="这里写图片描述"></p> 
<p><img src="https://images2.imgbox.com/90/f0/9nkPIpIJ_o.png" alt=""></p> 
<p><strong>nn.ConvTranspose2d</strong></p> 
<pre><code class="prism language-python">nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> 
                   out_channels<span class="token punctuation">,</span> 
                   kernel_size<span class="token punctuation">,</span> 
                   stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> 
                   padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
                   output_padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> 
                   groups<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> 
                   bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> 
                   dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<p>参数详解</p> 
<blockquote> 
 <ul><li><code>padding</code>(int or tuple, optional) - 输入的每一条边补充0的层数，高宽都增加2*padding</li><li><code>output_padding</code>(int or tuple, optional) - 输出边补充0的层数，高宽都增加padding</li><li><code>strides</code>:步长。每次窗口滑动距离</li></ul> 
</blockquote> 
<h4><a id="23_4_210"></a>2.3 跳跃连接(原文在第4章)</h4> 
<p>VGG,GoogleNet,LeNet</p> 
<p>采用的是VGG16的架构</p> 
<blockquote> 
 <p><strong>作用</strong>：连接底层信息到高层信息，从而提升精度。</p> 
</blockquote> 
<h5><a id="VGG16backbone_218"></a>使用VGG16作为backbone</h5> 
<p><img src="https://images2.imgbox.com/7e/8b/IsGOkKW4_o.png" alt="image-20220407094520496"></p> 
<p><img src="https://images2.imgbox.com/58/02/Zc29iirI_o.png" alt=""></p> 
<p><strong>看完VGG16这个模型，才能理解这个跳跃链接的效果。因为VGG16中对应的池化</strong></p> 
<ol><li>image经过多个conv和+一个max pooling变为pool1 feature，宽高变为1/2</li><li>pool1 feature再经过多个conv+一个max pooling变为pool2 feature，宽高变为1/4</li><li>pool2 feature再经过多个conv+一个max pooling变为pool3 feature，宽高变为1/8</li><li>…</li><li>直到pool5 feature，宽高变为1/32。</li></ol> 
<p><img src="https://images2.imgbox.com/cf/8b/Pvjt9UqL_o.png" alt="img"></p> 
<p>对于FCN-32s，直接对pool5 feature进行32倍上采样获得32x upsampled feature，再对32x upsampled feature每个点做softmax prediction获得32x upsampled feature prediction（即分割图）。</p> 
<h5><a id="FCN16s_242"></a>FCN-16s</h5> 
<p>对于FCN-16s，首先对pool5 feature进行2倍上采样获得2x upsampled feature，再把pool4 feature和2x upsampled feature<strong>逐点相加</strong>，然后对相加的feature进行16倍上采样，并softmax prediction，获得16x upsampled feature prediction。</p> 
<p><img src="https://images2.imgbox.com/96/d2/ry5bqjig_o.png" alt="image-20220330201131575"></p> 
<h5><a id="FCN8S_250"></a>FCN-8S</h5> 
<p>对于FCN-8s，首先进行pool4+2x upsampled feature<strong>逐点相加</strong>，然后又进行pool3+2x upsampled<strong>逐点相加</strong>，即进行更多次特征融合。具体过程与16s类似，不再赘述。</p> 
<p><img src="https://images2.imgbox.com/e3/fb/hoe5uPCW_o.png" alt="image-20220330202200701"></p> 
<p>作者在原文种给出3种网络结果对比，明显可以看出效果：FCN-32s &lt; FCN-16s &lt; FCN-8s，即<strong>使用多层feature融合有利于提高分割准确性</strong>。</p> 
<p><img src="https://images2.imgbox.com/7c/15/kEgHOWZ2_o.png" alt="img"></p> 
<h3><a id="3__264"></a>3. 分割网络</h3> 
<h4><a id="31__266"></a>3.1 训练过程</h4> 
<p>下面这个为模型的全部架构图</p> 
<p><img src="https://images2.imgbox.com/b3/0c/YIFQYtVM_o.png" alt="img"></p> 
<p>训练过程分为四个阶段，也体现了作者的设计思路，值得研究。</p> 
<p>第1阶段</p> 
<p>以经典的分类网络为初始化。最后两级是全连接（红色），参数弃去不用</p> 
<p><img src="https://images2.imgbox.com/34/9d/jzVf4cfQ_o.png" alt=""></p> 
<p>第2阶段</p> 
<p>从特征小图（<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         16 
        
       
         ∗ 
        
       
         16 
        
       
         ∗ 
        
       
         4096 
        
       
      
        16*16*4096 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord">6</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord">6</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">4</span><span class="mord">0</span><span class="mord">9</span><span class="mord">6</span></span></span></span></span>）预测分割小图（<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         16 
        
       
         ∗ 
        
       
         16 
        
       
         ∗ 
        
       
         21 
        
       
      
        16*16*21 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord">6</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord">6</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">2</span><span class="mord">1</span></span></span></span></span>），之后直接升采样为大图。</p> 
<p>反卷积（橙色）的步长为32，这个网络称为FCN-32s。</p> 
<p>这一阶段使用单GPU训练约需3天。</p> 
<p>1/32的图进行上采样-&gt;1/16 +maxpool4 (1/16) -&gt; 上采样</p> 
<p><img src="https://images2.imgbox.com/7f/a3/XkTA5l0F_o.png" alt=""></p> 
<p>第3阶段</p> 
<p>升采样分为两次完成（橙色×2）。<br> 在第二次升采样前，把第4个pooling层（绿色）的预测结果（蓝色）融合进来。使用跳级结构提升精确性。<br> 第二次反卷积步长为16，这个网络称为FCN-16s。<br> 这一阶段使用单GPU训练约需1天。</p> 
<p><img src="https://images2.imgbox.com/9f/a7/oRaF2MdC_o.png" alt=""></p> 
<p>第4阶段<br> 升采样分为三次完成（橙色×3）。<br> 进一步融合了第3个pooling层的预测结果。<br> 第三次反卷积步长为8，记为FCN-8s。<br> 这一阶段使用单GPU训练约需1天。</p> 
<p><img src="https://images2.imgbox.com/e7/ff/Eme5uYLz_o.png" alt=""></p> 
<h4><a id="32__317"></a>3.2 评价指标和损失</h4> 
<p><img src="https://images2.imgbox.com/dc/f8/VE23mnpu_o.png" alt=""></p> 
<p>fcn网络的输入batchsize是1，因为分割loss的计算在每一个像素点都一个真值（标签），相当于每一个像素点的都是一个分类任务，一个图像就有对应像素点个样本。所以分割任务的batch是一个图片，将一个图片最后在所有像素点上的分类loss加起来计算一次梯度的更新。</p> 
<p>评价指标使用了miou</p> 
<h4><a id="33__331"></a>3.3 实验尝试</h4> 
<p>作者还做了很多实验来尝试提升网络性能，很多都没有效果。这部分挺有意思的。</p> 
<ul><li> <p>作者尝试了减少最后一层的池化层的步长到1，可是这导致池化层后面的卷积层卷积核感受野很大。这样增大了计算量，这么大感受野的卷积核很难学习。作者也试过了用更小的过滤器来调整池化层上面的层，可是效果也不好。</p> </li><li> <p>作者也尝试了 shift and stitch trick，但是代价太大，性价比还不如层融合。得不偿失。</p> </li><li> <p>添加了一些训练数据，提升了mIOU</p> </li><li> <p>尝试了patch sampling，块抽样，发现相比于全图训练并没有带来更快地收敛。</p> <p><img src="https://images2.imgbox.com/04/a3/62Cvnfr3_o.png" alt=""></p> </li><li> <p>尝试通过修改损失值的权重或者采样loss来实现平衡训练集（class balance），发现虽然数据集的类很不平衡，其中四分之三都是背景，但是类平衡并不能带来性能的提升。</p> </li><li> <p>作者还尝试了数据增强，也并没有带来显著地提升。</p> </li></ul> 
<h3><a id="4__351"></a>4. 优缺点</h3> 
<p>传统 CNN 有几个缺点：</p> 
<ol><li>存储开销大，滑动窗口较大，每个窗口都需要存储空间来保存特征和判别类别，而且使用全连接结构，最后几层将近指数级存储递增</li><li>计算效率低，大量重复计算</li><li>滑动窗口大小是相对独立的，末端使用全连接只能约束局部特征。</li></ol> 
<p>为了解决上面的部分问题，FCN 将传统 CNN 中的全连接层转化成卷积层，对应 CNN 网络 FCN 把最后三层全连接层转换成为三层卷积层（4096，4096，1000）。</p> 
<p><strong>FCN 的缺点</strong>：</p> 
<ol><li>分割的结果不够精细。图像过于模糊或平滑，没有分割出目标图像的细节</li><li>因为模型是基于CNN改进而来，即便是用卷积替换了全连接，但是依然是独立像素进行分类，没有充分考虑像素与像素之间的关系</li><li>也是因为每个模型都有自己的不足，所以才会出现各式各样的模型来解决它们的问题。大家如果能分析出每个模型的缺点，那我们也可以构建新的模型去完善他们的模型，这其实就是创新的过程</li></ol> 
<h3><a id="5__367"></a>5. 代码</h3> 
<p><a href="https://www.kaggle.com/code/jokerak/fcn-voc" rel="nofollow">FCN实现VOC数据集训练 | Kaggle</a></p> 
<ul><li>准备数据集</li><li>加载数据集</li><li>实现网络</li><li>进行训练</li></ul> 
<p><img src="https://images2.imgbox.com/0e/51/EUBz9utZ_o.png" alt=""></p> 
<h3><a id="6__380"></a>6. 常见问题简述</h3> 
<h4><a id="_384"></a>什么是上采样？</h4> 
<p>说了半天，到底什么是上采样？</p> 
<p><strong>实际上，上采样（upsampling）一般包括2种方式：</strong></p> 
<ol><li><strong>Resize，如双线性插值直接缩放，类似于图像缩放（这种方法在原文中提到）</strong></li><li><strong>Deconvolution，也叫Transposed Convolution</strong></li></ol> 
<p>什么是Resize就不多说了，这里解释一下Deconvolution。</p> 
<p>对于一般卷积，输入蓝色4x4矩阵，卷积核大小3x3。当设置卷积参数pad=0，stride=1时，卷积输出绿色2x2矩阵，如图6。</p> 
<p><img src="https://images2.imgbox.com/85/3c/97n3qs1n_o.png" alt="img"></p> 
<p>而对于反卷积，相当于把普通卷积反过来，输入蓝色2x2矩阵（周围填0变成6x6），卷积核大小还是3x3。当设置反卷积参数pad=0，stride=1时输出绿色4x4矩阵，如图7，这相当于完全将图4倒过来（<a href="https://link.zhihu.com/?target=https%3A//github.com/vdumoulin/conv_arithmetic" rel="nofollow">其他更多卷积示意图点这里</a>）。</p> 
<p><img src="https://images2.imgbox.com/e8/1c/TNWpAzvc_o.png" alt="img"></p> 
<p>图7 Deconvolution forward示意图</p> 
<p>传统的网络是subsampling的，对应的输出尺寸会降低；<strong>upsampling的意义在于将小尺寸的高维度feature map恢复回去</strong>，以便做pixelwise prediction，获得每个点的分类信息。</p> 
<p><img src="https://images2.imgbox.com/41/0a/ncmpNzPR_o.png" alt="img"></p> 
<p>上采样在FCN网络中的作用如图8，明显可以看到经过上采样后恢复了较大的pixelwise feature map（其中最后一个层21-dim是因为PACSAL数据集有20个类别+Background）。这其实相当于一个Encode-Decode的过程。</p> 
<p><strong>具体的FCN网络结构，可以在<a href="https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/pascalcontext-fcn8s/train.prototxt">fcn caffe prototext</a>中查到，建议使用[Netscope](<a href="http://ethereon.github.io/netscope/#/editor" rel="nofollow">Netscope (ethereon.github.io)</a>)查看网络结构。这里解释里面的难点：</strong></p> 
<p>为了解决图像过小后 1/32 下采样后输出feature map太小情况，FCN原作者在第一个卷积层conv1_1加入pad=100。</p> 
<pre><code class="prism language-json">layer <span class="token punctuation">{<!-- --></span>
  <span class="token literal-property property">name</span><span class="token operator">:</span> <span class="token string">"conv1_1"</span>
  <span class="token literal-property property">type</span><span class="token operator">:</span> <span class="token string">"Convolution"</span>
  <span class="token literal-property property">bottom</span><span class="token operator">:</span> <span class="token string">"data"</span>
  <span class="token literal-property property">top</span><span class="token operator">:</span> <span class="token string">"conv1_1"</span>
  param <span class="token punctuation">{<!-- --></span>
    <span class="token literal-property property">lr_mult</span><span class="token operator">:</span> <span class="token number">1</span>
    <span class="token literal-property property">decay_mult</span><span class="token operator">:</span> <span class="token number">1</span>
  <span class="token punctuation">}</span>
  param <span class="token punctuation">{<!-- --></span>
    <span class="token literal-property property">lr_mult</span><span class="token operator">:</span> <span class="token number">2</span>
    <span class="token literal-property property">decay_mult</span><span class="token operator">:</span> <span class="token number">0</span>
  <span class="token punctuation">}</span>
  convolution_param <span class="token punctuation">{<!-- --></span>
    <span class="token literal-property property">num_output</span><span class="token operator">:</span> <span class="token number">64</span>
    <span class="token literal-property property">pad</span><span class="token operator">:</span> <span class="token number">100</span>             # pad<span class="token operator">=</span><span class="token number">100</span>
    <span class="token literal-property property">kernel_size</span><span class="token operator">:</span> <span class="token number">3</span>
    <span class="token literal-property property">stride</span><span class="token operator">:</span> <span class="token number">1</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>考虑如果不在conv1_1加入pad=100，会发生什么？</p> 
<p>假设输入图像高度为h。由于VGG中缩小输出feature map只在pooling层，经过每个pooling后输出高度变为：</p> 
<h3><a id="7__454"></a>7. 不重要部分讲解</h3> 
<h4><a id="71_patchwise3_456"></a>7.1 patchwise(原本第3章)</h4> 
<p>简单翻译：由于一副需要语义分割的图片中有很多我们不关注的区域，因此直接输入全卷积网络会有很多的多余计算，避免冗余的一个标准方法是只输入给网络图像中的随机小块（围绕感兴趣目标的小图像区域）而不是整图像，这样的“patchwise sampling”保证输入有足够的方差和训练集的有效表示。 可以从训练集中进行小块采样，或者直接对整图的损失进行采样，所以有这个说法“Patchwise training is loss sampling”，本文[fcn]后来实验发现patchwise training 比起直接训练整幅图 并没有大的提升，但是训练花费的时间更多了，因此本文是整幅图训练。</p> 
<h4><a id="72_shift_and_stitch_462"></a>7.2 shift and stitch</h4> 
<p>参考链接<a href="https://zhuanlan.zhihu.com/p/56035377" rel="nofollow">Shift and stitch理解 - 知乎 (zhihu.com)</a></p> 
<p>在Semantic Segmentation中，由于CNN网络的下采样，使得输出是coarse的，要想得到 pixel级别的dense prediction，其中一种方法是shift and stitch，介绍如下。</p> 
<blockquote> 
 <p>网上的关于shift-and-stitch的解释：</p> 
 <p>设原图与FCN所得输出图之间的降采样因子是f，那么对于原图的每个fxf 的区域（不重叠），“</p> 
 <p>shift the input x pixels to the right and y pixels down for every (x,y) ,0 &lt; x,y &lt; f."</p> 
 <p>把这个f x f区域对应的output作为此时区域中心点像素对应的output，</p> 
 <p>这样就对每个 fxf 的区域得到了 f2 个output，也就是每个像素都能对应一个output，所以成为了dense prediction</p> 
</blockquote> 
<p>假设降采样因子为<code>s</code>，那么output map（这里为了简单起见，仅考虑二维）的spatial size则是 input的 <code>1/s</code>， 平移 input map， 向左或向上（向右向下情况一样），偏移量为<code>(x,y)</code>， 其中，</p> 
<p>参考链接</p> 
<blockquote> 
 <p><a href="https://zhuanlan.zhihu.com/p/31428783" rel="nofollow">图像语义分割入门+FCN/U-Net网络解析 - 知乎 (zhihu.com)</a></p> 
 <p>https://blog.csdn.net/m0_37935211/article/details/83098997</p> 
 <p><a href="https://blog.csdn.net/qq_36269513/article/details/80420363">FCN的学习及理解（Fully Convolutional Networks for Semantic Segmentation）_moonuke的博客-CSDN博客_fcn</a></p> 
 <p><a href="https://blog.csdn.net/orange_littlegirl/article/details/80732058">深度学习语义分割(一)FCN论文解读_su扬帆启航的博客-CSDN博客_fcn论文解读</a></p> 
</blockquote>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/37334ade158a5bcd9d039852847a7e76/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">react从18降低版本到17的踩坑记</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ef61568bb20b704668916d8137a6562e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">企业级安全架构</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>