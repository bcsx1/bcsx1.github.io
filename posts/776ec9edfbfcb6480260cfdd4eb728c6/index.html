<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>史上最全 | BEV感知算法综述（基于图像/Lidar/多模态数据的3D检测与分割任务）... - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="史上最全 | BEV感知算法综述（基于图像/Lidar/多模态数据的3D检测与分割任务）..." />
<meta property="og:description" content="点击下方卡片，关注“自动驾驶之心”公众号
ADAS巨卷干货，即可获取
点击进入→自动驾驶之心技术交流群
后台回复【BEV综述】获取论文！
后台回复【ECCV2022】获取ECCV2022所有自动驾驶方向论文！
1摘要 以视觉为中心的俯视图（BEV）感知最近受到了广泛的关注，因其可以自然地呈现自然场景且对融合更友好。随着深度学习的快速发展，许多新颖的方法尝试解决以视觉为中心的BEV感知，但是目前还缺乏对该领域的综述类文章。本文对以视觉为中心的BEV感知及其扩展的方法进行了全面的综述调研，并提供了深入的分析和结果比较，进一步思考未来可能的研究方向。如下图所示，目前的工作可以根据视角变换分为两大类，即基于几何变换和基于网络变换。前者利用相机的物理原理，以可解释性的方式转换视图。后者则使用神经网络将透视图（PV）投影到BEV上。
2BEV感知任务 以视觉为中心的BEV感知指的是基于多个视角的图像序列，算法需要将这些透视图转换为BEV特征并进行感知，如输出物体的3D检测框或俯视图下的语义分割。相比于LiDAR，视觉感知的语义信息更丰富，但缺少准确的深度测量。
最常见的BEV感知便是3D检测，根据输入数据模态的不同，又可以划分为以下三种：
基于图像；
基于LiDAR；
基于多模态。
另一种常见感知是BEV分割：
地图分割（Map Segmentation）;
车道线分割。
3基于几何变换的PV2BEV 传统方案直接利用几何投影将透视图转换为BEV。进一步可以划分为基于同形异体和基于深度的PV2BEV。前者包括简化几何关系的早期工作或仅关注地面感知的近期工作；后者则更适用于实际场景。
基于同形异体的PV2BEV 3D空间中的点可以通过透视映射转换到图像空间，反之则存在困难。逆透视映射（IPM）[1]则解决了上述问题。IPM是将前视图转换为俯视图的开创性工作。该变换利用相机旋转单应性和各向异性缩放[34]，其中单应矩阵可以由相机的内外参推导出来。一些工作[35]使用CNN提取PV图像的语义特征，并估计图像中的垂直消影点和地平面消影线（地平线）来确定单应矩阵。经过IPM操作转换到BEV后，进一步可以进行如光流估计/检测/分割/运动预测/规划等下游任务。VPOE[36]使用Yolov3[37]作为检测主干来估计BEV下的车辆位置和方向。在实际应用中，相机的内外参可能无法获取，TrafCam3D[39] 基于双视角的网络结构提出了一种鲁棒的单应映射，来缓解IPM失真的问题。
但由于IPM极度依赖地面假设（flat-ground assumption，即所有点都在地面上的假设），所以基于IPM的方法通常无法准确检测位于地平面上方的物体，如建筑物、车辆和行人。一些方法利用丰富的语义信息缓解上述问题。OGMs[40]将PV中车辆的历史轨迹转换到BEV上，以遵循单应性的地面假设，进而避免了车身位于地面之上造成的失真。遵循这个想法，BEVStitch[41] 使用双分支来分割车辆和道路的轨迹，并分别通过IPM将它们转换为BEV，之后拼接为完整的BEV路线图。
和上述在预处理或后处理阶段使用IPM不同，也有一些方法将IPM融入到网络的训练中。Cam2BEV[44] 使用 IPM转换每个视角的特征图，从多个车载摄像头获取给定图像的整体BEV语义图。MVNet[45]基于IPM将2D特征投影到共享的BEV空间中，以融合多视角特征，并使用大卷积核来解决行人检测中的遮挡问题。
由于正视图和鸟瞰图之间的差距较大且变形严重，仅使用IPM不足以在BEV中获取无失真的图像或语义图。BridgeGAN[48]将单应视角作为中间视角，并提出了一种基于多个GAN网络的模型来学习PV和BEV之间的交叉视角转换。利用生成式对抗网络（GAN）[49]来增强生成的BEV图像的真实性。
总结 基于同形异体的方法依赖物理映射实现PV2BEV，可解释性强。IPM在下游感知任务的图像投影或特征投影中发挥作用。为了减少失真，语义信息、GAN都被得到了较广泛的应用以提升BEV特征的质量。核心映射通过矩阵乘法进行，不需要学习，简单明朗。但由于PV到BEV的真正转换并不适定（ill-posed），IPM仅通过硬假设解决了部分问题，PV整个特征图的有效BEV映射仍然有待验证和挖掘。下图对相关算法进行了概括总结。
基于深度的PV2BEV 很明显，基于IPM的方法丢失了重要的高度信息。既然降维不行，那么将2D升维到3D便可以解决这个问题。而深度信息可以将2D像素或者特征提升到3D空间中，基于此，基于深度预测的PV2BEV也是一大研究热点。受此启发，本文首先比较这些工作的方法设计，包括视角变换的方法，是否包含深度监督，以及如何与IPM结合等等。
基于点的视角变换 基于深度的PV2BEV方法建立在明确的3D表示上，如上图所示。与基于LiDAR的3D检测一样，可以划分为基于点的方法和基于体素的方法。基于点的方法直接使用深度估计将像素转换为点云，并散射到连续的3D空间中。这种方式更直接、更容易的集成单目深度估计和基于LiDAR的3D检测的成熟经验。开创性的工作Pseudo-LiDAR [53]，如下图所示，首先将深度图转换为伪LiDAR点，然后将其输入到基于LiDAR的3D检测器中。Pseudo-LiDAR&#43;&#43; [54]通过立体深度估计网络和损失函数提高了深度精度。AM3D [55]提出用互补的RGB特征来装饰伪点云。但是，上述方法存在两个通病：
数据泄露问题，验证集和测试集之间的性能差距是由于包含KITTI深度基准的数据泄露造成的[53、57]。
泛化问题，并且由于两个网络之间的梯度截断，在训练和部署期间可能会很复杂。
E2E Pseudo-LiDAR [58] 提出了一个Change-of-Representation（CoR）模块，实现了模型的端到端训练。尽管还有若干工作尝试解决这两个问题，但这种类型的方法本质上不如基于体素的方法，特别是对于大型室外场景。
基于体素的视角变换 与分布在连续3D空间中的点云相比，体素通过将3D空间离散化以构建用于特征变换的规则结构，从而为3D场景理解提供更有效的表示；之后的基于BEV的感知模块也可以直接附加。尽管它牺牲了局部的空间精度，但仍然在覆盖大规模场景结构信息方面更有效，并且与用于视角转换的端到端学习范式兼容。
具体来说，该方案基于深度信息直接在相应的3D位置投射对应的2D特征（而不是点）。先前的工作通过将2D特征图与对应的深度预测结果进行外积来实现。早期的工作假设分布是均匀的，即沿一条射线的所有特征都是相同的，如上图所示[59]。OFT[59]建立了一个内部表示，以确定图像中的哪些特征与鸟瞰图上的位置有关。它在均匀的3D栅格中构建了一个体素特征图，并通过投影对应图像特征来累积填充体素。之后，通过沿y轴对体素特征求和得到正交特征图，后面使用CNN提取BEV特征进行3D目标检测。值得注意的是，对于图像上的每个像素，网络为其分配相同的3D预测表示，即预测深度上的均匀分布。这类方法通常不需要深度监督，并且可以通过端到端的方式在视角转换后学习网络中的深度或3D位置信息。
相比之下，另一种范式显示预测深度分布并用来构建3D特征。如下图所示，LSS[60]预测深度上的分类分布和上下文向量，他们的外积可以确定沿透视射线的每个点的特征，这些特征接近真实的深度分布。此外，其将所有相机的预测结果融到一个场景下，从而减小校准误差。BEVDet [62]遵循了LSS范式，并提出了一个用于从BEV进行多视角相机3D检测的框架，该框架由图像视角编码器、视角转换器、BEV编码器和检测头组成。后续的BEVDet4D [63]在基于多摄像头的3D检测中利用了时间线索。
深度监督 先前的研究表明，当使用预测的深度分布来提升2D特征时，该分布的准确性十分重要。CaDDN[61]（上图b）使用经典方法对从投影LiDAR点派生的稀疏深度图进行插值，并利用它们来监督深度。基于双目3D检测的其他方法，DSGN[64]和LIGA-Stereo[65]（下图）也依赖于类似的监督，实验表明稀疏的LiDAR深度图更有效。其他不使用深度标签的工作只能从稀疏的实例标注中学习这种3D定位或深度信息，这对于网络学习来说要困难得多。除此之外，DD3D[66]和 MV-FCOS3D&#43;&#43;[67]指出深度估计和单目3D检测的预训练可以显著增强2D主干的特征表达能力。
与基于IPM的方法相结合 从上面的分析可知，基于IPM的方法适用于平坦的地面场景，并且只需要学习很少的参数便可高效运行。不依赖显示的深度预测和监督的方法适用于沿垂直方向的特征聚合。PanopticSeg[68]利用这两个优点，提出了一个用于全景分割的dense transformer模块，该模块由一个基于IPM的flat transformer和一个对3D空间建模的vertical transformer组成，然后进行错误纠正以生成平面BEV特征。
立体匹配的多视角聚合 除了单目深度估计，立体匹配可以在只有相机的情况下获取更准确的深度信息。其中，最常见的是双目立体匹配，其具有较大的重叠区域和较小的水平偏移。为了比较，之前的工作[60，62]使用多视角配置，例如，环视摄像头安装在自动驾驶汽车上，相邻视角之间的重叠区域通常非常小，这是因为环视相机的主要目标是使用更少的相机覆盖整个空间。在这种情况下，深度估计主要依赖单目，而基于BEV的方法仅在多视角感知的简单性和统一性方面具有优势。
相比之下，双目配置下的深度估计具有更重要的优点。最近的双目算法，如DSGN[64]和LIGA-Stereo[65]，通常使用平面扫描表示进行立体匹配和深度估计。然后从平面扫描特征中对体素和BEV特征进行采样，并进行3D检测。针对多视角设置的其他方法，例如ImVoxelNet[69]，也展示了这种基于体素的方式在室内场景中的有效性，其重叠区域在相邻区域中也更大。
总结 总结来说，基于深度的视角变换方法通常建立在显式3D表示、体素量化或连续3D空间中散射的点云之上。基于体素的方法使用统一的深度向量或显式预测的深度分布将2D特征提升到3D体素空间，并进行BEV感知。相比之下，基于点的方法将深度预测转换为伪LiDAR表示，然后使用自定义网络进行3D检测。下表展示了相关算法的结果：
由上表总结可知：
早期的方法利用伪LiDAR表示在第二阶段使用3D检测器，非端到端的训练方式导致了模型的复杂性并限制了性能；
由于计算效率和灵活性，最近的工作更多关注在基于体素的方法上。这种表示已被广泛应用于不同任务的仅相机方法中。
深度监督对于基于深度的方法很重要，因为在将透视图特征转换为鸟瞰图时，准确的深度分布可以提供必要的信息。
如DfM[70]、BEVDet4D[63]和MVFCOS3D&#43;&#43;[67] 中分析的那样，探索此类方法在时间建模中的潜在优势在未来有很大的研究价值。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/776ec9edfbfcb6480260cfdd4eb728c6/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-08-09T07:30:39+08:00" />
<meta property="article:modified_time" content="2022-08-09T07:30:39+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">史上最全 | BEV感知算法综述（基于图像/Lidar/多模态数据的3D检测与分割任务）...</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p style="text-align:center;">点击下方<strong>卡片</strong>，关注“<strong>自动驾驶之心</strong>”公众号</p> 
 <p style="text-align:center;">ADAS巨卷干货，即可获取</p> 
 <p style="text-align:justify;"><strong>点击进入</strong>→<a href="" rel="nofollow"><strong>自动驾驶之心技术交流群</strong></a></p> 
 <p style="text-align:justify;">后台回复【BEV综述】获取论文！</p> 
 <p style="text-align:justify;">后台回复【ECCV2022】获取ECCV2022所有自动驾驶方向论文！</p> 
 <h3>1摘要</h3> 
 <p style="text-align:justify;">以视觉为中心的俯视图（BEV）感知最近受到了广泛的关注，因其可以自然地呈现自然场景且对融合更友好。随着深度学习的快速发展，许多新颖的方法尝试解决以视觉为中心的BEV感知，但是目前还缺乏对该领域的综述类文章。本文对以视觉为中心的BEV感知及其扩展的方法进行了全面的综述调研，并提供了深入的分析和结果比较，进一步思考未来可能的研究方向。如下图所示，目前的工作可以根据视角变换分为两大类，即基于几何变换和基于网络变换。前者利用相机的物理原理，以可解释性的方式转换视图。后者则使用神经网络将透视图（PV）投影到BEV上。</p> 
 <img src="https://images2.imgbox.com/87/6b/kOqS4NhW_o.png" alt="66bbe4c116c07131a56cc6a445295685.png"> 
 <h3>2BEV感知任务</h3> 
 <p style="text-align:justify;">以视觉为中心的BEV感知指的是基于多个视角的图像序列，算法需要将这些透视图转换为BEV特征并进行感知，如输出物体的3D检测框或俯视图下的语义分割。相比于LiDAR，视觉感知的语义信息更丰富，但缺少准确的深度测量。</p> 
 <p style="text-align:justify;">最常见的BEV感知便是3D检测，根据输入数据模态的不同，又可以划分为以下三种：</p> 
 <ol><li><p style="text-align:justify;">基于图像；</p></li><li><p style="text-align:justify;">基于LiDAR；</p></li><li><p style="text-align:justify;">基于多模态。</p></li></ol> 
 <p style="text-align:justify;">另一种常见感知是BEV分割：</p> 
 <ol><li><p style="text-align:justify;">地图分割（Map Segmentation）;</p></li><li><p style="text-align:justify;">车道线分割。</p></li></ol> 
 <h3>3基于几何变换的PV2BEV</h3> 
 <p style="text-align:justify;">传统方案直接利用几何投影将透视图转换为BEV。进一步可以划分为基于同形异体和基于深度的PV2BEV。前者包括简化几何关系的早期工作或仅关注地面感知的近期工作；后者则更适用于实际场景。</p> 
 <h4>基于同形异体的PV2BEV</h4> 
 <p style="text-align:justify;">3D空间中的点可以通过透视映射转换到图像空间，反之则存在困难。逆透视映射（IPM）[1]则解决了上述问题。IPM是将前视图转换为俯视图的开创性工作。该变换利用相机旋转单应性和各向异性缩放[34]，其中单应矩阵可以由相机的内外参推导出来。一些工作[35]使用CNN提取PV图像的语义特征，并估计图像中的垂直消影点和地平面消影线（地平线）来确定单应矩阵。经过IPM操作转换到BEV后，进一步可以进行如光流估计/检测/分割/运动预测/规划等下游任务。VPOE[36]使用Yolov3[37]作为检测主干来估计BEV下的车辆位置和方向。在实际应用中，相机的内外参可能无法获取，TrafCam3D[39] 基于双视角的网络结构提出了一种鲁棒的单应映射，来缓解IPM失真的问题。</p> 
 <p style="text-align:justify;">但由于IPM极度依赖地面假设（flat-ground assumption，即所有点都在地面上的假设），所以基于IPM的方法通常无法准确检测位于地平面上方的物体，如建筑物、车辆和行人。一些方法利用丰富的语义信息缓解上述问题。OGMs[40]将PV中车辆的历史轨迹转换到BEV上，以遵循单应性的地面假设，进而避免了车身位于地面之上造成的失真。遵循这个想法，BEVStitch[41] 使用双分支来分割车辆和道路的轨迹，并分别通过IPM将它们转换为BEV，之后拼接为完整的BEV路线图。</p> 
 <p style="text-align:justify;">和上述在预处理或后处理阶段使用IPM不同，也有一些方法将IPM融入到网络的训练中。Cam2BEV[44] 使用 IPM转换每个视角的特征图，从多个车载摄像头获取给定图像的整体BEV语义图。MVNet[45]基于IPM将2D特征投影到共享的BEV空间中，以融合多视角特征，并使用大卷积核来解决行人检测中的遮挡问题。</p> 
 <p style="text-align:justify;">由于正视图和鸟瞰图之间的差距较大且变形严重，仅使用IPM不足以在BEV中获取无失真的图像或语义图。BridgeGAN[48]将单应视角作为中间视角，并提出了一种基于多个GAN网络的模型来学习PV和BEV之间的交叉视角转换。利用生成式对抗网络（GAN）[49]来增强生成的BEV图像的真实性。</p> 
 <h5>总结</h5> 
 <p style="text-align:justify;">基于同形异体的方法依赖物理映射实现PV2BEV，可解释性强。IPM在下游感知任务的图像投影或特征投影中发挥作用。为了减少失真，语义信息、GAN都被得到了较广泛的应用以提升BEV特征的质量。核心映射通过矩阵乘法进行，不需要学习，简单明朗。但由于PV到BEV的真正转换并不适定（ill-posed），IPM仅通过硬假设解决了部分问题，PV整个特征图的有效BEV映射仍然有待验证和挖掘。下图对相关算法进行了概括总结。</p> 
 <img src="https://images2.imgbox.com/46/44/PlW4w3Ro_o.png" alt="1025341473be0926d9c148d40c518715.png"> 
 <h4>基于深度的PV2BEV</h4> 
 <p style="text-align:justify;">很明显，基于IPM的方法丢失了重要的高度信息。既然降维不行，那么将2D升维到3D便可以解决这个问题。而深度信息可以将2D像素或者特征提升到3D空间中，基于此，基于深度预测的PV2BEV也是一大研究热点。受此启发，本文首先比较这些工作的方法设计，包括视角变换的方法，是否包含深度监督，以及如何与IPM结合等等。</p> 
 <img src="https://images2.imgbox.com/f4/75/XmD2cVcf_o.png" alt="dc1de22e7a1648de2f44673289c8b37a.png"> 
 <h5>基于点的视角变换</h5> 
 <p style="text-align:justify;">基于深度的PV2BEV方法建立在明确的3D表示上，如上图所示。与基于LiDAR的3D检测一样，可以划分为基于点的方法和基于体素的方法。基于点的方法直接使用深度估计将像素转换为点云，并散射到连续的3D空间中。这种方式更直接、更容易的集成单目深度估计和基于LiDAR的3D检测的成熟经验。开创性的工作Pseudo-LiDAR [53]，如下图所示，首先将深度图转换为伪LiDAR点，然后将其输入到基于LiDAR的3D检测器中。Pseudo-LiDAR++ [54]通过立体深度估计网络和损失函数提高了深度精度。AM3D [55]提出用互补的RGB特征来装饰伪点云。但是，上述方法存在两个通病：</p> 
 <ul><li><p style="text-align:justify;">数据泄露问题，验证集和测试集之间的性能差距是由于包含KITTI深度基准的数据泄露造成的[53、57]。</p></li><li><p style="text-align:justify;">泛化问题，并且由于两个网络之间的梯度截断，在训练和部署期间可能会很复杂。</p></li></ul> 
 <p style="text-align:justify;">E2E Pseudo-LiDAR [58] 提出了一个Change-of-Representation（CoR）模块，实现了模型的端到端训练。尽管还有若干工作尝试解决这两个问题，但这种类型的方法本质上不如基于体素的方法，特别是对于大型室外场景。</p> 
 <img src="https://images2.imgbox.com/88/9b/5IWM2la5_o.png" alt="c6d394e13c995168c8e85c21cfbd04cf.png"> 
 <h5>基于体素的视角变换</h5> 
 <p style="text-align:justify;">与分布在连续3D空间中的点云相比，体素通过将3D空间离散化以构建用于特征变换的规则结构，从而为3D场景理解提供更有效的表示；之后的基于BEV的感知模块也可以直接附加。尽管它牺牲了局部的空间精度，但仍然在覆盖大规模场景结构信息方面更有效，并且与用于视角转换的端到端学习范式兼容。</p> 
 <img src="https://images2.imgbox.com/9d/1e/LbupdI7y_o.png" alt="79d2ced2611338a71f7d35e3453ca503.png"> 
 <p style="text-align:justify;">具体来说，该方案基于深度信息直接在相应的3D位置投射对应的2D特征（而不是点）。先前的工作通过将2D特征图与对应的深度预测结果进行外积来实现。早期的工作假设分布是均匀的，即沿一条射线的所有特征都是相同的，如上图所示[59]。OFT[59]建立了一个内部表示，以确定图像中的哪些特征与鸟瞰图上的位置有关。它在均匀的3D栅格中构建了一个体素特征图，并通过投影对应图像特征来累积填充体素。之后，通过沿y轴对体素特征求和得到正交特征图，后面使用CNN提取BEV特征进行3D目标检测。值得注意的是，对于图像上的每个像素，网络为其分配相同的3D预测表示，即预测深度上的均匀分布。这类方法通常不需要深度监督，并且可以通过端到端的方式在视角转换后学习网络中的深度或3D位置信息。</p> 
 <p style="text-align:justify;">相比之下，另一种范式显示预测深度分布并用来构建3D特征。如下图所示，LSS[60]预测深度上的分类分布和上下文向量，他们的外积可以确定沿透视射线的每个点的特征，这些特征接近真实的深度分布。此外，其将所有相机的预测结果融到一个场景下，从而减小校准误差。BEVDet [62]遵循了LSS范式，并提出了一个用于从BEV进行多视角相机3D检测的框架，该框架由图像视角编码器、视角转换器、BEV编码器和检测头组成。后续的BEVDet4D [63]在基于多摄像头的3D检测中利用了时间线索。</p> 
 <img src="https://images2.imgbox.com/a8/05/fagg9UCy_o.png" alt="dba0163bf1ab0fe9047a7b4e05035503.png"> 
 <h5>深度监督</h5> 
 <p style="text-align:justify;">先前的研究表明，当使用预测的深度分布来提升2D特征时，该分布的准确性十分重要。CaDDN[61]（上图b）使用经典方法对从投影LiDAR点派生的稀疏深度图进行插值，并利用它们来监督深度。基于双目3D检测的其他方法，DSGN[64]和LIGA-Stereo[65]（下图）也依赖于类似的监督，实验表明稀疏的LiDAR深度图更有效。其他不使用深度标签的工作只能从稀疏的实例标注中学习这种3D定位或深度信息，这对于网络学习来说要困难得多。除此之外，DD3D[66]和 MV-FCOS3D++[67]指出深度估计和单目3D检测的预训练可以显著增强2D主干的特征表达能力。</p> 
 <img src="https://images2.imgbox.com/f1/8b/fO3YAfce_o.png" alt="dad9c9de93222e612e39eadec29a425c.png"> 
 <h5>与基于IPM的方法相结合</h5> 
 <p style="text-align:justify;">从上面的分析可知，基于IPM的方法适用于平坦的地面场景，并且只需要学习很少的参数便可高效运行。不依赖显示的深度预测和监督的方法适用于沿垂直方向的特征聚合。PanopticSeg[68]利用这两个优点，提出了一个用于全景分割的dense transformer模块，该模块由一个基于IPM的flat transformer和一个对3D空间建模的vertical transformer组成，然后进行错误纠正以生成平面BEV特征。</p> 
 <h5>立体匹配的多视角聚合</h5> 
 <p style="text-align:justify;">除了单目深度估计，立体匹配可以在只有相机的情况下获取更准确的深度信息。其中，最常见的是双目立体匹配，其具有较大的重叠区域和较小的水平偏移。为了比较，之前的工作[60，62]使用多视角配置，例如，环视摄像头安装在自动驾驶汽车上，相邻视角之间的重叠区域通常非常小，这是因为环视相机的主要目标是使用更少的相机覆盖整个空间。在这种情况下，深度估计主要依赖单目，而基于BEV的方法仅在多视角感知的简单性和统一性方面具有优势。</p> 
 <p style="text-align:justify;">相比之下，双目配置下的深度估计具有更重要的优点。最近的双目算法，如DSGN[64]和LIGA-Stereo[65]，通常使用平面扫描表示进行立体匹配和深度估计。然后从平面扫描特征中对体素和BEV特征进行采样，并进行3D检测。针对多视角设置的其他方法，例如ImVoxelNet[69]，也展示了这种基于体素的方式在室内场景中的有效性，其重叠区域在相邻区域中也更大。</p> 
 <h5>总结</h5> 
 <p style="text-align:justify;">总结来说，基于深度的视角变换方法通常建立在显式3D表示、体素量化或连续3D空间中散射的点云之上。基于体素的方法使用统一的深度向量或显式预测的深度分布将2D特征提升到3D体素空间，并进行BEV感知。相比之下，基于点的方法将深度预测转换为伪LiDAR表示，然后使用自定义网络进行3D检测。下表展示了相关算法的结果：<img src="https://images2.imgbox.com/24/be/X0Hdbk20_o.png" alt="1453d3fccbfebc4cd1733be4ec34778b.png"></p> 
 <p style="text-align:justify;">由上表总结可知：</p> 
 <ul><li><p style="text-align:justify;">早期的方法利用伪LiDAR表示在第二阶段使用3D检测器，非端到端的训练方式导致了模型的复杂性并限制了性能；</p></li><li><p style="text-align:justify;">由于计算效率和灵活性，最近的工作更多关注在基于体素的方法上。这种表示已被广泛应用于不同任务的仅相机方法中。</p></li><li><p style="text-align:justify;">深度监督对于基于深度的方法很重要，因为在将透视图特征转换为鸟瞰图时，准确的深度分布可以提供必要的信息。</p></li><li><p style="text-align:justify;">如DfM[70]、BEVDet4D[63]和MVFCOS3D++[67] 中分析的那样，探索此类方法在时间建模中的潜在优势在未来有很大的研究价值。</p></li></ul> 
 <h3>4基于网络的PV2BEV</h3> 
 <p style="text-align:justify;">另一种方式是以数据驱动的方式对视角变换进行建模，且隐式利用相机几何关系，使用CNN建模PV和BEV间的映射关系。为了涵盖复杂的变化，例如单应性，MLP和Transformer是基于网络的PV2BEV的常见方法。</p> 
 <h4>基于MLP的PV2BEV</h4> 
 <p style="text-align:justify;">多层感知机（MLP）在某种程度上可以看作是一个复杂的映射函数，并且已经取得了令人瞩目的成就。为了避免相机校准中的误差，一些方法使用MLP来学习相机校准的隐式表示，进而实现透视图和鸟瞰图之间的转换，如下图所示。</p> 
 <img src="https://images2.imgbox.com/78/9e/ydjhdG1J_o.png" alt="03c303a93688d47a18978a8ce5d847c3.png"> 
 <p style="text-align:justify;">总结来说，基于MLP的方法不需要使用相机校准的几何先验知识，并利用MLP作为映射函数来模拟从透视图到鸟瞰图的转换。尽管MLP在理论上可以实现转换功能，但由于缺乏深度信息和遮挡等原因，视角的转换仍然很困难。此外多视角图像通常的做法是各自转换后再进行后融合，这在一定程度上阻碍了基于MLP的方法利用重叠区域带来的几何潜力。下表列出了基于MLP的PV2BEV的结果，可以发现：</p> 
 <ul><li><p style="text-align:justify;">基于 MLP 的方法更关注单幅图像，而多视角融合仍然有待探索；</p></li><li><p style="text-align:justify;">基于MLP的方法已经被基于Transformer的方法超越。</p></li></ul> 
 <img src="https://images2.imgbox.com/a0/bc/jpFG6T6g_o.png" alt="088648c022803da89801713ef9a9e4de.png"> 
 <h4>基于Transformer的PV2BEV</h4> 
 <p style="text-align:justify;">除了MLP，带有交叉注意力的Transformer也可以实现PV2BEV，并且无需显式利用相机模型，下图展示了基于Transformer类方法的发展历程。基于MLP和基于Transformer的方法主要有以下三个区别：</p> 
 <ul><li><p style="text-align:justify;">首先，由于在推理阶段权重是固定的，所以MLP学习到的映射不依赖数据；相比之下，transformer中的交叉注意力依赖数据，而权重依赖于输入数据。这种数据依赖的属性使得transformer表达能力更强，但难以训练；</p></li><li><p style="text-align:justify;">其次，交叉注意力是permutation-invariant的（即不依赖输入的空间位置关系），也就是说transformer需要位置编码来区分输入的顺序，MLP则对空间的位置关系比较敏感；</p></li><li><p style="text-align:justify;">最后，基于Transformer的方法采用自上而下的策略，通过注意力机制构建查询并搜索相应的图像特征，这一点也与MLP不同。</p></li></ul> 
 <img src="https://images2.imgbox.com/c2/31/7Ogfq1sj_o.png" alt="ba629860bbeb2c32987dbe571ea7f412.png"> 
 <p style="text-align:justify;">Tesla[79]是第一个使用Transformer将透视图投影到BEV上的。该方法首先使用位置编码设计一组BEV queries，然后通过BEV queries和图像特征之间的交叉注意力进行视角转换。后续工作大都沿用这种范式。根据Transformer解码器中query的粒度，本文进一步细分为三类：基于稀疏query、基于密集query和基于混合query。</p> 
 <h5>基于稀疏Query的方法</h5> 
 <p style="text-align:justify;">query embeddings使网络直接输出稀疏感知结果，而无须显式进行图像特征的密集变换。这种设计对以目标为中心的感知任务（如3D目标检测）十分自然，但扩展到密集感知任务上（如分割）却非易事，下图展示了基于稀疏query的几种算法框架。</p> 
 <img src="https://images2.imgbox.com/44/95/ktK9fd4v_o.png" alt="7fed06b3d200c56f83ebd86a8991c935.png"> 
 <h5>基于密集Query的方法</h5> 
 <p style="text-align:justify;">对于基于密集query的方法，每个query都预先分配了3D空间或BEV空间中的空间位置。query个数由栅格化空间的空间分辨率决定，一般大于基于稀疏query方法中的query个数。密集的BEV表示可以通过密集query和图像特征间的交互来实现，进而用于3D检测、分割和运动预测等多个下游任务。</p> 
 <p style="text-align:justify;">Tesla[79]首先使用位置编码和context summary在BEV空间中生成密集的BEV query，然后使用query和多视角图像特征间的交叉注意力进行视角转换。在不考虑相机参数的情况下进行BEV query和图像特征之间的vanilla cross attention。Image2Map[21] 提出了一种单目BEV分割框架，使用3D几何约束简化了基于交叉注意力的交互从而减少了内存的消耗，如下图所示。</p> 
 <img src="https://images2.imgbox.com/cd/59/qZXzPpCu_o.png" alt="61805680309804a180bba21ac63f0f53.png"> 
 <h5>基于混合Query的方法</h5> 
 <p style="text-align:justify;">基于稀疏query的方法不适用于密集BEV任务，因此PETRv2[24]设计了一种混合query策略，使用密集分割query负责16×16大小的patch分割。</p> 
 <h5>稀疏Query vs. 密集Query</h5> 
 <p style="text-align:justify;">尽管基于稀疏query的方法在目标检测任务上取得了不错的成果，但其3D表示没有几何结构意义，因此难以实现密集预测任务。相比之下，具有显式空间分布的密集query为BEV空间提供了密集且统一的表示，可以很容易嵌入到不同的感知头中。然而，密集query的计算成本太大，因此需要提高注意力机制的效率以实现高分辨率特征图的感知。在过去几年中，高效Transformer结构[95]、[96]、[97]也获得了广泛的关注。但是这些工作一般聚焦在self attention[98]上，key和query是一样的。它们在交叉注意力中的有效性，如果key和query来自两个未对齐的集合，这方面仍然缺乏探索。</p> 
 <h5>几何线索</h5> 
 <p style="text-align:justify;">从概念上讲，基于Transformer的PV2BEV方法可以只依靠注意力机制实现视角转换，不一定需要几何先验知识。早期的方法[93]确实没有将任何几何信息（例如校准矩阵或深度信息）引入到transformer结构中。然而，排列不变性使得Transformer无法区分图像区域和BEV像素间的空间关系，从而使得网络收敛速度慢且依赖大量数据。目前有更多的方法试图利用3D几何约束加速模型收敛、提高数据效率。</p> 
 <ul><li><p style="text-align:justify;">校准矩阵：给定query的3D坐标（预定义或者使用query特征预测），相机校准矩阵定义了从BEV空间到图像平面的映射，反之亦然，为视觉特征和query交互提供基础。因此，校准矩阵以多种方式应用到基于Transformer的方法中。</p></li><li><p style="text-align:justify;">深度信息：虽然基于Transformer的PV2BEV方法不一定需要每个像素的深度来进行视角转换，但深度信息对于Transformer的几何推理仍然很重要。在nuScenes目标检测任务上，很多基于Transformer的方法都使用了深度预训练模型[10、66]。</p></li></ul> 
 <h5>总结</h5> 
 <p style="text-align:justify;">总结来说，基于Transformer的方法建模能力强、性能高且数据依赖性强，近两年发展的很快。除了作为视角转换，Transformer还可以作为通用主干或者检测头来代替anchor-based或anchor-free的方法。并且基于Transformer解码器的检测头使用二分匹配策略训练模型，已经摒弃了NMS等后处理步骤，目前已经普遍应用到3D检测任务中。</p> 
 <p style="text-align:justify;">nuScenes数据集是用于以视觉为中心的感知最常用的数据集，具有覆盖360度水平FOV的六个校准摄像头。下表列出了基于Transformer的相关算法在nuScenes检测和分割任务上的结果。<img src="https://images2.imgbox.com/39/de/3BREha3W_o.png" alt="30844dd72a0960b68410b2fe89132ea0.png"></p> 
 <img src="https://images2.imgbox.com/1f/86/Z2migV9s_o.png" alt="f7d9df8d37c6ee7ea07800d7525fad94.png"> 
 <p style="text-align:justify;">可以看出：</p> 
 <ul><li><p style="text-align:justify;">当考虑密集感知任务（例如道路分割）时，通常采用密集query，因为基于稀疏query的方法没有BEV空间的显式表示；</p></li><li><p style="text-align:justify;">正如在基于深度的视角变换方法中所提到的，时间信息对于基于Transformer的方法也很关键。具有时间融合的方法[24]、[28]、[77]在mAP和mAVE上的性能通常明显优于单帧方法。</p></li><li><p style="text-align:justify;">由于每个相机的感知范围是一个带有根轴的楔形，因此一些工作提议用基于非垂直轴的极坐标系[28]、[87]代替基于垂直轴的笛卡尔坐标系，如下图所示，未来也会是一个比较有意思的研究方向。</p></li></ul> 
 <img src="https://images2.imgbox.com/32/bd/1wU6mi9F_o.png" alt="a55b4ec85007921dac9a1031aebf8c52.png"> 
 <h3>5扩展</h3> 
 <p style="text-align:justify;">交通场景的BEV表示，包括精确的定位和比例信息，可以准确的映射到真实的物理世界中，这对很多下游任务有帮助。同时BEV还可以充当物理介质，为多传感器、时间戳等提供可解释的融合方式。下面介绍BEV下的两个主要扩展。</p> 
 <h4>BEV下的多任务学习</h4> 
 <p style="text-align:justify;">许多工作尝试用一个网络输出多个任务结果，虽然有一些工作证明了CNN可以在多个相关任务的联合优化中受益，但本文观察到3D目标检测和BEV分割的联合训练通常不会有增益，如下表所示，检测会掉点且不同类别的分割性能提升差异较大。未来可以探索不同感知任务的依赖关系，以互相促进。</p> 
 <img src="https://images2.imgbox.com/63/00/4hwUsBt0_o.png" alt="8b8cdeaefe185e289b6d32b3502b2eec.png"> 
 <h4>BEV下的融合</h4> 
 <p style="text-align:justify;"><strong>多模态融合</strong>：目前自动驾驶汽车最常见的三种传感器：摄像头、激光雷达和雷达。不同传感器的优缺点如下表所示。图像具有更丰富的颜色、纹理、边缘等外观特征，但对光照敏感，缺乏深度信息。LiDAR点云包含准确的深度信息和丰富的几何特征，但缺少纹理信息。雷达的感知范围比激光雷达更长，可以直接捕捉运动物体的速度，但点云极其稀疏和嘈杂，难以提取形状和尺度的视觉特征。未来多传感器融合将是很重要的研究方法，集成各个传感器的优点实现安全、高效的感知将是重中之重。但是，由于原始数据的表征存在巨大差距，多模态融合还有很长的路要走。</p> 
 <img src="https://images2.imgbox.com/6f/eb/Ri5VaTIY_o.png" alt="350ce404a9da9322e9e92b935293085c.png"> 
 <p style="text-align:justify;"><strong>时间融合</strong>：除了多模态融合，时间融合是稳定、可靠的感知系统的另一个关键组成部分。首先，由于累积了历史结果，可以减轻由相机的视角依赖特性引发的自遮挡和外部遮挡影响，有助于检测严重遮挡的物体并生成可靠的路线图。其次，时间线索是估计物体时间属性的必要条件，如速度、加速度、转向等等，有利于类别分类和运动预测。最后，虽然单幅图像的深度估计十分困难，但由连续图像形成的立体几何为绝对深度估计提供了重要的指导和深度研究的理论基础。如下表所示，将模型从3D空间提升到4D空间显著提升了整体检测性能，尤其对于速度和方向的预测。然而大多数模型只使用4帧信息，长期历史信息就被忽略了。</p> 
 <img src="https://images2.imgbox.com/c8/41/guIKUDF5_o.png" alt="bbe6fd323c95880863cf8c25e3ffa52f.png"> 
 <p style="text-align:justify;"><strong>多智能体融合</strong>：大多数工作都是基于单车智能，在处理遮挡和检测完整交通场景中的远处物体还存在一定困难。Vehicle-to-Vehicle（V2V）通信技术可以一定程度克服上述问题。CoBEVT [94]设计了一个多智能体多摄像头的感知框架，可以协同生成BEV地图预测。但是目前还没有可用的具有多智能体的真实世界数据集，所提出的框架仅在合成数据集上验证[125]，真实世界的泛化能力有待进一步探索。</p> 
 <h4>经验诀窍</h4> 
 <p style="text-align:justify;">以视觉为中心的感知方法通常涉及多种数据模式并在类不平衡的数据集上进行实验，因此需要各种保证几何关系的数据增强方法和具有较少注释的类别的训练技巧。此外，性能和效率的权衡也是一个重要问题。</p> 
 <h5>感知分辨率</h5> 
 <p style="text-align:justify;">近年来，显卡算力发展地很快，透视图分辨率和鸟瞰图网格尺寸显著增加。如下表所示，分辨率增加可以显著提升算法性能，但也会影响推理速度。尽管这些基于BEV的方法在nuScenes上取得了不错的结果，即使接近基于 LiDAR 的方法，高输入分辨率带来的高计算负担在部署上还存在一定壁垒，值得进一步探索。</p> 
 <img src="https://images2.imgbox.com/9d/ed/sqCqGvze_o.png" alt="445c072f4a5b59594ce163276bb2facd.png"> 
 <h5>模型设计</h5> 
 <p style="text-align:justify;">检测性能的另一个关键因素是使用不同的主干和检测头。最近很多工作[61]、[65]、[67]、[81] 提到，这种方法通常缺乏足够的语义监督来理解透视图。所以，很多方法[62]、[63]、[77]、[81]、[82] 使用经过基于单目方法的 3D检测[10]、[11]或深度估计[66] 预训练的PV主干。使用额外深度数据预训练的IA大型主干网络可以显著提升3D检测性能。检测头方面，除了激光雷达检测中使用的传统anchor-free、anchor-based和CenterPoint头外，基于Transformer的方法通常是端到端的。</p> 
 <h5>辅助任务</h5> 
 <p style="text-align:justify;">除了深度估计、单目2D/3D检测和2D车道线检测等经典的辅助任务之外，一些工作还设计了从跨模态中提取知识的方法，例如单目从立体学习[127]和立体从激光雷达学习[128]等设置。然而，这种新趋势仍然集中在小数据集的实验上，需要在大规模数据集上进一步验证和开发，大量的训练数据可能会削弱这种训练方法的好处。</p> 
 <p style="text-align:justify;">由于在训练过程中可以利用各种数据模式，例如图像、视频和 LiDAR 点云，为更好的表示学习设计辅助任务也成为最近研究的热点问题。除了深度估计 [61]、[65]、单目 2D 及 3D 检测 [11]、[71] 和 2D 车道检测 [31] 等经典辅助任务之外，一些工作还设计了从跨模态中提取知识的方案 诸如单目从立体学习[127]和立体从激光雷达学习[128]等设置。然而，这种新趋势仍然集中在小数据集的实验上，需要在大规模数据集上进一步验证和开发，大量的训练数据可能会削弱这种训练方法的好处。</p> 
 <h5>训练细节</h5> 
 <p style="text-align:justify;">最后，本文列出了几个重要的训练细节。首先，大多数都涉及视角转换和不同的模态，因此数据增强可以应用于透视图和BEV网格。最近的方法一般使用三种类型的数据增强：grid mask增强、图像数据增强和BEV数据增强，如下表所示。一些类别不平衡的问题仍然值得进一步探索。</p> 
 <img src="https://images2.imgbox.com/93/02/s3Ji9Iye_o.png" alt="11f2b1eb81d5c739f94dd1cf40b69ed0.png"> 
 <h3>6参考</h3> 
 <p style="text-align:left;">[1] Vision-Centric BEV Perception: A Survey</p> 
 <p style="text-align:center;">【<strong>自动驾驶之心</strong>】全栈技术交流群</p> 
 <p style="text-align:justify;"><strong>自动驾驶之心是首个自动驾驶开发者社区，聚焦目标检测、语义分割、全景分割、实例分割、关键点检测、车道线、目标跟踪、3D感知、多传感器融合、SLAM、高精地图、规划控制、AI模型部署落地等方向；</strong></p> 
 <p style="text-align:justify;"><strong>加入我们：</strong><a href="" rel="nofollow">自动驾驶之心技术交流群汇总！</a></p> 
 <p style="text-align:center;">自动驾驶之心【知识星球】</p> 
 <p style="text-align:justify;">想要了解更多自动驾驶感知（分类、检测、分割、关键点、车道线、3D感知、多传感器融合、目标跟踪）、自动驾驶定位建图（SLAM、高精地图）、自动驾驶规划控制、领域技术方案、AI模型部署落地实战、行业动态、岗位发布，欢迎扫描下方二维码，加入自动驾驶之心知识星球（三天内无条件退款），日常分享论文+代码，这里汇聚行业和学术界大佬，前沿技术方向尽在掌握中，期待交流！</p> 
 <p style="text-align:justify;"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/f5/eb/1OHseY6R_o.png" alt="94cef3d8797b96ac0aaacbe4b4007426.jpeg"></p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/32aeca26d81bf4620118a20365f4f6a9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Java FileWriter类的简介说明</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f15655e4e261ee91b12334a58e206c76/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">获取时间间隔的几种方法[c&#43;&#43; &amp; c#]</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>