<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>实例分割最全综述（上）：二阶段实例分割和一阶段实例分割 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="实例分割最全综述（上）：二阶段实例分割和一阶段实例分割" />
<meta property="og:description" content="作者 | fresher
原文链接：https://zhuanlan.zhihu.com/p/533568152
点击下方卡片，关注“自动驾驶之心”公众号
ADAS巨卷干货，即可获取
1. 实例分割简介 实例分割是结合目标检测和语义分割的一个更高层级的任务。
目标检测：区分出不同实例，用box进行目标定位；
语义分割：区分出不同类别，用mask进行标记；
实例分割：区分出不同实例，用mask进行标记；
因此：
实例分割需要在目标检测的基础上用更精细的mask进行定位，而非bbox；
实例分割需要在语义分割的基础上区分开同类别不同实例的mask；
纵观实例分割的算法发展也是遵循这两条路线：
一类是基于目标检测的自上而下的方案：首先通过目标检测定位出每个实例所在的box，进而对box内部进行语义分割得到每个实例的mask；
另一类是基于语义分割的自下而上的方案：首先通过语义分割进行逐像素分类，进而通过聚类或其他度量学习手段区分开同类的不同实例；
考虑到基于语义分割的的自下而上的实例分割算法（如Semantic Instance Segmentation with a Discriminative Loss Function，Deep Watershed Transform for Instance Segmentation）通常后处理步骤繁琐，且效果较差，本文主要探讨的是基于目标检测的自上而下的实例分割算法。
本文按照所采用的目标检测网络将实例分割算法划分为二阶段实例分割，一阶段实例分割，Query-based 实例分割三大类进行介绍。除此之外，按照对实例分割mask的表征方式的不同，介绍 Contour-based 和 Boundary Refinement 实例分割。
除此之外，一些方法（如：FCIS，TensorMask，deepmask，AdaptIS，MEInst）也都是非常经典的工作，本文由于篇幅原因，并未涉及。
2. 二阶段实例分割 2.1 Mask R-CNN Mask R-CNN是典型的自上而下的实例分割算法，其扩展自目标检测网络Faster R-CNN，在其基础上新增了mask预测分支。
Faster RCNN包含两个阶段, 第一个阶段, 是RPN结构, 用于生成RoI集合。第二个阶段利用RoI pooling从RoI中提出固定尺寸的特征, 然后进行class分类任务和box offset回归任务。
Mask RCNN使用了相同的two-stage结构, 第一阶段使用了相同的RPN网络。第二阶段, 利用RoI Align从RoI中提出固定尺寸的特征, 在执行class分类和box offset 回归任务的同时，MaskRCNN还会给每个RoI生成对应的二值mask；
因此，Mask R-CNN的主要工作集中在RoI Align和mask分支的设计，以下分别进行介绍：
(1) RoI Align 由于RPN阶段得到的proposals box的大小是不一样的，而为了进行后续的class分类，box回归和mask分割任务，必须有一种操作将不同尺寸的box特征图归化到相同的空间尺寸以方便进行batch运算，RoI Pooling和RoI Align的作用就是如此。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/5ccb6a15119f2df999f010378eeb1d49/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-23T11:30:51+08:00" />
<meta property="article:modified_time" content="2022-07-23T11:30:51+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">实例分割最全综述（上）：二阶段实例分割和一阶段实例分割</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <blockquote> 
  <p>作者 | fresher</p> 
  <p>原文链接：https://zhuanlan.zhihu.com/p/533568152</p> 
 </blockquote> 
 <p style="text-align:center;">点击下方<strong>卡片</strong>，关注“<strong>自动驾驶之心</strong>”公众号</p> 
 <p style="text-align:center;">ADAS巨卷干货，即可获取</p> 
 <h2>1. 实例分割简介</h2> 
 <p>实例分割是结合目标检测和语义分割的一个更高层级的任务。</p> 
 <ul><li><p style="text-align:left;">目标检测：区分出不同实例，用box进行目标定位；</p></li><li><p style="text-align:left;">语义分割：区分出不同类别，用mask进行标记；</p></li><li><p style="text-align:left;">实例分割：区分出不同实例，用mask进行标记；</p></li></ul> 
 <p>因此：</p> 
 <ul><li><p style="text-align:left;">实例分割需要在目标检测的基础上用更精细的mask进行定位，而非bbox；</p></li><li><p style="text-align:left;">实例分割需要在语义分割的基础上区分开同类别不同实例的mask；</p></li></ul> 
 <p>纵观实例分割的算法发展也是遵循这两条路线：</p> 
 <ul><li><p style="text-align:left;">一类是基于目标检测的自上而下的方案：首先通过目标检测定位出每个实例所在的box，进而对box内部进行语义分割得到每个实例的mask；</p></li><li><p style="text-align:left;">另一类是基于语义分割的自下而上的方案：首先通过语义分割进行逐像素分类，进而通过聚类或其他度量学习手段区分开同类的不同实例；</p></li></ul> 
 <p>考虑到基于语义分割的的自下而上的实例分割算法（如Semantic Instance Segmentation with a Discriminative Loss Function，Deep Watershed Transform for Instance Segmentation）通常后处理步骤繁琐，且效果较差，本文主要探讨的是基于目标检测的自上而下的实例分割算法。</p> 
 <p>本文按照所采用的目标检测网络将实例分割算法划分为二阶段实例分割，一阶段实例分割，Query-based 实例分割三大类进行介绍。除此之外，按照对实例分割mask的表征方式的不同，介绍 Contour-based 和 Boundary Refinement 实例分割。</p> 
 <p>除此之外，一些方法（如：FCIS，TensorMask，deepmask，AdaptIS，MEInst）也都是非常经典的工作，本文由于篇幅原因，并未涉及。</p> 
 <h2>2. 二阶段实例分割</h2> 
 <h3>2.1 Mask R-CNN</h3> 
 <p><img src="https://images2.imgbox.com/b6/e9/rs6T5FDY_o.png" alt="e29f656dd729f55d607e34736f048088.png"></p> 
 <p>Mask R-CNN是典型的自上而下的实例分割算法，其扩展自目标检测网络Faster R-CNN，在其基础上新增了mask预测分支。</p> 
 <ul><li><p style="text-align:left;">Faster RCNN包含两个阶段, 第一个阶段, 是RPN结构, 用于生成RoI集合。第二个阶段利用RoI pooling从RoI中提出固定尺寸的特征, 然后进行class分类任务和box offset回归任务。</p></li><li><p style="text-align:left;">Mask RCNN使用了相同的two-stage结构, 第一阶段使用了相同的RPN网络。第二阶段, 利用RoI Align从RoI中提出固定尺寸的特征, 在执行class分类和box offset 回归任务的同时，MaskRCNN还会给每个RoI生成对应的二值mask；</p></li></ul> 
 <p>因此，Mask R-CNN的主要工作集中在RoI Align和mask分支的设计，以下分别进行介绍：</p> 
 <h4>(1) RoI Align</h4> 
 <p>由于RPN阶段得到的proposals box的大小是不一样的，而为了进行后续的class分类，box回归和mask分割任务，必须有一种操作将不同尺寸的box特征图归化到相同的空间尺寸以方便进行batch运算，RoI Pooling和RoI Align的作用就是如此。</p> 
 <p>RoI Pooling存在两次量化过程:</p> 
 <ul><li><p style="text-align:left;">将box量化为整数坐标值；</p></li><li><p style="text-align:left;">将量化后的box区域分割成 <em>k*k</em> 个bins, 并对每一个bins的边界量化为整数；</p></li></ul> 
 <p>RoI Pooling这种粗糙的量化方式对于Mask R-CNN新增的mask分支会产生很大的量化误差，因此，提出了RoI Align方法，取消RoI Pooling中涉及的两次量化操作, 使用双线性插值的方法获得坐标为浮点数的像素点上的图像数值，其具体流程如下:</p> 
 <ul><li><p style="text-align:left;">遍历每一个候选区域, 保持浮点数边界不做量化；</p></li><li><p style="text-align:left;">将候选区域分割成 <em>k*k</em> 个bins, 每个bins的边界也不做量化；</p></li><li><p style="text-align:left;">在每个bins中sample四个point，使用双线性插值的方法计算出这四个位置的值, 然后取最大；</p></li></ul> 
 <h4>(2) Mask 分支</h4> 
 <p><img src="https://images2.imgbox.com/6b/64/c5uKVBrC_o.png" alt="c8b3be12b1eedade5faf7a76bf9e49ac.png"></p> 
 <p>由于mask分支需要对目标进行精细的mask预测，因此，mask分支采用比分类和回归分支更高的特征分辨率。具体来讲，将经过RPN阶段筛选出来的RoI经过RoI Align层将其尺寸统一为14X14，然后使用4层全卷积层和一层反卷积层以及最终的分类层为每个RoI预测一个28X28的mask。</p> 
 <h4>(3) 总结</h4> 
 <p>Mask R-CNN提出一种极简的思路来进行实例分割，借助目标检测领域的成熟发展，可以通过简单的替换Mask R-CNN的Faster R-CNN为更优的检测器便可以稳定提升实例分割的指标。</p> 
 <p>但是，RoI Align为了将不同尺寸的RoI统一到相同的[公式]尺寸进行batch运算，会导致大目标的空间特征出现信息损失，特别是在边缘部分，导致大目标特别是轮廓复杂的大目标的边缘预测效果较差。</p> 
 <p><img src="https://images2.imgbox.com/3c/09/2Ubu2fu7_o.png" alt="6976f39cf93ec1a71d580076108e9c36.png"></p> 
 <h3>2.2 Cascade Mask R-CNN</h3> 
 <p>Cascade Mask R-CNN 也是探讨如何在检测器Cascade R-CNN基础上设计mask分支。</p> 
 <h4>(1) Motivation</h4> 
 <p>Cascade R-CNN主要针对 Faster R-CNN 中 R-CNN 部分采用单一 IoU 阈值进行正负样本选取会产生训练和测试过程中的不匹配的问题。具体来讲：</p> 
 <ul><li><p style="text-align:left;">training阶段，RPN网络提出了2000左右的proposals，这些proposals在送入R-CNN结构前，需要首先计算每个Proposals和GT之间的IoU，并通过一个IoU阈值（如0.5）把这些Proposals分为正样本和负样本，并对这些正负样本按一定比例采样，进而送入R-CNN进行class分类和box回归。</p></li><li><p style="text-align:left;">inference阶段，RPN网络提出了300左右的proposals，这些proposals被送直接入到R-CNN结构中，因为没有GT用于采样。</p></li></ul> 
 <p>因此，此处所描述的不匹配问题就在于：</p> 
 <ul><li><p style="text-align:left;">training阶段的输入proposals质量更高(被采样过，IoU&gt;threshold)</p></li><li><p style="text-align:left;">inference阶段的输入proposals质量相对较差（没有被采样过，可能包括很多IoU&lt;threshold的）。</p></li></ul> 
 <p>通常threshold取0.5时，mismatch问题还不会很严重。而为了得到更精准的box，最直接的办法就是提高正样本的IoU阈值，那此时的mismatch问题就会更加严重。同时，提高IoU阈值会导致满足阈值条件的proposals比之前少了许多，极容易产生过拟合。</p> 
 <p><img src="https://images2.imgbox.com/1d/b6/5UPa1IYb_o.png" alt="85b715961b26e4a59d441607b1a51677.png"></p> 
 <h4>(2) Box级联</h4> 
 <p>上述的实验表明，由于采用单一的IoU阈值会产生mismatch的问题，因此，无法通过一味提升IoU阈值来获取更精准的box预测。本文提出一种级联结构，并在不同层逐渐增大IoU阈值，来缓解mismatch问题并产生更精确的box预测。</p> 
 <p style="text-align:left;"><img src="https://images2.imgbox.com/83/76/513kyKKH_o.png" alt="eb11c913c520483093ed6dfa14d04118.png"></p> 
 <p>Cascade R-CNN的结构如上图所示，首先在不同的stage采用不同的IoU阈值，同时不同stage也采用不同的H（H表示R-CNN Head）。</p> 
 <h5>1) 递增IoU阈值</h5> 
 <p>下图中，横轴 Input IoU 表示 RPN 输出的 proposal 与 gt bbox 的 IoU，纵轴 Output IoU 是经过 R-CNN 的 box 分支回归输出后与gt bbox 的 IoU，不同线条代表不同阈值训练出来的 detector。可以发现，Input IoU 在 0.55-0.6 范围内 proposal 阈值设置为 0.5 时 detector 性能最好，在 0.6~0.75 阈值为 0.6 的 detector 性能最佳，而到了 0.75 之后就是阈值为 0.7 的 detector 性能最好了。只有输入 proposal 自身的 IoU 分布和 detector 训练用的阈值 IoU 较为接近的时候，detector 性能才最好。</p> 
 <p><img src="https://images2.imgbox.com/36/03/ZTwQPd5p_o.png" alt="ea710e088dd9c50c75f564168c7be9e8.png"></p> 
 <h5>2) 差异化R-CNN Head</h5> 
 <p>下图显示逐stage增大IoU后，正样本不仅没有减少，而且稍稍有增加。这是因为经过前一个stage的refinement，后一个stage的输出box变得更精确了，即与GT的IoU提升了。因此，Cascade R-CNN级联多个 R-CNN 模块，并且不断提高 IoU 阈值，不仅不会出现mismatch和过拟合，反而可以逐阶段修正box预测。</p> 
 <p><img src="https://images2.imgbox.com/40/14/k5pzxrMe_o.png" alt="a2b1667c8dfdf5ada3b5e0d9a91d4b36.png"></p> 
 <h4>(3) Mask级联</h4> 
 <p>为了将Cascade R-CNN目标检测器推广到实例分割，作者提出了(b)(c)(d)三种策略来新增mask分支。在测试时:</p> 
 <ul><li><p style="text-align:left;">(b)(c)(d)三种结构都只采用最后一个阶段输出的检测框裁剪RoI进行分割;</p></li><li><p style="text-align:left;">不同之处在于，(d)结构中，最后一个stage输出的RoI会送入三个mask分支分别进行预测，然后取均值进行ensemble。(d)结构在文中的消融实验中也是性能最好的。</p></li></ul> 
 <p><img src="https://images2.imgbox.com/88/e9/AvvIVcj0_o.png" alt="dbe9f9d8787148c534b1eaddae8a6260.png"></p> 
 <h4>(4) 总结</h4> 
 <ul><li><p style="text-align:left;">Cascade R-CNN通过级联box head 来进行逐阶段的box refinement；</p></li><li><p style="text-align:left;">Cascade Mask R-CNN在其基础上添加级联mask head并在测试阶段对三个stage的mask预测概率进行均值集成。</p></li><li><p style="text-align:left;">然而，Cascade Mask R-CNN的三个stage的mask head之间没有类似box head的逐stage的refinement的过程，而是各自独立预测，最终进行集成，这也是后续HTC模型改进的关键点。</p></li><li><p style="text-align:left;">此外，Cascade Mask R-CNN仍然存在类似Mask R-CNN的大目标边缘预测粗糙的问题，这也是二阶段实例分割算法的通病。</p></li></ul> 
 <h3>2.3 HTC</h3> 
 <p>HTC的主要创新点有两个：</p> 
 <ul><li><p style="text-align:left;">设计mask分支的级联，以便将前一个stage的mask信息流传递到下一个stage；</p></li><li><p style="text-align:left;">添加语义分支和语义分割监督来增强特征的上下文语义特征；</p></li></ul> 
 <p>下图中展示了在Cascade R-CNN的基础上添加mask分支的四种依次递进的设计：</p> 
 <ul><li><p style="text-align:left;">(a)图是标准的Cascade Mask R-CNN，当前 stage 会接受 RPN 或者 上一个 stage 回归过的框作为输入，并行预测box和mask；</p></li><li><p style="text-align:left;">(b)图中在每个stage是将refine后的box输入到mask分支；</p></li><li><p style="text-align:left;">(c)图在(b)图的基础上添加了mask分支之间的信息流，每次将前一个stage的mask特征输入到当前stage进行sum融合；</p></li><li><p style="text-align:left;">(d)图进一步引入了语义分割监督，来增强mask分支的特征语义上下文信息；</p></li></ul> 
 <p><img src="https://images2.imgbox.com/0e/8e/6m9tKwDl_o.png" alt="08a1a0e0f1f602a2da5e5fbc54a67c24.png"></p> 
 <h4>(1) mask 信息流</h4> 
 <p>为了在不同stage的mask分支之间添加信息流，作者将前一个stage的特征式经过一个卷积之后与当前stage RoI Align之后的特征进行sum融合并输入到当前mask分支。</p> 
 <p>在实际训练时，前一个stage的RoI和当前stage的RoI并不一致，那么如果直接将前一个stage的特征拿过来与当前stage特征sum融合便会遇到空间不对齐的问题。因此，在实际训练过程中，需要用当前stage的RoI重新执行一遍前一个mask分支，这样就解决了mask特征不对齐的问题。测试时，与Cascade Mask R-CNN一致，只采用了最后一个stage的RoI输出，也不会遇到mask特征不对齐的问题。</p> 
 <p><img src="https://images2.imgbox.com/a2/eb/O91htHc0_o.png" alt="1a6b84fd07af2f354c9a873afe802949.png"></p> 
 <h4>(2) 语义监督</h4> 
 <p>为更好的区分前背景，进一步将语义分割引入到实例分割框架中，以获得更好的 spatial context。具体来讲：</p> 
 <ul><li><p style="text-align:left;">将FPN输出的不同level的特征图分别经过后插值到同一分辨率并sum融合；</p></li><li><p style="text-align:left;">紧接着通过4层全卷积后，分别预测语义分割特征以及语义分割预测结果；</p></li><li><p style="text-align:left;">语义分割特征通过RoIAlign及element-wise sum与box、mask特征进行融合；</p></li><li><p style="text-align:left;">语义分割预测结果需要添加语义分割损失对该语义分支进行监督；</p></li></ul> 
 <p><img src="https://images2.imgbox.com/55/ad/2aI16i0x_o.png" alt="1423da66c1b3ede98f09ec2abc723cfd.png"></p> 
 <h4>(3) 总结</h4> 
 <ul><li><p style="text-align:left;">HTC算是二阶段实例分割的终结了，一直在COCO霸榜直至22年6月上旬被Mask DINO以微弱的优势超越；</p></li><li><p style="text-align:left;">二阶段实例分割采用RoI Align的通病就不再赘述；</p></li><li><p style="text-align:left;">本文所提出采用mask信息流级联与语义监督虽然都不是特别新颖的创新点，但是都可以作为实例分割算法的稳定提点策略；</p></li></ul> 
 <h2>3. 一阶段实例分割</h2> 
 <h3>3.1 局部mask与全局mask之争</h3> 
 <p>在介绍一阶段实例分割前，我们引入局部mask和全局mask的概念，这也是二阶段实例分割与一阶段实例分割的本质差别。如果这一块看的时候有点懵逼，就先跳过，等看完一阶段部分再回来看也行。</p> 
 <img src="https://images2.imgbox.com/4d/1e/5KaoaDaF_o.png" alt="a7e6b5d19ccea4d52023637dcb0b3f72.png"> 
 <h4>(1) 局部mask</h4> 
 <p>如上图所示，第二章介绍的二阶段实例分割采用的就是局部mask，将box内部的区域全部裁剪出来并通过RoI Align统一到相同的尺寸；</p> 
 <p>其优点是：</p> 
 <ul><li><p style="text-align:left;">mask分支简单易学：裁剪后的mask特征构成简单，只包含实例的前景和少量的背景，因此，mask分支设计简单，仅仅通过4层全卷积就可以得到mask；</p></li><li><p style="text-align:left;">小目标效果较好：Mask R-CNN输出的mask分辨率为28×28，COCO的小目标定义是size&lt;32×28，换算到1/4特征图就是8×8，所以小目标特征在经过RoI Align后会被放大，细节特征就可以较好的保留。此外，由于目标撑满整个mask，小目标的mask在监督时不会遇到正负样本严重不均衡的状况。</p></li></ul> 
 <p>其缺点是：</p> 
 <ul><li><p style="text-align:left;">大目标效果差：RoI Align后的尺寸较小，会导致大目标特别是轮廓复杂的大目标边缘分割较为粗糙；</p></li><li><p style="text-align:left;">box偏差：当box预测存在偏差时，仅仅对box内部进行实例mask预测无法预测box外的mask区域；</p></li></ul> 
 <h4>(2) 全局mask</h4> 
 <p>全局mask不需要经过裁剪和RoI Align的过程，而是直接从整个特征图中预测某个实例。</p> 
 <p>其优点是：</p> 
 <ul><li><p style="text-align:left;">大目标效果好：大目标的特征不会经过RoI Align操作导致细节特征严重丢失；而且大目标的损失函数优化时，正负样本还是相对均衡的。</p></li></ul> 
 <p>其缺点是：</p> 
 <ul><li><p style="text-align:left;">小目标效果差：为了减少计算量，全局mask通常在1/4或1/8特征图进行，小目标在此分辨率下边缘预测不佳；且交叉熵损失函数优化时会遇到背景负样本占据大多数的情况，优化效果不佳，需搭配Dice Loss这一类的损失函数进行优化；加大了模型学习难度：局部mask分支获取到的特征是已经用box剔除了大部分背景的实例特征，是有box这个强先验加持的；而全局mask拿到的是全局特征，那么全局mask分支就还需要具备定位能力，分别解析出不同的实例。</p></li></ul> 
 <h3>3.2 YOLACT</h3> 
 <p>之所以这篇文章给了五星，是因为在我看来，YOLACT算是后来的BlendMask，EmbedMask，Condinst这一系列文章的雏形。这一系列方法都是通过预测一组实例特定的参数与共享的1/4或者1/8的全局特征通过某种操作（通道加权求和，卷积或者聚类等）来预测一组实例的mask。</p> 
 <p>如果上面这句话理解有点困难的话，再拿Mask R-CNN举个例子：</p> 
 <ul><li><p style="text-align:left;">问 "Mask R-CNN是如何预测一组实例的mask的呢？” 答 "(1) 每个实例的特征都不一样；(2) 但所有实例共享相同的卷积参数；”</p></li><li><p style="text-align:left;">那我们扩展一下，问 "如果所有实例共享相同的特征的情况下如何预测一组实例mask呢？” 答 "那就要处理每个实例的卷积的卷积核参数不一样。”</p></li><li><p style="text-align:left;">继续扩展，问 "那每个实例的特征和卷积参数可不可以都不一样呢？”。答 "当然可以，这就是下文即将要介绍的文章BlendMask和QueryInst算法的思路。”</p></li><li><p style="text-align:left;">总结来讲，每个实例具有不同的mask区域，那么用于合成该实例的实例特征或者卷积操作的卷积核参数总有一个要是实例特定的，即随着实例的不同而变化的。当然了，这里的操作并不一定非得是卷积。</p></li></ul> 
 <p>接下来详细看一下YOLACT是怎么用共享的全局特征来进行实例分割的。YOLACT是在目标检测网络RetinaNet的基础上进行改造的。</p> 
 <h4>(1) RetinaNet</h4> 
 <p>RetinaNet本质上是由ResNet+FPN+两个FCN子网络（分类和回归）组成：</p> 
 <ul><li><p style="text-align:left;">Backbone选择ResNet来作为特征提取网络；</p></li><li><p style="text-align:left;">FPN用来产生更强的包含多尺度目标区域信息的feature map，包含到；</p></li><li><p style="text-align:left;">最后在FPN的多个feature map上分别使用两个结构相同但是不共享参数的FCN子网络，从而完成目标框类别分类和bbox位置回归任务；</p></li></ul> 
 <p>RetinaNet同时也是一个Anchor-based的模型：</p> 
 <ul><li><p style="text-align:left;">不同分辨率上anchor的尺寸anchor-size：；</p></li><li><p style="text-align:left;">每个anchor-size对应着三种放缩系数scale：；</p></li><li><p style="text-align:left;">每个anchor-size对应着三种长宽比ratio：；</p></li></ul> 
 <p>也就是说，在到的每个特征图的每个像素网格上都会预设9个anchor：</p> 
 <ul><li><p style="text-align:left;">每个anchor都会预测一个长度为c的类别向量（如下图分类分支预测的，c就是类别数，a=9，是anchor数）；和一个长度为4的bbox回归向量（如下图回归分支预测的，4 就是box四个回归量，a=9，是anchor数）。</p></li></ul> 
 <p>训练时的anchor正负样本分配策略如下：</p> 
 <ul><li><p style="text-align:left;">当anchor与ground-truth的IoU大于等于0.5时，这一类anchor就是正样本；</p></li><li><p style="text-align:left;">若IoU在[0,0.4)这个区间内，这一类anchor就是负样本，也就是背景。</p></li><li><p style="text-align:left;">IoU在[0.4,0.5)这个区间内的anchor在训练时是不计损失的。</p></li></ul> 
 <p><img src="https://images2.imgbox.com/8a/d7/rOH8aIkv_o.png" alt="d9d4c7791bcbd1e5767d0b6b7950d78c.png"></p> 
 <h4>(2) 实例Mask系数</h4> 
 <p>实例mask系数就是在RetinaNet分类和回归分支的基础上，并行添加一个mask系数预测分支，也就是如上图所示，预测为 的分支，即接下来的采用通道加权操作合成实例mask的通道加权的系数，就是通道的数目。实际操作会对预测出来的系数采用进行非线性变换。</p> 
 <h4>(3) Mask特征</h4> 
 <p>Mask特征直接在FPN 特征图上接一个全卷积网络，将分辨率上采样至1/4并将通道映射为，最终的shape为。</p> 
 <h4>(4) 实例Mask预测</h4> 
 <p>最终的实例mask的预测就采用下图所示的公式合成：</p> 
 <img src="https://images2.imgbox.com/96/5a/wulrZV52_o.png" alt="a03ccd95a3890cce73d71147e542a51a.png"> 
 <ul><li><p style="text-align:left;">P就是（3）中预测的Mask特征，shape为；</p></li><li><p style="text-align:left;">C为正样本的实例mask系数矩阵，shape为；</p></li><li><p style="text-align:left;">sigema为sigmoid激活函数；</p></li></ul> 
 <p>这个矩阵乘法背后的含义就是对预测的mask特征的每个通道采用对应的mask系数进行加权求和，其本质就是一个1×1×k×1 的卷积操作，卷积核为1，输入通道为k，输出通道也为1。</p> 
 <h4>(5) 总结</h4> 
 <p>YOLACT通过预测一组实例特定的mask加权系数与共享的全局mask特征来生成实例mask，省去了RoI Align生成局部特征图的过程，网络简洁，速度实时；由于YOLACT采用的RetinaNet检测网络的性能本身较差，YOLACT的整体指标与二阶段Mask R-CNN相比还是差了许多的，29.8 vs 35.7。</p> 
 <h3>3.2 BlendMask</h3> 
 <p>BlendMask与YOLACT整体来讲是很像的，给检测head部分新增一个实例相关的参数（Boxes Attens）预测分支，在1/4特征图上生成mask特征（Bases）。接下来照旧挨个模块简介一下：</p> 
 <p><img src="https://images2.imgbox.com/cc/9e/ZezSXNpu_o.png" alt="85e213498efc5ea7aeea90c094781c97.png"></p> 
 <h4>(1) BlendMask vs YOLACT</h4> 
 <ul><li><p style="text-align:left;">目标检测网络由RetinaNet替换为FCOS；</p></li><li><p style="text-align:left;">mask特征（Bases）的生成由YOLACT的几层简单的卷积替换为DeepLab V3+的decoder模块，也就是ASPP模块，能够产生更强的上下文语义特征。shape为。论文最终采用的K=4，这是一个令人惊讶的通道数字，难以想象这么少的通道可以支撑最终的实例mask预测效果；</p></li><li><p style="text-align:left;">mask系数（Boxes Attens）也由原本简单的每个通道一个的加权系数变成每个像素每个通道一个加权系数，粒度更细，即Boxes Attens已经学习出了实例空间位置分布信息。shape为，是正样本实例的数目；</p></li><li><p style="text-align:left;">mask合成这一块比YOLACT要复杂一些，将的mask特征用RoI Align裁剪并缩放为，R最终设置为56×56，Boxes Attens本身的shape为， M比R要小，这里设置为14，这主要是为了减少Boxes Attens预测分支的计算量和预测难度，所以这里需要先将Boxes Attens插值为 ，最终便可以如下图所示合成实例mask 。</p></li></ul> 
 <p><img src="https://images2.imgbox.com/b0/db/JnzQVGAx_o.png" alt="f5cf183853fcb576d0940ce9b2373b15.png"></p> 
 <h4>(2) 总结</h4> 
 <ul><li><p style="text-align:left;">BlendMask不知道该说他是大杂烩还是集大成者 ，既有YOLACT似的实例mask系数预测，也存在Mask R-CNN似的RoI Align与局部mask；</p></li><li><p style="text-align:left;">虽然BlendMask采用了RoI Align，但是其采用的尺寸是56, 比Mask R-CNN这一类方法采用28还是要精细一些的；</p></li></ul> 
 <h3>3.3 EmbedMask * * *</h3> 
 <p>EmbedMask与YOLACT同样很相似，不同之处在于：</p> 
 <ul><li><p style="text-align:left;">YOLACT采用共享全局特征与实例mask系数通过通道加权组合得到实例mask。之前讲过，这个通道加权相当于一个1×1卷积；</p></li><li><p style="text-align:left;">EmbedMask也是要预测一组实例特定的参数与全局共享特征图通过某种操作来得到实例mask，不过此处的操作不是卷积，而是聚类操作。具体来讲，给每个实例学习一组向量（相当于类心向量），然后计算实例向量与每个像素的向量之间的欧式距离，当距离小于给定阈值时则认为当前像素属于当前实例。</p></li></ul> 
 <h4>(1) 卷积 vs 聚类</h4> 
 <p>EmbedMask的详细网络结构如下，基于FCOS目标检测网络进行实例分割设计，蓝色的部分是新添加的模块，主要包含实例Embedding预测模块（Proposal Embedding），全局特征图生成模块（Pixel Embedding），以及可学习实例聚类阈值预测模块（Proposal Margin）三个模块。</p> 
 <p><img src="https://images2.imgbox.com/1b/4c/GWqgJJux_o.png" alt="0223a9224f6f185047d0517db01a944f.png"></p> 
 <ul><li><p style="text-align:left;">全局特征图生成模块（Pixel Embedding）：采用作为输入，后接4层3×3卷积层得到，最终的shape为；</p></li><li><p style="text-align:left;">Embedding预测模块（Proposal Embedding）：在FCOS原有检测head（classification+box regression+centerness）的基础上新增一个分支，给每个像素（FCOS每个像素视为一个anchor，可以预测一个实例）预测一个实例embedding，那么正样本（FCOS正样本选取方式请移步FCOS原文）的实例embedding就构成了一个的矩阵；</p></li><li><p style="text-align:left;">实例mask合成：有了实例embedding和pixel embedding，那么最直接的做法就是采用下列的公式来合成实例的mask。具体的逻辑就是聚类那一套，只不过这里我把类心也给你了，不用像kmeans一样反复优化类心了。当某个像素embedding与某个实例embedding的距离小于给定阈值时，就可以将该像素归属于该实例，这样就可以得到每个实例的mask。</p></li></ul> 
 <img src="https://images2.imgbox.com/af/4d/OwKBMIBO_o.png" alt="53b6653e9f802b48ad103312cb19b79a.png"> 
 <h4>(2)可学习实例聚类阈值</h4> 
 <p>（1）中介绍的是一种固定阈值聚类方案，那么，如下图所示（虚线圆半径代表聚类阈值），大目标应该用大的阈值，小目标应该用小的阈值。阈值取得大了，小目标就会和其他目标混淆被分割出来，阈值取得小了，大目标只能分割出一部分来。那么，可学习的阈值就应运上马了。</p> 
 <p><img src="https://images2.imgbox.com/ee/6c/3PUegV94_o.png" alt="e01e0bc85240d3c628c5b4944a9369af.png"><img src="https://images2.imgbox.com/e8/ce/lLoP44yq_o.png" alt="6b5f1d1eec539c54603eff3e866e27c3.png"><img src="https://images2.imgbox.com/42/c2/n6xyd3GM_o.png" alt="4984170270475dd2519546229722bce9.png"><img src="https://images2.imgbox.com/34/5a/2Og4agZq_o.png" alt="d7949d4424e28610bce6ef6ffec56867.png"><img src="https://images2.imgbox.com/fe/45/67C7ipQu_o.png" alt="fecbc25b62ce1cf88c3ec70996f4129d.png"><img src="https://images2.imgbox.com/6f/91/spEkOPML_o.png" alt="122ad50a1c5110eebaceb30f4f6e6d20.png"><img src="https://images2.imgbox.com/1c/b0/kQCKdSwP_o.png" alt="5dfc6bcd945fab4baa4b21e9c38a7a9b.png"><img src="https://images2.imgbox.com/59/12/RERuY087_o.png" alt="40dee63e6a03593f5305c87cd159c546.png"><img src="https://images2.imgbox.com/dd/70/fyofX8mh_o.png" alt="ac715bb707b6dbfd6f887aa40fcbf02c.png"></p> 
 <h4>(3) 总结</h4> 
 <ul><li><p style="text-align:left;">EmbedMask采用预YOLACT系列文章相似的做法，只不过合成实例mask的方式变成了聚类，想法新颖，也确实可行。这也为其他方法探索除了卷积和聚类之外的其他动态操作提供了可能，如动态BN（AdadpIS），动态ReLU，特征图共享，BN和ReLU的参数是依据实例预测的；</p></li><li><p style="text-align:left;">但是，聚类操作的类心embedding和pixel embedding都是给定的，那就是要求mask特征图已经能够准确将不同实例区分开了。这在一些实例与背景差异比较大的情况下可行，但是当不同实例，特别是同类别实例挨得比较近的情况下，特征边界往往很难明显划分。</p></li><li><p style="text-align:left;">而且，聚类操作嵌入到模型中，相比于卷积操作来讲不够自然，需要进行更多的后处理操作，如本文的高斯概率映射。</p></li></ul> 
 <h3>3.4 CondInst</h3> 
 <h4>(1) Motivation</h4> 
 <p>当两个属于同一类别的目标同时出现时，要想将两者进行实例层面的区分，上述已经介绍的方法的解决思路如下：</p> 
 <ul><li><p style="text-align:left;">二阶段的方法通过RoI Cropping将目标区分开来，这样相当于显式引入了实例的box位置先验信息；</p></li><li><p style="text-align:left;">YOLACT这一类的一阶段方法并没有实例位置先验信息，所以只能强行让不同实例的语义特征学习的足够差异化；</p></li></ul> 
 <p>那么，也给YOLACT这一类的一阶段方法引入位置先验信息，就可以减缓同类目标语义特征学习的足够差异化的压力，也就是说，两个同类目标虽然语义特征相似，但是位置特征是不同的，那这样也可以将两者在实例层面区分开来。</p> 
 <h4>(2) 模型结构</h4> 
 <p>CondInst同样采用FCOS作为目标检测网络；</p> 
 <p><img src="https://images2.imgbox.com/59/e9/CvoxldxR_o.png" alt="5dc61d54b0cb45eeed5e1ff4bf9603d2.png"></p> 
 <p>CondInst的全局特征图同时包含语义特征和位置特征：</p> 
 <img src="https://images2.imgbox.com/eb/d6/0omhyPx5_o.png" alt="30a0e9e67821910e7d0355b95cd2da15.png"> 
 <p><img src="https://images2.imgbox.com/5a/96/X1H6ax0L_o.png" alt="26278c1c7c5a91ad3609b9f1ca383ec1.png"></p> 
 <p>Controller Head：CondInst类似YOLACT在检测head部分添加了一个并行的分支用来学习每个实例特定的动态卷积参数。此处的动态卷积与YOLACT中的mask系数是相似的操作：</p> 
 <ul><li><p style="text-align:left;">虽然YOLACT的mask系数相当于的卷积，但是YOLACT当时可能并没想那么深入；</p></li><li><p style="text-align:left;">CondInst明确了在此处使用动态卷积的方案，直接学习了三层所需的weights和biases共计169个参数，具体构成如下：（8是指语义特征channel为8，相对位置特征channel为2）；</p></li></ul> 
 <img src="https://images2.imgbox.com/fa/68/XPHtAWv0_o.png" alt="06686724ea24e65763aa5ff1d1101edf.png"> 
 <h4>(3) 总结</h4> 
 <ul><li><p style="text-align:left;">CondInst兼具YOLACT的简洁结构与实用性能（超过Mask R-CNN）；</p></li><li><p style="text-align:left;">CondInst最终采用的特征图channel数为8，这并不是为了彰显模型的强大（能够在少量的通道情况下得到不错的效果），更多的原因是CondInst在实现过程中涉及到repeat操作，需要将特征图复制正样本的数目那么多份，如果通道数太多，肯定行不通的。</p></li><li><p style="text-align:left;">至于为什么要repeat，主要是因为YOLACT，CondInst都是只将正样本输入到mask head，但是不同图片输出的正样本数目不一致，那就的需要在batch维度加一个for循环（时间成本增加），或者直接将特征图repeat多份（内存增加）。下图整理了几种全局mask具体生成实例mask的方案。</p> 
   <ul><li><p style="text-align:left;">YOLACT可以采用for循环或者repeat两种方案；</p></li><li><p style="text-align:left;">CondInst只能采用repeat的方案，这主要是因为其采用了多层卷积，实现层面必须用组卷积来实现，也就必须要进行repeat；</p></li><li><p style="text-align:left;">Mask2former还没介绍，这里简单提一下，该算法在每张图设置固定的proposal数目，每次将所有proposal输入到mask head，并且采用类似YOLACT的mask组合系数，所以并不涉及for循环或者repeat。</p></li></ul></li></ul> 
 <p><img src="https://images2.imgbox.com/bf/00/7U0djms0_o.png" alt="1db95eca2a5af3b4c0032f8d6897c470.png"></p> 
 <p>这实际上是一个balanced的过程：</p> 
 <ul><li><p style="text-align:left;">CondInst的卷积层更多，由于实现限制只能采用较少的特征图通道；</p></li><li><p style="text-align:left;">Mask2former直接一步bmm，所以通道可以保持正常的256；</p></li><li><p style="text-align:left;">通道肯定多点好啦，特别是对于颜色复杂的大目标，具体合成实例mask的卷积操作也是一样啦，卷积层越多，能够对特征的变化和拟合就越强，如果能找到一种实现方式，同时采用多层卷积操作和大channel就最好啦。</p></li></ul> 
 <h3>3.5 SOLOv2</h3> 
 <p>SOLO共出了两个版本，v1跟本章前面的几篇文章相关性都不大，v2在v1的基础上引入了动态卷积的方案。那就先介绍一下v1:</p> 
 <p><img src="https://images2.imgbox.com/a6/14/eEgaNKBL_o.png" alt="fa8286e2102bc361806ee2dd20aac51d.png"></p> 
 <p>SOLOv1在网格上进行实例分割的方案有点类似YOLO，都是在网格上进行实例目标的检出，SOLOv1是做实例分割，YOLO是进行目标检测。但是SOLOv1并不像CondInst这一类方法，其并没有保留目标检测网络的bbox分支，而是只有分类分支和mask分支。接下来分别从SOLOv1的网格正负样本分配和classification分支及mask分支的设计两个方面展开。</p> 
 <p>(1) 网格正负样本分配策略 具体来讲，SOLO会首先将FPN不同层的特征图等分为 S×S个格子。正负样本的分配涉及到两个方面：</p> 
 <ul><li><p style="text-align:left;">一是FPN同一层正负样本分配的策略；</p></li><li><p style="text-align:left;">二是FPN不同层正负样本分配的策略；</p></li></ul> 
 <p>FPN同一层：对于给定gt mask的质心、宽度w和高度h，设置比例因子，如0.2。被缩小后的box覆盖住的几个格子就是正样本，每一个gt平均有3个正样本。</p> 
 <p>FPN不同层：但是，如果两个目标同时覆盖某个网格呢，为了处理这种情况，作者首先做了个统计：在验证集中，绝大多数（约98.9%）的实例要么中心位置不同，要么大小不同。因此，如果两个目标中心位置接近，那就通过尺度将其分配到FPN的不同层去，这样就能解决掉98.9%的冲突。</p> 
 <p>论文设置scale_ranges=((1, 96), (48, 192), (96, 384), (192, 768), (384, 2048))，分别对应不同FPN的输出层。当实例的尺度落入某一个区间，则该FPN分支负责该实例的预测。由于不同区间存在重叠的情况，因此会存在不同FPN层预测相同的目标，这样同样会增加正样本的数量。</p> 
 <img src="https://images2.imgbox.com/3c/c7/a6R8sohK_o.png" alt="bcef25dcb6ea521fbfab1a0d4af7b52a.png"> 
 <p><img src="https://images2.imgbox.com/ad/55/d9p381Rf_o.png" alt="494410404658935ec02713d42da45802.png"><img src="https://images2.imgbox.com/6e/90/4C0FZvrN_o.png" alt="630a230d60b222d942078ddd61d21cef.png"><img src="https://images2.imgbox.com/06/a5/a3Co6qbY_o.png" alt="f2c8f02711dd7dfbf2c2791169f64759.png"></p> 
 <h4>(3) SOLOv2 动态卷积</h4> 
 <p>SOLOv2采用类似CondInst的动态卷积方式，给每个网格代表的实例预测一组卷积参数，将其FPN各个分辨率融合后的特征图进行动态卷积来得到实例mask。</p> 
 <p><img src="https://images2.imgbox.com/34/e0/SEiDWYUl_o.png" alt="f0efd685b52638d0b347e0532f2a9389.png"></p> 
 <h4>(4) 总结</h4> 
 <img src="https://images2.imgbox.com/d4/a0/M8POgUW8_o.png" alt="433f51b81c647c4f5a5fb1be6803ef9d.png"> 
 <p>SOLOv2的mAP刚刚与Mask R-CNN齐平，但是在小目标和大目标的性能上出现了明显的反差</p> 
 <ul><li><p style="text-align:left;">对于小目标效果差，我认为这是由于SOLOv2始终没有放弃网格的方案导致的，网格稀疏的情况下会导致小目标的召回率较低，网格过密计算代价就高了；</p></li><li><p style="text-align:left;">大目标效果好，我认为有三方面原因：</p> 
   <ul><li><p style="text-align:left;">一方面是由于采用全局mask，会比Mask R-CNN好是自然的；</p></li><li><p style="text-align:left;">另一方面是预测实例mask的特征是FPN融合后的，比仅使用1/4特征图语义信息更丰富；</p></li><li><p style="text-align:left;">最后是与同为全局mask的condinst相比的，SOLOv2的特征图通道为256，这能够编码更丰富的信息相比于CondInst的8个通道。下图是CondInst中关于通道数的一个超参实验，当通道数为16时，Apl能产生一个点的提升相比于通道数为8时。</p></li></ul></li></ul> 
 <img src="https://images2.imgbox.com/4b/11/UiG8ulmG_o.png" alt="59037be52e15d0af8a293118618b491d.png"> 
 <p style="text-align:center;">【<strong>自动驾驶之心</strong>】全栈技术交流群</p> 
 <p style="text-align:left;"><strong>自动驾驶之心是首个自动驾驶开发者社区，聚焦目标检测、语义分割、全景分割、实例分割、关键点检测、车道线、目标跟踪、3D感知、多传感器融合、SLAM、高精地图、规划控制、AI模型部署落地等方向；</strong></p> 
 <p style="text-align:justify;"><strong>加入我们：</strong><a href="" rel="nofollow">自动驾驶之心技术交流群汇总！</a></p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ed5892c7f512b2f4f24243025b8c8cd7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【硬件】什么是负电压？怎么产生负电压？</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6c776d6300ab896aed3de538ec4be825/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">什么是堆栈以及堆栈的区别</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>