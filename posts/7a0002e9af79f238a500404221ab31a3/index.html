<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>语义分割 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="语义分割" />
<meta property="og:description" content="Introduction Semantic Segmentation:将图像中每个像素分配到某个对象类别。图像语义分割中存在3种挑战：（1）特征分辨率减少，（2）不同尺度下的物体的存在状况，（3）由于深度卷积神经网络的不变性造成的定位精度减少。
第一个挑战是由 基于分类的卷积神经网络包含重复最大池化和降采样（步长跨度）操作造成的。深度卷积神经网络采用全卷积方式的时候，会明显降低特征地图的空间分辨率。 第二个挑战是object以多尺度形式存在于图像中。 第三个挑战是object分类器要求对空间变换具有不变性，内在地限制了深度卷积神经网络的空间精度。
在深度学习应用到计算机视觉领域之前，开发的大多数成功的语义分割系统都是采用手工特征的单调分类器，比如，提升方法（论文【Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context, IJCV,2009.】），随机森林（论文【Semantic texton forests for image categorization and segmentation, in CVPR, 2008】），或支持向量机（论文【Class segmentation and object localization with superpixel neighborhoods, in ICCV,2009】）。通过整合更丰富的信息，内容（论文【 Semantic segmentation with second-order pooling, in ECCV, 2012】）和结构化的预测技术（论文【Efficient inference in fully connected crfs with gaussian edge potentials, in NIPS, 2011】、【Multiscale conditional random fields for image labeling, in CVPR, 2004】、【Associative hierarchical crfs for object class image segmentation, in ICCV,2009】、【CPMC: Automatic object segmentation using constrained parametric min-cuts, PAMI, vol." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/7a0002e9af79f238a500404221ab31a3/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2017-09-27T10:32:22+08:00" />
<meta property="article:modified_time" content="2017-09-27T10:32:22+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">语义分割</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3 id="introduction">Introduction</h3> 
<p><strong>Semantic Segmentation:将图像中每个像素分配到某个对象类别。图像语义分割中存在3种挑战：（1）特征分辨率减少，（2）不同尺度下的物体的存在状况，（3）由于深度卷积神经网络的不变性造成的定位精度减少。</strong></p> 
<p><strong>第一个挑战</strong>是由 基于分类的卷积神经网络包含重复最大池化和降采样（步长跨度）操作造成的。深度卷积神经网络采用全卷积方式的时候，会明显降低特征地图的空间分辨率。 <br> <strong>第二个挑战</strong>是object以多尺度形式存在于图像中。 <br> <strong>第三个挑战</strong>是object分类器要求对空间变换具有不变性，内在地限制了深度卷积神经网络的空间精度。</p> 
<p>在深度学习应用到计算机视觉领域之前，开发的大多数成功的语义分割系统都是采用手工特征的单调分类器，比如，提升方法（论文【Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context, IJCV,2009.】），随机森林（论文【Semantic texton forests for image categorization and segmentation, in CVPR, 2008】），或支持向量机（论文【Class segmentation and object localization with superpixel neighborhoods, in ICCV,2009】）。通过整合更丰富的信息，内容（论文【 Semantic segmentation with second-order pooling, in ECCV, 2012】）和结构化的预测技术（论文【Efficient inference in fully connected crfs with gaussian edge potentials, in NIPS, 2011】、【Multiscale conditional random fields for image labeling, in CVPR, 2004】、【Associative hierarchical crfs for object class image segmentation, in ICCV,2009】、【CPMC: Automatic object segmentation using constrained parametric min-cuts, PAMI, vol. 34,no. 7, pp. 1312–1328, 2012】），取得了实质性的进展，但这些系统的性能受限于特征的表达能力。过去几年里，深度学习在图像分类上取得了突破性的进展，迅速转移到语义分割中。既然这个任务包含了分割和分类，那么一个中心问题是如何合并这两个任务呢。</p> 
<h3 id="related-work">Related Work</h3> 
<p><strong>第一类深度卷积神经网络的语义分割主要采用自下而上的串联图像分割，再串联深度卷积神经网络区域分类。</strong>比如，论文(【 Rich feature hierarchies for accurate object detection and semantic segmentation,in CVPR, 2014.】、【Simultaneous detection and segmentation,” in ECCV, 2014】)中使用了论文(【Multiscale combinatorial grouping, in CVPR, 2014】、【Selective search for object recognition, IJCV, 2013】)的<strong>bounding box proposals和masked regions</strong>，作为输入提供给深度卷积神经网络，整合形状信息提供给分类流程。类似的，论文【Feedforward semantic segmentation with zoom-out features, in CVPR】使用超像素表示。尽管这些方法可以从良好的分割产生的形状边缘受益，但它们无法从错误中恢复。</p> 
<p>第二类方法用卷积计算的深度卷积神经网络特征用于密集图像标注，将独立获取的分割结合在一起。在第一阶段中论文【Learning hierarchical features for scene labeling, PAMI, 2013】在多个图像分辨率上使用深度卷积神经网络，再使用分割树平滑预测结果。最近，论文【Hypercolumns for object segmentation and fine-grained localization,in CVPR, 2015】建议使用跳层，并联结深度神经网络中计算的中间特征地图用于像素分类。还有，论文【Convolutional feature masking for joint object and stuff segmentation, arXiv:1412.1283, 2014】建议用局部方案池化中间特征地图。这些工作仍然是采用的从深度卷积分类器的结果中解耦合的分割算法，因此有提早做决策的风险。</p> 
<p>第三类工作使用深度卷积神经网络直接提供类别级像素标注，甚至可以放弃分割。论文(【Fully convolutional networks for semantic segmentation, in CVPR, 2015】、【Combining the best of graphical models and convnets for semantic segmentation, arXiv:1412.4313, 2014】）的免分割方法直接用全卷积方法将深度卷积神经网络应用到整幅图像上，将深度卷积神经网络的最后全连接层转换成卷积层。为了处理前面简介中提出的空间定位问题，论文【Fully convolutional networks for semantic segmentation, in CVPR, 2015】在中间特征地图上进行上采样，连接分值，论文【Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture, arXiv:1411.4734, 2014】，将生成粗略的结果放入另外一个深度卷积神经网络上，从粗到细优化预测结果。在这些工作基础上，对特征分辨率进行控制，引入多尺度池化技术，在深度卷积神经网络的最上层集成稠密连接的条件随机场（论文【Efficient inference in fully connected crfs with gaussian edge potentials,in NIPS, 2011】）。我们发现这会产生明显好的分割结果，尤其是在物体边缘。深度卷积神经网络和条件随机场当然不是最新的，但之前的工作只是尝试了局部连接条件随机场模型。具体来说，论文 【Combining the best of graphical models and convnets for semantic segmentation,arXiv:1412.4313, 2014】用条件随机场作为基于深度卷积神经网络的重排序系统，论文【 Learning hierarchical features for scene labeling, PAMI, 2013】将超像素作为结点用于局部配对条件随机场，用图切割用于离散推理。这样，它们的模型受限于超像素计算的误差，或者忽略了长距离依赖。我们的方法将每个像素看做是条件随机场的结点，用于接收卷积神经网络的一元势能。关键是，论文【Efficient inference in fully connected crfs with gaussian edge potentials,in NIPS, 2011】中的全连接条件随机场中的高斯条件随机场势能可以抓取远程依赖，同时模型服从快速平均场推理。我们注意到平均场推理用于传统图像分割任务，论文(【Parallel and deterministic algorithms from mrfs: Surface reconstruction, PAMI, vol. 13, no. 5, pp. 401–412, 1991】、【A common framework for image segmentation, IJCV, vol. 6, no. 3, pp. 227–243, 1991】、【Computational analysis and learning for a biologically motivated model of boundary detection,” Neurocomputing, vol. 71, no. 10, pp. 1798–1812, 2008】)，但这些老的模型受限于短程连接。在一些独立的工作中，论文57使用了一个非常相似的密集连接条件随机场模型优化深度卷积神经网络的结果，用于物料分类。然而，论文【Material recognition in the wild with the materials in context database,arXiv:1412.0623, 2014】的深度卷积神经网络模块是用稀疏点监督上训练的，而不是每个像素点的稠密监督。</p> 
<p>下面介绍深度学习在语义分割中的发展历程，<strong>笔者本着学习的态度进行总结，如有错误之处，欢迎交流讨论。</strong></p> 
<h3 id="paper01-cvpr2015-fully-convolutional-networks-for-semantic-segmentation">Paper01 [CVPR2015]: 《Fully Convolutional Networks for Semantic Segmentation》</h3> 
<p><strong>本论文为CVPR15 Best Paper arxiv:1411 PAMI16 加州大学伯克利分校</strong> </p> 
<p><img src="https://images2.imgbox.com/86/3f/GlPkBDwL_o.png" alt="这里写图片描述" title=""></p> 
<p><img src="https://images2.imgbox.com/bf/33/yvocYsPL_o.png" alt="这里写图片描述" title=""> <br> <strong>本网络架构特点</strong></p> 
<p>(1):第一次end-to-end训练全卷积网络[ <strong>FCN</strong> ]在semantic segmentation Task上，基本网络采用了AlexNet、VGG、GoogleNet；最后实验表明FCN_VGG16高于FCN_AlexNet和FCN_GoogLeNet。 <br> <img src="https://images2.imgbox.com/71/96/aQUIp9W4_o.png" alt="这里写图片描述" title=""> <br> (2):把全连接层转化为卷积层，卷积核大小就是输入的feature map的size. <br> <img src="https://images2.imgbox.com/3e/77/Rap7wNxa_o.png" alt="这里写图片描述" title=""></p> 
<p><img src="https://images2.imgbox.com/db/e6/0HCYKnFp_o.png" alt="这里写图片描述" title=""> <br> (3):输入图片可以任意大小</p> 
<p>(4):定义了一个<strong>skip connection</strong>， 融合深层的语义信息(semantic information from a deep,coarse layer)和浅层的表征信息(appearance information from a shallow ,fine layer to produce accurate and detailed segmentations )；并通过实验证明这种融合可以提高分割性能。<strong>低分辨率的特征（高层的feature map）通过上采用与高分辨率的特征（低层的feature map)融合，该unsampling过程是可学习的（通过deconvolution和activation functions实现可学习的非线性采样）</strong></p> 
<p><img src="https://images2.imgbox.com/a5/62/w08eoCmY_o.png" alt="这里写图片描述" title=""></p> 
<p><img src="https://images2.imgbox.com/f5/aa/RmGGqzEf_o.png" alt="这里写图片描述" title=""></p> 
<p><img src="https://images2.imgbox.com/4a/ec/vBCbzbSv_o.png" alt="这里写图片描述" title=""> <br> <strong>比较FCNs</strong> <br> <img src="https://images2.imgbox.com/e4/f0/W5GSKiOA_o.png" alt="这里写图片描述" title=""> <br> (5) Fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to <strong>62.2%</strong> mean IoU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one ﬁfth of a second for a typical image. <br> <img src="https://images2.imgbox.com/05/fe/LEXLIfoF_o.png" alt="这里写图片描述" title=""> <br> (6)Fully convolutional networks are a rich class of models, of which modern classiﬁcation convnets are a special case. Recognizing this, extending these classiﬁcation nets to segmentation, and improving the architecture with multi resolution layer combinations dramatically improves the state of-the-art, while simultaneously simplifying and speeding up learning and inference.</p> 
<p><img src="https://images2.imgbox.com/c6/58/ITbeqqvV_o.png" alt="这里写图片描述" title=""></p> 
<h3 id="paper02-cvpr2015-hypercolumns-for-object-segmentation-and-fine-grained-localization">Paper02 [CVPR2015]: 《Hypercolumns for Object Segmentation and Fine-grained Localization》</h3> 
<p><strong>本论文为CVPR15 arxiv: v1:1411 v2:1504</strong> </p> 
<p><img src="https://images2.imgbox.com/c8/ed/VFtOfSix_o.png" alt="这里写图片描述" title=""> <br> <strong>本网络架构特点：</strong></p> 
<p><strong>(1)</strong>针对CNN最后层输出特征(feature map) too coarse spatially to allow precise in localization；而早期的层的特征对于位置非常准确，而不具备强的语义特征。<strong>the last layer of the CNN is the most sensitive to category-level semantic information and the most invariant to “nuisance” variables such as pose, illumination, articulation, precise location and so on.</strong>作者提出了一个 <strong>Hypercolumns</strong></p> 
<blockquote> 
 <blockquote> 
  <p><strong>define the hypercolumn at a pixel as the vector of activations of all CNN units above that pixel</strong></p> 
 </blockquote> 
</blockquote> 
<p><img src="https://images2.imgbox.com/eb/5d/Hb4D8HIp_o.png" alt="这里写图片描述" title=""></p> 
<p>(2)性能比SDS好(SDS只用了最后一层特征)</p> 
<p>(3)在 keypoint localization,part labeling任务上取得了new state-of-the-art performance.</p> 
<h3 id="paper03-iccv2015-conditional-random-fields-as-recurrent-neural-networks">Paper03 [ICCV2015]: 《Conditional Random Fields as Recurrent Neural Networks》</h3> 
<p><strong>本论文为ICCV15 Paper arxiv: v1:1502 v2:1504 v3:1604</strong> <br> <img src="https://images2.imgbox.com/9d/73/9mfHFgfZ_o.png" alt="这里写图片描述" title=""> <br> <strong>本网络架构特点：</strong> <br> <strong>(1)</strong>提出一种新的卷积网路（由CNN和基于概率图模型[PGM]的条件随机场[CRF]组成，同时具备CNN和CRF的优越性质。 <br> <strong>(2)</strong> Integrating CRF modeling with CNN，making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm,avoiding offline post-processing methods for object delineation. <br> <img src="https://images2.imgbox.com/e8/85/40roMkxg_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>(3)</strong> 当时在VOC12 val取得了new state-of-the-art performance.比FCN-8s,FCN-8s+CRF(post-processing)高出了8.3个点，6.1个点（训练集without COCO）.在VOC12 test性能高于FCN_8s、Hypercolumn、DeepLab-Msc（72.0% vs 62.2% 62.6% 71.6%3） <br> <img src="https://images2.imgbox.com/ca/b6/v6ML8Wvm_o.png" alt="这里写图片描述" title=""></p> 
<p><img src="https://images2.imgbox.com/07/c1/6SnwwRgm_o.png" alt="这里写图片描述" title=""></p> 
<h3 id="paper04-iclr2015-semantic-image-segmentation-with-deep-convolutional-nets-and-fully-connected-crfs">Paper04 [ICLR2015]： 《Semantic image segmentation with deep convolutional nets and fully connected CRFs》</h3> 
<p><strong>本论文为ICLR15 Paper arxiv: v1:1412 v4:1607</strong> <br> <strong>【DeepLab_v1】</strong> <br> 后面作者在Paper3得基础上继续改善，提出了ASPP(利用多尺度)，详细参考后面的<strong>DeepLab_v2</strong> : 《DeepLab: semantic image segmentation with Deep Convolutional Nets, Atrous Convolution and fully connected CRFs》，在TPAMI上发表. 和<strong>DeepLab_v３</strong> :《Rethinking Atrous Convolution for Semantic Image Segmentation》</p> 
<p><img src="https://images2.imgbox.com/94/8c/PiOUexQj_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>本网络架构特点：</strong> <br> <strong>(1)Problem:</strong> <strong>responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation.</strong> This is due to the very invariance properties that make DCNNs good for <strong>high-level tasks[classification]</strong>,ａｎｄ it can hamper <strong>low-level tasks,such as pose estimation and semantic segmentation.</strong> ，proposing overcome this poor localization property of deep networks by**combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF)**。</p> 
<blockquote> 
 <blockquote> 
  <p>Even though　ｓｏｍｅ works[such as <strong>FCN、Ｈｙｐｅｒｃｏlumns</strong>] still employ segmentation algorithms that are decoupled from the DCNN classifier’s results, we believe it is advantageous that segmentation is only used at a later stage, avoiding the commitment to <strong>premature decisions[过早决策]</strong>.</p> 
 </blockquote> 
</blockquote> 
<p>(2)提出了<strong>hole algorithm</strong>(或者称atrous algorithm,<strong>膨胀卷积</strong>)，Careful network re-purposing and a novel application of the ’hole’ algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU. <br> <img src="https://images2.imgbox.com/01/39/c02z2ghN_o.png" alt="这里写图片描述" title=""> <br> (3)他们提出的方法称为<strong>“DeepLab”</strong>,<strong>在VOC12 semantic image segmentation task reaching 71.6% IOU accuracy in the test set,new state-of-the-art.</strong> （DeepLab-MSc-CRF-LargeFov,采用输入多尺度和CRF做post-processing） <br> (4)CNN特征提取采用的是VGG-16,后面再TPAMI上发表的DeepLab_v2采用是ResNet.</p> 
<blockquote> 
 <blockquote> 
  <p><strong>The three main advantages of our “DeepLab” system are</strong> <br> <strong>(i)</strong> speed: by virtue of the ‘atrous’ algorithm, our dense DCNN operates at 8 fps, while Mean Field Inference for the fully-connected CRF requires 0.5 second. <br> <strong>(ii)</strong> accuracy: we obtain state-of-the-art results on the PASCAL semantic segmentation challenge, outperforming the second-best approach of Mostajabi et al. (2014) by a margin <br> of 7.2% <br> <strong>(iii)</strong> simplicity: our system is composed of a cascade of two fairly well-established modules, DCNNs and CRFs.</p> 
 </blockquote> 
</blockquote> 
<p><strong>(5)整个网络详细结构：</strong> Convertｉｎｇ the fully-connected layers of　VGG-16 into convolutional　ones and run the network in a convolutional fashion on the image at its original resolution. However　this is not enough as it yields very sparsely computed detection scores (with a stride of 32 pixels).so skipping subsampling after the last two max-pooling layers ,and then modify the convolutional filters in the layers that follow them by introducing zeros to increase their length (2×in the last three convolutional layers and 4× in the first fully connected layer).</p> 
<h3 id="paper05-cvpr2016-efficient-piecewise-trainingof-deep-structured-models-for-semantic-segmentation">Paper05 [CVPR2016]： 《Efficient Piecewise Trainingof Deep Structured Models for Semantic Segmentation》</h3> 
<p><strong>本论文为CVPR16 Paper arxiv: v1:1504 v2:1606</strong> </p> 
<p><img src="https://images2.imgbox.com/05/80/70zVZ1Uz_o.png" alt="这里写图片描述" title=""> <br> <img src="https://images2.imgbox.com/23/38/Rs4QBXrO_o.png" alt="这里写图片描述" title=""></p> 
<blockquote> 
 <blockquote> 
  <p>Note: FeatMap-Net is a convolutional network(e.g VGG ) to generate a feature map.</p> 
 </blockquote> 
</blockquote> 
<p><strong>本网络架构特点：</strong> <br> <strong>(1)</strong>提出利用context information (including “patch-patch” context and “patch-background” context)来改善语义分割质量。因此设计了<strong>Contextual Deep CRFs model</strong>，结构如下： <br> <img src="https://images2.imgbox.com/8d/16/TSbJdCI8_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>(2)</strong> For learning from <strong>the patch-patch context</strong>, we formulate Conditional Random Fields (CRFs) with CNN-based pairwise potential functions to capture semantic correlations between neighboring patches.</p> 
<p><strong>(3)</strong> For capturing <strong>the patch-background context</strong>, we show that a network design with <strong>traditional multi-scale image input</strong> and <strong>sliding pyramid pooling</strong> is effective for improving performance. <br> <img src="https://images2.imgbox.com/65/92/aS8WhnIe_o.png" alt="这里写图片描述" title=""> <br> <strong>sliding pyramid pooling结构如下：</strong> <br> <img src="https://images2.imgbox.com/91/5d/Md0PAAW8_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>(4)</strong> CNN-CRF联合训练极具挑战，通过Piecewise Training来缓解。 <br> <strong>(5)</strong> achieving new state-of-the-art performance on a number of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an intersection-over-union score of <strong>78.0 on the challenging PASCAL VOC 2012 dataset.</strong> <br> <img src="https://images2.imgbox.com/53/99/rJV976T5_o.png" alt="这里写图片描述" title=""></p> 
<h3 id="paper06-iccv2015-learning-deconvolution-network-for-semantic-segmentation">Paper06 [ICCV2015]： 《Learning Deconvolution Network for Semantic Segmentation》</h3> 
<p><strong>本论文为ICCV15 Paper arxiv:1505</strong> </p> 
<p><img src="https://images2.imgbox.com/2e/51/XhUQfkTf_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>DeconvNet 网络架构特点：</strong> <br> (1提出了<strong>Deconvolution network</strong>(由deconvolution and unpooling layer组成,以ＶＧＧ16为基础)，通过集成<strong>Deconvolution network</strong>到FCN，缓解现存基于<strong>FCN</strong>方法的局限性，在PASCAL VOC 2012 test取得了当时最好的正确率72.5%(比<span class="MathJax_Preview"></span><span class="MathJax" id="MathJax-Element-1-Frame" style=""> 
   
   <span class="math" id="MathJax-Span-1" style="width: 3.883em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.144em; height: 0px; font-size: 123%;"><span style="position: absolute; clip: rect(1.355em, 1000em, 2.55em, -0.457em); top: -2.222em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3" style="font-family: MathJax_Math; font-style: italic;">F<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.106em;"></span></span><span class="mi" id="MathJax-Span-4" style="font-family: MathJax_Math; font-style: italic;">C<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.045em;"></span></span><span class="msubsup" id="MathJax-Span-5"><span style="display: inline-block; position: relative; width: 1.616em; height: 0px;"><span style="position: absolute; clip: rect(1.377em, 1000em, 2.385em, -0.457em); top: -2.222em; left: 0em;"><span class="mi" id="MathJax-Span-6" style="font-family: MathJax_Math; font-style: italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.085em;"></span></span><span style="display: inline-block; width: 0px; height: 2.222em;"></span></span><span style="position: absolute; top: -2.018em; left: 0.836em;"><span class="texatom" id="MathJax-Span-7"><span class="mrow" id="MathJax-Span-8"><span class="mn" id="MathJax-Span-9" style="font-size: 70.7%; font-family: MathJax_Main;">8</span><span class="mi" id="MathJax-Span-10" style="font-size: 70.7%; font-family: MathJax_Math; font-style: italic;">s</span></span></span><span style="display: inline-block; width: 0px; height: 2.168em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.222em;"></span></span></span><span style="border-left: 0em solid; display: inline-block; overflow: hidden; width: 0px; height: 1.204em; vertical-align: -0.27em;"></span></span> 
  </span><script type="math/tex" id="MathJax-Element-1">FCN_{8s}</script> 高出了 10.3% ，比DeepLab+CRF(v1)71.6%高出了0.9个点. <br> <img src="https://images2.imgbox.com/08/a3/YBxrvk0M_o.png" alt="这里写图片描述" title=""> <br> **FCN局限性： <br> 固定大小的receptive field <br> FCN直接通过unpooling到相应输入size ，过于简单粗暴。** <br> <img src="https://images2.imgbox.com/68/f7/TRb7ZLAx_o.png" alt="这里写图片描述" title=""></p> 
<p>(2)在Deconvolution Network中，低层捕捉对象的粗糙特征(位置、形状、区域)，高层捕捉更复杂的表征(比如类别等)</p> 
<p>(3)coarse-to-fine structures of an object 是通过一系列deconvolution 操作来重构。</p> 
<p>(4)通过可视化实验，We can observe that coarse-to ﬁne object structure sarere constructed through the propagation in the deconvolutional layers; lower layers tend to capture overall coarse conﬁguration of an object (e.g. location, shapeand region), while more complex patterns are discovered in higher layers. Note that unpooling and deconvolution play differentroles for the construction of segmentation masks. Unpooling captures example-speciﬁc structures by tracing the original locations with strong activations back to image space. </p> 
<p><strong>（５）网络详细配置如下：</strong> <br> <img src="https://images2.imgbox.com/57/76/Q3m1VaZN_o.png" alt="这里写图片描述" title=""></p> 
<h3 id="paper07-iclr2016-parsenet-looking-wider-to-see-better">Paper07 [ICLR2016]: 《ParseNet Looking wider to see better》</h3> 
<p><strong>本论文为ICLR16 Paper arxiv:1506</strong> </p> 
<p><img src="https://images2.imgbox.com/4d/c6/6PeH9oxW_o.png" alt="这里写图片描述" title=""></p> 
<p>**(1)**adding global Context to fully convolutional networks for semantic segmentation,using the average feature for a layer to augment the feature at each location.</p> 
<p><strong>(２)</strong> ｐｒｏposing global Context can tackle <strong>local confusions</strong>。As for　semantic segmentation, per pixel classification, is often <strong>ambiguous [模糊不清]**in the presence of only local information. However, **the task becomes much simpler if contextual information, from the whole image, is available.</strong></p> 
<p>**(３)**an end-to-end simple and effective convolutional network.在SIFTFLOW和PASCAL-Context数据集达到了new-state-of-the-art performance.在VOC12 test 69.8% mean IoU, ParseNet 没有使用 post processing，但性能接近使用了post processing的DeepLab_CRF_LargeFOV(v1)[70.3%]。此外，ParseNet速度更快，结构更简单，比起DeepLab. <br> <img src="https://images2.imgbox.com/8b/38/jNoNw6Ef_o.png" alt="这里写图片描述" title=""></p> 
<h3 id="paper08-cvpr2016-attention-to-scale-scale-aware-semantic-image-segmentation">Paper08 [CVPR2016]: 《Attention to Scale: Scale-aware Semantic Image Segmentation》</h3> 
<p><strong>本论文为CVPR16 Paper arxiv:1511</strong> </p> 
<p><img src="https://images2.imgbox.com/62/d3/jAvb44UT_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>本网络架构特点：</strong></p> 
<blockquote> 
 <blockquote> 
  <p><strong>基于DeepLab-LargeFOV(而DeepLab-LargeFOV则是从VGG16修改到全卷机网络而来的)</strong></p> 
 </blockquote> 
</blockquote> 
<p>(1)提出<strong>attention机制( learns to softly weight the multi-scale features at each pixel location.)</strong>，实现多尺度感知； <br> <img src="https://images2.imgbox.com/5e/a5/iI2PmvQG_o.png" alt="这里写图片描述" title=""> <br> (2)<strong>利用attention机制融合多尺度特征，不仅提高了性能(over average_ or max pooling baseline)</strong>,而且通过可视化证明了不同位置、尺度的特征的重要性。 <br> <img src="https://images2.imgbox.com/69/ef/fnnCUOst_o.png" alt="这里写图片描述" title=""> <br> (3)以DeepLab-LargeFOV为baseline，在PASCAL-Person-Part、VOC12-val、VOC12-test、COCO子集性能分别提升了4.48%、6.8%、3.84%、6.4%、4.56% <br> <img src="https://images2.imgbox.com/06/92/GVXxUwyH_o.png" alt="这里写图片描述" title=""></p> 
<h3 id="paper09-iclr2016-multi-scale-context-aggregation-by-dilated-convolutions">Paper09 [ICLR2016]: 《Multi-scale Context Aggregation by dilated convolutions》</h3> 
<p><strong>本论文为ICLR16 Paper arxiv:1511</strong> </p> 
<p><strong>本网络架构特点：</strong> <br> (1)proposing Context module that uses <strong>dilated convolutions（膨胀卷积）</strong> to systematically aggregate mutiscale Contextual information without losing resolution. <br> <img src="https://images2.imgbox.com/81/c3/x44kaWA3_o.gif" alt="这里写图片描述" title=""> <br> （2）this Network is based on VGG-16,removing the last two pooling and striding layers和余下用于分类使用的全连接层，而FCN仍然保留了它们，while DeepLabv1 replaced striding by dilation but kept the pooling layer. <br> (3)只使用前端网络，比FCN8s,DeepLab-Msc（v1）再VOC12 test性能更高（67.6% vs 62.2%、62.9%）,并且高于DeepLab-CRF[66.4%]，Contex+CRF-RNN再VOC12 test 取得了75.3% mean Iou. <br> <img src="https://images2.imgbox.com/95/28/DQxTe7d8_o.png" alt="这里写图片描述" title=""></p> 
<h3 id="paper10-eccv2016-higher-order-conditional-random-fields-in-deep-neural-networks">Paper10 [ECCV2016]: 《Higher Order Conditional Random Fields in Deep Neural Networks》</h3> 
<p><strong>本论文为ECCV16 Paper arxiv:1511</strong> <br> <img src="https://images2.imgbox.com/07/1a/2Kwa7ZhW_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>本网络架构特点：</strong> <br> (1)与CRF_RNN作者相同，作者在CRF_RNN基础上加 <strong>higher order potentials</strong>,that is based on object detections and superpixels,and demonstrate it can be included in a CRF embedded within a deep network. <br> <img src="https://images2.imgbox.com/0d/98/hiM8D5mK_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>（2）Superpixel先验</strong> ，enforce the consistency of the semantic segmentation output feature of the image <br> <strong>(3)object detection先验</strong>，Intuitively，an object detector with a high recall can help the semantic segmentation algorithm by finding objects appearing in an image. <br> <strong>(4)</strong> 在VOC12 test上比CRF_RNN高了3.2个点，以CRF_RNN为基础，+det_potential比+superpixel_potential性能要高，end-to-end训练比分段训练性能高。在VOC12 test上取得了new-state-of-the-art,77.9%,使用了COCO+VOC数据训练。 <br> <img src="https://images2.imgbox.com/03/66/m7Cr7xQD_o.png" alt="这里写图片描述" title=""></p> 
<h3 id="paper11-eccv2016-laplacian-pyramid-reconstruction-and-refinement-for-semantic-segmentation">Paper11 [ECCV2016]: 《 Laplacian Pyramid Reconstruction and Refinement for Semantic Segmentation 》</h3> 
<p><strong>本论文为ECCV16 Paper arxiv:1605</strong> <br> <img src="https://images2.imgbox.com/24/ff/9OOGWEf9_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>本网络架构特点：</strong></p> 
<p>(1)证明了高层的feature map的spatial resolution很low ，但高层的feature map的高维特征包含了sub-pixel的位置信息。 <br> <img src="https://images2.imgbox.com/61/e7/AtQzyfmd_o.png" alt="这里写图片描述" title=""></p> 
<p>（2）describing a multi-resolution reconstruction feature maps and multiplicative gating to successive refine segment boundaries reconstructed from lower-resolution maps.Boundary masking机制，<strong>也可以理解成attention机制</strong>，<strong>高低层特征融合的时候进行加权求和</strong> 。 <br> (3)低分辨率上采样到高分辨率的unsampling是可学习过程（不同于标准的unsampling）,该过程称之为<strong>Reconstruction</strong>(双线性插值可以看成是Reconstruction的一种特殊版本)，这种Reconstruction可以增加spatial accuracy. <br> <img src="https://images2.imgbox.com/8f/df/QXN5VZid_o.png" alt="这里写图片描述" title=""></p> 
<p>（4）以VGG-16为前端，在VOC12 test ，训练只用VOC training data.性能高于FCN-8s，Hypercolumn, DeepLab-MSc+CRF(v1),CRF_RNN,DeconvNet。以ResNet101为前端，在VOC12 test，训练利用VOC and COCO性能在VOC上可比肩DeepLabv2(LRR-CRF 79.3% DeepLab-CRF(V2) 79.7%) <br> <img src="https://images2.imgbox.com/ba/85/UOfPqbJU_o.png" alt="这里写图片描述" title=""></p> 
<h3 id="paper12-pami2016-deeplabv2-semantic-image-segmentation">Paper12 [PAMI2016]: 《DeepLab_v2 :semantic image segmentation 》</h3> 
<p><strong>《DeepLab semantic image segmentation with Deep Convolutional Nets, Atrous Convolution and fully connected CRFs》</strong></p> 
<p><strong>本论文为TPAMI16 Paper arxiv:1606</strong> <br> <img src="https://images2.imgbox.com/8d/5d/tgeCLr7Q_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>本网络架构特点：</strong> <br> (1)以Resnet为基础，提出了<strong>“atrous convolution”</strong>(膨胀卷积)，在不增加参数和计算开销的情况下，增大了filter的感受野,DeepLab_v1中也采用了atrous convolution。 <br> <img src="https://images2.imgbox.com/77/c4/qWUhSp9Z_o.png" alt="这里写图片描述" title=""> <br> (2)提出了<strong>ASPP</strong>(Atrous spatial pyramid pooling，膨胀空间金字塔pooling),实现多尺度。</p> 
<blockquote> 
 <blockquote> 
  <p>proposing atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales.</p> 
 </blockquote> 
</blockquote> 
<p><img src="https://images2.imgbox.com/ba/3d/hblDJBbq_o.png" alt="这里写图片描述" title=""></p> 
<p>(3)组合深度卷积神经网络和CRF，提高object boundary的准确率。</p> 
<blockquote> 
 <blockquote> 
  <p>传统方法中，条件随机场用于平滑带噪声的分割图，论文(【 GrabCut: Interactive foreground extraction using iterated graph cuts, in SIGGRAPH,2004.】、【Robust higher order potentials for <br> enforcing label consistency, IJCV, vol. 82, no. 3, pp. 302–324,2009.】)。通常，这些模型将邻近结点耦合，有利于将相同标记分配给空间上接近的像素。定性的说，这些短程条件随机场主函数会清除构建在局部手动特征上层弱分类器的错误预测。与这些弱分类器相比，现代深度卷积神经网络架构，比如像我们在本文中用的，生成score map和定性不同的语义标记预测。score map通常非常平滑，生成相同的分类结果。在这种情形下，用短程条件随机场可能就不好了，我们的目标是要恢复详细的局部结构而不是平滑它。用局部条件随机场关联中的反差灵敏势，可以增强定位，但还是会漏掉细小结构，通常都需要处理离散优化问题。为了克服短程条件随机场的局限，我们在系统中整合了论文【Efficient inference in fully connected crfs with gaussian edge potentials, in NIPS, 2011】的全连接随机场模型。 <br> <img src="https://images2.imgbox.com/1c/61/AdmcRb8L_o.png" alt="这里写图片描述" title=""> <br> <img src="https://images2.imgbox.com/18/34/VWHbyQAs_o.png" alt="这里写图片描述" title=""> <br> 第一个是像素位置（记为p）和RGB颜色（记为I）间的双向核，第二个核是像素位置。超参数σασβσγ控制高斯核的尺度。第一个核强制位置相近、像素值相似的像素具有相同的分类(label)，第二个核在强制平滑时只考虑空间上的接近程度。</p> 
 </blockquote> 
</blockquote> 
<p>(4)DeepLap系统在VOC12上取得了79.7%的mIOU. <br> <img src="https://images2.imgbox.com/50/d8/ivqufCP2_o.png" alt="这里写图片描述" title=""></p> 
<h3 id="paper13-cvpr2017-full-resolution-residual-networks-for-semantic-segmentation-in-street-scenes">Paper13 [CVPR2017]: 《Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes》</h3> 
<p><strong>本论文为CVPR17 Paper arxiv:1611_v1</strong> </p> 
<p><img src="https://images2.imgbox.com/04/95/IyMzHaEU_o.png" alt="这里写图片描述" title=""> <br> <strong>本网络架构特点：</strong> <br> (1)一个网络两支处理流，residual stream和pooling stream。</p> 
<blockquote> 
 <blockquote> 
  <p>One stream carries information at the full image resolution, enabling precise adherence to segment boundaries.The other stream undergoes a sequence of pooling operations to obtain robust features for recognition. The two streams are coupled at the full image resolution using residuals.</p> 
 </blockquote> 
</blockquote> 
<p>(2)解决了当前基于预训练方法主体网络无法加入新结构，比如batch normalization or new activation。 <br> (3)是所有当前没有预训练的模型中，在Cityscapes dataset上去的了IoU score 71.8%,new state-of-the-art performance. <br> <img src="https://images2.imgbox.com/1e/fb/AWJ2lyzZ_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>(4)网络具体配置：</strong> <br> <img src="https://images2.imgbox.com/05/59/WY3UXmNO_o.png" alt="这里写图片描述" title=""></p> 
<h3 id="paper14-cvpr2017-refinenet-multi-path-refinement-networks-for-high-resolution-semantic-segmentation">Paper14 [CVPR2017]: 《RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation》</h3> 
<p><strong>本论文为CVPR17 Paper arxiv:1611</strong> <br> <img src="https://images2.imgbox.com/e5/9f/nQU5w0f1_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>本网络架构特点：</strong> <br> <strong>(1)</strong> repeated subsampling operations like <strong>pooling</strong> or <strong>convolution striding</strong> in deep CNNs lead to a significant de-crease in the initial image resolution,reducing the final image prediction typically by a factor of 32 or 16,which result in losing much of the finer image structure.In order to tackle the limitation,this paper proposes <strong>RefineNet, a generic multi-path refinement network.</strong></p> 
<blockquote> 
 <blockquote> 
  <p>RefineNet explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections.multi-path refinement network (RefineNet) which exploits features at multiple levels of abstraction for high-resolution semantic segmentation. RefineNet <strong>refines low-resolution (coarse) semantic features with fine grained low-level features in a recursive manner to generate high-resolution semantic feature maps</strong>. the model is flexible in that it can be cascaded and modified easily.</p> 
 </blockquote> 
</blockquote> 
<p><strong>(2)**proposing **Residual Convolution Unit[RCU]</strong>, <strong>Multi resolution fusion,Chained residual pooling.</strong></p> 
<blockquote> 
 <blockquote> 
  <p>chained residual pooling is able to capture background context from a large image region. It does so by　efficiently pooling　features with multiple window sizes and　fusing them together with residual connections and learnable weights.</p> 
 </blockquote> 
</blockquote> 
<p><strong>(3)</strong> achieving an intersection-over-union score of <strong>83.4 on the challenging PASCAL VOC 2012 dataset</strong>, which is the best <br> reported result to date.</p> 
<blockquote> 
 <blockquote> 
  <p>achievｉｎｇ new state-of-the-art performance on 7 public datasets, including PASCAL VOC 2012, PASCAL-Context, NYUDv2, SUN-RGBD, Cityscapes, ADE20K, and the object parsing Person-Parts dataset. In particular, we achieve an IoU score of 83.4 on the PASCAL VOC 2012 dataset, outperforming the currently best approach DeepLab by a large margin.</p> 
 </blockquote> 
</blockquote> 
<p><img src="https://images2.imgbox.com/68/cd/aAQZlYyU_o.png" alt="这里写图片描述" title=""></p> 
<h3 id="paper15-cvpr2017-pyramid-scene-parsing-network">Paper15 [CVPR2017]: 《Pyramid Scene Parsing Network》</h3> 
<p><strong>本论文为CVPR17 Paper arxiv:1612</strong> </p> 
<p><img src="https://images2.imgbox.com/51/d2/SA6RizlA_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>本网络架构特点：</strong> <br> (1)以Resnet为基础，提出了<strong>PSPNet(Pyramid scene parsing network)</strong>,金字塔场景分割网络，使用了辅助损失(auxiliary loss)，在VOC12上获得了85.4%mIOU。 <strong>new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.</strong> <br> <img src="https://images2.imgbox.com/52/e2/1fCi0I5S_o.png" alt="这里写图片描述" title=""> <br> (2)获得ImageNet Scene parsing challenging 2016 第一名。</p> 
<h3 id="paper15-arxiv2017-ｔｕｓｉｍｐｌｅ-understanding-convolution-for-semantic-segmentation">Paper15 [arXiv2017]: 《ＴｕＳｉｍｐｌｅ Understanding Convolution for Semantic Segmentation》</h3> 
<p><strong>本论文 arxiv:1702</strong> <br> <img src="https://images2.imgbox.com/36/c2/SQ8BsjCG_o.png" alt="这里写图片描述" title=""> <br> <strong>本网络架构特点：</strong> <br> <strong>[Ｔusimple]</strong></p> 
<p><strong>(1)</strong>以Resnet为基础，design Dense upsampling convolution[<strong>DUC</strong>] to generate pixel-level prediction,which is able <strong>to capture and decode more detailed information that is generally missing in bilinear upsampling</strong>. Because Bilinear upsampling is not learnable and may lose fine details.</p> 
<p><strong>（２)</strong>膨胀卷积(Dilated Convolution)的局限性： <br> <strong>the “gridding issue”</strong> caused by the standard dilated convolution dilated convolution operation loses some neighboring information,and the problem gets worse when the rate of dilation increases,generally in higher layers when the receptive field is large: the convolutional kernel is too sparse to cover any local information,since the non-zero values are too far apart. <br> <img src="https://images2.imgbox.com/2c/cb/X0FM7VBC_o.png" alt="这里写图片描述" title=""> <br> 针对该局限性提出了Hybrid Dilated Convolution[<strong>HDC</strong>]来解决， <br> <strong>the goal of HDC is to let the final size of the RF of a series of　convolutional operations fully covers a square region without any holes or missing edges.</strong></p> 
<p><strong>(3)**achieving a state-of-the-art result of **83.1% mIOU on VOC12 test</strong> ; We also have achieved state-of-the-art overall on the KITTI road estimation benchmark and Cityscapes dataset.</p> 
<h3 id="paper16-iccv2017-large-kernel-matters-improve-semantic-segmentation-by-global-convolutional-network">Paper16 [ICCV2017]: 《Large Kernel Matters ——Improve Semantic Segmentation by Global Convolutional Network》</h3> 
<p><strong>本论文为ICCV17 Paper arxiv:1703</strong> <br> <img src="https://images2.imgbox.com/c1/9e/FgQ3Dp5Q_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>本网络架构特点：</strong> <br> <strong>（１）</strong>问题　A well-designed segmentation model should deal with <strong>classification and localization</strong>.</p> 
<blockquote> 
 <blockquote> 
  <p>For the classification task, the models are required to be in-variant to various transformations like translation and rotation. But for the localization task, models should be transformation-sensitive, i.e., precisely locate every pixel for each semantic category. </p> 
 </blockquote> 
</blockquote> 
<p><strong>（２）</strong> proposing a <strong>Global Convolutional Network[GCN]</strong> to address both the classification and localization issues for the semantic segmentation. <br> <img src="https://images2.imgbox.com/75/88/FLScndqD_o.png" alt="这里写图片描述" title=""> <br> <strong>(３)</strong> proposing a <strong>Boundary Refinement[BR]</strong> ｂｌｏｃk to further improve the localization performance near the object boundary. <br> <strong>(4)</strong>作者进一步设计了Resnet50_GCN，用ＧＣＮ——ｂｏｔｔｌｅｎｅｃｋ代替了Resnet中原本的bottleneck,在ImageNet上分类正确率有所下降，但分割性能更好。 <br> <img src="https://images2.imgbox.com/22/8a/o7vVWWMN_o.png" alt="这里写图片描述" title=""> <br> <strong>(5)</strong> achieves state-of-art performance on two public benchmarks and significantly outperforms previous results, <strong>82.2% (vs 80.2%) on PASCAL VOC2012 dataset and 76.9% (vs 71.8%) on Cityscapes dataset.</strong> <br> <img src="https://images2.imgbox.com/98/1b/PcOrz82H_o.png" alt="这里写图片描述" title=""> <br> <img src="https://images2.imgbox.com/28/39/2I7BrHoo_o.png" alt="这里写图片描述" title=""></p> 
<h3 id="paper17-iccv2017-deformable-convolutional-networks">Paper17 [ICCV2017]: 《Deformable Convolutional Networks》</h3> 
<p><strong>本论文为ICCV17 Paper arxiv:1703</strong></p> 
<p><strong>本网络架构特点：</strong> <br> (1)提出了deformable convolution和deformable RoI pooling(通过额外的offset,根据任务学习到的offset，来增强spatial sampling locations) <br> <img src="https://images2.imgbox.com/e4/5d/E03YM0tN_o.png" alt="这里写图片描述" title=""> </p> 
<p><img src="https://images2.imgbox.com/50/29/9zYdo4hx_o.png" alt="这里写图片描述" title=""> <br> (2)解决了当前主流方法中(卷积单元固定位置采用、RoI pooling输出固定spatial bin的局限性，这二者都没有很好应对object几何变换) <br> <img src="https://images2.imgbox.com/33/89/8zVdgfh8_o.png" alt="这里写图片描述" title=""></p> 
<h3 id="paper18-iccv2017-scale-adaptive-convolutions-for-scene-parsing">Paper18 [ICCV2017]: 《Scale-adaptive Convolutions for Scene Parsing》</h3> 
<p><strong>本论文为ICCV17 Paper </strong> <br> <img src="https://images2.imgbox.com/d9/7c/Buu68p5n_o.png" alt="这里写图片描述" title="">**</p> 
<p><strong>本网络架构特点：</strong> <br> (1)The CNNs with standard convolutions can only handle a single scale due to the fixed-size receptive fields,however scene images usually contain stuff(e.g. sky,wall) and objects(e.g. people,cars) with various sizes,leading two critical drawbacks:</p> 
<blockquote> 
 <blockquote> 
  <p><strong>I):objects which are enough larger than the receptive fields often have inconsistent parsing predictions, since the receptive fields may cover only small part of the large objects;</strong> <br> <strong>II):small objects are often ignored and mislabeled to the background because the receptive fields cover too much background instead of focusing on the small objects.</strong></p> 
 </blockquote> 
</blockquote> 
<p>作者针对这两个问题，proposing the scale-adaptive Convolutions,which are capable of automatically learning flexible-size receptive field,dealing with objects of various sizes.</p> 
<p>（２）Scale-adaptive Convolutions can be considered as the generalized convoluitons with adaptive dialation parameters . <br> <img src="https://images2.imgbox.com/83/0d/Y2UQTp4X_o.png" alt="这里写图片描述" title=""> <br> <strong>（３）整个架构以ＤｅｅｐLab_v2为基础框架，在ＡＤＥ２０Ｋ和Cityscapes验证了该方法的有效性。</strong> <br> <img src="https://images2.imgbox.com/28/30/vZCD8uou_o.png" alt="这里写图片描述" title=""></p> 
<h3 id="paper19-arxiv2017-rethinking-atrous-convolution-for-semantic-image-segmentation">Paper19 [arXiv2017]: 《Rethinking Atrous Convolution for Semantic Image Segmentation》</h3> 
<p><strong>Paper arxiv:1706</strong> <br> <img src="https://images2.imgbox.com/b1/d9/8ML61AwZ_o.png" alt="这里写图片描述" title=""> <br> <strong>【DeepLab_v3】</strong> <br> <strong>本网络架构特点：</strong> <br> <strong>(1)</strong> 改进了空间维度上的金字塔空洞池化方法(ASPP)； <br> <strong>(2)</strong> 该模块级联了多个空洞卷积结构。</p> 
<blockquote> 
 <blockquote> 
  <p><strong>具体解释：</strong> <br> (1)与在DeepLab v2网络、空洞卷积中一样，这项研究也用空洞卷积/多空卷积来改善ResNet模型。 <br> (2)这篇论文还提出了三种改善ASPP的方法，涉及了像素级特征的连接、加入1×1的卷积层和三个不同比率下3×3的空洞卷积，还在每个并行卷积层之后加入了批量归一化操作。 <br> (3)级联模块实际上是一个残差网络模块，但其中的空洞卷积层是以不同比率构建的。这个模块与空洞卷积论文中提到的背景模块相似，但直接应用到中间特征图谱中，而不是置信图谱。置信图谱是指其通道数与类别数相同的CNN网络顶层特征图谱。 <br> (4)该论文独立评估了这两个所提出的模型，尝试结合将两者结合起来并没有提高实际性能。两者在验证集上的实际性能相近，带有ASPP结构的模型表现略好一些，且没有加入CRF结构。 <br> (5)这两种模型的性能优于DeepLabv2模型的最优值，文章中还提到性能的提高是由于加入了批量归一化层和使用了更优的方法来编码多尺度背景。</p> 
 </blockquote> 
</blockquote> 
<p><strong>(3)</strong> The proposed ‘DeepLabv3’ system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark. <br> <img src="https://images2.imgbox.com/2f/fa/7zpCwQh7_o.png" alt="这里写图片描述" title=""></p> 
<h3 id="paper20-iccv2017-foveanet-perspective-aware-urban-scene-parsing">Paper20 [ICCV2017]: 《FoveaNet: Perspective-aware Urban Scene Parsing》</h3> 
<p><strong>本论文为ICCV17 Paper arXiv:1708</strong> <br> <img src="https://images2.imgbox.com/81/de/g28jGMsU_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>本网络架构特点：</strong> <br> <strong>(1)</strong> proposing to consider perspective geometry in urban scene parsing and introduce a perspective estimation network for learning the global perspective geometry of urban scene images. <br> <strong>解决的问题：</strong> <br> <strong>a)</strong> Most of the current solutions employ generic image parsing models that treat all scales and locations in the images equally and <strong>do not consider the geometry property of car-captured urban scene images.</strong> Thus, they suffer from heterogeneous [多种多样的]object scales caused by perspective projection [透视投影]of cameras on actual scenes and inevitably encounter parsing failures on distant objects as well as other boundary and recognition errors.图像周围区域的large object 经常会有“broken down” error.例如large object容易分割出肖的区域到别处其它不同但相似的类，如下图 <br> <img src="https://images2.imgbox.com/be/9f/LfBkAojo_o.png" alt="这里写图片描述" title=""></p> 
<blockquote> 
 <blockquote> 
  <p>presenting a new <strong>perspective-aware CRFs model</strong> that is able to reduce the typical “broken-down” errors in parsing peripheral regions of a scene image.</p> 
 </blockquote> 
</blockquote> 
<p><strong>b)</strong>在图像中消失点处的密集smll objects分割效果差</p> 
<blockquote> 
 <blockquote> 
  <p>developing a <strong>perspective-aware parsing network</strong> that addresses the scale heterogeneity issues well for urban scene images and gives accurate parsing on small objects crowding around the vanishing point.</p> 
 </blockquote> 
</blockquote> 
<p><strong>(2)</strong>本文提出了一个 <strong>FoveaNet</strong> model,充分利用了透视投影来解决通用parsing model不能解决的问题a) and b). <br> <strong>(3)</strong>设计的<strong>Perspective Estimation Network</strong>[PEN]产生聚焦区[Fovea region]，对密集的小目标群体有非常好的分割性能。 <br> <img src="https://images2.imgbox.com/3b/0c/WK6LHt3T_o.png" alt="这里写图片描述" title=""></p> 
<p><img src="https://images2.imgbox.com/de/dc/QgG0vFxD_o.png" alt="这里写图片描述" title=""> <br> <strong>（４）</strong> <strong>ＦoveaNet provide new state-of-the-art performance on City spaces.</strong> 超过了DeepLabv2和ＬＲＲ <br> <img src="https://images2.imgbox.com/e2/a3/LR5NGRGd_o.png" alt="这里写图片描述" title=""></p> 
<h3 id="paper21-cvpr2017-difficulty-aware-semantic-segmentationvia-deep-layer-cascade">Paper21 [CVPR2017] 《 Difficulty-Aware Semantic Segmentationvia Deep Layer Cascade》</h3> 
<p><strong>本论文为CVPR17 Paper </strong> <br> <img src="https://images2.imgbox.com/b7/65/qv48Rk5k_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>本网络架构【ＬＣ】特点：</strong> <br> (1)propose a novel deep layer cascade (LC) method to improve the accuracy and speed of semantic segmentation. Unlike the conventional model cascade (MC) that is composed of multiple independent models, LC treats a single deep model as a cascade of several sub-models.Earlier sub-models are trained to handle easy and confident regions, and they progressively feed-forward harder regions to the next sub-model for processing.</p> 
<p>(2)前端主干网络采用的是Ｉnception-Resnet-V2(带atrous convolution) <br> (3)在没有多尺度、ｐｏｓt-processing,在voc12 test上表现比ＤｅｅpLabv2更好<strong>【７０．４２％ vs 73.91%】</strong>。加上多尺度和post-processing在ＶＯＣ１２ ｔｅｓｔ取得了８２.7 mean IoU. <strong>[DeepLabv2_COCO_Msc_ASPP:79.7%]</strong> <br> <img src="https://images2.imgbox.com/78/e4/eNY6hjDj_o.png" alt="这里写图片描述" title=""> <br> 在Cityscapes test set实验结果如下： <br> <img src="https://images2.imgbox.com/8b/f3/XRoCVUib_o.png" alt="这里写图片描述" title=""></p> 
<h3 id="paper22-bmvc2017-semantic-segmentation-with-reverse-attention">Paper22 [BMVC2017] 《Semantic Segmentation with Reverse Attention》</h3> 
<p><strong>本论文为BMVC17 Paper arXiv:1707 </strong> <br> <img src="https://images2.imgbox.com/4d/47/M5nz3ZYi_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>本网络架构【ＬＣ】特点：</strong> <br> (1):propose a <strong>reverse attention network</strong> (RAN) architecture that trains the network to capture the opposite concept (i.e., what are not associated with a target class) as well. The RAN is a three-branch network that performs the <strong>direct, reverse and reverse-attention</strong> learning processes simultaneously.</p> 
<blockquote> 
 <p>Traditionally, the convolutional classifiers are taught to learn the representative semantic features of labeled semantic objects.</p> 
</blockquote> 
<p>（２）Being built upon the DeepLabv2-LargeFOV,the RAN achieves the state-of-the-art mean IoU score (48.1%) for the　challenging PASCAL-Context dataset. Significant performance improvements are also observed for the PASCAL-VOC, Person-Part, NYUDv2　and ADE20K datasets.<strong>在ＶＯＣ 12 test上取得了80.5% mean IoU[DeepLabv2-LargeFOV:79.1%,DeepLabV2-ASPP:79.7%]</strong> <br> <img src="https://images2.imgbox.com/82/c2/aXwJK7K5_o.png" alt="这里写图片描述" title=""></p> 
<p><img src="https://images2.imgbox.com/80/bc/GsDPKIFC_o.png" alt="这里写图片描述" title=""></p> 
<h3 id="paper23-arxiv2016-mixed-context-networks-for-semantic-segmentation">Paper23 [arXiv2016] 《Mixed context networks for semantic segmentation》</h3> 
<p><strong>本论文为 arXiv:1610 </strong> <br> <img src="https://images2.imgbox.com/27/0a/Hbh1sibV_o.png" alt="这里写图片描述" title=""> <br> <strong>本网络架构特点：</strong> <br> (1)proposing <strong>mixed context network</strong> <strong>[MCN]</strong></p> 
<blockquote> 
 <p>FCN-based systems gained great improvement in this area. Unlike classification networks, combining features of different layers plays an important role in these dense prediction models, as these features contains information of different levels. A number of models have been proposed to show how to use these features. However, what is the best architecture to make use of features of different layers is still a question</p> 
</blockquote> 
<p>(2)proposing message passing network <strong>[MPN]</strong>,</p> 
<blockquote> 
 <p>the CRF-RNN is memory cost, especially when the class number is large, for example 150 in ADE20K. <br> <img src="https://images2.imgbox.com/46/2d/NGoaro3H_o.png" alt="这里写图片描述" title=""></p> 
</blockquote> 
<p>(3)以VGG-16为基础，在VOC12 test <strong>取得了81.4% mean IoU.</strong></p> 
<h3 id="paper24-eccv2016-fast-exact-and-multi-scale-inference-for-semantic-segmentation-with-deep-g-crfs">Paper24 [ECCV2016] 《Fast, Exact and Multi-Scale Inference for Semantic Segmentation with Deep G-CRFs》</h3> 
<p>《Fast, Exact and Multi-Scale Inference for Semantic Image Segmentation with Deep Gaussian CRFs》 <br> <strong>本论文为 ECCV2016 arXiv:1603 </strong> <br> <img src="https://images2.imgbox.com/e0/89/tX9zZ7Y7_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>本网络架构特点：</strong> <br> (1):combine the virtues of Gaussian Conditional Random Feilds <strong>(G-CRF)</strong> with Deep Learning. <br> (2) proposing structured prediction task having a unique global optimum that is obtained exactly form the solution of a linear system. <br> <img src="https://images2.imgbox.com/1e/dd/fC5ltxCw_o.png" alt="这里写图片描述" title=""></p> 
<blockquote> 
 <p>where A denotes the symmetric N × N matrix of pairwise terms, and B denotes the N × 1 vector of unary terms. <strong>the pairwise terms A and the unary terms B are learned from the data using a fully convolutional network.</strong></p> 
 <p><strong>[Inference ]</strong> Given A and B, inference involves solving for the value of x that minimizes the energy function. If (A + λI) is symmetric positive definite, then E(x) has a unique global minimum at: <br> <img src="https://images2.imgbox.com/96/6e/i0DYSbHK_o.png" alt="这里写图片描述" title=""></p> 
</blockquote> 
<p>(3)our pairwise terms do not have to be simple hand-crafted expressions, as in the line of works building on the DenseCRF[<strong>such as CRFasRNN</strong>]</p> 
<p>(4)proposing multi-resolution architectures to couple information across scales in a joint optimization framework, yielding systematic improvements.(比简单的跨层特征融合高出０．３个点。 <br> <img src="https://images2.imgbox.com/64/26/lqj4rVZJ_o.png" alt="这里写图片描述" title=""></p> 
<p>(5)以ＤｅｅｐＬａｂ-V2 Resnet-101为基础，在voc12 test上，Ｄｅｅｐ G-CRF achieving 80.2% mean IoU. <br> <img src="https://images2.imgbox.com/51/12/yMEx6LuS_o.png" alt="这里写图片描述" title=""></p> 
<h3 id="paper25-iccv2017-dense-and-low-rank-gaussian-crfs-using-deep-embeddings">Paper25 [ICCV2017] 《Dense and Low-Rank Gaussian CRFs Using Deep Embeddings》</h3> 
<p><strong>本论文为 ICCV2017 </strong> <br> 与《Fast, Exact and Multi-Scale Inference for Semantic Image Segmentation with Deep Gaussian CRFs》的作者相同 <br> <img src="https://images2.imgbox.com/1c/a1/T9wXCy06_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>本网络架构[Dense G-CRF]特点：</strong> <br> （１）作者在Sparse G-CRF的基础上，针对Sparse G-CRF的局限性</p> 
<blockquote> 
 <p>While the Deep G-CRF model described above allows for efficient and exact inference, in practice it only captures interactions in small (4−,8− and 12−connected) neighborhoods. The model may thereby lose some of its power by ignoring a richer set of long-range interactions. The extension to fully-connected graphs is technically challenging because of the non-sparse matrix A it involves. Assuming an image size of 800 × 800 pixels, 21 labels (PASCAL VOC benchmark), and a network with a spatial down-sampling factor of 8 [5, 6], the number of variables is N = (100 × 100) × 21 and the number of elements in A would be N 2 ∼ 10 10 . This is prohibitively large due to both memory and computational requirements.</p> 
</blockquote> 
<p>To <strong>overcome this challenge</strong>, we advocate forcing A to be a low-rank. In particular, we propose decomposing the N × N matrix A into a product of the form <br> <img src="https://images2.imgbox.com/f9/df/tVaiRXaB_o.png" alt="这里写图片描述" title=""></p> 
<p>(2)introducing a structured prediction model that endows the Deep Gaussian Conditional Random Field (G-CRF) with a densely connected graph structure. We keep memory and computational complexity under control by expressing the pairwise interactions as inner products of low-dimensional, learnable embeddings. The G-CRF system matrix is therefore low-rank, allowing us to solve the resulting system in a few milliseconds on the GPU by using conjugate gradient. As in G-CRF, inference is exact, the unary and pairwise terms are jointly trained end-to-end by using analytic expressions for the gradients, while we also develop even faster, Potts-type variants of our embeddings. <br> <img src="https://images2.imgbox.com/b7/f4/sQs7u974_o.png" alt="这里写图片描述" title=""> <br> (3)在ＶＯＣ１２ test取得了８０．４％ mean IoU. <br> <img src="https://images2.imgbox.com/0a/8a/sLuriLWY_o.png" alt="这里写图片描述" title=""></p> 
<h3 id="paper26-iccv2017-segmentation-aware-convolutional-networks-using-local-attention-masks">Paper26 [ICCV2017] 《Segmentation-Aware Convolutional Networks Using Local Attention Masks》</h3> 
<p><strong>本论文为 ICCV2017 </strong> <br> <img src="https://images2.imgbox.com/7a/9c/rr6jU9bC_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>本网络架构[Segaware]特点：</strong> <br> (1)Introducing an approach to integrate segmentation in-formation within a convolutional neural network (CNN). This counter-acts the tendency of CNNs to smooth information across regions and increases their spatial precision. <br> and set up a CNN to provide <strong>an embedding space</strong> where region co-membership can be estimated based on Euclidean distance. We <strong>use these embeddings to compute a local attention mask relative to every neuron position</strong>. We incorporate such masks in CNNs and replace the convolution operation with a “segmentation-aware” variant that allows a neuron to selectively attend to inputs coming from its own region <br> <img src="https://images2.imgbox.com/55/2b/aEyX7Zcj_o.png" alt="这里写图片描述" title=""></p> 
<p>（２）以DeepLabv2为基础，在voc 12 test上取得了７９．８％ mean IoU <br> <img src="https://images2.imgbox.com/65/fb/mDHKSUuT_o.png" alt="这里写图片描述" title=""></p> 
<h2 id="附录１ｓｅmantic-segmentation-datasets">附录１：Ｓｅmantic Segmentation Datasets</h2> 
<h3 id="1the-pascal-voc2011-and-sbd">1.The PASCAL VOC2011 and SBD</h3> 
<p>The PASCAL VOC 2011 segmentation challenge training set labels 1112 images.[Semantic contours from inverse detectors ,in ICCV2011] have collected labels for a much larger set of 8498 PASCAL training images. <br> The train/val/test splits of PASCAL VOC segmentation challenge and SBD diverge. Most notably VOC 2011 segval intersects with SBD train. Care must be taken for proper evaluation by excluding images from the train or val splits. <br> We train on the 8,498 images of SBD train. We validate on the non-intersecting set defined in the included seg11valid.txt.[736张] 因为有部分VOC11 val.txt的图片包含在SBD 训练集(8498张)</p> 
<h3 id="2the-pascal-voc2012">2.The PASCAL VOC2012</h3> 
<p>The PASCAL VOC 2012 segmentation benchmark[34] involves 20 foreground object classes and one background class. The original dataset contains <br> 1464(train),1449 (val), and 1456 (test) pixel-level labeled images for training, validation, and testing, respectively. The dataset is augmented by the extra annotations provided by [Semantic contours from inverse detectors, in ICCV, 2011],resulting in 10582 (trainaug) training images.</p> 
<h3 id="3voc2012-vs-voc2011">3.VOC2012 vs. VOC2011</h3> 
<p>For VOC2012 the majority of the annotation effort was put into increasing the size of the segmentation and action classification datasets, andno additional annotation was performed for the classification/detection tasks. The list below summarizes the differences in the data between VOC2012 and VOC2011.</p> 
<pre><code>Classification/Detection: The 2012 dataset is the same as that used in 2011. No additional data has been annotated. For this reason, participants are not allowed to run evaluation on the VOC2011 dataset, and this option on the evaluation server has been disabled.

Segmentation: The 2012 dataset contains images from 2008-2011 for which additional segmentations have been prepared. As in previous years the assignment to training/test sets has been maintained. The total number of images with segmentation has been increased from 7062 to 9993.
</code></pre> 
<h3 id="4the-pascal-context">4.The PASCAL-Context</h3> 
<p>[200~PASCAL-Context provides whole scene annotations of PASCAL VOC 2010. While there are over 400 distinct classes, we follow the 59 class task defined by [26] that　picks the most frequent classes.</p> 
<p>The PASCAL-Context dataset [35] provides detailed semantic labels for the whole scene, including both object (e.g., person) and stuff (e.g., sky). Following [The role of context for object detection and semantic segmentation in the wild,in CVPR, 2014], the proposed models are evaluated on the most frequent 59 classes along with one background category. The training set and validation set contain 4998 and 5105 images. <br> /data/pascal-context/trainval/文件夹提供了trainval=10103张图片的标注 <br> 训练时原始图片.jpg去/data/pascal/VOC2010/JPEGImages/下读取</p> 
<h3 id="5nyudv2">5.NYUDv2</h3> 
<p>NYUDv2 [30] is an RGB-D dataset collected using the Microsoft Kinect. It has 1449 RGB-D images, with pixel-wise labels that have been coalesced into a 40 class semantic segmentation task by Gupta et al. [13]. We report results on the standard split of 795 training images and 654 testing images.</p> 
<h3 id="6sift-flow">6.SIFT Flow</h3> 
<p>SIFT Flow is a dataset of 2,688 images with pixel labels for 33 semantic categories (“bridge”, “mountain”, “sun”),as well as three geometric categories (“horizontal”, “vertical”, and “sky”).the standard split into 2,488 training and 200 test images,</p> 
<h3 id="７cityscapes-dataset">７．Cityscapes Dataset</h3> 
<p>The Cityscapes dataset [5] contains　5,000　images,including 2,975 images in training set, 500　images in validation set and 1,525 images in test set. The images in this dataset are collected in street scenes from 50　different cities, with high quality pixel-level annotations of　19 semantic classes and high resolution of 2048×1024. Intersection over Union (IoU) averaged over all the categoriesis adopted for evaluation. <br> <strong>Part I</strong></p> 
<blockquote> 
 <blockquote> 
  <p>gtFine_trainvaltest.zip (241MB) <br> fine annotations for train and val sets (3475 annotated images[train:2975 val:500]) and dummy annotations (ignore regions) for the test set (1525 images)</p> 
  <p>gtCoarse.zip (1.3GB) <br> coarse annotations for train and val set (3475 annotated images) and train_extra (19998 annotated images)</p> 
 </blockquote> 
</blockquote> 
<p><strong>Others</strong></p> 
<blockquote> 
 <blockquote> 
  <p>leftImg8bit_trainvaltest.zip (11GB) [md5] <br> left 8-bit images - train, val, and test sets (5000 images)</p> 
  <p>leftImg8bit_trainextra.zip (44GB) [md5] <br> left 8-bit images - trainextra set (19998 images)</p> 
  <p>camera_trainvaltest.zip (2MB) [md5] <br> intrinsic and extrinsic camera parameters for train, val, and test sets</p> 
  <p>camera_trainextra.zip (8MB) [md5] <br> intrinsic and extrinsic camera parameters for trainextra set</p> 
  <p>vehicle_trainvaltest.zip (2MB) [md5] <br> vehicle odometry + GPS coordinates + temperature for train, val, and test sets</p> 
  <p>vehicle_trainextra.zip (7MB) [md5] <br> vehicle odometry + GPS coordinates + temperature for trainextra set</p> 
  <p>leftImg8bit_demoVideo.zip (6.6GB) [md5] <br> video sequences for qualitative evaluation, left 8-bit images only</p> 
 </blockquote> 
</blockquote> 
<h3 id="8ade20k-dataset">8.ADE20K Dataset</h3> 
<p>The ADE20K dataset [34] is a large scale dataset recently released by ImageNet Large Scale Visual Recognition Challenge 2016 (ILSVRC2016). This dataset contains 150 semantic classes for scene parsing, with 20,210 images for training, 2,000 images for validation and 3,351 images for testing. Pixel-level annotations are provided for entire images. This dataset is more scene-centric with a diverse range of object categories. The performance is evaluated based on both pixel-wise accuracy and the Intersection over Union (IoU) averaged over all the semantic categories.</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e617996e02d64f333edfea5295360672/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">IDEA运行缓慢解决方案</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/96048f0766da2c2f8ed555b64e2e99c3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">vue， vux调用微信点击图片，上传图片，删除图片，接口，其中选图接口，苹果手机显示有问题，查看不到图片，提交会提示fail not exist，解决如下...</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>