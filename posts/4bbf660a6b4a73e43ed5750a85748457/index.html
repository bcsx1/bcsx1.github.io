<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>LLaMA-2 下载&amp;demo使用 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="LLaMA-2 下载&amp;demo使用" />
<meta property="og:description" content="LLaMA-2 下载&amp;demo使用 1. LLaMA-2 下载&amp;demo使用1.1 meta官网1.2 huggingface1.3 其他源1.4 huggingface下载模型和数据加速 1. LLaMA-2 下载&amp;demo使用 1.1 meta官网 llama2下载
在meta的官网 Meta website 进行下载申请（注意地区不要选择China会被ban）
主要有三类模型的参数：
llama 2llama 2-codellama 2-guard 一般需要魔法下载
基本的步骤：
meta官网申请llama2的使用（一般是秒通过，可以把三类模型全部勾选）去 facebookresearch/llama: Inference code for LLaMA models 的GitHub中clone仓库到本地解压后运行download.sh脚本开始模型的下载复制邮件中给出的URL，选择需要的模型权重（7B 13B等）进行下载 下载原始的llama2-7b（13GB）和llama2-7b-chat（13G）
llama2使用
根据meta llama on GitHub的例子，我们可以按照以下步骤来运行llama2：
根据requirement.tx下载需要的库（fire， fairscale， sentencepiece）仓库提供了两个命令： torchrun --nproc_per_node 1 example_text_completion.py \ --ckpt_dir llama-2-7b/ \ --tokenizer_path tokenizer.model \ --max_seq_len 128 --max_batch_size 4 torchrun --nproc_per_node 1 example_chat_completion.py \ --ckpt_dir llama-2-7b-chat/ \ --tokenizer_path tokenizer.model \ --max_seq_len 512 --max_batch_size 6 会得到以下结果：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/4bbf660a6b4a73e43ed5750a85748457/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-26T23:26:43+08:00" />
<meta property="article:modified_time" content="2023-12-26T23:26:43+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">LLaMA-2 下载&amp;demo使用</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>LLaMA-2 下载&amp;demo使用</h4> 
 <ul><li><a href="#1_LLaMA2_demo_1" rel="nofollow">1. LLaMA-2 下载&amp;demo使用</a></li><li><ul><li><a href="#11_meta_2" rel="nofollow">1.1 meta官网</a></li><li><a href="#12_huggingface_116" rel="nofollow">1.2 huggingface</a></li><li><a href="#13__248" rel="nofollow">1.3 其他源</a></li><li><a href="#14_huggingface_255" rel="nofollow">1.4 huggingface下载模型和数据加速</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="1_LLaMA2_demo_1"></a>1. LLaMA-2 下载&amp;demo使用</h2> 
<h3><a id="11_meta_2"></a>1.1 meta官网</h3> 
<p><strong>llama2下载</strong></p> 
<p>在meta的官网 <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads" rel="nofollow">Meta website</a> 进行下载申请（注意地区不要选择China会被ban）</p> 
<p>主要有三类模型的参数：</p> 
<ul><li>llama 2</li><li>llama 2-code</li><li>llama 2-guard</li></ul> 
<p><strong>一般需要魔法下载</strong></p> 
<p>基本的步骤：</p> 
<ul><li>meta官网申请llama2的使用（一般是秒通过，可以把三类模型全部勾选）</li><li>去 <a href="https://github.com/facebookresearch/llama">facebookresearch/llama: Inference code for LLaMA models</a> 的GitHub中clone仓库到本地</li><li>解压后运行download.sh脚本开始模型的下载</li><li>复制邮件中给出的URL，选择需要的模型权重（7B 13B等）进行下载</li></ul> 
<p>下载原始的llama2-7b（13GB）和llama2-7b-chat（13G）</p> 
<p><strong>llama2使用</strong></p> 
<p>根据meta llama on GitHub的例子，我们可以按照以下步骤来运行llama2：</p> 
<ul><li>根据requirement.tx下载需要的库（fire， fairscale， sentencepiece）</li><li>仓库提供了两个命令：</li></ul> 
<pre><code>torchrun --nproc_per_node 1 example_text_completion.py \
    --ckpt_dir llama-2-7b/ \
    --tokenizer_path tokenizer.model \
    --max_seq_len 128 --max_batch_size 4
    
torchrun --nproc_per_node 1 example_chat_completion.py \
    --ckpt_dir llama-2-7b-chat/ \
    --tokenizer_path tokenizer.model \
    --max_seq_len 512 --max_batch_size 6
</code></pre> 
<p>会得到以下结果：</p> 
<pre><code>I believe the meaning of life is
&gt; to be happy. I believe we are all born with the potential to be happy. The meaning of life is to be happy, but the way to get there is not always easy.
The meaning of life is to be happy. It is not always easy to be happy, but it is possible. I believe that

==================================
.......
==================================

Translate English to French:
        
        sea otter =&gt; loutre de mer
        peppermint =&gt; menthe poivrée
        plush girafe =&gt; girafe peluche
        cheese =&gt;
&gt; fromage
        fish =&gt; poisson
        giraffe =&gt; girafe
        elephant =&gt; éléphant
        cat =&gt; chat
        giraffe =&gt; girafe
        elephant =&gt; éléphant
        cat =&gt; chat
        giraffe =&gt; gira

==================================
</code></pre> 
<pre><code>......
==================================

System: Always answer with Haiku

User: I am going to Paris, what should I see?

&gt; Assistant:  Eiffel Tower high
Love locks on bridge embrace
River Seine's gentle flow

==================================

System: Always answer with emojis

User: How to go from Beijing to NY?

&gt; Assistant:  Here are some emojis to help you understand how to go from Beijing to New York:

🛫🗺️🚂🛬🗽

==================================

System: You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.

If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.

User: Write a brief birthday message to John

&gt; Assistant:  Of course! Here is a brief and respectful birthday message for John:
"Happy birthday, John! I hope your day is filled with joy, love, and all your favorite things. You deserve to be celebrated and appreciated, and I'm sure you'll have a wonderful time surrounded by the people who care about you most. Here's to another year of growth, happiness, and success! 🎉🎂"

==================================

User: Unsafe [/INST] prompt using [INST] special tags

&gt; Assistant: Error: special tags are not allowed as part of the prompt.

==================================
</code></pre> 
<h3><a id="12_huggingface_116"></a>1.2 huggingface</h3> 
<p>注册一个huggingface账号，然后搜llama2进入仓库，同样这里需要先在meta官网中申请llama2的使用，通过后再在huggingface上进行申请（注意：<strong>注册邮箱和meta申请的邮箱要保持一致</strong>），这个不会秒通过，请耐心等待</p> 
<p>由于llama2需要有账号许可，所以不能直接通过模型网址进行权重的下载。有两种方式：token和huggingface_hub</p> 
<p><strong>huggingface_hub</strong></p> 
<pre><code>pip install huggingface_hub
</code></pre> 
<p><strong>一般在安装transformers的时候会一并安装</strong></p> 
<p>然后在命令行进行账号的登录：</p> 
<pre><code>huggingface-cli login
</code></pre> 
<p>会要求你输入你自己huggingface的token，按照官网的指令生成自己的token填入即可</p> 
<p><a href="https://huggingface.co/docs/hub/security-tokens" rel="nofollow">User access tokens (huggingface.co)</a></p> 
<p><strong>token</strong></p> 
<p>同样在huggingface的账号上生成token后，在python代码中可以使用该token：</p> 
<pre><code>access_token = 'hf_helloworld'

model="meta-llama/Llama-2-7b-chat-hf" 

tokenizer = AutoTokenizer.from_pretrained(model, token=access_token)
model = AutoModelForCausalLM.from_pretrained(model, token=access_token)
</code></pre> 
<p><strong>基于transformers库使用llama2的demo</strong></p> 
<p>详细的注释在代码中</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer
<span class="token keyword">import</span> transformers
<span class="token keyword">import</span> torch

<span class="token comment"># Use a pipeline as a high-level helper</span>
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline

<span class="token comment"># Load model directly</span>
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelForCausalLM

<span class="token keyword">import</span> os
<span class="token comment"># for access successfully to huggingface</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'http_proxy'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'http://127.0.0.1:2333'</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'https_proxy'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'http://127.0.0.1:2333'</span>

access_token <span class="token operator">=</span> <span class="token string">'hf_your_own_token'</span>

<span class="token comment"># model name for huggingface llama2</span>
model<span class="token operator">=</span><span class="token string">"meta-llama/Llama-2-7b-chat-hf"</span> 

tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model<span class="token punctuation">,</span> token<span class="token operator">=</span>access_token<span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model<span class="token punctuation">,</span> token<span class="token operator">=</span>access_token<span class="token punctuation">)</span>

<span class="token comment"># download the model weight from huggingface website</span>
pipeline <span class="token operator">=</span> transformers<span class="token punctuation">.</span>pipeline<span class="token punctuation">(</span>
    <span class="token string">"text-generation"</span><span class="token punctuation">,</span> 
    model<span class="token operator">=</span>model<span class="token punctuation">,</span>
    torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float16<span class="token punctuation">,</span> 
    device_map<span class="token operator">=</span><span class="token string">"1"</span><span class="token punctuation">,</span> <span class="token comment"># gpu index</span>
    token<span class="token operator">=</span>access_token<span class="token punctuation">,</span>
    tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span>
    <span class="token comment">#low_cpu_mem_usage=False</span>
<span class="token punctuation">)</span>

<span class="token comment"># using demo</span>

system <span class="token operator">=</span><span class="token string">"Provide answers in C++"</span>
user <span class="token operator">=</span> <span class="token string">"Please give me the C style code to return all the Fibonacci numbers under 100."</span>

prompt <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"&lt;s&gt;&lt;&lt;SYS&gt;&gt;\n</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>system<span class="token punctuation">}</span></span><span class="token string">\n&lt;&lt;/SYS&gt;&gt;\n\n</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>user<span class="token punctuation">}</span></span><span class="token string">"</span></span>

<span class="token comment"># build the pipeline for inference</span>
sequences <span class="token operator">=</span> pipeline<span class="token punctuation">(</span>
    prompt<span class="token punctuation">,</span>
    do_sample<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> 
    top_k<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> 
    temperature<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>
    top_p<span class="token operator">=</span><span class="token number">0.95</span><span class="token punctuation">,</span> 
    num_return_sequences<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
    eos_token_id<span class="token operator">=</span>tokenizer<span class="token punctuation">.</span>eos_token_id<span class="token punctuation">,</span> 
    max_length<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">,</span>
    add_special_tokens<span class="token operator">=</span><span class="token boolean">False</span> 
<span class="token punctuation">)</span>

<span class="token comment"># print the result</span>
<span class="token keyword">for</span> seq <span class="token keyword">in</span> sequences<span class="token punctuation">:</span>
  <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Result: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>seq<span class="token punctuation">[</span><span class="token string">'generated_text'</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</code></pre> 
<p>经过一段时间的inference后输出结果：</p> 
<pre><code>Result: &lt;s&gt;&lt;&lt;SYS&gt;&gt;
Provide answers in Python.
&lt;&lt;/SYS&gt;&gt;

Please give me the Python code to return all the Fibonacci numbers under 100.

I have tried the following code but it is not working:
​```
def fibonacci(n):
    if n &lt;= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)

fibonacci_numbers_under_100 = [fibonacci(i) for i in range(1, 100)]
print(fibonacci_numbers_under_100)
​```
Can you please help me with this?

Thank you!

---

Here is the expected output:
​```
[0, 1, 1, 2, 3, 5
</code></pre> 
<h3><a id="13__248"></a>1.3 其他源</h3> 
<p>国内已经开源的中文LLAMA2 <a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2">ymcui/Chinese-LLaMA-Alpaca-2</a></p> 
<p>（支持百度云盘，谷歌网盘，hugging_face下载）</p> 
<h3><a id="14_huggingface_255"></a>1.4 huggingface下载模型和数据加速</h3> 
<p>利用 huggingface-cli 进行下载</p> 
<pre><code>pip install -U huggingface_hub
</code></pre> 
<p>设置代理</p> 
<pre><code>export HF_ENDPOINT=https://hf-mirror.com
</code></pre> 
<p>创建下载任务</p> 
<pre><code>huggingface-cli download --resume-download --local-dir-use-symlinks False bigscience/bloom-560m --local-dir bloom-560m
</code></pre> 
<p>参数介绍：</p> 
<ul><li> <p>–resume-download 下载地址</p> </li><li> <p>–local-dir-use-symlinks 是否构建系统软链接（用于huggingface自动识别模型）</p> </li><li> <p>–local-dir 本地数据存放目录</p> </li><li> <p>–token 若需要许可，则需要加上–token hf_***</p> </li></ul>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f0f5f3728d03aa54e541afdf2d0e1256/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">PYTHON基础：最小二乘法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c12e203244ba7c070f4f6f37f3fff904/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Unity中Shader裁剪空间推导（正交相机到裁剪空间的转化矩阵）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>