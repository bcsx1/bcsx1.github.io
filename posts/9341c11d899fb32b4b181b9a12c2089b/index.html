<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【统计学习方法】EM算法实现之隐马尔科夫模型HMM - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【统计学习方法】EM算法实现之隐马尔科夫模型HMM" />
<meta property="og:description" content="1 基本概念 1.1 马尔科夫链（维基百科） 马尔可夫链（英语：Markov chain），又称离散时间马尔可夫链（discrete-time Markov chain，缩写为DTMC），因俄国数学家安德烈·马尔可夫得名，为状态空间中经过从一个状态到另一个状态的转换的随机过程。该过程要求具备“无记忆”的性质：下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关。这种特定类型的“无记忆性”称作马尔可夫性质。
1.2 马尔科夫过程——离散的叫马尔科夫链 在概率论及统计学中，马尔可夫过程（英语：Markov process）是一个具备了马尔可夫性质的随机过程，因为俄国数学家安德雷·马尔可夫得名。马尔可夫过程是不具备记忆特质的（memorylessness）。换言之，马尔可夫过程的条件概率仅仅与系统的当前状态相关，而与它的过去历史或未来状态，都是独立、不相关的。
具备离散状态的马尔可夫过程，通常被称为马尔可夫链。马尔可夫链通常使用离散的时间集合定义，又称离散时间马尔可夫链。有些学者虽然采用这个术语，但允许时间可以取连续的值。
1.3 隐马尔科夫模型定义 隐马尔可夫模型（Hidden Markov Model；缩写：HMM）或称作隐性马尔可夫模型，是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来作进一步的分析，例如模式识别。
在正常的马尔可夫模型中，状态对于观察者来说是直接可见的。这样状态的转换概率便是全部的参数。而在隐马尔可夫模型中，状态并不是直接可见的，但受状态影响的某些变量则是可见的。每一个状态在可能输出的符号上都有一概率分布。因此输出符号的序列能够透露出状态序列的一些信息。
隐马尔科夫模型是关于时序的概率模型，描述一个由隐藏的马尔科夫链随机生成不可观测的状态随机序列，再由各个状态生成一个可观测的观测随机序列的过程。
1.4 隐马尔科夫模型参数的确定 1.4.1 初始概率分布 可能是状态1，状态2 ... 状态n，于是就有个N点分布：
状态1状态2...状态n概率.. 即：对应个n维的向量。
上面这个n维的向量就是初始概率分布，记做π。
1.4.2 状态转移矩阵A 因为和不独立，所以是状态1的概率有：是状态1时是状态1，是状态2时是状态1,..., 是状态n时是状态1，如下表
\状态1状态2...状态n状态1P11P12...P1n状态2P21P22...P2n...............状态nPn1Pn2...Pnn 即：-&gt;对应n*n的矩阵。
同理： -&gt; 对应个n*n的矩阵。
上面这些n*n的矩阵被称为状态转移矩阵，用An*n表示。
当然了，真要说的话， -&gt; 的状态转移矩阵一定都不一样，但在实际应用中一般将这些状态转移矩阵定为同一个，即：只有一个状态转移矩阵。
1.4.3 观测矩阵B 如果对于有：状态1, 状态2, ..., 状态n，那的每一个状态都会从下面的m个观测中产生一个：观测1, 观测2, ..., 观测m，所以有如下矩阵：
\观测1观测2...观测m状态1P11P12...P1m状态2P21P22...P2m...............状态nPn1Pn2...Pnm 这可以用一个n*m的矩阵表示，也就是观测矩阵，记做Bn*m。
由于HMM用上面的π，A，B就可以描述了，于是我们就可以说：HMM由初始概率分布π、状态转移概率分布A以及观测概率分布B确定，为了方便表达，把A, B, π 用 λ 表示，即：
λ = (A, B, π)
2 隐马尔科夫模型 隐马尔科夫模型是一个生成模型，也就意味着一旦掌握了其底层结构，就可以产生数据。
HMM模型一共有三个经典的问题需要解决：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/9341c11d899fb32b4b181b9a12c2089b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-11T08:59:43+08:00" />
<meta property="article:modified_time" content="2023-03-11T08:59:43+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【统计学习方法】EM算法实现之隐马尔科夫模型HMM</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>1 基本概念</h2> 
<h3>1.1 马尔科夫链（维基百科）</h3> 
<p>马尔可夫链（英语：Markov chain），又称离散时间马尔可夫链（discrete-time Markov chain，缩写为DTMC），因俄国数学家安德烈·马尔可夫得名，为状态空间中经过从一个状态到另一个状态的转换的随机过程。该过程要求具备“无记忆”的性质：下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关。这种特定类型的“无记忆性”称作马尔可夫性质。</p> 
<p></p> 
<h3>1.2 马尔科夫过程——离散的叫马尔科夫链</h3> 
<p>在概率论及统计学中，马尔可夫过程（英语：Markov process）是一个具备了马尔可夫性质的随机过程，因为俄国数学家安德雷·马尔可夫得名。马尔可夫过程是不具备记忆特质的（memorylessness）。换言之，马尔可夫过程的条件概率仅仅与系统的当前状态相关，而与它的过去历史或未来状态，都是独立、不相关的。</p> 
<p>具备离散状态的马尔可夫过程，通常被称为马尔可夫链。马尔可夫链通常使用离散的时间集合定义，又称离散时间马尔可夫链。有些学者虽然采用这个术语，但允许时间可以取连续的值。</p> 
<p></p> 
<h3>1.3 隐马尔科夫模型定义</h3> 
<p>隐马尔可夫模型（Hidden Markov Model；缩写：HMM）或称作隐性马尔可夫模型，是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来作进一步的分析，例如模式识别。</p> 
<p>在正常的马尔可夫模型中，状态对于观察者来说是直接可见的。这样状态的转换概率便是全部的参数。而在隐马尔可夫模型中，状态并不是直接可见的，但受状态影响的某些变量则是可见的。每一个状态在可能输出的符号上都有一概率分布。因此输出符号的序列能够透露出状态序列的一些信息。</p> 
<p>隐马尔科夫模型是关于时序的概率模型，描述一个由隐藏的马尔科夫链随机生成不可观测的状态随机序列，再由各个状态生成一个可观测的观测随机序列的过程。</p> 
<h3>1.4 隐马尔科夫模型参数的确定</h3> 
<h4>1.4.1 初始概率分布<img alt="\pi" class="mathcode" src="https://images2.imgbox.com/6d/ee/AVpRpv2E_o.gif"></h4> 
<p> <img alt="i_1" class="mathcode" src="https://images2.imgbox.com/2e/e4/AgDN2ldM_o.gif">可能是状态1，状态2 ... 状态n，于是<img alt="i_1" class="mathcode" src="https://images2.imgbox.com/40/6d/vuqlSFLf_o.gif">就有个N点分布：</p> 
<table border="1" cellpadding="1" cellspacing="1" style="width:456px;"><tbody><tr><td style="width:95px;"><img alt="i_1" class="mathcode" src="https://images2.imgbox.com/0a/4d/T317woNd_o.gif"></td><td style="width:77px;">状态1</td><td style="width:77px;">状态2</td><td style="width:85px;">...</td><td style="width:94px;">状态n</td></tr><tr><td style="width:95px;">概率</td><td style="width:77px;"><img alt="P_1" class="mathcode" src="https://images2.imgbox.com/e1/ec/DBuw4ZN9_o.gif"></td><td style="width:77px;"><img alt="P_2" class="mathcode" src="https://images2.imgbox.com/46/a6/CSjpheeB_o.gif"></td><td style="width:85px;">..</td><td style="width:94px;"><img alt="P_n" class="mathcode" src="https://images2.imgbox.com/24/82/5pGTzSuh_o.gif"></td></tr></tbody></table> 
<p>即：<img alt="i_1" class="mathcode" src="https://images2.imgbox.com/d3/c6/sychWDeR_o.gif">对应个n维的向量。</p> 
<p>上面这个n维的向量就是初始概率分布，记做π。</p> 
<h4>1.4.2 状态转移矩阵A</h4> 
<p>因为<img alt="i_2" class="mathcode" src="https://images2.imgbox.com/f4/81/nS1NMFRN_o.gif">和<img alt="i_1" class="mathcode" src="https://images2.imgbox.com/b4/de/Td7PXVzs_o.gif">不独立，所以<img alt="i_2" class="mathcode" src="https://images2.imgbox.com/21/fb/Q90fs1Fu_o.gif">是状态1的概率有：<img alt="i_1" class="mathcode" src="https://images2.imgbox.com/47/fe/SMjejofN_o.gif">是状态1时<img alt="i_2" class="mathcode" src="https://images2.imgbox.com/95/55/vOGVTWUO_o.gif">是状态1，<img alt="i_1" class="mathcode" src="https://images2.imgbox.com/7b/21/5z1jZyF9_o.gif">是状态2时<img alt="i_2" class="mathcode" src="https://images2.imgbox.com/72/0c/eZeSXu8s_o.gif">是状态1,..., <img alt="i_1" class="mathcode" src="https://images2.imgbox.com/c1/2b/i2lR35j3_o.gif">是状态n时<img alt="i_2" class="mathcode" src="https://images2.imgbox.com/f8/0a/oyL9EHDC_o.gif">是状态1，如下表</p> 
<table border="1" cellpadding="1" cellspacing="1" style="width:396px;"><tbody><tr><td style="width:72px;"><img alt="i_2" class="mathcode" src="https://images2.imgbox.com/88/a6/sbXNWC6S_o.gif">\<img alt="i_1" class="mathcode" src="https://images2.imgbox.com/e3/c8/UM3tMjh0_o.gif"></td><td style="width:58px;">状态1</td><td style="width:78px;">状态2</td><td style="width:64px;">...</td><td style="width:96px;">状态n</td></tr><tr><td style="width:72px;">状态1</td><td style="width:58px;">P11</td><td style="width:78px;">P12</td><td style="width:64px;">...</td><td style="width:96px;">P1n</td></tr><tr><td style="width:72px;">状态2</td><td style="width:58px;">P21</td><td style="width:78px;">P22</td><td style="width:64px;">...</td><td style="width:96px;">P2n</td></tr><tr><td style="width:72px;">...</td><td style="width:58px;">...</td><td style="width:78px;">...</td><td style="width:64px;">...</td><td style="width:96px;">...</td></tr><tr><td style="width:72px;">状态n</td><td style="width:58px;">Pn1</td><td style="width:78px;">Pn2</td><td style="width:64px;">...</td><td style="width:96px;">Pnn</td></tr></tbody></table> 
<p>即：<img alt="i_1" class="mathcode" src="https://images2.imgbox.com/e6/46/qQsr8Iu5_o.gif">-&gt;<img alt="i_2" class="mathcode" src="https://images2.imgbox.com/6b/b6/YTWHXv11_o.gif">对应n*n的矩阵。</p> 
<p>同理：<img alt="i_n" class="mathcode" src="https://images2.imgbox.com/74/b0/eSSYbrJh_o.gif"> -&gt; <img alt="i_{n+1}" class="mathcode" src="https://images2.imgbox.com/eb/c0/cR5vtFN6_o.gif">对应个n*n的矩阵。</p> 
<p>上面这些n*n的矩阵被称为状态转移矩阵，用An*n表示。</p> 
<p>      当然了，真要说的话，<img alt="i_n" class="mathcode" src="https://images2.imgbox.com/29/40/wSyiSM6S_o.gif"> -&gt; <img alt="i_{n+1}" class="mathcode" src="https://images2.imgbox.com/f3/44/IprsBRou_o.gif">的状态转移矩阵一定都不一样，但在实际应用中一般将这些状态转移矩阵定为同一个，即：只有一个状态转移矩阵。</p> 
<h4>1.4.3 观测矩阵B</h4> 
<p>  如果对于<img alt="i_n" class="mathcode" src="https://images2.imgbox.com/fb/c0/6FtpTDq6_o.gif">有：状态1, 状态2, ..., 状态n，那<img alt="i_n" class="mathcode" src="https://images2.imgbox.com/d0/33/OsQ0Pvgo_o.gif">的每一个状态都会从下面的m个观测中产生一个：观测1, 观测2, ..., 观测m，所以有如下矩阵：</p> 
<table border="1" cellpadding="1" cellspacing="1" style="width:500px;"><tbody><tr><td><img alt="i_n" class="mathcode" src="https://images2.imgbox.com/08/77/EAr7xqjT_o.gif">\<img alt="O_m" class="mathcode" src="https://images2.imgbox.com/6e/24/HwTU769W_o.gif"></td><td>观测1</td><td>观测2</td><td>...</td><td>观测m</td></tr><tr><td>状态1</td><td>P11</td><td>P12</td><td>...</td><td>P1m</td></tr><tr><td>状态2</td><td>P21</td><td>P22</td><td>...</td><td>P2m</td></tr><tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><td>状态n</td><td>Pn1</td><td>Pn2</td><td>...</td><td>Pnm</td></tr></tbody></table> 
<p>这可以用一个n*m的矩阵表示，也就是观测矩阵，记做Bn*m。</p> 
<p>由于HMM用上面的π，A，B就可以描述了，于是我们就可以说：HMM由初始概率分布π、状态转移概率分布A以及观测概率分布B确定，为了方便表达，把A, B, π 用 λ 表示，即：</p> 
<p>            λ = (A, B, π)</p> 
<h2>2 隐马尔科夫模型</h2> 
<p>隐马尔科夫模型是一个生成模型，也就意味着一旦掌握了其底层结构，就可以产生数据。</p> 
<p>HMM模型一共有三个经典的问题需要解决：</p> 
<p>　　　　1） 评估观察序列概率。即给定模型<img alt="\lambda (A,B,\pi)" class="mathcode" src="https://images2.imgbox.com/e0/6c/9dQwZGSe_o.gif">和观测序列<img alt="O(O_1,O_2,O_3...O_T)" class="mathcode" src="https://images2.imgbox.com/f3/19/Dijdf1db_o.gif">，计算在模型λ下观测序列𝑂出现的概率<img alt="P(O|\lambda)" class="mathcode" src="https://images2.imgbox.com/60/cf/9NqeZt5e_o.gif">。这个问题的求解需要用到前向后向算法，我们在这个系列的第二篇会详细讲解。这个问题是HMM模型三个问题中最简单的。</p> 
<p>　　　　2）模型参数学习问题。即给定观测序列<img alt="O(O_1,O_2,O_3...O_T)" class="mathcode" src="https://images2.imgbox.com/bb/7b/a4y0TrAO_o.gif">，估计模型<img alt="\lambda (A,B,\pi)" class="mathcode" src="https://images2.imgbox.com/65/5c/wCjJOg9D_o.gif">的参数，使该模型下观测序列的条件概率<img alt="P(O|\lambda)" class="mathcode" src="https://images2.imgbox.com/75/59/H84aSpFZ_o.gif">最大。这个问题的求解需要用到基于EM算法的鲍姆-韦尔奇算法， 我们在这个系列的第三篇会详细讲解。这个问题是HMM模型三个问题中最复杂的。</p> 
<p>　　　　3）预测问题，也称为解码问题。即给定模型<img alt="\lambda (A,B,\pi)" class="mathcode" src="https://images2.imgbox.com/b7/85/M1tGcgcK_o.gif">和观测序列<img alt="O(O_1,O_2,O_3...O_T)" class="mathcode" src="https://images2.imgbox.com/a3/3f/7Kz8dOhk_o.gif">，求对给定观测序列条件概率<img alt="P(I|O)" class="mathcode" src="https://images2.imgbox.com/2b/95/2EZqBRNB_o.gif">最大的状态序列 <img alt="I(i_1,i_2,i_3...i_T)" class="mathcode" src="https://images2.imgbox.com/1f/02/kWvQ7J0Z_o.gif">。即给定观测序列条件下，最可能出现的对应的状态序列，这个问题的求解需要用到基于动态规划的维特比算法，我们在这个系列的第四篇会详细讲解。这个问题是HMM模型三个问题中复杂度居中的算法。</p> 
<h3>2.1 概率计算问题</h3> 
<p>穷举算法，有N中状态，经过T时间，则有<img alt="N^{T}" class="mathcode" src="https://images2.imgbox.com/f2/4e/IlLs8X5K_o.png">种可能(因为每一种状态都可能观测到<img alt="O_{t}" class="mathcode" src="https://images2.imgbox.com/cf/2e/Rp3QWHuN_o.png">)，计算量巨大。但是很多路径是重复，基于动态规划的知识，我们可以重复利用很多节点的计算结果，所以出来了前向算法和后向算法。</p> 
<p><img alt="" height="340" src="https://images2.imgbox.com/a2/b7/Ali952Fr_o.png" width="485"></p> 
<h4>2.1.1 前向概率</h4> 
<p>给定隐马尔可夫模型<img alt="\lambda (A,B,\pi)" class="mathcode" src="https://images2.imgbox.com/1f/17/TR0fSTPp_o.gif">，定义到时刻t为止的观测序列为<img alt="O(O_1,O_2,O_3...O_T)" class="mathcode" src="https://images2.imgbox.com/92/e9/GDsf2Uw0_o.gif">，且状态为<img alt="" height="22" src="https://images2.imgbox.com/d1/3d/M0hXw4ZR_o.png" width="15">的概率为前向概率，记作：</p> 
<p><img alt="a_t(i) = P(O_1,O_2,O_3...O_t,i_t=q_i|\lambda)" class="mathcode" src="https://images2.imgbox.com/b5/7e/FHaVspIN_o.gif"></p> 
<p>可以递推地求得前向概率<img alt="a_t(i)" class="mathcode" src="https://images2.imgbox.com/5b/4f/xQg3jmPa_o.gif">及观测序列概率<img alt="P(O|\lambda)" class="mathcode" src="https://images2.imgbox.com/2d/a5/A3TDChqf_o.gif">。</p> 
<p></p> 
<p>定义解析：由于每个状态生成一个观测变量，那么在t时刻就会生成t个观测变量，在t时刻处于状态i的概率就是前向概率。</p> 
<h4>2.1.1 前向算法</h4> 
<p>算法的目的：根据<span style="color:#f33b45;"><strong>初始参数</strong></span>和<span style="color:#f33b45;"><strong>观测序列</strong></span>求出观测序列概率</p> 
<p><strong>前向概率</strong>的定义是：当第t个时刻的状态为i时，前面的时刻分别观测到<img alt="O_1,O_2,O_3...O_T" class="mathcode" src="https://images2.imgbox.com/d8/fc/zKnC199y_o.gif">的概率，即：</p> 
<p><img alt="\alpha_i(t) = P(O_1,O_2,O_3...O_t|q_t=i,\lambda)" class="mathcode" src="https://images2.imgbox.com/ef/df/fIl7ZZCW_o.gif"></p> 
<p><img alt="" height="244" src="https://images2.imgbox.com/93/21/wnoa9N7E_o.png" width="476"></p> 
<p>从上图可以看出，我们可以基于时刻t时各个隐藏状态的前向概率，再乘以对应的状态转移概率，即<img alt="a_t(j)a_{ji}" class="mathcode" src="https://images2.imgbox.com/89/8f/Rf8iGLNX_o.gif">就是在时刻t观测到<img alt="O(O_1,O_2,O_3...O_T)" class="mathcode" src="https://images2.imgbox.com/ed/c5/y5HjtKdI_o.gif">，并且时刻t隐藏状态<img alt="q_j" class="mathcode" src="https://images2.imgbox.com/38/52/jUU0Jv5q_o.gif">, 时刻t+1隐藏状态<img alt="q_i" class="mathcode" src="https://images2.imgbox.com/c6/6e/smN2hBRs_o.gif">的概率。如果将像上面所有的线对应的概率求和，即<img alt="\sum_{j=1}^{N}a_t(j)a_{ji}" class="mathcode" src="https://images2.imgbox.com/72/eb/K2EBboY5_o.gif">就是在时刻t观测到<img alt="O(O_1,O_2,O_3...O_T)" class="mathcode" src="https://images2.imgbox.com/71/e1/E214HpLK_o.gif">，并且时刻t+1隐藏状态<img alt="q_i" class="mathcode" src="https://images2.imgbox.com/98/84/TwPuAAI3_o.gif">的概率。继续一步，由于观测状态<img alt="O_{t+1}" class="mathcode" src="https://images2.imgbox.com/5c/95/7xamReIi_o.gif">只依赖于t+1时刻隐藏状态<img alt="q_i" class="mathcode" src="https://images2.imgbox.com/26/50/PEtQ2bMz_o.gif">, 这样<img alt="\left [ \sum_{j=1}^{N}a_t(j)a_{ji} \right ]b_i(O_{t+1})" class="mathcode" src="https://images2.imgbox.com/24/fa/zahdP3PR_o.gif">就是在在时刻t+1观测到<img alt="O(O_1,O_2,O_3...O_T,O_{T+1})" class="mathcode" src="https://images2.imgbox.com/e4/82/wAGmhHjk_o.gif">，并且时刻t+1隐藏状态<img alt="q_i" class="mathcode" src="https://images2.imgbox.com/3d/8a/3kmZmHCO_o.gif">的概率。而这个概率，恰恰就是时刻t+1对应的隐藏状态i的前向概率，这样我们得到了前向概率的递推关系式如下：</p> 
<p><img alt="a_{t+1}(i) = \left [ \sum_{j=1}^{N} a_t(j)a_{ji}\right ]b_i(o_{t+1})" class="mathcode" src="https://images2.imgbox.com/c7/ea/mqB9at5e_o.gif"></p> 
<p><strong>前向算法步骤：</strong></p> 
<p>输入：HMM模型<img alt="\lambda (A,B,\pi)" class="mathcode" src="https://images2.imgbox.com/f4/19/aNXB8XkV_o.gif">，观测序列<img alt="O(O_1,O_2,O_3...O_T)" class="mathcode" src="https://images2.imgbox.com/5d/40/Aqyfo7af_o.gif"></p> 
<p>输出：观测序列概率𝑃(𝑂|𝜆)</p> 
<p>1) 计算时刻1的各个隐藏状态前向概率：</p> 
<p><img alt="\alpha_1(i) = \pi_i b_i(O_1),i=1,2...N" class="mathcode" src="https://images2.imgbox.com/04/4d/9DSxjNGx_o.gif"></p> 
<p></p> 
<p>2) 递推时刻2,3,...𝑇2,3,...T时刻的前向概率：</p> 
<p></p> 
<p><img alt="a_{t+1}(i) = \left [ \sum_{j=1}^{N} a_t(j)a_{ji} \right ]b_i(o_{t+1}),i=1,2...N" class="mathcode" src="https://images2.imgbox.com/cb/f2/SnA2glPs_o.gif"></p> 
<p>3) 计算最终结果：</p> 
<p><img alt="P(O|\lambda) = \sum_{i=1}^{N}a_T(i)" class="mathcode" src="https://images2.imgbox.com/d2/2c/CODGTIob_o.gif"></p> 
<p>PS：这里的 <img alt="a_i(t)" class="mathcode" src="https://images2.imgbox.com/73/4d/XaoBuV4U_o.gif">中i表示第i号状态，t表示第t时刻。有的教程中会把i和t位置调换一下，变成 <img alt="a_t(i)" class="mathcode" src="https://images2.imgbox.com/5c/b9/ch0QLFfO_o.gif">，其实都一样。</p> 
<p>代码实现：</p> 
<pre><code class="language-python">def forward(obs_seq):
    """前向算法"""
    N = A.shape[0]
    T = len(obs_seq)
    
    # F保存前向概率矩阵
    F = np.zeros((N,T))
    F[:,0] = pi * B[:, obs_seq[0]]  # 初始状态输出序列元素0概率

    for t in range(1, T):
        for n in range(N):
            # t时刻各个状态，是t-1时刻各个状态转移过来的
            # A[:,n]表示各个状态转移到状态n
            # F[:,t-1]表示从初始时刻到t-1处于该状态的概率（不准确的理解）
            # B[n, obs_seq[t]]表示n状态输出obs_seq[t]观察值的概率
            # 利用点乘 累加 所有状态
            F[n,t] = np.dot(F[:,t-1], (A[:,n])) * B[n, obs_seq[t]]

    return F
</code></pre> 
<h4>2.1.2 后向算法</h4> 
<p> 后向算法和前向算法非常类似，都是用的动态规划，唯一的区别是选择的局部状态不同，后向算法用的是“后向概率”。</p> 
<p><strong> 后向概率</strong>的定义是：当第t个时刻的状态为i时，后面的时刻分别观测到<img alt="O_{t+1},O_{t+2},O_{t+3}...O_T" class="mathcode" src="https://images2.imgbox.com/57/09/t5mGfVW2_o.png">的概率，即：</p> 
<p><img alt="\beta_i(t) = P(O_{t+1},O_{t+2},O_{t+3.}..O_T|q_t=i,\lambda)" class="mathcode" src="https://images2.imgbox.com/7b/08/FjQAtrCv_o.png"></p> 
<p></p> 
<p><img alt="" src="https://images2.imgbox.com/6f/25/F2myTbsL_o.png"></p> 
<p>后向概率的动态规划递推公式和前向概率是相反的。现在我们假设我们已经找到了在时刻t+1时各个隐藏状态的后向概率<img alt="\beta_{t+1}(j)" class="mathcode" src="https://images2.imgbox.com/75/9d/aknVB79e_o.gif">，现在我们需要递推出时刻t时各个隐藏状态的后向概率。如下图，我们可以计算出观测状态的序列为<img alt="O_{t+1},O_{t+2},O_{t+3}...O_T" class="mathcode" src="https://images2.imgbox.com/6c/34/xZLfteBz_o.png">， t时隐藏状态<img alt="q_i" class="mathcode" src="https://images2.imgbox.com/34/c3/0Z0Z543s_o.gif">, 时刻t+1隐藏状态为<img alt="q_j" class="mathcode" src="https://images2.imgbox.com/75/06/iGXTsgAU_o.gif">的概率为<img alt="a_{ij}\beta_{t+1}(j)" class="mathcode" src="https://images2.imgbox.com/5d/44/VqqWCovL_o.gif">, 接着可以得到观测状态的序列为<img alt="O_{t+1},O_{t+2},O_{t+3}...O_T" class="mathcode" src="https://images2.imgbox.com/88/f9/RruiCkfy_o.png">， t时隐藏状态为<img alt="q_i" class="mathcode" src="https://images2.imgbox.com/61/6d/uCf7RM1L_o.gif">, 时刻t+1隐藏状态为<img alt="q_j" class="mathcode" src="https://images2.imgbox.com/85/e7/VwabfYlz_o.gif">的概率为<img alt="a{ij}b{j}(O_{t+1})\beta_{t+1}(j)" class="mathcode" src="https://images2.imgbox.com/2a/fb/PBm2jLvb_o.gif">, 则把下面所有线对应的概率加起来，我们可以得到观测状态的序列为<img alt="O_{t+1},O_{t+2},O_{t+3}...O_T" class="mathcode" src="https://images2.imgbox.com/3c/14/rYKqU5Xm_o.png">， t时隐藏状态为<img alt="q_i" class="mathcode" src="https://images2.imgbox.com/98/9c/ga5furzg_o.gif">的概率为<img alt="\sum _{j=1}^{N}a_{ij}b_j(O_{t+1})\beta_{t+1}(j)" class="mathcode" src="https://images2.imgbox.com/c2/6e/AJkfPeRo_o.gif">，这个概率即为时刻t的后向概率。</p> 
<p>这样我们得到了后向概率的递推关系式如下：</p> 
<p><img alt="\beta_t (i)=\sum_{j=1}^{N}a_{ij}b_j(O_{t+1})\beta_{t+1}(j)" class="mathcode" src="https://images2.imgbox.com/7e/54/uDUbAOyr_o.gif"></p> 
<p>后向算法步骤：</p> 
<p>代码实现：</p> 
<pre><code class="language-python">
def backward(obs_seq):
    """后向算法"""
    N = A.shape[0]
    T = len(obs_seq)
    # X保存后向概率矩阵
    X = np.zeros((N,T))

    #t=T，T+1已经不观察了，后向概率都为1
    X[:,-1:] = 1

    for t in reversed(range(T-1)):
        for n in range(N):
            #t时刻转移概率*(t+1)时刻的后向概率
            X[n,t] = np.sum(X[:,t+1] * A[n,:] * B[:, obs_seq[t+1]])

    return X</code></pre> 
<p></p> 
<h3>2.3 学习问题</h3> 
<p>Baum-Welch算法也就是EM算法，己知观测序列<img alt="O(O_1,O_2,O_3...O_T)" class="mathcode" src="https://images2.imgbox.com/e9/97/xGBFWoxB_o.gif">,估计模型参数<img alt="\lambda (A,B,\pi)" class="mathcode" src="https://images2.imgbox.com/94/3e/ySPvn6Uo_o.gif">，使得在该模型下观测序列概率<img alt="P(O|\lambda)" class="mathcode" src="https://images2.imgbox.com/c7/3a/rJSYo8H2_o.gif">最大。即用极大似然估计的方法估计参数。</p> 
<p></p> 
<h3>2.4 预测问题（<span style="color:#f33b45;"><strong>viterbi算法</strong></span>）</h3> 
<p>参考视频：<a href="https://www.youtube.com/watch?v=RKDIpPyeTTk" rel="nofollow" title="https://www.youtube.com/watch?v=RKDIpPyeTTk">https://www.youtube.com/watch?v=RKDIpPyeTTk</a></p> 
<p>参考文章：<a href="https://www.zhihu.com/question/20136144" rel="nofollow" title="如何通俗地讲解 viterbi 算法？ - 知乎">如何通俗地讲解 viterbi 算法？ - 知乎</a></p> 
<p><span style="color:#f33b45;"><strong>删掉了不可能是答案的路径，就是viterbi算法（维特比算法）的重点，因为后面我们再也不用考虑这些被删掉的路径了</strong></span></p> 
<p></p> 
<p>代码实现：</p> 
<pre><code class="language-python">    def viterbi(self, obs_seq):
        """
        Returns
        -------
        V : numpy.ndarray
            V [s][t] = Maximum probability of an observation sequence ending
                       at time 't' with final state 's'
            观察序列
        prev : numpy.ndarray
            Contains a pointer to the previous state at t-1 that maximizes
            V[state][t]
        """
        N = self.A.shape[0]
        T = len(obs_seq)
        prev = np.zeros((T - 1, N), dtype=int)

        # DP matrix containing max likelihood of state at a given time
        # 隐状态个数, 观测序列长度
        V = np.zeros((N, T))

        # 0时刻观测状态 =  状态初始概率 * 每个状态输出观测概率
        V[:,0] = self.pi * self.B[:,obs_seq[0]]

        for t in range(1, T):
            for n in range(N):
                # 上一时刻状态概率 * 状态转移概率(每一个状态转移到状态n) * 观察概率
                seq_probs = V[:,t-1] * self.A[:,n] * self.B[n, obs_seq[t]]
                # prev用于记录最大概率是从哪个状态转移过来的 -- &gt;记录转移路径
                prev[t-1,n] = np.argmax(seq_probs)

                # 这里找出概率当前状态最大的概率,实质就是删除不可能的路径 --&gt; 记录最大概率
                V[n,t] = np.max(seq_probs)

        return V, prev</code></pre> 
<p></p> 
<h2>3 代码实现</h2> 
<p>完整实现</p> 
<pre><code>import numpy as np

class HMM:
    """
    Order 1 Hidden Markov Model

    Attributes
    ----------
    A : numpy.ndarray
        State transition probability matrix
    B: numpy.ndarray
        Output emission probability matrix with shape(N, number of output types)
    pi: numpy.ndarray
        Initial state probablity vector

    Common Variables
    ----------------
    obs_seq : list of int
        list of observations (represented as ints corresponding to output
        indexes in B) in order of appearance
    T : int
        number of observations in an observation sequence
    N : int
        number of states
    """

    def __init__(self, A, B, pi):
        self.A = A
        self.B = B
        self.pi = pi

    def _forward(self, obs_seq):
        N = self.A.shape[0]
        T = len(obs_seq)

        F = np.zeros((N,T))
        F[:,0] = self.pi * self.B[:, obs_seq[0]]

        for t in range(1, T):
            for n in range(N):
                F[n,t] = np.dot(F[:,t-1], (self.A[:,n])) * self.B[n, obs_seq[t]]

        return F

    def _backward(self, obs_seq):
        N = self.A.shape[0]
        T = len(obs_seq)

        X = np.zeros((N,T))
        X[:,-1:] = 1

        for t in reversed(range(T-1)):
            for n in range(N):
                X[n,t] = np.sum(X[:,t+1] * self.A[n,:] * self.B[:, obs_seq[t+1]])

        return X

    def observation_prob(self, obs_seq):
        """ P( entire observation sequence | A, B, pi ) """
        return np.sum(self._forward(obs_seq)[:,-1])

    def state_path(self, obs_seq):
        """
        Returns
        -------
        V[last_state, -1] : float
            Probability of the optimal state path
        path : list(int)
            Optimal state path for the observation sequence
        """
        V, prev = self.viterbi(obs_seq)

        # Build state path with greatest probability
        last_state = np.argmax(V[:,-1])
        path = list(self.build_viterbi_path(prev, last_state))

        return V[last_state,-1], reversed(path)

    def viterbi(self, obs_seq):
        """
        Returns
        -------
        V : numpy.ndarray
            V [s][t] = Maximum probability of an observation sequence ending
                       at time 't' with final state 's'
        prev : numpy.ndarray
            Contains a pointer to the previous state at t-1 that maximizes
            V[state][t]
        """
        N = self.A.shape[0]
        T = len(obs_seq)
        prev = np.zeros((T - 1, N), dtype=int)

        # DP matrix containing max likelihood of state at a given time
        V = np.zeros((N, T))
        V[:,0] = self.pi * self.B[:,obs_seq[0]]

        for t in range(1, T):
            for n in range(N):
                seq_probs = V[:,t-1] * self.A[:,n] * self.B[n, obs_seq[t]]
                prev[t-1,n] = np.argmax(seq_probs)
                V[n,t] = np.max(seq_probs)

        return V, prev

    def build_viterbi_path(self, prev, last_state):
        """Returns a state path ending in last_state in reverse order."""
        T = len(prev)
        yield(last_state)
        for i in range(T-1, -1, -1):
            yield(prev[i, last_state])
            last_state = prev[i, last_state]

    def baum_welch_train(self, obs_seq):
        N = self.A.shape[0]
        T = len(obs_seq)

        forw = self._forward(obs_seq)
        back = self._backward(obs_seq)

        # P( entire observation sequence | A, B, pi )
        obs_prob = np.sum(forw[:,-1])
        if obs_prob &lt;= 0:
            raise ValueError("P(O | lambda) = 0. Cannot optimize!")

        xi = np.zeros((T-1, N, N))
        for t in range(xi.shape[0]):
            xi[t,:,:] = self.A * forw[:,[t]] * self.B[:,obs_seq[t+1]] * back[:, t+1] / obs_prob

        gamma = forw * back / obs_prob

        # Gamma sum excluding last column
        gamma_sum_A = np.sum(gamma[:,:-1], axis=1, keepdims=True)
        # Vector of binary values indicating whether a row in gamma_sum is 0.
        # If a gamma_sum row is 0, save old rows on update
        rows_to_keep_A =  (gamma_sum_A == 0)
        # Convert all 0s to 1s to avoid division by zero
        gamma_sum_A[gamma_sum_A == 0] = 1.
        next_A = np.sum(xi, axis=0) / gamma_sum_A


        gamma_sum_B = np.sum(gamma, axis=1, keepdims=True)
        rows_to_keep_B = (gamma_sum_B == 0)
        gamma_sum_B[gamma_sum_B == 0] = 1.

        obs_mat = np.zeros((T, self.B.shape[1]))
        obs_mat[range(T),obs_seq] = 1
        next_B = np.dot(gamma, obs_mat) / gamma_sum_B

        # Update model
        self.A = self.A * rows_to_keep_A + next_A
        self.B = self.B * rows_to_keep_B + next_B
        self.pi = gamma[:,0] / np.sum(gamma[:,0])</code></pre> 
<p></p> 
<p></p> 
<p>参考：</p> 
<p><a href="https://www.cnblogs.com/sddai/p/8475424.html" rel="nofollow" id="cb_post_title_url" title="隐马尔可夫(HMM)、前/后向算法、Viterbi算法">隐马尔可夫(HMM)、前/后向算法、Viterbi算法</a></p> 
<p><a href="https://www.jianshu.com/p/eccb9eb9a921" rel="nofollow" title="【火炉炼AI】机器学习044-创建隐马尔科夫模型">【火炉炼AI】机器学习044-创建隐马尔科夫模型</a></p> 
<p><a href="https://mp.weixin.qq.com/s/rV1VuZuMJh8g2GCF1b9mQQ" rel="nofollow" title="隐马尔可夫模型HMM及Python实现">隐马尔可夫模型HMM及Python实现</a></p> 
<p><a href="https://blog.csdn.net/u014688145/article/details/53046765?locationNum=7&amp;fps=1" title="隐马尔可夫模型之Baum-Welch算法详解">隐马尔可夫模型之Baum-Welch算法详解</a></p> 
<p><a href="https://blog.csdn.net/continueOo/article/details/77893587#em%E7%AE%97%E6%B3%95%E5%AF%A6%E4%BE%8B%E7%90%86%E8%A7%A3" title="HMM超详细讲解+代码">HMM超详细讲解+代码</a></p> 
<p></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/360ea06b724327f543d9bde5a7eaefc2/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">记一次使用hutool的http工具调用外部接口同步人员信息</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b7605adae061ccc76e00b7d55440427e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Element-UI侧边导航栏切换展示不同的右侧主体内容</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>