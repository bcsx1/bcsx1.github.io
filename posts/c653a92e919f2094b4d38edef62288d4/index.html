<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【具身智能评估7】ProcTHOR: Large-Scale Embodied AI Using Procedural Generation - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【具身智能评估7】ProcTHOR: Large-Scale Embodied AI Using Procedural Generation" />
<meta property="og:description" content="论文标题：ProcTHOR: Large-Scale Embodied AI Using Procedural Generation
论文作者：Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Jordi Salvador, Kiana Ehsani, Winson Han, Eric Kolve, Ali Farhadi, Aniruddha Kembhavi, Roozbeh Mottaghi
论文原文：https://arxiv.org/abs/2206.06994
论文出处：–
论文被引：69（12/18/2023）
论文代码：https://github.com/allenai/procthor，183 star
项目主页：https://procthor.allenai.org/
Abstract 海量数据集和大容量模型推动了计算机视觉和自然语言理解领域的许多最新进展。这项工作提供了一个平台，使具身人工智能也能取得类似的成功。我们提出的 PROCTHOR 是一个程序化生成具身人工智能环境的框架。PROCTHOR 使我们能够对多样化、交互式、可定制和高性能虚拟环境的任意大型数据集进行采样，以训练和评估导航、交互和操作任务中的具身智能体。我们通过 10,000 个生成的房屋样本和一个简单的神经模型展示了 PROCTHOR 的功能和潜力。在 PROCTHOR 上仅使用 RGB 图像训练的模型，在没有显式映射和人类任务监督的情况下，在导航、重新排列和手臂操纵等 6 项具身人工智能基准测试中取得了最先进的结果，其中包括目前正在进行的 Habitat 2022、AI2-THOR Rearrangement 2022 和 RoboTHOR 挑战赛。我们还通过在 PROCTHOR 上进行预训练，而不对下游基准进行微调，在这些基准上展示了强大的 零样本结果，击败了以前使用下游训练数据的最先进系统。
1 Introduction 通过使用大规模训练数据，计算机视觉和自然语言处理模型变得越来越强大。最近的模型，如 CLIP [ 93 ]、DALL-E [ 95 ]、GPT-3 [ 10 ] 和 Flamingo [ 3 ]，都使用了大量与任务无关的数据来预训练大型神经架构，这些架构在下游任务（包括零样本和少镜头设置）中表现出色。相比之下，Embodied AI（E-AI）研究界主要是在场景少得多的模拟器中训练智能体[ 94 , 63 , 27 ]。由于任务的复杂性和需要较长的规划时间，表现最好的 E-AI 模型在有限的训练场景中仍会过拟合，因此对未知环境的泛化能力较差。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/c653a92e919f2094b4d38edef62288d4/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-19T04:15:00+08:00" />
<meta property="article:modified_time" content="2023-12-19T04:15:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【具身智能评估7】ProcTHOR: Large-Scale Embodied AI Using Procedural Generation</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p>论文标题：ProcTHOR: Large-Scale Embodied AI Using Procedural Generation<br> 论文作者：Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Jordi Salvador, Kiana Ehsani, Winson Han, Eric Kolve, Ali Farhadi, Aniruddha Kembhavi, Roozbeh Mottaghi<br> 论文原文：https://arxiv.org/abs/2206.06994<br> 论文出处：–<br> 论文被引：69（12/18/2023）<br> 论文代码：https://github.com/allenai/procthor，183 star<br> 项目主页：https://procthor.allenai.org/</p> 
</blockquote> 
<h2><a id="Abstract_10"></a>Abstract</h2> 
<p>海量数据集和大容量模型推动了计算机视觉和自然语言理解领域的许多最新进展。这项工作提供了一个平台，使具身人工智能也能取得类似的成功。我们提出的 PROCTHOR 是一个<strong>程序化生成具身人工智能环境的框架</strong>。PROCTHOR 使我们能够对多样化、交互式、可定制和高性能虚拟环境的任意大型数据集进行采样，以训练和评估导航、交互和操作任务中的具身智能体。我们通过 10,000 个生成的房屋样本和一个简单的神经模型展示了 PROCTHOR 的功能和潜力。在 PROCTHOR 上仅使用 RGB 图像训练的模型，在没有显式映射和人类任务监督的情况下，在导航、重新排列和手臂操纵等 6 项具身人工智能基准测试中取得了最先进的结果，其中包括目前正在进行的 Habitat 2022、AI2-THOR Rearrangement 2022 和 RoboTHOR 挑战赛。我们还通过在 PROCTHOR 上进行预训练，而不对下游基准进行微调，在这些基准上展示了强大的 零样本结果，击败了以前使用下游训练数据的最先进系统。</p> 
<h2><a id="1_Introduction_14"></a>1 Introduction</h2> 
<p>通过使用大规模训练数据，计算机视觉和自然语言处理模型变得越来越强大。最近的模型，如 CLIP [ 93 ]、DALL-E [ 95 ]、GPT-3 [ 10 ] 和 Flamingo [ 3 ]，都<strong>使用了大量与任务无关的数据来预训练大型神经架构，这些架构在下游任务（包括零样本和少镜头设置）中表现出色</strong>。相比之下，Embodied AI（E-AI）研究界主要是在场景少得多的模拟器中训练智能体[ 94 , 63 , 27 ]。<strong>由于任务的复杂性和需要较长的规划时间，表现最好的 E-AI 模型在有限的训练场景中仍会过拟合，因此对未知环境的泛化能力较差</strong>。</p> 
<p>近年来，E-AI 模拟器的功能越来越强大，支持物理、机械手、物体状态、可变形物体、流体和真实模拟对等[63, 101 , 104 , 37 , 124]，但将其<strong>扩展到数以万计的场景仍然具有挑战性</strong>。现有的 E-AI 环境要么是人工设计的 [63, 37]，要么是通过真实结构的3D扫描获得的 [101, 94]。前一种方法需要3D艺术家花费大量时间设计3D资产，在大型空间中合理地布置它们，并在这些环境中仔细配置合适的纹理和照明。后者则需要在许多真实世界环境中移动专用摄像机，然后将生成的图像拼接在一起，形成场景的3D重建。<strong>这些方法都不具备可扩展性，将现有场景库扩大数倍也不现实</strong>。</p> 
<blockquote> 
 <p>现有的 E-AI 环境分为两类：</p> 
 <ul><li>专家手动设计：AI2-THOR，ThreeDWorld</li><li>真实环境扫描：Habitat，HM3D</li></ul> 
</blockquote> 
<p><img src="https://images2.imgbox.com/ab/f0/v9lUlyGG_o.png" alt="在这里插入图片描述"></p> 
<p>我们介绍的 PROCTHOR 是一个基于 AI2-THOR [63 ]的框架，用于为 E-AI 研究<strong>程序化地生成完全交互的物理环境</strong>。给定一个房间规格（例如，一栋有 3 个卧室、3 个浴室和 1 个厨房的房子），PROCTHOR 就能生成符合这些要求的大量不同的平面图（图 1）。</p> 
<ul><li>由 108 种物体类型和 1633 个完全可交互实例组成的大型资产库用于自动填充每个平面图，确保物体的摆放合理、自然和逼真。</li><li>用户还可以改变每个场景中照明元素（人工照明和模拟 skybox）的强度和颜色，以模拟室内照明和一天中时间的变化。</li><li>资产（如家具和水果）和较大的结构（如墙壁和门）可以分配各种颜色和纹理，这些颜色和纹理是从每类资产的合理颜色和材料集中抽取的。</li><li>布局、资产、位置和照明的多样性共同构成了一个任意庞大的环境集，使 PROCTHOR 的规模远远超出了目前模拟器所支持的场景数量。</li><li>此外，PROCTHOR 还支持<strong>动态材料随机化（dynamic material randomizations）</strong>，每次将环境加载到内存中进行训练时，都可以对单个资产的颜色和材料进行随机化。</li><li>重要的是，与使用3D扫描生成的环境不同，PROCTHOR 生成的场景包含的物体<strong>既支持各种不同的物体状态（如打开、关闭、破碎等），又具有完全的交互性，因此可以由使用机械臂的智能体进行实际操作</strong>。</li></ul> 
<p>我们还展示了由艺术家设计的3D建筑 <strong>ARCHITECTHOR，这是一套由 10 座高质量的全交互式房屋组成的3D模型，旨在作为家庭环境研究的测试环境</strong>。与 AI2-iTHOR（单个房间）和 RoboTHOR（视觉多样性较低）环境相比，ARCHITECTHOR 包含更大、更多样和更逼真的房屋。</p> 
<p>我们通过对包含 10,000 所房屋（命名为 PROCTHOR-10K）的环境进行采样，展示了 PROCTHOR 的易用性和有效性，该环境由各种布局组成，小到 1 个房间的房屋，大到 10 个房间的房屋。我们在 PROCTHOR-10K 上使用非常简单的神经架构（CNN+RNN）训练智能体（没有深度传感器，仅使用 RGB 通道，没有明确的映射，也没有人类任务监督），并在多个导航和交互基准上生成了最先进的（SoTA）模型。截至太平洋时间 2022 年 6 月 14 日上午 10 点，我们获得了 ：</p> 
<ul><li> <p>1）RoboTHOR ObjectNav Challenge [5]</p> 
  <ul><li>零样本的性能优于之前使用 RoboTHOR 训练场景的 SoTA</li><li>通过微调，SPL 比以前的 SoTA 提高了 8.8 分；</li></ul> </li><li> <p>2）Habitat ObjectNav Challenge 2022 [ 79 ]</p> 
  <ul><li>成绩名列前茅，SPL 比下一个最好的提交作品提高了 3 分以上；</li></ul> </li><li> <p>3）1-phase Rearrangement Challenge 2022 [ 4 ]</p> 
  <ul><li>成绩名列前茅，Prop Fixed Strict 从 0.19 提高到 0.245；</li></ul> </li><li> <p>4）AI2-iTHOR ObjectNav</p> 
  <ul><li>零样本性能已经超过了之前基于 AI2-iTHOR 训练的模型，经过微调，成功率达到了 77.5%；</li></ul> </li><li> <p>5）ArmPointNav [ 33 ]</p> 
  <ul><li>使用 RGB 时，零样本性能数字超过了之前的 SoTA 结果；</li></ul> </li><li> <p>6）ArchitecTHOR ObjectNav</p> 
  <ul><li>成功率从 18.5% 大幅提高到 31.4%。</li><li>最后，消融分析清楚地显示了从 10 个场景到 100 个场景再到 1K 个场景，最后到 10K 个场景的优势，并表明通过调用 PROCTHOR 生成更大的环境可以获得进一步的改进。</li></ul> </li></ul> 
<p>总之，我们的贡献包括：</p> 
<ul><li>1）PROCTHOR，一个允许以程序化方式生成无限数量的多样化、完全交互式模拟环境的框架；</li><li>2）ARCHITECTHOR，一套由3D艺术家设计的新房屋，用于 E-AI 评估；</li><li>3）六项 E-AI 基准的 SoTA 结果，涵盖操作和导航任务，包括强大的 0-shot 结果。PROCTHOR 将开源，并将发布这项工作中使用的代码。</li></ul> 
<h2><a id="2_Related_Work_68"></a>2 Related Work</h2> 
<p>Embodied AI platforms.</p> 
<p>在过去几年里，人们开发了各种具身人工智能平台[ 63 , 101 , 104 , 124 , 37, 121 ]。这些平台针对不同的设计目标。</p> 
<ul><li>AI2-THOR[63]及其变体（ManipulaTHOR[33]和RoboTHOR[27]）是在Unity游戏引擎中构建的，重点关注<strong>智能体与物体之间的交互、物体状态变化以及精确的物理模拟</strong>。</li><li>与 AI2-THOR 不同，Habitat[101] 提供由房屋3D扫描构建的场景，但<strong>物体和场景不可交互</strong>。</li><li>iGibson [ 104 ] 包含逼真的场景，但<strong>只有有限的交互</strong>（如推）。</li><li>iGibson 2.0 [ 69 ] 扩展了 iGibson，重点关注合成场景中的<strong>家务劳动和物体状态变化</strong>，并包含虚拟现实接口。</li><li>ThreeDWorld [37] 以<strong>高保真物理仿真</strong>为目标，如液体和可变形物体仿真。</li><li>VirtualHome [92] 是为通过程序<strong>模拟人类活动</strong>而设计的。</li><li>RLBench [53]、RoboSuite [133] 和 Sapien [124] 的目标是<strong>细粒度操作</strong>。</li><li>PROCTHOR的主要优势在于，我们可以程序化地生成各种交互场景，从而在具身人工智能（Embodied AI，E-AI）的背景下，<strong>对数据增强和大规模训练进行研究</strong>。</li></ul> 
<p>Large-scale datasets.</p> 
<p>大规模数据集已在不同领域取得重大突破，如图像分类 [28 , 67]、视觉与语言 [18 , 111]、3D理解 [ 14 , 125 ]、自动驾驶 [11 , 107 ] 和机器人物体操纵 [ 91, 82 ]。然而，用于<strong>E-AI 研究的交互式大规模数据集并不多</strong>。PROCTHOR 包含程序化生成的交互式房屋。因此，该框架中的场景数量非常庞大。与我们最接近的作品有 [94, 90, 72]。</p> 
<ul><li>HM3D [94] 是一个最新的框架，其中包含 1000 个使用真实环境的 3D 扫描生成的场景。PROCTHOR 有几个主要区别： 
  <ul><li>1）HM3D 包含静态场景，而 PROCTHOR 不同，它的场景是交互式的，即<strong>物体可以移动和改变状态，物体的光照和纹理可以改变，物理引擎可以决定场景的未来状态</strong>；</li><li>2）HM3D 需要扫描房屋并清理数据，因此扩大 HM3D 的规模具有挑战性，而我们可以程序化地生成更多房屋；(3) HM3D 只能用于导航任务（因为没有物理模拟和物体交互），而 PROCTHOR 可用于导航以外的任务。</li></ul> </li><li>OpenRooms [ 72] 在数据来源（3D扫描）和数据集大小方面与 HM3D 相似。不过，<strong>OpenRooms 是交互式的</strong>。OpenRooms 还<strong>局限于扫描的房屋集，而且注释一个新场景需要花费大量时间（例如，为一个物体标注材料需要 1 分钟）</strong>，而 PROCTHOR 则不存在这些问题。</li><li>Megaverse [ 90 ] 是另一个大规模的具身人工智能平台，其中包括程序化生成的环境。虽然它<strong>模拟速度快</strong>，但它只包含<strong>外观简化</strong>的类似游戏的环境。相比之下，PROCTHOR 在外观、物理和物体交互的复杂性方面都模仿了现实世界中的房屋。</li></ul> 
<p>Scene generation.</p> 
<p>计算机视觉和图形学界对室内场景合成进行了广泛的研究。</p> 
<ul><li>[16, 17, 13] 解决了从文本描述生成3D场景的问题。</li><li>[120 , 47 , 85] 学习生成房屋平面图。</li><li>[98, 129 , 20, 113 , 56 , 70] 使用生成模型生成室内场景。</li><li><strong>[131 , 100] 为室内场景中的查询位置提出潜在物体</strong>。</li><li>还有人使用程序生成[57 , 31]和无监督学习[29]为AI合成网格世界环境。</li></ul> 
<p>PROCTHOR 是专为具身人工智能研究而设计的，它具有以下特点：</p> 
<ul><li>1）<strong>所有场景都是交互式的，具有物理功能，物体的摆放尊重世界的物理特性（例如，没有两个物体会互相穿过）</strong>；</li><li>2）<strong>有各种形式的场景增强，例如，在遵循某些常识性规则的情况下，物体摆放的随机化、物体和结构外观的变化以及光照的变化</strong>。</li></ul> 
<h2><a id="3_PROCTHOR_108"></a>3 PROCTHOR</h2> 
<p><strong>PROCTHOR 是一个程序化生成 E-AI 环境的框架。它扩展了 AI2-THOR，因此继承了 AI2-THOR 的大型资产库、机器人智能体和精确的物理模拟</strong>。与 AI2-THOR 中设计人员精心创建的场景一样，PROCTHOR 中的环境也是完全交互式的，支持导航、物体操作和多智能体交互。</p> 
<p><img src="https://images2.imgbox.com/d1/0b/NSqOLUxZ_o.png" alt="在这里插入图片描述"></p> 
<p>图 2 显示了 PROCTHOR 生成场景的高级程序示意图。给定一个房间规格（如带 1 间卧室和 1 间浴室的房子），PROCTHOR 使用多阶段条件采样来反复生成平面图（floor plans）、创建外墙结构、采样照明和门，然后采样包括大型、小型和墙面物体在内的资产，挑选颜色和纹理，并确定资产在场景中的适当位置。有关程序生成和采样机制的详情，请参阅附录：多样性、交互性、可定制性、规模和效率。</p> 
<h3><a id="Diversity_117"></a>Diversity</h3> 
<p>PROCTHOR 可以创建丰富多样的环境。在视觉和 NLP 领域，使用多样化数据对模型进行预训练取得了成功，与此相对应，我们在多个 E-AI 任务中展示了这种多样性的实用性。PROCTHOR 中的场景在多个方面都表现出了多样性：</p> 
<p>平面图（floor plans）的多样性。给定房间规格后，我们首先采用<strong>迭代边界切割（iterative boundary cutting）<strong>获得外部场景布局（从简单的矩形到复杂的多边形）。然后使用 Lopes 等人[73] 的</strong>递归布局生成算法</strong>将场景划分为所需的房间。最后，我们使用一组<strong>用户定义的约束条件</strong>来确定房间之间的连通性。这些程序会产生自然的房间布局（例如，卧室通常通过一扇门与相邻的浴室相连，浴室通常只有一个入口等）。如图 3 所示，PROCTHOR 使用这种程序生成了多种多样的平面图。</p> 
<p><img src="https://images2.imgbox.com/87/cf/JsJkYClE_o.png" alt="在这里插入图片描述"></p> 
<p>资产的多样性。PROCTHOR 从其数据库中收集了 108 个类别的 1633 件家庭资产（图 4 中的示例），在场景中填充了大大小小的资产。虽然许多资产继承自 AI2-THOR，但我们<strong>也引入了新的资产，如门窗和台面，这些都是由3D平面设计师手工设计的</strong>。资产实例分为训练/验证/测试子集，并且可以交互，即可以在场景中挑选和放置物体，有些物体有多种状态（如灯可以开或关），还有一些物体由具有刚体运动的部件组成（如微波炉上的门）。</p> 
<p><img src="https://images2.imgbox.com/94/df/NKGd4ZK5_o.png" alt="在这里插入图片描述"></p> 
<p>材料多样性。墙壁有两种材料：40 种纯色（流行色）或 122 种墙壁纹理（如砖和瓷砖）。我们还提供 55 种地板材料。<strong>整栋房屋的天花板材料是从墙壁材料中抽取的</strong>。PROCTHOR 还提供了随机化物体材料的功能。<strong>材料只在类别内随机化</strong>，这样可以确保物体的外观和行为与它们所代表的类别一致。</p> 
<p><img src="https://images2.imgbox.com/83/96/H6bXRuxX_o.png" alt="在这里插入图片描述"></p> 
<p>物体摆放的多样性。资产类别有多种<strong>软注释（soft annotations）</strong>，有助于将它们真实地放置在房屋内。这些软注释包括房间分配（例如，沙发放在客厅而不是浴室）和位置分配（例如，冰箱沿着墙壁，电视机不在地板上）。我们还提出了<strong>语义资产组（Semantic Asset Group，SAG）的概念，即通常共同出现的资产组（如餐桌和四把椅子），因此必须使用从属采样（dependent sampling）进行采样和放置</strong>。</p> 
<ul><li>给定布局后，对位于地面上的单个资产和 SAG 进行采样并反复放置，确保房间有足够的地面空间供智能体导航和操作物品。</li><li>然后，放置窗户和油画等墙面物体。</li><li>最后，放置表面物体（位于其他资产之上的物体），例如厨房台面上的杯子。</li></ul> 
<p>通过这种取样，可以在任何布局中选择和放置大量不同的物体。图 6 展示了这种变化。</p> 
<p><img src="https://images2.imgbox.com/ac/53/RyWv5DF9_o.png" alt="在这里插入图片描述"></p> 
<p>照明的多样性。PROCTHOR 支持<strong>单个定向光源</strong>（类似太阳）和<strong>多个点光源</strong>（类似灯泡）。通过改变这些光源的<strong>颜色、强度和位置</strong>，我们可以模拟出不同的人工照明，通常是在房屋内以及一天中的不同时间。如图 7 所示，照明对渲染图像有很大影响。</p> 
<p><img src="https://images2.imgbox.com/fd/80/fO4X3zoQ_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Interactivity_152"></a>Interactivity</h3> 
<p>PROCTHOR 的一个关键特性是<strong>能够与物体互动，改变其位置或状态</strong>（图 8）。这种能力是许多E-AI任务的基础。像HM3D[94]这样通过静态3D扫描创建的数据集则<strong>不具备这种能力</strong>。PROCTHOR 支持带有手臂的智能体，它们能够操纵物体并相互交互。</p> 
<p><img src="https://images2.imgbox.com/0b/fd/FbdtdriB_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Customizability_159"></a>Customizability</h3> 
<p>PROCTHOR 支持多种房间、资产、材料和照明规格。只需几行简单的说明，就能轻松生成客户感兴趣的环境。图 9 展示了这些不同场景（教室、图书馆和办公室）的示例。</p> 
<p><img src="https://images2.imgbox.com/b0/66/EihV40T9_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Scale_and_Efficiency_166"></a>Scale and Efficiency</h3> 
<p>PROCTHOR 目前使用 16 种不同的场景规范作为场景生成过程的种子。这些规范可产生超过 1,000 亿个布局。PROCTHOR 使用 18 种不同的语义资产组和 1633 种资产。这可以产生大约 2000 万个独特的资产组。每个资产都可以放置在多个位置。此外，每栋房屋都会按比例缩放，并使用各种照明。这种布局、资产、材料、位置和照明的多样性使我们能够生成任意庞大的房屋集——<strong>既可以静态生成并存储为数据集，也可以在每次迭代训练时动态生成</strong>。场景<strong>以 JSON 格式有效表示，并在运行时加载到 AI2-THOR，从而使存储房屋的内存开销变得异常高效</strong>。此外，场景生成过程是全自动且快速的，PROCTHOR 可为 E-AI 模型的训练提供高帧频（详见第 4 章）。</p> 
<h2><a id="4_PROCTHOR10K_170"></a>4 PROCTHOR-10K</h2> 
<p>我们使用第 3 节中描述的程序生成过程获得的 10,000 套完全交互式房屋样本来展示 PROCTHOR 的功能和潜力，我们将其命名为 PROCTHOR -10K。另外还有 1,000 套验证房屋和 1,000 套测试房屋可供评估。附录中详细说明了训练/验证/测试的资产分配情况。所有房屋都是完全可浏览的，允许智能体在没有任何交互的情况下穿越每个房间。就规模而言，<strong>PROCTHOR -10K 是最大的具身人工智能交互式家庭环境集之一</strong>。</p> 
<ul><li>AI2-iTHOR [63] 包含 120 个场景</li><li>RoboTHOR [27] 有 89 个场景</li><li>iGibson [104] 有 15 个场景</li><li>Habitat Matterport 3D [94] 有 1,000 个静态（非交互式）场景</li><li>Habitat 2.0 [109] 有 105 个场景布局</li></ul> 
<p>将规模扩展到 10K 房屋以上既简单又便宜。这组 10K 房屋是在配备 4 个英伟达 RTX A5000 GPU 的本地工作站上于 1 小时内生成的。图 11 显示了 PROCTHOR-10K 中以自我为中心和自上而下的房屋视图示例。</p> 
<p><img src="https://images2.imgbox.com/db/6c/vY19X4Xk_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Scene_statistics_185"></a>Scene statistics</h3> 
<p>PROCTHOR -10K 中的房屋是根据 16 种不同的房间规格生成的。房间规格示例如下 该数据集中的房屋少则 1 间，多则 10 间。图 10 显示了这些生成房屋的面积分布（中）和房间数量（右）。通过使用房间规格，我们可以非常容易地改变房屋的大小和复杂程度。PROCTHOR -10K 比 AI2-iTHOR [ 63 ] 和 ROBOTHOR [27]（偏重于房间大小的场景）以及 Gibson [ 123 ] 和 HM3D [94]（偏重于大型房屋）涵盖了更广泛的场景。</p> 
<p><img src="https://images2.imgbox.com/4b/be/SCIi7eWJ_o.png" alt="在这里插入图片描述"></p> 
<p><strong>这些房屋中的每个房间都包含 95 个不同类别的物体，包括冰箱、台面、床、马桶和室内植物等常见家居物体，以及门道和窗户等结构物体</strong>。图 10（左）显示了每栋房屋每个房间的物品数量分布情况，从中可以看出 PROCTHOR -10K 中的房屋都有很多物品。它们还包含通过 18 个不同语义资产组采样的物体。语义资产组（SAG）的示例包括带 4 把椅子的餐桌或带 2 个枕头的床。考虑到我们庞大的资产库和 SAG，我们可以创建 1930 万种组实例组合。</p> 
<h3><a id="Rendering_speed_194"></a>Rendering speed</h3> 
<p>大规模训练的一个关键要求是高渲染速度，因为训练算法需要数百万次迭代才能收敛。表 1 显示了这些统计数据。实验在配有 8 个英伟达 Quadro RTX 8000 GPU 的服务器上运行。在使用 1 个 GPU 的实验中，我们使用了 15 个进程，而在使用 8 个 GPU 的实验中，我们使用了 120 个进程，平均分配给各个 GPU。PROCTHOR 尽管有更大的房子，却能提供与 iTHOR 和 RoboTHOR 环境相当的帧速率（详见附录），因此它的速度足以在合理的时间内训练数亿步的大型模型。</p> 
<p><img src="https://images2.imgbox.com/a4/d7/fl97hfzP_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="5_Experiments_201"></a>5 Experiments</h2> 
<h3><a id="Tasks_203"></a>Tasks</h3> 
<p>我们现在介绍在 PROCTHOR -10K 上预先训练的模型在几个<strong>导航和操作基准</strong>上的结果，以证明大规模训练的好处。我们考虑了ObjectNav（指向特定物体类别的导航）：</p> 
<ul><li>PROCTHOR</li><li>ARCHITECTHOR</li><li>RoboTHOR[27]</li><li>HM3D[94]</li><li>AI2-iTHOR[63]</li></ul> 
<p>我们还考虑了两个基于操作的任务：</p> 
<ul><li>ArmPointNav[33]：智能体使用机械臂将物体从源位置移动到3D坐标框架中指定的目标位置。</li><li>1-phase Room Rearrangement[115]：目标是移动物体或改变其状态，以达到目标场景状态。</li></ul> 
<h3><a id="Models_218"></a>Models</h3> 
<p>我们用于所有任务的<strong>模型都由一个 CNN（编码视觉信息）和一个 GRU（捕捉时间信息）组成</strong>。</p> 
<ul><li>我们特意在所有任务中使用了简单的架构，以展示大规模训练的优势。</li><li>我们的 ObjectNav 和 Rearrangement 模型使用了 [58] 基于 CLIP 的架构。</li><li>我们的 ArmPointNav 模型使用了一个更简单的视觉编码器，包含 3 个卷积层；我们发现这比 CLIP 编码器更有效。</li><li>所有模型均采用 AllenAct [116] 框架进行训练，训练详情请参见附录。</li></ul> 
<h3><a id="Results_227"></a>Results</h3> 
<p>我们展示了两种情况下的结果：在下游基准所提供的训练场景上进行零样本和微调后的结果。</p> 
<ul><li>零样本实验展示了在 PROCTHOR 基础上训练的<strong>模型对新环境的泛化能力</strong>；</li><li>微调实验展示了从 PROCTHOR 中学习到的<strong>表征是否可以作为快速调优的良好初始化</strong>。</li></ul> 
<p>在所有实验中，我们<strong>只使用 RGB 图像</strong>（不使用深度和其他模式）。</p> 
<p>与 PROCTHOR 相比，其他环境具有不同的外观统计、布局和物体分布，因此零样本尤其具有挑战性。</p> 
<ul><li>ARCHITECTHOR 和 AI2-iTHOR [63] 是艺术家设计的高保真场景，具有高质量的阴影和照明。</li><li>HM3D 由房屋的 3D 扫描构建而成，与合成环境有很大差异。</li><li>RoboTHOR [27] 房屋使用的墙板和地板具有非常特殊的纹理。</li></ul> 
<p>零样本迁移结果。仅在 PROCTHOR 上训练并进行了零样本评估的模型在 3 个基准上的表现优于之前的 SoTA 模型（参见表 2 中的零样本行）。这些结果非常出色，因为<strong>这些模型不仅可以泛化到未见过的物体和场景，还可以泛化到不同的外观和布局统计</strong>。</p> 
<p>微调结果。利用每个基准的训练数据对模型进行进一步微调后，在所有基准上都取得了最先进的结果（参见表 2 的微调行）。值得注意的是，截至太平洋时间 2022 年 6 月 14 日上午 10 点，我们的模型在三个公开排行榜上排名第一：Habitat 2022 ObjectNav challenge、AI2-THOR Rearrangement 2022 challenge 和 RoboTHOR ObjectNav challenge。值得注意的是，<strong>我们的模型仅使用了非常简单的架构和 RGB 图像就取得了这些成绩</strong>。其他技术通常使用更复杂的架构，包括映射或视觉里程测量模块，并使用深度图像等额外的感知传感器。</p> 
<p><img src="https://images2.imgbox.com/0c/59/RcFIxKQ1_o.png" alt="在这里插入图片描述"></p> 
<p>表 2： 在 ProcTHOR 上训练的模型的结果，以及在多个 E-AI 基准上进行的零样本评估和微调。a）EmbCLIP [58] 在 ROBOTHOR 上训练，b）EmbCLIP [58] 在 AI2-iTHOR 上训练，c）在 Habitat 2022 ObjectNav 排行榜[79]上提交 。 d）对于 HM3D，我们给出了使用标准 EmbCLIP 架构（使用 CLIP 预训练的 ResNet50 主干网）和 Large 模型（使用更大的 CLIP 主干网 CNN 和更宽的 RNN）进行预训练的结果，详见补充资料。 e）使用了[33]中的模型，但在带有 RGB 输入的完整 iTHOR 数据上进行了重新训练。粉色背景的是零样本结果，即模型在 PROCTHOR -10K 上进行预训练，不使用评估基准的任何训练数据。</p> 
<h3><a id="Scale_ablation_251"></a>Scale ablation</h3> 
<p>为了评估规模的影响，我们在 10、100、1,000 和 10,000 座房屋上对模型进行了训练。在这项实验中，我们没有使用任何材料增强。如表 3 所示，<strong>当我们使用更多房屋进行训练时，模型的性能就会提高</strong>，这证明了大规模数据对具身人工智能任务的益处。<br> <img src="https://images2.imgbox.com/ed/82/KuK1jbUl_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="6_Conclusion_255"></a>6 Conclusion</h2> 
<p>我们提出了 PROCTHOR，这是一个程序化生成任意大型交互式物理房屋集的框架，用于具身人工智能研究。我们对生成的 10,000 所房屋进行了简单模型的预训练，并在 6 个具身导航和操作基准测试中展示了最先进的结果，其中 3 个基准测试的结果甚至超过了之前的最先进水平。</p> 
<h2><a id="Acknowledgements_259"></a>Acknowledgements</h2> 
<p>我们要感谢该项目中使用的开源包背后的团队，包括AI2-THOR[63]、AllenAct[116]、Habitat[101]、Datasets[68]、NumPy[45]、PyTorch[88]、Pandas[78]、Wandb[8]、Shapely[41]、Hydra[126]、SciPy[112]、UMAP[77]、NetworkX[44]、EvalAI[127]、TensorFlow[1]、OpenAI Gym[9]、Seaborn[114]、PySAT[50]和Matplotlib[49]。</p> 
<h2><a id="A_ProcTHOR_Assets_263"></a>A ProcTHOR Assets</h2> 
<p><img src="https://images2.imgbox.com/04/24/CvBVHQpL_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="B_House_Generation_268"></a>B House Generation</h2> 
<h3><a id="B4_Connecting_Rooms_270"></a>B.4 Connecting Rooms</h3> 
<p><img src="https://images2.imgbox.com/2e/38/3cMlo9n9_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="B8_Object_Placement_275"></a>B.8 Object Placement</h3> 
<h4><a id="B81_Assets_277"></a>B.8.1 Assets</h4> 
<p>ProcTHOR资产数据库由108种物体类型的1633种交互式家庭资产组成（更多详细信息，请参阅附录A）。大部分资产来自AI2-THOR。窗户、门和台面都建在AI2-THOR房间的外部，这使我们无法将房间作为独立资产生成。因此，我们还手工制作了21扇窗户、20扇门和33个台面。</p> 
<p><img src="https://images2.imgbox.com/21/d5/fNNVyCWs_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="B82_Semantic_Asset_Groups_SAGs_284"></a>B.8.2 Semantic Asset Groups (SAGs)</h4> 
<p>语义资产组（SAG）提供了一种灵活多样的方式来编码哪些物体可能出现在彼此附近。SAG的力量在于其支持随机资产和轮换采样的能力。通过我们用户友好的拖放式web界面，SAG可以在几秒钟内创建和导出。</p> 
<p><img src="https://images2.imgbox.com/ba/14/SAr39m6D_o.png" alt="在这里插入图片描述"></p> 
<p>图 26 举例说明了我们如何构建一个将两把椅子推到餐桌边上的 SAG。SAG 包括两个椅子采样器和一个餐桌采样器。资产采样器包含一组可采样的唯一 3D 建模资产实例。当 SAG 实例化时，每个资产采样器会随机选择其中一个实例。资产采样器也可以链接，即多个采样器每次对同一资产实例进行采样。在这里，通过链接可以将同一把椅子的多个实例放置在餐桌上，而不是每个取样器独立取样不同的椅子。</p> 
<p>在 SAG 中随机采样资产的能力具有惊人的表现力。例如，考虑一个包含电视柜、电视、沙发和扶手椅采样器的 SAG。如果每个采样器都能从 30 个不同的 3D 建模资产实例中采样，那么就有超过 800K 种独特的实例组合可以从 SAG 中采样。</p> 
<p><strong>资产采样器定义了资产之间的相对位置</strong>。SAG 是通过资产采样器自上而下的正交图像来构建的，如图 26a 所示。在这里，两个椅子取样器都是餐桌取样器的父取样器。每个子资产采样器与其父资产采样器垂直锚定在 V = {上、中、下}，水平锚定在 H = {左、中、右}。例如，在图 26a 中，两个椅子采样器都与父采样器垂直固定在顶部，水平固定在中心。但是，左侧椅子取样器的枢轴位置垂直于中心，水平于右侧，而右侧椅子取样器的枢轴位置垂直于中心，水平于左侧。图 27 举例说明了如何将植物或落地灯取样器放置在扶手椅取样器周围。每个子资产采样器都有一个 (x, y) 偏移量，即从父采样器锚点到子采样器枢轴位置的距离。</p> 
<p><img src="https://images2.imgbox.com/22/bb/G3tRQdO2_o.png" alt="在这里插入图片描述"></p> 
<p>对资产采样器进行相对定位的目的是防止网格之间相互剪切。例如，以图 26a 中的 SAG 为例，如果餐桌采样器采样的餐桌尺寸是当前餐桌的两倍，会发生什么情况？椅子不会停留在固定的全局位置，也不会与新的餐桌发生碰撞，而是会反应性地向后移动，并重新定位，使其保持在较大餐桌的下方。此外，考虑到从资产采样器中采样的实例尺寸往往大不相同。例如，一张桌子可能是方形的，而另一张桌子则是长条形的。如果我们只使用 CENTER CENTER 枢轴和偏移量，就无法可靠地将包含不同大小物体的资产采样器直接放置在彼此旁边而不会造成剪切。</p> 
<p><img src="https://images2.imgbox.com/18/aa/c8acOW78_o.png" alt="在这里插入图片描述"></p> 
<p>虽然设置锚点和枢轴位置可以解决很多网格剪切问题，但仍可能出现一些情况。图 28 显示了一个例子，如果我们的餐桌采样器采样的是一张短餐桌，那么它可能会夹到某些椅子上。这种问题在实践中并不多见，但物体剪切会导致房屋的逼真度和交互性降低。<strong>为了解决剪切问题，我们使用拒绝采样对 SAG 中的资产进行重新采样，直到采样资产的三维网格都不发生剪切</strong>。</p> 
<p>在 PROCTHOR -10K 中，<strong>我们构建了 18 个 SAG，这些 SAG 可通过超过 2,000 万种独特的资产组合进行实例化</strong>。其中包括桌子周围的椅子、床顶上的枕头、沙发和扶手椅、电视柜顶上的电视、水槽顶上的水龙头以及带椅子的书桌等语义资产组。</p> 
<h4><a id="B83_Floor_Object_Placement_309"></a>B.8.3 Floor Object Placement</h4> 
<p><img src="https://images2.imgbox.com/e8/0c/EG6jOfRi_o.png" alt="在这里插入图片描述"><br> 图29：详细说明地板物体如何放置在房间中的示意图。首先，我们通过从每个角点绘制水平和垂直分隔符，将房间开放式平面图的自上而下视图矩形化。然后，我们构造在分隔符内形成的所有可能的矩形。然后，我们对其中一个矩形进行采样，并将物体放置在该矩形内。采样物体的自上而下边界框（带边距）显示为蓝色。然后从打开的楼层平面中减去边界框，然后再次重复该过程。</p> 
<p><img src="https://images2.imgbox.com/4d/8f/CFPG82dy_o.png" alt="在这里插入图片描述"></p> 
<p>图30：放置在房间的边缘、角落和中间时对象的有效旋转。放置在房间边缘或角落的对象始终背对墙。场景中间的对象可以在任何方向上旋转。通过约束对象的旋转，我们可以确保房间边缘的对象（如冰箱或抽屉）仍然可以打开。</p> 
<h4><a id="B84_Wall_Object_Placement_318"></a>B.8.4 Wall Object Placement</h4> 
<p><img src="https://images2.imgbox.com/c3/e8/B8JP4Gsy_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="B85_Surface_Object_Placement_323"></a>B.8.5 Surface Object Placement</h4> 
<p><img src="https://images2.imgbox.com/6f/9b/C6uiRFkp_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="B12_Limitations_and_Future_Work_328"></a>B.12 Limitations and Future Work</h4> 
<p>ProcTHOR-10K仅使用一层房屋。我们计划在ProcTHORv2.0中支持多层房屋。这将使我们能够捕捉更广泛的房屋，并提供更好的微调结果。此外，我们计划通过利用许多开源3D资产数据库来扩大我们的资产数据库，如ABO[26]、PartNet[81]、ShapeNet[15]、谷歌扫描对象[30]和CO3D[97]等。</p> 
<h2><a id="C_PROCTHOR_Datasheet_332"></a>C PROCTHOR Datasheet</h2> 
<p>创建该数据集是为了能够在更加多样化的环境中训练模拟的具身智能体。</p> 
<h3><a id="_336"></a>数据集可以用于哪些（其他）任务？</h3> 
<p>在AI2-THOR中可以执行的任何任务都可以在ProcTHOR中执行。例如，在E-AI中，房屋可用于：</p> 
<ul><li>导航[58，89，118，132，117，128，74，130]</li><li>多智能体交互[51，52，2]</li><li>重排和交互[115，36，39，23，106]</li><li>操纵[33，86，32，122]</li><li>Sim2Real transfer[27，54，66]</li><li>EVL [105，87，48，65，42，55]</li><li>视听导航[22，38，21]</li><li>虚拟现实交互[119，83，46]</li></ul> 
<p>在更广泛的计算机视觉领域，数据集可用于研究：</p> 
<ul><li>物体检测[64]</li><li>NeRFs[80，110，43，71]</li><li>分割、深度和最佳流量估计[35，43]</li><li>生成建模[59，62，61]</li><li>遮挡推理[34]</li><li>姿态估计[19]</li></ul> 
<p>我们从JSON规范加载程序生成房屋的框架还可以研究场景杂乱的生成，构建更逼真的程序生成房屋，以及开发综合生成空间，以训练工厂[84]、办公室、杂货店[76]和完整的程序生成城市中的具体代理。</p> 
<h2><a id="D_ARCHITECTHOR_360"></a>D ARCHITECTHOR</h2> 
<h2><a id="E_Input_Modalities_362"></a>E Input Modalities</h2> 
<h2><a id="F_Experiment_details_364"></a>F Experiment details</h2> 
<h2><a id="G_Performance_Benchmark_366"></a>G Performance Benchmark</h2> 
<p>为了计算 "分析 "部分所示的 FPS 性能基准，我们将房屋分为小型房屋（1-3 个房间的房屋）和大型房屋（7-10 个房间的房屋）。对于导航基准，我们执行移动和旋转操作。在交互基准中，我们执行了推动物体的操作。在查询环境数据时，我们会在每个时间步从环境中获取一个不常提供的元数据（例如，检查智能体的尺寸）。在每个时间步骤，我们都会从智能体的自我中心视角渲染一张 3 × 224 × 224 RGB 图像。实验在配有 8 个英伟达 Quadro RTX 8000 GPU 的服务器上进行。我们在单 GPU 测试中使用了 15 个进程，在 8 GPU 测试中使用了 120 个进程，平均分配给各个 GPU。表 11 显示了与 AI2-iTHOR 和 RoboTHOR 的比较。</p> 
<p><img src="https://images2.imgbox.com/37/a6/wDD8t0pu_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="H_Broader_Impact_373"></a>H Broader Impact</h2>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/79cdc51e324ae5de76127c807d54f940/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Linux shell编程学习笔记36：read命令</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c38556283168c0390b60352f9ff10250/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【具身智能评估8】BEHAVIOR-1K: A Benchmark for Embodied AI with 1,000 Everyday Activities and ...</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>