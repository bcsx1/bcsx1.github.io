<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>还在为DST模型刷不动而感到苦恼吗？来试试无监督DST吧，DSI等你来战！ - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="还在为DST模型刷不动而感到苦恼吗？来试试无监督DST吧，DSI等你来战！" />
<meta property="og:description" content="本文介绍一篇西湖大学联合哈尔滨工业大学 SCIR 实验室和北京理工大学发表于 IJCAI 2020 的论文 Dialogue State Induction Using Neural Latent Variable Models。
论文链接：
https://www.ijcai.org/Proceedings/2020/0532.pdf
代码链接：
https://github.com/taolusi/dialogue-state-induction
PPT链接：
https://taolusi.github.io/qingkai_min/assets/pdf/20-ijcai-dsi_slides.pdf
视频链接：
https://www.bilibili.com/video/BV1fV41127tq
对话状态跟踪模块是任务型对话系统中的核心部件，目前主流的对话状态跟踪的方法需要在大量人工标注的数据上进行训练。然而，对于现实世界中的各种客户服务对话系统来说，人工标注的过程存在代价高、标注慢、错误率高以及难以覆盖数量庞大的不同领域等问题。
基于这些问题，我们提出了一个新的任务：对话状态推理，目标是从大量无标注的客服对话记录中自动挖掘对话状态，并提出了两个基于神经隐变量的模型来实现无监督的对话状态推理，同时我们在下游的对话生成任务中进行了验证，实验结果表明，相比于缺少对话状态的对话系统，使用我们推理得到的对话状态可以获得更好的表现。
背景
1.1 任务型对话系统
任务型对话系统（task-oriented dialogue system）的目标是协助用户完成特定的任务，比如订机票、打车、日程管理等。
一个典型的任务型对话系统可以分为四个模块：自然语言理解（natural language understanding, NLU）、对话状态追踪（dialogue state tracking, DST）、策略学习（dialogue policy）以及自然语言生成（natural language generation, NLG），在这个过程中需要跟各种各样的数据库进行查询甚至更新等操作。
▲ 图1 任务型对话系统
（图片来源于：Gao J, Galley M, Li L. Neural approaches to conversational AI[C]//The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval. 2018: 1371-1374.）" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/faf14db6641df9f3fb793f2531983d16/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-09-07T13:01:12+08:00" />
<meta property="article:modified_time" content="2020-09-07T13:01:12+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">还在为DST模型刷不动而感到苦恼吗？来试试无监督DST吧，DSI等你来战！</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/d6/7f/eOuLztRG_o.png"></p> 
 <h2><br></h2> 
 <h2>本文介绍一篇西湖大学联合哈尔滨工业大学 SCIR 实验室和北京理工大学发表于 IJCAI 2020 的论文 <strong><em>Dialogue State Induction Using Neural Latent Variable Models</em></strong>。<br></h2> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/dc/82/2VZxXDfa_o.png"></p> 
 <figcaption></figcaption> 
 <p style="text-align: left"><strong>论文链接：</strong></p> 
 <p style="text-align: left"><strong></strong>https://www.ijcai.org/Proceedings/2020/0532.pdf</p> 
 <p style="text-align: left"><strong>代码链接：</strong></p> 
 <p style="text-align: left"><strong></strong>https://github.com/taolusi/dialogue-state-induction</p> 
 <p style="text-align: left"><strong>PPT链接：</strong></p> 
 <p style="text-align: left"><strong></strong>https://taolusi.github.io/qingkai_min/assets/pdf/20-ijcai-dsi_slides.pdf</p> 
 <p style="text-align: left"><strong>视频链接：</strong></p> 
 <p style="text-align: left"><strong></strong>https://www.bilibili.com/video/BV1fV41127tq</p> 
 <p style="text-align: justify">对话状态跟踪模块是任务型对话系统中的核心部件，目前主流的对话状态跟踪的方法需要在大量人工标注的数据上进行训练。然而，对于现实世界中的各种客户服务对话系统来说，人工标注的过程存在代价高、标注慢、错误率高以及难以覆盖数量庞大的不同领域等问题。</p> 
 <p style="text-align: justify">基于这些问题，我们提出了一个新的任务：<strong>对话状态推理</strong>，目标是从大量无标注的客服对话记录中自动挖掘对话状态，并提出了两个基于神经隐变量的模型来实现无监督的对话状态推理，同时我们在下游的对话生成任务中进行了验证，实验结果表明，相比于缺少对话状态的对话系统，使用我们推理得到的对话状态可以获得更好的表现。</p> 
 <p style="text-align: left"><img src="https://images2.imgbox.com/44/88/KpgYgUrW_o.png"></p> 
 <p style="text-align: left"><strong>背景</strong></p> 
 <p style="text-align: left"><strong>1.1 任务型对话系统</strong></p> 
 <p style="text-align: justify">任务型对话系统（task-oriented dialogue system）的目标是协助用户完成特定的任务，比如订机票、打车、日程管理等。</p> 
 <p style="text-align: justify">一个典型的任务型对话系统可以分为四个模块：自然语言理解（natural language understanding, NLU）、对话状态追踪（dialogue state tracking, DST）、策略学习（dialogue policy）以及自然语言生成（natural language generation, NLG），在这个过程中需要跟各种各样的数据库进行查询甚至更新等操作。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/6f/56/sQaDO5DZ_o.png"></p> 
 <p style="text-align: center">▲ 图1 任务型对话系统</p> 
 <p style="text-align: left">（图片来源于：Gao J, Galley M, Li L. Neural approaches to conversational AI[C]//The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval. 2018: 1371-1374.）</p> 
 <p style="text-align: left"><strong>1.2 对话跟踪模块及其限制</strong></p> 
 <p>对话状态跟踪模块是任务型对话系统的一个核心模块，对话状态表示了用户在每轮对话所寻求的内容的关键信息，它是从对话开始到当前轮的所有信息的累积，表示形式是一些 slot-value pairs 的集合，inform 表示用户对所寻求的内容的限制，request 表示用户想要寻求哪些内容。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/6f/55/1ZOOIdRJ_o.png" height="269" width="1132"></p> 
 <p style="text-align: center">▲ 图2 对话状态表示</p> 
 <p>传统的 DST 的做法是先通过 NLU 模块对用户对话进行意图识别和槽位解析，然后将结果输入到 DST 模块中，由 DST 模块处理 NLU 模块带来的一些不确定性，得到最终的对话状态。目前越来越多的工作直接通过一个端到端的 DST 模型来直接处理用户对话（以及历史对话记录）得到当前的对话状态，如图 3 所示。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/31/da/DZPHyu1s_o.png" height="563" width="1132"></p> 
 <p style="text-align: center">▲ 图3 端到端的DST</p> 
 <p>端到端的 DST 模型虽然已经在标准的数据集（如 EMNLP 最佳资源论文 MultiWOZ 数据集）取得了绝大的成功，但是这种方法依赖于人工标注的大量语料，由于对话状态本身的复杂性非常高，需要对语义以及上下文有一个整体的理解，所以导致人工标注的过程存在两个非常大的问题：</p> 
 <ul><li><p>昂贵的标注代价：在 MultiWOZ2.0 数据集中一共标注了 8438 组对话，一共竟然用了 1249 个标注人员。</p></li><li><p>过高的标注错误率：根据相关论文统计，MultiWOZ2.0 数据集中有大约 40% 的标注错误；而在此基础上更新的 MultiWOZ2.1 数据集也有超过 30% 的错误；目前最新版已经更新到了 MultiWOZ2.2。</p></li></ul> 
 <p>从这两点可以看出想要构建大规模高质量的数据集存在非常大的困难，这也导致虽然对于任务型对话系统的研究在一些领域上已经取得了一定的成功，但是也非常受限于这些已经标注好大量语料的领域，在实际应用中，会有大量新的领域存在，面对每一个新的领域都要人工标注新的数据集是不太实际的。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/33/2e/hzdlEsDt_o.png" height="297" width="641"></p> 
 <p style="text-align: center">▲ 图4 端到端的DST所面临的新领域问题<br></p> 
 <p style="text-align: left"><img src="https://images2.imgbox.com/dd/47/RisV7Pll_o.png"></p> 
 <p style="text-align: left"><strong>动机</strong></p> 
 <p style="text-align: left"><strong>2.1 新任务：对话状态推理 (dialogue state induction, DSI)</strong></p> 
 <p>在这种情况下，如何能够从对话记录中自动发现对话状态显得十分重要。我们假设可以获得大量来自不同领域的对话记录，而这些对话记录是没有对话状态标注的，通常这样的语料是相对容易获得的，比如可以来源于不同商业服务中已经积累的人工客服与用户的对话记录。</p> 
 <p>基于这种假设，我们提出了一个新的任务：对话状态推理（dialogue state induction, DSI），任务的目标是从大量的生对话语料中自动推理得到对话状态，并能更好的用于下游任务比如策略学习和对话生成。</p> 
 <p>DSI 与 DST 的区别如图 5 所示，和 DST 相似的是，DSI 的输出也是 slot-value pairs 形式的对话状态，不同的是，在训练过程中 DST 依赖于对话语料以及人工标注的对话状态，而 DSI 不依赖于人工标注，可以在生对话中自动生成 slot-value pairs。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/91/f0/qwz2oW1Y_o.png"></p> 
 <p style="text-align: center">▲ 图5 DST与DSI对比</p> 
 <p>除了我们提出的 DSI 任务之外，谷歌的研究者也提出了通过 zero-shot DST 的方案来缓解标注数据缺失的问题，他们主要利用了一些相似的领域之间可能存在相似的槽位的这一特点，比如订火车票和订机票两个领域都存在像出发地、到达地和出发时间、到达时间等槽位，通过对领域和槽位进行自然语言的描述来建立不同领域之间的联系。</p> 
 <p>这种方法确实可以在一定程度上解决未知领域的问题，但是这种方法同样需要对不同领域的槽位进行高质量（一致性高）的标注，同时在面对新的与已知的领域关联性不大的领域时（比如金融领域），很难从已知的领域迁移到这些全新的领域。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/9a/05/sCjZHIl1_o.png"></p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/53/c4/r09H5whH_o.png"></p> 
 <p style="text-align: center">▲ 图6 Zero-shot DST及其限制</p> 
 <p>而我们提出的 DSI 任务进一步减轻了对人工标注的依赖，我们希望通过数据驱动的方式，采用无监督的方法从生对话中自动挖掘出对话状态的信息。</p> 
 <p style="text-align: left"><img src="https://images2.imgbox.com/12/44/H16l7NxL_o.png"></p> 
 <p style="text-align: left">方法</p> 
 <p>我们的方法分为两个步骤：1）候选词抽取（通过 POS tag、NER 和 coreference）；2）为候选词分配合适的槽位。</p> 
 <p>其中第一步是一个预处理的过程，第二步我们建立了两个无监督的神经隐变量模型 DSI-base 和 DSI-GM 来为每个候选词分配一个槽位索引（slot-index），其中 DSI-GM 可以认为是 DSI-base 的一个变种。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/54/d3/64KUCOrE_o.png" height="421" width="826"></p> 
 <p style="text-align: center">▲ 图7 两个步骤来解决DSI</p> 
 <p style="text-align: left"><strong>3.1 VAE简介</strong></p> 
 <p>在说 VAE 之前，先来看一下它的前身 AutoEncoder (AE)。AE 是非常知名的自编码器，它通过自监督的训练方式，能够从原始特征获得一个潜在的特征编码，实现了自动化的特征工程，并且达到了降维和泛化的目的。它的网络结构很简单，有编码和解码两个部分组成：</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/a1/f5/zHO5fFUL_o.png" height="497" width="671"></p> 
 <p style="text-align: center">▲ 图8 AutoEncoder</p> 
 <p>从图中可以看出，AutoEncoder 被认为自监督是因为它的目标就是输入本身，不需要额外的标签。AutoEncoder 虽然是由编码器和解码器两部分构成的，但是它的重点是通过编码得到隐藏层的向量，用来表示输入的潜在的特征。</p> 
 <p>解码的作用是重建输入数据，通过优化使得重建的输入和真实的输入尽可能的类似，从而达到压缩数据的目的。尽管通过这种方式得到的隐变量的维度要远小于输入的原始数据，但是这种隐藏空间的分布并不是连续的，导致在解码的过程中，解码器很难正常的重建输入数据。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/53/aa/tJ2ulzS7_o.png" height="524" width="840"></p> 
 <p style="text-align: center">▲ 图9 Variational AutoEncoder</p> 
 <p>VAE 假设经过神经网络编码后的隐变量符合标准的高斯分布，也就是从一个高斯分布中采样得到隐变量 z，具体的做法不再通过编码器直接得到一个隐变量 z，而是得到两个向量，这两个向量作为高斯分布的参数，从这个分布中采样得到隐变量 z，然后通过解码器重建得到输入数据。</p> 
 <p>VAE 的这种假设使得抽取出来的隐藏特征直接拟合了已知的潜在概率分布，从而得到了连续完整的潜在空间，拥有了更强大的表达能力。</p> 
 <p>VAE 训练过程的 loss 分为两个部分：一个是重建项（reconstruction term），还有一个是正则项（regularization term）。第一部分的目的是让隐变量能够尽可能表示输入数据的特征，第二个部分是使得编码得到的后验分布尽可能接近先验的标准高斯分布。</p> 
 <p style="text-align: left"><strong>3.2 模型1：DSI-base</strong></p> 
 <p>在 DSI 的任务设置里，对话状态对我们来说是未知的，而对话状态又表示了当前对话中最关键的信息，因此我们将对话状态表示为隐变量 z。</p> 
 <p>和 VAE 类似，我们的模型也分为三个部分：Encoder、Sampling 和 Decoder。</p> 
 <p>Encoder 的目的是通过神经网络将对话输入编码得到一个多元高斯分布的参数  
    <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -442 603 658" style="vertical-align: -0.489ex;width: 1.364ex;height: 1.489ex;"> 
     <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
      <g> 
       <g> 
        <path d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path> 
       </g> 
      </g> 
     </g> 
    </svg> 和  
    <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -431 571 442" style="vertical-align: -0.025ex;width: 1.292ex;height: 1ex;"> 
     <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
      <g> 
       <g> 
        <path d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path> 
       </g> 
      </g> 
     </g> 
    </svg>，这个高斯分布应该能够尽量好的表示当前的对话状态并尽可能地贴近标准的多元高斯分布；Sampling 利用当前的高斯分布采样得到表示当前对话状态的隐变量 z；Decoder 的目的是通过神经网络将隐变量 z 解码为当前的对话输入。</p> 
 <p>我们以下边这句对话为例来解释一下我们的模型（蓝色字表示在预处理过程得到的 candidates）：</p> 
 <p>I need to take a train out of Chicago, I will be leaving Dallas on Wednesday.</p> 
 <p>首先介绍 encoder 部分：在 encoder 中，我们首先将对话表示为两种形式：one-hot（oh）表示和 contextualized embedding（ce）表示。</p> 
 <p>oh 是对句子的一种离散化的向量表示，oh 的长度是语料中所有候选词的数量，当前对话中出现了哪些候选词，就将其对应的位置置为 1，其余的位置置为 0，然后通过 linear transformation + SoftPlus + Dropout 将 oh 编码为 encoded_oh。</p> 
 <p>ce 是对子的一种连续性的向量表示，将当前对话经过了一个预训练好的语言模型（如 ELMo、BERT），可以得到每个单词在当前对话中的上下文表示，我们将每个候选词的上下文表示取出，先对所有的候选词做 AvgPooling 得到当前句子的整体表示，然后然后通过 linear transformation + SoftPlus + Dropout 将 ce 编码为 encoded_ce。</p> 
 <p>然后将 encoded_oh 和 encoded_ce 拼接起来，分别经过 linear transformation + BatchNorm 得到后验高斯分布的参数 posterior  
    <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -442 603 658" style="vertical-align: -0.489ex;width: 1.364ex;height: 1.489ex;"> 
     <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
      <g> 
       <g> 
        <path d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path> 
       </g> 
      </g> 
     </g> 
    </svg> 和posterior  
    <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -431 571 442" style="vertical-align: -0.025ex;width: 1.292ex;height: 1ex;"> 
     <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
      <g> 
       <g> 
        <path d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path> 
       </g> 
      </g> 
     </g> 
    </svg>。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/8f/e4/Is3arW44_o.png"></p> 
 <p style="text-align: center">▲ 图10 Encoder</p> 
 <p>接下来是 sampling 过程：在得到高斯分布的参数之后我们要通过随机采样得到隐变量 z，但是随机采样的过程是没办法求导的，所以没办法进行梯度的回传，这时候需要采用一个重参数化技巧（reparameterization trick），将 z 的采样过程转变为  
    <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -789 9252.8 1039" style="vertical-align: -0.566ex;width: 20.934ex;height: 2.351ex;"> 
     <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
      <g> 
       <g> 
        <path d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path> 
       </g> 
       <g transform="translate(825.2, 0)"> 
        <path d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path> 
       </g> 
       <g transform="translate(1825.4, 0)"> 
        <path d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path> 
       </g> 
       <g transform="translate(2618.7, 0)"> 
        <path d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path> 
       </g> 
       <g transform="translate(3340.9, 0)"> 
        <path d="M190 -22Q124 -22 76 11T27 107Q27 174 97 232L107 239L99 248Q76 273 76 304Q76 364 144 408T290 452H302Q360 452 405 421Q428 405 428 392Q428 381 417 369T391 356Q382 356 371 365T338 383T283 392Q217 392 167 368T116 308Q116 289 133 272Q142 263 145 262T157 264Q188 278 238 278H243Q308 278 308 247Q308 206 223 206Q177 206 142 219L132 212Q68 169 68 112Q68 39 201 39Q253 39 286 49T328 72T345 94T362 105Q376 103 376 88Q376 79 365 62T334 26T275 -8T190 -22Z"></path> 
       </g> 
       <g transform="translate(3806.9, 0)"> 
        <path d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path> 
       </g> 
       <g transform="translate(4251.6, 0)"> 
        <path d="M190 -22Q124 -22 76 11T27 107Q27 174 97 232L107 239L99 248Q76 273 76 304Q76 364 144 408T290 452H302Q360 452 405 421Q428 405 428 392Q428 381 417 369T391 356Q382 356 371 365T338 383T283 392Q217 392 167 368T116 308Q116 289 133 272Q142 263 145 262T157 264Q188 278 238 278H243Q308 278 308 247Q308 206 223 206Q177 206 142 219L132 212Q68 169 68 112Q68 39 201 39Q253 39 286 49T328 72T345 94T362 105Q376 103 376 88Q376 79 365 62T334 26T275 -8T190 -22Z"></path> 
       </g> 
       <g transform="translate(4995.3, 0)"> 
        <path d="M55 166Q55 241 101 304T222 367Q260 367 296 349T362 304T421 252T484 208T554 189Q616 189 655 236T694 338Q694 350 698 358T708 367Q722 367 722 334Q722 260 677 197T562 134H554Q517 134 481 152T414 196T355 248T292 293T223 311Q179 311 145 286Q109 257 96 218T80 156T69 133Q55 133 55 166Z"></path> 
       </g> 
       <g transform="translate(6051.1, 0)"> 
        <g> 
         <g> 
          <path d="M343 705Q358 705 358 698Q360 696 370 658T411 524T484 319Q536 174 590 82L595 73L615 152Q646 274 683 407Q729 571 752 637T799 727Q852 780 937 788Q939 788 947 788T958 789H962Q979 789 979 765Q979 722 951 692Q942 683 924 683Q888 681 859 672T818 654T803 639Q784 608 708 322T631 15Q631 14 630 15Q630 17 629 15Q628 14 628 12Q621 -4 601 -17T560 -31Q550 -31 546 -28T530 -7Q484 67 458 123T398 272Q352 392 314 514L306 535V534Q306 533 296 488T272 379T234 239T185 100T127 -7T61 -50Q34 -50 4 -34T-27 8Q-27 33 -12 61T18 90Q21 90 36 77T87 57H92Q109 57 123 78T162 173Q206 299 232 417T265 599T276 667Q284 681 304 693T343 705Z"></path> 
         </g> 
        </g> 
       </g> 
       <g transform="translate(7030.1, 0)"> 
        <path d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path> 
       </g> 
       <g transform="translate(7419.1, 0)"> 
        <path d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path> 
       </g> 
       <g transform="translate(7919.1, 0)"> 
        <path d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path> 
       </g> 
       <g transform="translate(8363.8, 0)"> 
        <path d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path> 
       </g> 
       <g transform="translate(8863.8, 0)"> 
        <path d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path> 
       </g> 
      </g> 
     </g> 
    </svg>，这样使得采样过程变得可导。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/4c/28/YmYF6P2i_o.png" height="185" width="799"></p> 
 <p style="text-align: center">▲ 图11 Sampling</p> 
 <p>最后是 decoder 部分：在得到隐变量 z 之后我们需要通过一个 decoder 来重新得到当前对话的表示 oh 和 ce。这时我们首先通过 transformation+Softmax 得到另一个隐变量 s，隐变量 s 表示了当前对话中 slot 的概率分布，s 的维度是预设的 slot 的数量，是一个调整的超参数。</p> 
 <p>接下来我们首先通过一个 linear transormation+BatchNorm 得到重建的 oh 表示，reconstructed_oh 的维度也是所有 candidate 的数量。</p> 
 <p>除此之外，我们还想要得到重建得到上下文表示 ce，这是我们将上下文 ce 的重建转换为重建一个能够表示上下文分布规律的多元高斯分布，也就是说对这个多元高斯分布采样能够以比较更好地得到当前的 ce。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/15/23/VaeeZAVk_o.png"></p> 
 <p style="text-align: center">▲ 图12 Decoder</p> 
 <p>我们的模型的优化过程和 VAE 类似，模型的 loss 分为两个部分：重建项和正则项，我们的重建项又分为两个部分，一部分是对 oh 表示的重建，另一部分是对 ce 表示的重建。</p> 
 <p>对 oh 的重建我们通过对 reconstructed_oh 和真实的 oh 求 cross entropy loss：</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/3f/3a/0sZNcMQ7_o.png" height="147" width="830"></p> 
 <p>对 ce 的重建我们转化为提高我们重建得到的多元高斯分布采样得到 ce 的概率，也就是说我们将高斯分布取得候选词的概率取反作为 loss，然后将所有的候选词对应的 loss 相加，这样我们优化 loss 就是不断提高重建的高斯分布能够得到候选词上下文表示的概率。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/5b/a6/Qa539ij8_o.png" height="231" width="966"></p> 
 <p>对于正则项来说，我们计算后验的高斯分布和标准的高斯分布的 KL 散度作为 loss，通过优化 loss 来不断拉近后验的高斯分布与标准高斯分布之间的距离。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/61/bd/Hzxshgz4_o.png" height="66" width="852"></p> 
 <p>模型训练完之后，我们看一下模型是如何进行预测的？通过和训练过程完全相同的 encoder 和 sampling 模块，得到隐变量 z，然后通过 linear transformation+softmax 得到隐变量 s，此时的 s 代表当前对话中不同 slot 的分布概率，但是这个概率针对每一轮的对话来说的，我们并不知道概率较高得几个 slot 分别对应哪些 candidate，那么怎么对不同的 candidate 进行区分呢？</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/97/ef/bmif5Lhz_o.png"></p> 
 <p>我们先来看一下除了 encoder，模型在 decoder 模块中学到了什么。首先从第一部分 oh 的重建过程，由 slot 的表示 s 到 oh 的 linear transformation 中可以得到一个 [slot_num, vocab_len] 的参数矩阵 W，如下图所示。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/22/a5/McvWvKBG_o.png"></p> 
 <p>那么这个参数矩阵 W 的含义是什么呢？做完 softmax 后的隐变量 s 可以代表不同的 slot 的概率，和参数矩阵相乘可以看作将是对 W 每行的参数进行加权求和，得到 reconstructed_oh，然后和实际的 oh 求交叉熵。</p> 
 <p>因为在 oh 中当前对话出现的 candidate 被置为 1，其余的 candidate 被置为 0，这些被置为 1 得 candidate 可能属于当前对话不同的 slot，而同一个 slot 中出现的 candidate 有很高的重复性（比如火车的起点是一个固定的集合）。</p> 
 <p>那么经过多次数据优化之后的结果就是隐变量 s 中当前对话中出现的 slot 对应的那些维接近 1，不出现的 slot 的那些维接近 0，s 中接近 1 的那些维映射到参数矩阵 W 的对应行中，这些 slot 中经常得 candidate 对应的那些维接近 1，其余的接近 0。</p> 
 <p>这样经过对 W 每一行的加权平均相当于取出 slot 对应的 candidate 相加得到 oh，那么 W 的每一行就可以看作某个 slot 中所有 candidate 出现的概率。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/0b/41/dfM23uiC_o.png"></p> 
 <p>这样我们可以利用参数矩阵 W 来判断每个 candidate 属于各个 slot 的概率，和我们训练过程中求交叉熵 loss 的优化方式一样，我们将 candidate 的 one-hot 表示与 W 的每一行点对点相乘，可以看作在每一个 slot 下能够得到当前 candidate 的概率。</p> 
 <p>和当前对话的 oh 表示可能存在多个 1 不同的是，每个 candidate 的 one-hot 表示只有一维是 1，和 W 的每一行点积的过程可以看作是将 W 中的对应列取出的过程，这样就得到了 candidate 得到属于每个 slot 的概率。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/5e/9c/giPyRCkt_o.png"></p> 
 <p>同样我们看一下模型在对 ce 的 decode 过程中学到了什么？我们可以在重建得到高斯分布的两个参数  
   <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -442 603 658" style="vertical-align: -0.489ex;width: 1.364ex;height: 1.489ex;"> 
    <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
     <g> 
      <g> 
       <path d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path> 
      </g> 
     </g> 
    </g> 
   </svg> 和  
   <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -431 571 442" style="vertical-align: -0.025ex;width: 1.292ex;height: 1ex;"> 
    <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
     <g> 
      <g> 
       <path d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path> 
      </g> 
     </g> 
    </g> 
   </svg> 的过程中得到两个参数矩阵，接下来我们也来分析一下这两个参数矩阵的含义。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/c4/46/XJmBfqZ8_o.png" height="589" width="1078"></p> 
 <p>类似于 oh 重建过程，得到参数  
    <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -442 603 658" style="vertical-align: -0.489ex;width: 1.364ex;height: 1.489ex;"> 
     <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
      <g> 
       <g> 
        <path d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path> 
       </g> 
      </g> 
     </g> 
    </svg> 和  
    <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -431 571 442" style="vertical-align: -0.025ex;width: 1.292ex;height: 1ex;"> 
     <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
      <g> 
       <g> 
        <path d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path> 
       </g> 
      </g> 
     </g> 
    </svg> 的过程也可以看做是对两个参数矩阵的加权求和过程： 
    <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -442 3538 658" style="vertical-align: -0.489ex;width: 8.005ex;height: 1.489ex;"> 
     <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
      <g> 
       <g> 
        <path d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path> 
       </g> 
       <g transform="translate(451, 0)"> 
        <path d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path> 
       </g> 
       <g transform="translate(917, 0)"> 
        <path d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path> 
       </g> 
       <g transform="translate(1350, 0)"> 
        <path d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path> 
       </g> 
       <g transform="translate(1835, 0)"> 
        <path d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path> 
       </g> 
       <g transform="translate(2435, 0)"> 
        <path d="M0 -62V-25H499V-62H0Z"></path> 
       </g> 
       <g transform="translate(2935, 0)"> 
        <path d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path> 
       </g> 
      </g> 
     </g> 
    </svg> 和  
    <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -442 3506 504" style="vertical-align: -0.14ex;width: 7.932ex;height: 1.14ex;"> 
     <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
      <g> 
       <g> 
        <path d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path> 
       </g> 
       <g transform="translate(451, 0)"> 
        <path d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path> 
       </g> 
       <g transform="translate(917, 0)"> 
        <path d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path> 
       </g> 
       <g transform="translate(1350, 0)"> 
        <path d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path> 
       </g> 
       <g transform="translate(1835, 0)"> 
        <path d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path> 
       </g> 
       <g transform="translate(2435, 0)"> 
        <path d="M0 -62V-25H499V-62H0Z"></path> 
       </g> 
       <g transform="translate(2935, 0)"> 
        <path d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path> 
       </g> 
      </g> 
     </g> 
    </svg> 是对当前对话整体的上下文的一个表示，而 s 表示了当前对话中每个 slot 出现概率，那么这两个参数矩阵中每一行就是每个 slot 对应的上下文的表示，多个不同的 slot 进行加权求和，得到了整体的上下文表示。</p> 
 <p>我们将两个参数矩阵中对应的每一行  
    <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -442 897 658" style="vertical-align: -0.489ex;width: 2.029ex;height: 1.489ex;"> 
     <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
      <g> 
       <g> 
        <g> 
         <path d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path> 
        </g> 
        <g transform="translate(603, -150) scale(0.707)"> 
         <path d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path> 
        </g> 
       </g> 
      </g> 
     </g> 
    </svg> 和  
    <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -431 865 588.8" style="vertical-align: -0.357ex;width: 1.957ex;height: 1.332ex;"> 
     <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
      <g> 
       <g> 
        <g> 
         <path d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path> 
        </g> 
        <g transform="translate(571, -150) scale(0.707)"> 
         <path d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path> 
        </g> 
       </g> 
      </g> 
     </g> 
    </svg> 取出来，可以作为每个 slot 对应的能够表示相应上下文的高斯分布的参数，由此得到了 slot_num 个不同的多元高斯分布。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/fa/2c/enUPwEqW_o.png"></p> 
 <p>我们利用 slot_num 个不同的高斯分布，可以计算每个高斯分布采样得到 candidate 的 ce 的概率，作为 candidate 属于不同 slot 的概率。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/76/6c/jRqZbAJW_o.png" height="428" width="649"></p> 
 <p>我们将得到的三个概率相乘，最终得到 candidate 属于每个 slot 的概率，取其中最大的作为当前 candidate 对应的 slot。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/0d/be/pI9sOavS_o.png"></p> 
 <p>因为我们采取的是无监督的方式来为每一个 candidate 预测 slot，对应于每个 candidate 我们得到的是 slot 的索引，和所有无监督的方式一样，我们需要有一个后处理的为每一个 slot 的索引分配一个合适的标记。</p> 
 <p>如图所示，我们将分配到同一个 slot 下的 candidate 聚集起来，通过多数投票的方式，观察其中大多数的 candidate 实际应该属于哪个合适的label，将这个 label 分配给当前的 slot，这样得到了每个 slot 对应 label 的一个字典，通过这个字典将预测的 slot 索引替换为实际的 label。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/3b/94/gRufNhOk_o.png"></p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/e3/8e/HkXdfBWg_o.png"></p> 
 <p style="text-align: left"><strong>3.3 模型2：DSI-GM</strong></p> 
 <p>目前任务型对话系统的构建都是建立在多领域的语料之上，其中一些领域可能有相似的 slot，比如 train 和 taxi 这两个领域可能有相似的 slot 包括 arrive by、leave at、destination 和 departure。</p> 
 <p>而相似的 slot 之间上下文可能是非常类似的，比如对于 arrive by 这个 slot 来说，不同的领域可能共享了“I need by arrive by xxx”，而对于 departure 这个 slot 来说，不同的领域可能共享了“I need departure from xxx”。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/61/5a/ybwQQJ25_o.png" height="385" width="993"></p> 
 <p>针对这种情况，我们提出了 DSI-GM 模型来解决这个问题。在 DSI-base 模型中，所有不同的领域和槽位都通过一个高斯分布来进行建模，这样不同领域的相似的 slot 因为其具有相似的上下文可能导致模型无法对这些 slot 进行区分。</p> 
 <p>在 DSI-GM 模型中，我们利用高斯混合模型（A Mixture-of-Gaussians）来对隐变量 z 进行建模，每一个单个的子模型来对一个领域进行建模，这样在避免了不同领域的相似 slot 被混合在一起的情况，可以在领域这个维度上对对话进行更加清晰地建模。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/a7/a7/nJy4etwv_o.png" height="517" width="1164"></p> 
 <p>DSI-GM 模型实际是什么样的呢？相对于 DSI-base 模型在通过 encoder 和 sampling 得到隐变量之后直接预测得到 slot 来说，DSI-GM 在得到隐变量 z 之后，通过和先验的 K 个高斯分布子模型进行对比。</p> 
 <p>首先预测出当前对话的领域，在此基础上，在利用 z 和 decoder 模块预测得到 slot，当前的领域和 slot 共同组成了当前的 slot（实际 slot 是由领域和 slot 两部分组成），DSI-GM 模型具体的训练和预测过程请详见我们的论文。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/3f/36/hujvDFPk_o.png" height="428" width="1097"></p> 
 <p style="text-align: left"><img src="https://images2.imgbox.com/49/bd/49X2vB7l_o.png"></p> 
 <p style="text-align: left"><strong>实验</strong></p> 
 <p>我们在 MultiWOZ2.1 和 SGD 两个数据集上进行了实验，我们从两个层面对我们的结果进行了评价，一个是传统的 joint level，joint level 的 dialogue state 是从对话开始到当前对话所有的信息的累积，另一个是 turn level，turn level 的 dialogue state 是仅对当前轮对话的局部信息的表示。</p> 
 <p>在这两个层面上，我们分别通过两个指标对结果进行了评价，一个是 accuracy，当对话状态中所有的 slot-value pairs 被识别对才认为是正确；另一个是 state matching（precision, recall, F1），用来评价推理得到的 slot-value pairs 的覆盖率。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/ce/b7/MBbblFjX_o.png" height="174" width="998"></p> 
 <p>从结果中可以看出，相对于随机分配的 slot，通过我们的模型可以有效地推理出对话状态，我们采用的高斯混合模型在 DSI-base 的基础上有明显的提高。但是从 accuracy 来看，我们的结果还比较低，这也反映出了无监督的 DSI 任务的难度，仍然有很大的提升空间。</p> 
 <p>除此之外，我们还将推理得到的对话状态用于下游的 act prediction 和对话生成上，下游任务的模型我们采用的是 [Chen et al., 2019] Wenhu Chen, Jianshu Chen, Pengda Qin, Xifeng Yan, and William Yang Wang. Semantically conditioned dialog response generation via hierarchical disentangled self-attention. In ACL, 2019。</p> 
 <p>从实验结果可以看出，相对于没有对话状态，通过 DSI-GM 得到的对话状态对下游任务有明显的提升，而相比于人工标注的对话状态还有一定的差距，这也体现了我们的这个任务还有较大的提升空间。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/31/5a/NB0HaLNV_o.png" height="384" width="1103"></p> 
 <p>我们分析了多个不同的领域的结果，通过将结果最好的领域 attraction 和结果最差的领域 hotel 对比，我们发现不同领域中 slot 的数量对结果有直接的影响，slot 的数量较少的领域更容易进行推理是比较显然的，但是如何对不同的领域进行建模是未来值得考虑的问题。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/6b/d7/EBilV8Ou_o.png" height="324" width="1126"></p> 
 <p>除此之外，我们还就 DSI-base 和 DSI-GM 在隐变量 z 上的表现进行了进一步的分析，我们通过 t-SNE 来对 z 进行了降维，图中每一种颜色代表一个领域，从图中可以看出，DSI-GM 在对不同领域的区分上确实取得了更好的效果，这也进一步证明了我们的高斯混合模型的作用。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/bd/b4/SpX9EUJC_o.png" height="341" width="829"></p> 
 <p style="text-align: left"><img src="https://images2.imgbox.com/b8/d9/FE5syflp_o.png"></p> 
 <p style="text-align: left"><strong>结论</strong></p> 
 <p>我们提出了一个新的任务：<strong>对话状态推理（dialogue state induction，DSI），</strong>目标是从无标注的对话记录中自动推理得到对话状态。</p> 
 <p>我们提出了两个<strong>基于神经隐变量的模型通过无监督</strong>的方式来对对话状态进行自动推理。</p> 
 <p>我们提出的任务针对了当前任务型对话研究中面临的非常实际的标注困难的问题，我们的提出的任务是一个比较有前景同时很有挑战性的任务。</p> 
 <p>一方面，我们在 IJCAI 的 review 也说到了：<strong>“This problem is important and interesting, this area should attract more attention. This work has great potential of motivating follow-up research.”</strong>，证明我们的这个工作的意义，是有利于促进后续的研究工作的。</p> 
 <p>另一方面，由对话状态本身的复杂性到无监督的任务设定使得这个任务还是有非常大的提升空间，我们提出的两个基于神经隐变量的模型也是抛砖引玉，提供了一种解决这个任务的思路，大家可以以我们的模型作为 baseline，提出更多的方法来解决这个任务。</p> 
 <p><strong>更多阅读</strong></p> 
 <p style="text-align: center"><a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw%3D%3D&amp;chksm=96ea77d6a19dfec0e8b98ad0d4db689bf619b5d61acedac59193f6c65b1b79a90b6528d112a7&amp;idx=1&amp;lang=zh_CN&amp;mid=2247510102&amp;scene=21&amp;sn=64c3b416a06ff1a0ab9cd992bbda0ee6&amp;token=1669762347#wechat_redirect" rel="nofollow"><img src="https://images2.imgbox.com/a7/72/uEwtdaQk_o.png"></a></p> 
 <p style="text-align: center"><a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw%3D%3D&amp;chksm=96ea7786a19dfe909de4c0e3e07112add69720d1c0a3f7a4ab65018846624e7a013ca45ecd4f&amp;idx=1&amp;lang=zh_CN&amp;mid=2247510022&amp;scene=21&amp;sn=52caeb3ab93aebb81a0eda49104e8ea1&amp;token=1669762347#wechat_redirect" rel="nofollow"><img src="https://images2.imgbox.com/44/27/kSWdeQXM_o.png"></a></p> 
 <p style="text-align: center"><a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw%3D%3D&amp;chksm=96ea784ea19df158d6bb52820ce281eb46fdbfef481058438e080b04096324fedfac1719ed38&amp;idx=2&amp;lang=zh_CN&amp;mid=2247509966&amp;scene=21&amp;sn=4f7a4172069993580bfbee06fe0139b0&amp;token=1669762347#wechat_redirect" rel="nofollow"><img src="https://images2.imgbox.com/fa/7f/xBnnxkd2_o.png"></a></p> 
 <p><img src="https://images2.imgbox.com/a1/97/RkJvZvOH_o.png"></p> 
 <p><strong>#投 稿 通 道#</strong></p> 
 <p><strong> 让你的论文被更多人看到 </strong></p> 
 <p>如何才能让更多的优质内容以更短路径到达读者群体，缩短读者寻找优质内容的成本呢？<strong>答案就是：你不认识的人。</strong></p> 
 <p>总有一些你不认识的人，知道你想知道的东西。PaperWeekly 或许可以成为一座桥梁，促使不同背景、不同方向的学者和学术灵感相互碰撞，迸发出更多的可能性。 </p> 
 <p>PaperWeekly 鼓励高校实验室或个人，在我们的平台上分享各类优质内容，可以是<strong>最新论文解读</strong>，也可以是<strong>学习心得</strong>或<strong>技术干货</strong>。我们的目的只有一个，让知识真正流动起来。</p> 
 <p>???? <strong>来稿标准：</strong></p> 
 <p>• 稿件确系个人<strong>原创作品</strong>，来稿需注明作者个人信息（姓名+学校/工作单位+学历/职位+研究方向） </p> 
 <p>• 如果文章并非首发，请在投稿时提醒并附上所有已发布链接 </p> 
 <p>• PaperWeekly 默认每篇文章都是首发，均会添加“原创”标志</p> 
 <p>???? <strong>投稿邮箱：</strong></p> 
 <p>• 投稿邮箱：hr@paperweekly.site </p> 
 <p>• 所有文章配图，请单独在附件中发送 </p> 
 <p>• 请留下即时联系方式（微信或手机），以便我们在编辑发布时和作者沟通</p> 
 <p style="text-align: center">????</p> 
 <p style="text-align: center">现在，在<strong>「知乎」</strong>也能找到我们了</p> 
 <p style="text-align: center">进入知乎首页搜索<strong>「PaperWeekly」</strong></p> 
 <p style="text-align: center">点击<strong>「关注」</strong>订阅我们的专栏吧</p> 
 <p style="text-align: left"><strong>关于PaperWeekly</strong><br></p> 
 <p style="text-align: left">PaperWeekly 是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。如果你研究或从事 AI 领域，欢迎在公众号后台点击<strong>「交流群」</strong>，小助手将把你带入 PaperWeekly 的交流群里。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/d3/da/V1U7G5zq_o.png"></p> 
 <p style="text-align: right"><img src="https://images2.imgbox.com/61/d7/yneoh5oS_o.png"></p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4d9d6c4a79109e76e4831ba3312cb405/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Hbase（二）Client客户端</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e40a9e43a9542c12ee69c62654446206/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Java集合--HashMap</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>