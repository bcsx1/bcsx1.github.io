<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>（2023|CVPR，中文，扩散，知识增强，去噪专家组合）ERNIE-ViLG 2.0：利用知识增强的去噪专家组合改进文本到图像扩散模型 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="（2023|CVPR，中文，扩散，知识增强，去噪专家组合）ERNIE-ViLG 2.0：利用知识增强的去噪专家组合改进文本到图像扩散模型" />
<meta property="og:description" content="ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts
公众号：EDPJ（添加 VX：CV_EDPJ 或直接进 Q 交流群：922230617 获取资料）
目录
0. 摘要
1. 简介
2. 方法 2.1. 基础 2.2. 知识增强扩散模型 2.3. 去噪专家混合 3. 实验 3.1 实现细节
3.2 结果
3.3 分析 4. 相关工作
5. 风险、限制和未来工作 6. 结论
参考
附录
A. 训练细节
B. 详细的自动评估
D. 详细消融研究 D.1 知识增强消融 D.2 混合去噪专家消融 D.3 图像质量比较 S. 总结
S.1 主要贡献
S.2 架构和方法 0. 摘要 扩散模型的最新进展彻底改变了文本到图像生成的流行技术。 虽然现有的方法可以在文本条件下生成逼真的高分辨率图像，但仍有几个悬而未决的问题需要解决，这限制了图像保真度和文本相关性的进一步提高。 在本文中，我们提出了 ERNIE-ViLG 2.0，一种大规模中文文本到图像的扩散模型，通过以下方式逐步提升生成图像的质量：（1）将关键元素的细粒度文本和视觉知识融入到图像中 （2）在不同的去噪阶段使用不同的去噪专家。 通过所提出的机制，并在双语提示集 ViLG-300 上进行并排人工评估，ERNIE-ViLG 2." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/81cb3fceb6c488087acb1f8015bb11dd/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-13T16:30:47+08:00" />
<meta property="article:modified_time" content="2023-10-13T16:30:47+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">（2023|CVPR，中文，扩散，知识增强，去噪专家组合）ERNIE-ViLG 2.0：利用知识增强的去噪专家组合改进文本到图像扩散模型</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p style="text-align:center;"><strong>ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts</strong></p> 
<p style="text-align:center;"><strong>公众号：EDPJ（添加 VX：CV_EDPJ 或直接进 Q 交流群：922230617 获取资料）</strong></p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="0.%20%E6%91%98%E8%A6%81-toc" style="margin-left:0px;"><a href="#0.%20%E6%91%98%E8%A6%81" rel="nofollow">0. 摘要</a></p> 
<p id="1.%20%E7%AE%80%E4%BB%8B-toc" style="margin-left:0px;"><a href="#1.%20%E7%AE%80%E4%BB%8B" rel="nofollow">1. 简介</a></p> 
<p id="2.%20%E6%96%B9%E6%B3%95%C2%A0-toc" style="margin-left:0px;"><a href="#2.%20%E6%96%B9%E6%B3%95%C2%A0" rel="nofollow">2. 方法 </a></p> 
<p id="2.1.%20%E5%9F%BA%E7%A1%80%C2%A0-toc" style="margin-left:40px;"><a href="#2.1.%20%E5%9F%BA%E7%A1%80%C2%A0" rel="nofollow">2.1. 基础 </a></p> 
<p id="2.2.%20%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%C2%A0-toc" style="margin-left:40px;"><a href="#2.2.%20%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%C2%A0" rel="nofollow">2.2. 知识增强扩散模型 </a></p> 
<p id="2.3.%20%E5%8E%BB%E5%99%AA%E4%B8%93%E5%AE%B6%E6%B7%B7%E5%90%88%C2%A0-toc" style="margin-left:40px;"><a href="#2.3.%20%E5%8E%BB%E5%99%AA%E4%B8%93%E5%AE%B6%E6%B7%B7%E5%90%88%C2%A0" rel="nofollow">2.3. 去噪专家混合 </a></p> 
<p id="3.%20%E5%AE%9E%E9%AA%8C%C2%A0-toc" style="margin-left:0px;"><a href="#3.%20%E5%AE%9E%E9%AA%8C%C2%A0" rel="nofollow">3. 实验 </a></p> 
<p id="3.1%20%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82-toc" style="margin-left:40px;"><a href="#3.1%20%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82" rel="nofollow">3.1 实现细节</a></p> 
<p id="3.2%20%E7%BB%93%E6%9E%9C-toc" style="margin-left:40px;"><a href="#3.2%20%E7%BB%93%E6%9E%9C" rel="nofollow">3.2 结果</a></p> 
<p id="3.3%20%E5%88%86%E6%9E%90%C2%A0-toc" style="margin-left:40px;"><a href="#3.3%20%E5%88%86%E6%9E%90%C2%A0" rel="nofollow">3.3 分析 </a></p> 
<p id="4.%20%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C-toc" style="margin-left:0px;"><a href="#4.%20%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C" rel="nofollow">4. 相关工作</a></p> 
<p id="5.%20%E9%A3%8E%E9%99%A9%E3%80%81%E9%99%90%E5%88%B6%E5%92%8C%E6%9C%AA%E6%9D%A5%E5%B7%A5%E4%BD%9C%C2%A0-toc" style="margin-left:0px;"><a href="#5.%20%E9%A3%8E%E9%99%A9%E3%80%81%E9%99%90%E5%88%B6%E5%92%8C%E6%9C%AA%E6%9D%A5%E5%B7%A5%E4%BD%9C%C2%A0" rel="nofollow">5. 风险、限制和未来工作 </a></p> 
<p id="6.%20%E7%BB%93%E8%AE%BA-toc" style="margin-left:0px;"><a href="#6.%20%E7%BB%93%E8%AE%BA" rel="nofollow">6. 结论</a></p> 
<p id="%E5%8F%82%E8%80%83-toc" style="margin-left:0px;"><a href="#%E5%8F%82%E8%80%83" rel="nofollow">参考</a></p> 
<p id="%E9%99%84%E5%BD%95-toc" style="margin-left:0px;"><a href="#%E9%99%84%E5%BD%95" rel="nofollow">附录</a></p> 
<p id="A.%20%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82-toc" style="margin-left:0px;"><a href="#A.%20%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82" rel="nofollow">A. 训练细节</a></p> 
<p id="B.%20%E8%AF%A6%E7%BB%86%E7%9A%84%E8%87%AA%E5%8A%A8%E8%AF%84%E4%BC%B0-toc" style="margin-left:0px;"><a href="#B.%20%E8%AF%A6%E7%BB%86%E7%9A%84%E8%87%AA%E5%8A%A8%E8%AF%84%E4%BC%B0" rel="nofollow">B. 详细的自动评估</a></p> 
<p id="D.%20%E8%AF%A6%E7%BB%86%E6%B6%88%E8%9E%8D%E7%A0%94%E7%A9%B6%C2%A0-toc" style="margin-left:0px;"><a href="#D.%20%E8%AF%A6%E7%BB%86%E6%B6%88%E8%9E%8D%E7%A0%94%E7%A9%B6%C2%A0" rel="nofollow">D. 详细消融研究 </a></p> 
<p id="D.1%20%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA%E6%B6%88%E8%9E%8D%C2%A0-toc" style="margin-left:40px;"><a href="#D.1%20%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA%E6%B6%88%E8%9E%8D%C2%A0" rel="nofollow">D.1 知识增强消融 </a></p> 
<p id="D.2.%20%E6%B7%B7%E5%90%88%E5%8E%BB%E5%99%AA%E4%B8%93%E5%AE%B6%E6%B6%88%E8%9E%8D%C2%A0-toc" style="margin-left:40px;"><a href="#D.2.%20%E6%B7%B7%E5%90%88%E5%8E%BB%E5%99%AA%E4%B8%93%E5%AE%B6%E6%B6%88%E8%9E%8D%C2%A0" rel="nofollow">D.2 混合去噪专家消融 </a></p> 
<p id="D.3%20%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E6%AF%94%E8%BE%83%C2%A0-toc" style="margin-left:40px;"><a href="#D.3%20%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E6%AF%94%E8%BE%83%C2%A0" rel="nofollow">D.3 图像质量比较 </a></p> 
<p id="S.%20%E6%80%BB%E7%BB%93-toc" style="margin-left:0px;"><a href="#S.%20%E6%80%BB%E7%BB%93" rel="nofollow">S. 总结</a></p> 
<p id="S.1%C2%A0%20%E4%B8%BB%E8%A6%81%E8%B4%A1%E7%8C%AE-toc" style="margin-left:40px;"><a href="#S.1%C2%A0%20%E4%B8%BB%E8%A6%81%E8%B4%A1%E7%8C%AE" rel="nofollow">S.1 主要贡献</a></p> 
<p id="S.2%20%E6%9E%B6%E6%9E%84%E5%92%8C%E6%96%B9%E6%B3%95%C2%A0-toc" style="margin-left:40px;"><a href="#S.2%20%E6%9E%B6%E6%9E%84%E5%92%8C%E6%96%B9%E6%B3%95%C2%A0" rel="nofollow">S.2 架构和方法 </a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="0.%20%E6%91%98%E8%A6%81" style="background-color:transparent;">0. 摘要</h2> 
<p>扩散模型的最新进展彻底改变了文本到图像生成的流行技术。 虽然现有的方法可以在文本条件下生成逼真的高分辨率图像，但仍有几个悬而未决的问题需要解决，这限制了图像保真度和文本相关性的进一步提高。 在本文中，我们提出了 <strong>ERNIE-ViLG 2.0</strong>，一种大规模中文文本到图像的扩散模型，通过以下方式逐步提升生成图像的质量：（1）将关键元素的细粒度文本和视觉知识融入到图像中 （2）在不同的去噪阶段使用不同的去噪专家。 通过所提出的机制，并在双语提示集 ViLG-300 上进行并排人工评估，ERNIE-ViLG 2.0（https://wenxin.baidu.com/ernie-vilg）不仅在 MS-COCO 上达到了新的最先进水平，零样本 FID-30k 得分为 6.75，而且在图像保真度以及图像-文本对齐方面显着优于最新模型。</p> 
<h2 id="1.%20%E7%AE%80%E4%BB%8B">1. 简介</h2> 
<p>近年来，在文本到图像生成领域取得了令人难以置信的进展。借助大规模的训练数据和模型参数，各种文本到图像生成模型现在能够生动地描绘文本提示所描述的视觉场景，使任何人都能够创建精美的图像，而无需精湛的绘画技巧。在所有类型的图像生成方法中，扩散模型 [9] 因其能够根据文本提示生成高度逼真的图像而引起越来越多的关注。在给定文本提示的情况下，这些模型通过迭代去噪步骤将高斯噪声转化为与提示一致的图像。在过去的几年里，诸如 LDM [25]、GLIDE [18]、DALL-E 2 [22] 和 Imagen [26] 等文本到图像扩散模型在文本相关性和图像保真度方面取得了令人印象深刻的性能。尽管取得了这些进展，现有方法对扩散模型的探索仍处于初级阶段。当我们深入研究文本到图像扩散模型的原理和实施时，仍然存在许多机会可以进一步提高生成图像的质量。</p> 
<p>首先，在每个去噪步骤的学习过程中，所有文本标记与图像区域进行互动，所有图像区域对最终损失函数的贡献是均等的。然而，文本和图像的视觉场景包含许多元素（即文本词语和视觉对象），不同的元素通常对表达场景语义具有不同的重要性 [42]。不加区分的学习过程可能导致模型忽略了场景中的一些关键元素和相互作用，因此面临文本图像不一致的风险，如属性混淆问题，尤其是对包含具体属性的多个对象的文本提示 [22]。其次，在将视野从单个步骤扩展到整个去噪过程时，我们可以发现不同去噪阶段的要求也不相同。在早期阶段，输入图像具有较高的噪声，模型需要在几乎纯噪声的基础上勾勒出语义布局和骨架。相比之下，在接近图像输出的后续步骤中，去噪主要意味着在几乎完成的图像基础上改进细节 [25]。在实践中，现有模型通常在所有步骤中使用相同的 U-Net，这意味着同一组参数必须学习不同的去噪能力。</p> 
<p>在本文中，我们提出了 <strong>ERNIE-ViLG 2.0</strong>，这是一种改进的文本到图像扩散模型，采用了<strong>知识增强的去噪专家混合</strong>方法，以融合有关视觉场景的额外知识并分离不同步骤中的去噪能力。具体来说，我们使用文本解析器和目标检测器来提取输入文本-图像对中场景的关键元素，然后在学习过程中引导模型更多地关注它们的对齐，希望模型能处理各种对象和属性之间的关系。此外，我们将去噪步骤划分为几个阶段，并为每个阶段使用特定的去噪 “专家”。通过多个专家的混合，模型可以涵盖更多的参数并更好地学习每个去噪阶段的数据分布，而不会增加推理时间，因为在每个去噪步骤中只激活一个专家。</p> 
<p>通过来自视觉场景的额外知识和去噪专家混合机制，我们训练了 ERNIE-ViLG 2.0 并将模型规模扩大到 240 亿个参数。在 MS-COCO 数据集上的实验表明，我们的模型在无预训练情况下的 FID-30k 得分达到 6.75，创下了先前文本到图像模型的最新纪录，且详细的消融研究证实了每种提出策略的贡献。除了自动度量指标外，我们还收集了 300 个双语文本提示，以从不同角度评估生成图像的质量，并实现英文和中文文本到图像模型的公平比较。人工评估结果再次表明，ERNIE-ViLG 2.0 在图像文本对齐和图像保真度方面明显优于其他最近的方法，包括 DALL-E 2 [22] 和 Stable Diffusion [25]。 </p> 
<p>总结一下，本文的主要贡献如下：</p> 
<ul><li>(1) 我们将文本和视觉知识融入文本到图像扩散模型，有效提高了对细粒度语义控制的能力，并减轻了生成图像中对象与属性不匹配的问题。</li><li>(2) 我们提出了去噪专家混合机制，以改进去噪过程，该机制可以适应不同去噪步骤的特性，并将模型规模扩大到 240 亿个参数，使其成为目前最大的文本到图像模型。</li><li>(3) ERNIE-ViLG 2.0 在 MS-COCO 上实现了 zero-shot FID-30k 得分的最新纪录，达到了 6.75 分，超越了 DALL-E 2 和 Stable Diffusion 在中英双语提示集 ViLG-300 的人工评估中。 </li></ul> 
<h2 id="2.%20%E6%96%B9%E6%B3%95%C2%A0">2. 方法 </h2> 
<p>在训练过程中，文本到图像扩散模型接收成对的输入（x，y），包括图像 x 和其文本描述 y，最终目标是基于 y 生成 x。为了实现这一目标，首先通过文本编码器 f_θ(·) 将 y 编码为 f_θ(y)，然后，以 f_θ(y) 为条件的去噪网络 ε_θ(·) 学习从高斯噪声生成 x。 为了帮助模型生成与文本描述（即文本提示）准确匹配的高质量图像，ERNIE-ViLG 2.0 使用场景中关键元素的文本和视觉知识增强了文本编码器 f_θ(·) 和去噪网络 ε_θ(·)。此外，ERNIE-ViLG 2.0 使用去噪专家的混合以精细化图像生成过程，不同的专家负责去噪过程中的不同生成步骤。ERNIE-ViLG 2.0 的整体架构如图 2 所示，具体细节在以下子节中描述。</p> 
<p><img alt="" height="524" src="https://images2.imgbox.com/40/de/5uI8Ib8D_o.png" width="1200"></p> 
<h3 id="2.1.%20%E5%9F%BA%E7%A1%80%C2%A0">2.1. 基础 </h3> 
<p>去噪扩散概率模型（Denoising diffusion probabilistic models，DDPM）是一类最近在文本到图像生成领域表现出出色才能的基于得分的生成模型 [9]。DDPM 的扩散过程旨在将对初始数据样本 x 进行 T 步迭代，每一步都会添加对角高斯噪声，最终将其转化为各向同性高斯分布： </p> 
<p class="img-center"><img alt="" height="26" src="https://images2.imgbox.com/34/fe/xPF2sVGH_o.png" width="476"></p> 
<p>在上述公式中，序列 {x_t} 从 x_0 = x 开始，到 x_T ~ N(0，I) 结束，每一步添加的噪声 ε_t ~ N(0, I)，{α_t}_(1...T) 是一个预定义的时间表 [30, 32]。去噪过程是扩散过程的逆过程，通过迭代去噪步骤 t = T,...,1，将高斯噪声 x_T ~ N(0, I) 转换回数据分布 x_0。在训练期间，对于给定的图像 x，模型通过抽样高斯噪声 ε ~ N(0, I) 来计算 x_t： </p> 
<p class="img-center"><img alt="" height="28" src="https://images2.imgbox.com/6c/f4/WppKEU8R_o.png" width="386"></p> 
<p class="img-center"><img alt="" height="35" src="https://images2.imgbox.com/3a/4f/6BRQ1XFJ_o.png" width="136"></p> 
<p>给定 x_t，去噪网络的目标是通过预测噪声 ε 来恢复 x_0。它是通过如下损失函数学习： </p> 
<p class="img-center"><img alt="" height="31" src="https://images2.imgbox.com/8f/9e/s6jus5sI_o.png" width="437"></p> 
<p>通过预测的 ε_θ(x_t, t)，我们可以通过转化方程(2) 来得到在步骤 t 的 x_0 预测： </p> 
<p class="img-center"><img alt="" height="53" src="https://images2.imgbox.com/be/94/gyZBpLLS_o.png" width="435"></p> 
<p>在图 2 中，我们可视化了在训练期间多个时间步骤内采样的 x_t 和预测的 ^x_(0,t)。在 DDPM 的推断过程中，x_0 是未知的，因此模型基于 x_t 和 ^x(0,t) 迭代生成 x_(t-1)：</p> 
<p class="img-center"><img alt="" height="109" src="https://images2.imgbox.com/df/77/LnSrEXoU_o.png" width="519"></p> 
<p>其中，ε'_t ~ N(0, I) 是采样的高斯噪声分布。 </p> 
<p>去噪网络 ε_θ(·) 通常是通过 U-Net [9] 实现的。为了让 ε_θ(·) 能够根据文本提示进行条件化，首先使用文本编码器 f_θ(·) 提取文本表示 f_θ(y) ∈ R^(n_y * d_y)，然后通过跨模态注意力层 [18] 将其馈送到 ε_θ(·) 中。形式上，U-Net 的表示 φ_i ∈ R^(n_x * d) 在投影后与文本表示 f_θ(y) 连接，然后通过一个注意力层实现跨模态交互。 </p> 
<p class="img-center"><img alt="" height="181" src="https://images2.imgbox.com/48/bb/6v3AVXPV_o.png" width="467"></p> 
<p>这里，i 是 U-Net 层的索引，[;] 是连接运算符，W^(i)_Q、W^(i)_(K_x)、W^(i)_(V_x) ∈ R^(d*d) 和 W^(i)_(K_y)、W^(i)_(V_y) ∈ R^(d_y * d) 是可学习的投影层，n_x 和 n_y 分别是编码的图像和文本的长度。 </p> 
<p>在推断过程中，给定文本提示 y，去噪 U-Net ε_θ(·) 使用无分类器引导 [10] 和去噪扩散隐式模型 (Denoising Diffusion Implicit Models，DDIM) 采样 [31] 来预测在文本 y 的条件下的图像样本 x。 </p> 
<h3 id="2.2.%20%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%C2%A0" style="background-color:transparent;">2.2. 知识增强扩散模型 </h3> 
<p>文本到图像模型接收描述图像场景的文本提示，然后以关键对象和相应属性细节来描绘它。换句话说，文本和图像都旨在展示一个视觉场景，在这个场景中，关键元素具有不同的表达方式，比如文本中的关键词或图像中的显著区域。然而，简单的扩散模型不能区分元素的重要性，并且无差别地迭代去噪过程。 ERNIE-ViLG 2.0 将额外的文本和视觉知识纳入学习阶段，希望增强扩散模型对细粒度语义的感知能力。</p> 
<p><strong>文本知识</strong>。理想的文本到图像模型应该专注于文本提示中提到的所有关键语义。为了区分功能词和描述关键语义的词，我们采用了一个现成的词性标注工具包来提取输入提示的词汇知识，然后通过以下方式改进学习过程：(1) 在输入序列中插入特殊标记，(2) 增加具有特定词性标签（tag）的标记在注意力层中的权重。具体来说，我们选择了 50% 的样本，在每个词的开头插入特殊标记，其中每个词性标签对应一个特殊标记。对于选择的样本，我们还根据词汇分析的结果增强关键词的注意力权重。这样，方程 (7) 被修改为：</p> 
<p class="img-center"><img alt="" height="56" src="https://images2.imgbox.com/fc/3f/BN1MqlAW_o.png" width="511"></p> 
<p>其中，</p> 
<p class="img-center"><img alt="" height="30" src="https://images2.imgbox.com/97/1d/VdGzS6oT_o.png" width="187"></p> 
<p>是一个辅助权重矩阵，用于缩放原始的注意力，而且 </p> 
<p class="img-center"><img alt="" height="51" src="https://images2.imgbox.com/ba/54/Y2XukT30_o.png" width="493"></p> 
<p>这里，w^ij_a 是 token tok_i 和 tok_j 之间的注意力权重的缩放因子，w_a 是一个超参数，x 指的是所有图像的标记，而 y_key 表示文本中的关键词（关键词被定义为现代汉语中的虚词（notional words），即名词、动词、形容词、数词、量词和代词）。图 2 给出了一个示例，其中特殊标记 "[a]" 和 "[n]" 分别用于形容词和名词。 </p> 
<p><strong>视觉知识</strong>。与文本提示中的虚词类似，图像中也有显著的区域，例如人、树木、建筑和输入中提到的对象。为了提取这种视觉知识，我们对 50% 的训练样本应用了一个目标检测器 [1]，然后通过启发式策略从结果中选择引人注目的对象。 由于扩散模型的损失函数直接作用于图像空间，我们可以通过修改等式 (3) 来为相应区域分配更高的权重，从而促使模型关注这些对象的生成：</p> 
<p class="img-center"><img alt="" height="30" src="https://images2.imgbox.com/48/90/FQsmPu8O_o.png" width="459"></p> 
<p class="img-center"><img alt="" height="53" src="https://images2.imgbox.com/1d/3c/YNw3eMNO_o.png" width="434"></p> 
<p>这里，W_l 是权重矩阵，n_h 和 n_w 分别是图像空间的高度和宽度，w_l 是一个超参数，los_ij 是图像空间中第 i 行和第 j 列的损失项，x_key 是对应于关键对象的区域。正如图 2 所示，计算 L' 时，“狗” 和 “猫” 的区域被分配了更大的权重。 </p> 
<p>现在出现了一个新的问题：作为一种细粒度的知识，所选对象可能不出现在文本提示中，因此会困扰模型在学习单词和对象之间的对齐。一个直观的想法是首先获取每个区域的对象和属性类别，然后将相应的类别标签与原始文本提示结合起来，以实现细粒度描述，从而确保输入包含粗粒度和细粒度信息。例如，如图 2 所示，检测到的对象 "碗" 没有包含在标题中，因此我们将其附加到原始描述中。此外，我们还使用图像标题模型 [38] 为图像生成文本，并随机替换原始提示为生成的标题，因为许多图像的生成标题比原始提示更简洁，揭示了更准确的语义。 </p> 
<p>值得注意的是，上述策略仅限于训练阶段。通过随机选择一部分样本来应用这些额外的增强策略，模型应该能够从各种角度感知知识的提示，并在推断阶段为给定文本生成更高质量的图像，即使没有特殊标记、注意力强化或文本精炼。 </p> 
<p></p> 
<h3 id="2.3.%20%E5%8E%BB%E5%99%AA%E4%B8%93%E5%AE%B6%E6%B7%B7%E5%90%88%C2%A0">2.3. 去噪专家混合 </h3> 
<p><img alt="" height="524" src="https://images2.imgbox.com/82/f9/GG8vUjHr_o.png" width="1200"></p> 
<p>回想一下，扩散过程是通过一系列扩散步骤 t = 1，...，T 来迭代地用高斯噪声损坏图像，而 DDPM [9] 被训练为通过去噪步骤 t = T，...，1 来还原扩散过程。在去噪过程中，所有步骤的目标都是去噪噪声输入，且它们一起将完全随机的噪声逐渐转化为有意义的图像。尽管它们共享相同的目标，但根据输入的噪声比例，这些去噪步骤的难度有所不同。图 2 通过展示训练中的一些 x_t 和去噪预测 ^x_(0,t) 的示例来说明这种差异。对于接近 T 的时间步 t，例如图中的 t = T、t_k、t_j，去噪网络的输入 x_t 受到严重噪声的影响，这些步骤的网络主要处理生成任务，即从噪声生成图像。相反，对于接近 1 的时间步 t，例如 t = t_i、t_1，输入 x_t 接近原始图像，这些步骤的网络需要精练图像细节。 </p> 
<p>DDPM 对去噪网络的实现没有特定的假设，也就是说，理论上去噪过程不需要相同的去噪网络来处理所有步骤。然而，大多数以前的文本到图像扩散方法 [18,22,25,26] 遵循通用实现方式，为整个去噪过程采用单个去噪网络。考虑到不同时间步骤的任务不同，我们推测使用相同的参数集来学习不同任务可能会导致性能不佳。 </p> 
<p>基于这一观点，我们进一步提出了 "Mixture-of-Denoising-Experts" (MoDE) 模型，其主要动机是利用多个专门的专家网络来适应不同时间步骤的不同任务。由于相邻时间步骤的输入相似，去噪任务也相似，我们将所有时间步骤均匀地分成 n 个块 (S_1， ...，S_i， ... ，S_n)，每个块包含连续的时间步骤，分配给一个去噪专家。换句话说，同一块内的时间步骤由相同组的网络参数去噪。在实践中，我们为所有去噪专家共享相同的文本编码器，同时为不同的时间步骤块使用不同的 U-Net 专家网络： </p> 
<p class="img-center"><img alt="" height="29" src="https://images2.imgbox.com/ee/4d/LwliXaIJ_o.png" width="425"></p> 
<p>其中，ε_(θ,i)(x_t, t) 是第 i 个专家网络。MoDE 通过采用专家网络来专门处理不同的去噪阶段，从而提高了模型的性能。 </p> 
<p>直观地说，当使用更多专家时，每个块包含更少的时间步骤，因此每个专家可以更好地专注于学习分配给它的特定去噪步骤的特征。同时，由于每个步骤只激活一个专家网络，增加专家的数量不会影响推断时的计算开销。因此，ERNIE-ViLG 2.0 可以灵活扩展扩散模型的参数，使专家能够更好地拟合数据分布，而不增加推断时间。 </p> 
<h2 id="3.%20%E5%AE%9E%E9%AA%8C%C2%A0">3. 实验 </h2> 
<p>在本节中，首先介绍 ERNIE-ViLG 2.0 的实现细节。然后，我们使用自动度量和人工评估来比较模型。最后，我们通过定量消融研究和定性案例来进一步分析结果。 </p> 
<h3 id="3.1%20%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82">3.1 实现细节</h3> 
<p>为了降低学习复杂性，我们使用扩散模型生成图像的表示，这些图像表示位于图像自编码器（遵循 “潜在扩散模型” [25]）的潜在空间中。首先，预训练一个图像编码器，将图像 x ∈ R^(n_h*n_w*3) 从像素空间转换到潜在空间</p> 
<p class="img-center"><img alt="" height="23" src="https://images2.imgbox.com/3a/30/GfKwDBpU_o.png" width="45"></p> 
<p class="img-center"><img alt="" height="27" src="https://images2.imgbox.com/0e/2c/ZO7rxIZD_o.png" width="103"></p> 
<p>并训练一个图像解码器将其转换回来。这里，</p> 
<p class="img-center"><img alt="" height="30" src="https://images2.imgbox.com/ad/de/np4xC8ZB_o.png" width="175"></p> 
<p>分别表示图像的原始/潜在高度和宽度，我们在本文将像素空间和隐藏空间统称为图像空间。然后，我们固定自编码器，训练扩散模型从文本提示 y 生成 ^x。在推断时，我们采用预训练的图像解码器将 ^x 转换为像素级图像输出。 </p> 
<p>ERNIE-ViLG 2.0 包含一个基于 Transformer 的文本编码器，拥有 13 亿参数，以及 10 个去噪 U-Net 专家，每个专家拥有 22 亿参数，总共大约有 24 亿参数。对于整合知识的超参数，注意力权重尺度 w_a 设置为 0.01，损失权重尺度 w_l 设置为0.1（均从 [0.01, 0.1, 0.5, 1] 中选择）。对于MoDE 策略，所有时间步被分成 10 个块。模型使用 AdamW [15] 进行优化，学习率固定为0.0009， β1 = 0.9， β2 = 0.999，权重衰减为 0.01。ERNIE-ViLG 2.0 在 320 块 Tesla A100 GPU 上训练了 18 天。更多的训练细节请参考附录 A。 </p> 
<p>训练数据包括 1.7 亿图像-文本对，包括公开可用的英语数据集，如 LAION [28]，以及一系列内部的中文数据集。图像自编码器也是在相同的数据集上进行训练的。对于带有英语标题的图像，我们使用百度翻译 API 将它们翻译成中文版本。 </p> 
<h3 id="3.2%20%E7%BB%93%E6%9E%9C">3.2 结果</h3> 
<p><strong>在 MS-COCO 上的自动评估</strong>。与之前的工作 [22, 25, 26] 一样，我们在 MS-COCO 的 256*256 图像上进行了零样本 FID-30k 评估，其中从验证集中随机选择了 3 万张图像，然后将英语标题自动翻译成中文。 </p> 
<p class="img-center"><img alt="" height="490" src="https://images2.imgbox.com/a2/de/qZklYu2Q_o.png" width="548"></p> 
<p>表 1 显示，ERNIE-ViLG 2.0 在文本到图像生成方面取得了新的业界最佳表现，在 MS-COCO 上实现了 6.75 的零样本 FID-30k。受到 DALLE [23] 和 Parti [43] 的启发，我们根据由预训练的 CLIP 模型 [21] 计算的图像文本对齐分数对批量采样的图像进行了重新排名（每个文本提示只有 4 张图像，与 DALL-E 使用的 512 张图像和 Parti 使用的 16 张图像相比）。在 CLIP 模型中，文本是 MS-COCO 中的初始英语标题，而图像是从自动翻译的中文标题生成的。此外，即使没有重新排名策略，我们发现 ERNIE-ViLG 2.0 也能够击败最新的基于扩散的模型（如 DALL-E2 [22] 和 Imagen [26]），zreo-shot FID-30k 为 7.23。关于模型大小和重新排名策略的详细比较，请参阅附录B。 </p> 
<p><strong>在 ViLG-300 上进行人工评估</strong>。ERNIE-ViLG 2.0 以中文提示作为输入，生成高分辨率的图像，这与最近的以英语为主的文本到图像模型不同。在此，我们引入了 ViLG-300（https://github.com/PaddlePaddle/ERNIE/tree/ernie-kit-open-v1.0/Research/ERNIE-ViLG2/data/ViLG-300.csv），这是一个支持中文和英文文本到图像模型系统评估和比较的双语提示集。ViLG-300 包含来自 16 个类别的 300 个提示，由 Draw-Bench [26]（英文）和 ERNIE-ViLG [45]（中文）中使用的提示集组成。有关构建过程的更多详细信息，请参见附录 C.1。 </p> 
<p>通过 ViLG-300，我们可以在 ERNIE-ViLG 2.0 和 DALL-E 2（https://openai.com/dall-e-2/）、Stable Diffusion（https://beta.dreamstudio.ai/dream）等模型之间进行有说服力的比较。在评估中，五名评分员被呈现了 ERNIE-ViLG 2.0 和所比较模型生成的两组图像。然后，他们被要求从图像文本对齐和图像保真度这两个维度进行比较，然后选择他们更喜欢的模型，或者回应两个模型之间没有可测量的差异。在整个过程中，评分员不知道图像是由哪个模型生成的，我们也没有应用任何筛选策略来对评分结果进行过滤。图 3 显示，人工评分员在图像文本对齐（与 DALL-E2 和Stable Diffusion 分别相比，分别为 56.5%±3.8% 和 68.2%±3.8%）和图像保真度（分别为 58.8%±3.6% 对 DALL-E 2，66.5%±3.5% 对 Stable Diffusion）两个方面更喜欢 ERNIE-ViLG 2.0，这再次证明了 ERNIE-ViLG 2.0 借助知识增强和混合去噪专家策略能够生成与文本相符的高质量图像。我们提供了附录 C.2 中各个类别的比较。除了文本相关性和图像保真度，我们还观察到 ERNIE-ViLG 2.0 可以生成比基准模型更锐利和贴图更好的图像。有关详细讨论，请参阅附录 D.3。 </p> 
<p><img alt="" height="364" src="https://images2.imgbox.com/31/cd/eWl0d66Z_o.png" width="728"></p> 
<p class="img-center"><img alt="" height="486" src="https://images2.imgbox.com/5f/75/iTnnh8Wl_o.png" width="547"></p> 
<h3 id="3.3%20%E5%88%86%E6%9E%90%C2%A0">3.3 分析 </h3> 
<p>为了检验我们的设计理念的有效性，我们进行了两组消融研究。与主实验类似，我们在这里也提供了自动度量和直观的案例来展示 ERNIE-ViLG 2.0 中每个策略的优势。 </p> 
<p><strong>知识增强策略</strong>。在这部分，我们关注各种知识增强策略对性能的影响，通过训练一系列轻量级模型，这些模型具有 500M 的文本编码器、870M 的 U-Net 和 500M 的训练样本。图 5a 中的帕累托曲线和图 10 中的收敛曲线（附录D.1）展示了在学习过程中整合知识带来的图像保真度、图像文本对齐和收敛速度方面的显著性能提升。具体来说，（1）文本知识的好处主要体现在精确的细粒度语义控制中（w/ textual），（2）只使用对象知识可能无法稳定提升性能（w/ object），而考虑合成描述是充分利用视觉知识的有效解决方案（w/ visual）。图 6 提供了更多的视觉比较，以直观展示每种策略带来的变化。在处理复杂的提示时，基线模型面临一些问题，例如缺少关键对象或属性分配错误。此时，文本知识有助于模型准确理解每个对象的属性，但生成的图像有时会出现新的扭曲问题。此外，视觉知识促进了高保真图像的生成，但它不能很好地理解文本中的特定实体。最终，这两种知识的结合和谐地促使模型从单模和多模视角提高了保真度并提高了细粒度视觉场景中的图像文本对齐。在附录 D.1 中，我们还定量衡量了不同知识源带来的 CLIP 分数改进，这与定性观察相符。 </p> 
<p><img alt="" height="473" src="https://images2.imgbox.com/d2/54/VqWBC6h9_o.png" width="1089"></p> 
<p class="img-center"><img alt="" height="436" src="https://images2.imgbox.com/0a/cd/3Ly1yRrF_o.png" width="548"></p> 
<p><strong>混合去噪专家策略</strong>。基于上述轻量级设置，我们进一步使用 500M 样本对基线模型进行训练，然后为每个去噪专家训练 200M 样本。图 5b 显示，随着去噪专家数量的增加，总体性能逐渐提高，证明了扩大 U-Net 的大小也是实现更好图像质量的有效解决方案。图 6 提供了更多展示。当专家数量从 1 增加到 10 时，模型不仅能更好地处理不同元素之间的耦合，还能生成具有更自然纹理的图像。例如，时钟上的数字变得更清晰，狼和西装的比例更和谐，模型可以生成更逼真的图片，而不是卡通绘画。我们还尝试分析去噪专家数量和训练样本数量的影响，发现使用更多专家网络比使用一个网络来训练更多样本具有更好的性能（详见附录D.2）。 </p> 
<p class="img-center"><img alt="" height="500" src="https://images2.imgbox.com/c9/98/i55fCHgb_o.png" width="1200"></p> 
<p>图 7 进一步可视化了在去噪专家中，图像特征到文本表示的交叉注意力映射，涵盖了 1,000 步的去噪过程，其中这些步骤由不同的专家进行去噪。如图所示，不同的去噪步具有不同的注意力分布。具体而言，接近 1,000 步的注意力图几乎均匀分布在整个图像上，这是因为这些步的输入接近高斯噪声，图像布局不清晰，因此所有图像标记都必须关注文本提示以生成图像轮廓。当时间步接近 1 时，注意力图更集中在前景对象上。对于这些时间步，去噪网络的输入接近最终图像，布局清晰，只有图像的少数部分需要关注文本以填补对象的细节。这些观察结果再次说明了去噪时间步之间的差异，并展示了需要使用多个专家来分离不同时间步的必要性。 </p> 
<h2 id="4.%20%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C">4. 相关工作</h2> 
<p><strong>文本到图像生成</strong>。文本到图像生成是根据自然语言描述合成图像的任务。早期的研究采用生成对抗网络（GANs）[7] 根据文本生成图像 [36,41,44,47]。受到 transformer 在生成任务中的成功 [37] 的启发，模型如 ERNIE-ViLG [45]、DALL-E [23]、Cogview [2]、Make-A-Scene [6] 和 Parti [43] 也将文本到图像生成探索为一个序列到序列问题，使用自回归 transformer  作为生成器，以文本/图像标记作为输入/输出序列。最近，另一系列作品采用了扩散模型 [30]，将其形成为一个迭代去噪任务 [9, 27, 31]。通过在去噪步骤中添加文本条件，LDM [25]、DALL-E 2 [22] 和 Imagen [26] 等实践不断刷新文本到图像生成的记录。基于扩散模型作为骨干，ERNIE-ViLG 2.0 提出了将场景知识和混合去噪专家机制纳入去噪过程的方法。</p> 
<p><strong>知识增强的预训练模型</strong>。尽管 transformer 在大规模数据上的预训练效益显著，但许多尝试在学习过程中添加知识，以引导它们关注关键元素。对于基于语言的任务，知识增强模型使用知识掩码策略 [11,34] 或知识感知的预训练任务 [33,35] 来理解语言数据分布。至于视觉-语言多模态区分模型，OSCAR [14]、ERNIE-ViL [42] 和 ERNIE-Layout [19] 利用对象标签、场景图和文档布局（document layout）等额外知识，帮助模型更好地对齐语言和视觉模态。在多模态生成模型中，Make-A-Scene [6] 通过整合领域特定的感知知识，强调了对象和面部区域的重要性。虽然当前的文本到图像扩散模型存在属性不对齐问题 [22]，但它们没有使用任何特定的对象知识。因此，ERNIE-ViLG 2.0 利用图像和文本中关键元素的知识来增强扩散模型，从而在生成的图片中实现更好的细粒度图像文本对齐。 </p> 
<p><strong>专家混合</strong>。神经网络中的专家混合（Mixture-of-Experts，MoE）是将参数的特定部分分为子集，每个子集被称为一个专家 [4,29]。在前向传播期间，路由器（router）将专家分配给不同的输入，每个输入只与分配给的专家进行交互。路由器是 MoE 的关键部分。在语言任务中，最常见的策略是匹配算法，该算法将每个文本标记分配给线性前馈层中的多个专家 [5,8,12,24]。尽管大多数实践只在一个线性层中制定了多个专家，但也有一些工作将整个语言模型作为专家 [13]。除了自然语言处理任务，MoE 的思想还被应用到视觉模型 [20] 和多模态 transformer 中的模态专家混合 [17,39,40]。在 ERNIE-ViLG 2.0 中，MoDE 机制将多个去噪 U-Net 作为专家，并使用去噪步骤索引作为固定的路由器来确定使用哪个专家。 </p> 
<h2 id="5.%20%E9%A3%8E%E9%99%A9%E3%80%81%E9%99%90%E5%88%B6%E5%92%8C%E6%9C%AA%E6%9D%A5%E5%B7%A5%E4%BD%9C%C2%A0">5. 风险、限制和未来工作 </h2> 
<p><strong>模型使用和数据偏见</strong>。经过大规模图像文本数据培训的文本到图像生成模型都面临着与生成不当和数据偏见有关的类似风险 [22, 25, 26]。考虑到文本到图像模型有助于人们以更少的努力实现他们的想象，模型的恶意使用可能会导致意想不到的欺骗性或有害后果。此外，由于模型是在包含从网站抓取的图像和其替代文本（alt-text）的数据集上进行训练的，生成的图像可能会在数据集和网站中展示社会和文化偏见。 </p> 
<p class="img-center"><img alt="" height="336" src="https://images2.imgbox.com/96/83/8VkWPqxT_o.png" width="545"></p> 
<p><strong>字符渲染</strong>。图 8 显示了两个成功的字符渲染案例（a、b）和一个失败案例（c）。对于 ERNIE-ViLG 2.0 来说，字符渲染是一项具有挑战性的任务，原因有两点。首先，训练数据包含了中文文本-图像对和翻译成中文的英文文本-图像对。当文本提示中提到字符时，图像中的字符可能是中文或英文，模型很难同时学习两种语言中的相应字符。在我们观察到的成功的字符渲染案例中，字符可以是中文中常见的词语，在英文中没有精确匹配，例如图 8a 中的“福”（英文中的 “blessing, happiness, good luck”）或在英文和中文图像中相同的数字，例如图 8b 中的 “20”。 使字符渲染变得困难的第二个原因可能是中文字符是由笔画的复杂组合构成，没有像英文字母那样的基本组件。在图 8c 中，模型确实学会了应该在右上角写一些中文字符，但它只在那里绘制了无意义的笔画。</p> 
<p><strong>混合去噪专家的变体</strong>。第 3.3 节表明，使用更多的去噪专家可以提高模型性能。这表明使用并行的 U-Net 专家是增强去噪网络的有效方法。由于计算能力的限制，本文只尝试使用最多 10 个专家，但我们认为探索更多的去噪专家和多个文本编码器作为专家是一个有意义的未来方向。在这方面，我们可以进一步扩大模型规模，并允许它在类似的推理时间内更好地学习数据分布。 </p> 
<h2 id="6.%20%E7%BB%93%E8%AE%BA">6. 结论</h2> 
<p>我们提出了 ERNIE-ViLG 2.0，这是基于扩散模型的首个大规模中文文本到图像生成模型。为了提高对场景语义的细粒度控制，我们将场景的视觉和文本知识纳入扩散模型中。为了解耦不同去噪步骤的模型参数，我们引入了 MoDE，并将模型参数扩展到 24B，同时具有相对较短的推理时间。实验证明，ERNIE-ViLG 2.0 在 MS-COCO 上达到了最新的水平，每个提出的机制都对最终结果有所贡献。为了使中文和英文文本到图像模型能够进行公平比较，我们收集了双语提示集 ViLG-300，人工评估表明 ERNIE-ViLG 2.0 在文本相关性和图像保真度方面优于强大的基线模型。进一步的分析表明，不同的知识来源在不同方面提高了生成效果，并且使用更多的专家会导致更好的图像质量。在未来，我们计划丰富外部的图像文本对齐知识，并扩大多个专家的使用，以推进生成技术。另请参阅附录，获取有关训练和评估的更多详细信息。 </p> 
<p></p> 
<h2 id="%E5%8F%82%E8%80%83">参考</h2> 
<p>Feng Z, Zhang Z, Yu X, et al. ERNIE-ViLG 2.0: Improving text-to-image diffusion model with knowledge-enhanced mixture-of-denoising-experts[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 10135-10145.</p> 
<p></p> 
<h2 id="%E9%99%84%E5%BD%95">附录</h2> 
<h2 id="A.%20%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82">A. 训练细节</h2> 
<p class="img-center"><img alt="" height="785" src="https://images2.imgbox.com/f8/0b/V4lkhPx8_o.png" width="546"></p> 
<p>算法 1 显示了 ERNIE-ViLG 2.0 的训练伪代码。我们首先使用（图像，文本）对作为输入，在文本中找到关键词，使用开源的词性标注工具 “jieba”，并使用目标检测器 [1] 在图像中找到显著区域。然后，这些附加信息将用于增强知识的训练。训练过程分为两个阶段。在第一阶段，我们训练一个具有 2.2B 参数的 U-Net 和一个具有 1.3B 参数的文本编码器，共进行 350,000 个步骤。在第二阶段，文本编码器是共享的，我们训练 10 个去噪专家，进行 90,000 个步骤，这些专家继承了第一阶段的 U-Net 参数。ERNIE-ViLG 2.0 中使用的未完成的超参数已提供在 Table 2 中。 </p> 
<h2 id="B.%20%E8%AF%A6%E7%BB%86%E7%9A%84%E8%87%AA%E5%8A%A8%E8%AF%84%E4%BC%B0">B. 详细的自动评估</h2> 
<p><img alt="" height="364" src="https://images2.imgbox.com/6e/97/0FSdE07n_o.png" width="1141"></p> 
<p>表格 3 详细比较了自动评估分数，包括模型大小和重新排序策略。在第一训练阶段结束时，我们的 具有一个去噪专家的 35 亿参数模型的 FID-30K 指标为 8.07（w/o reranking），比类似模型大小的 DALL-E 2 [22]（10.39）要好，但比我们最终的24亿参数模型（7.23，10 个专家，w/o reranking）差。完成整个训练过程后，我们的 24 亿参数模型（6.75，w/ 4 reranking images）在具有类似参数数量和较少重新排序图像的情况下超越了 Parti [43]（7.23，w/ 16 reranking images），这些比较表明额外的知识和模型扩展都对我们模型的最终性能有贡献。 </p> 
<h2 id="D.%20%E8%AF%A6%E7%BB%86%E6%B6%88%E8%9E%8D%E7%A0%94%E7%A9%B6%C2%A0">D. 详细消融研究 </h2> 
<h3 id="D.1%20%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA%E6%B6%88%E8%9E%8D%C2%A0">D.1 知识增强消融 </h3> 
<p><img alt="" height="527" src="https://images2.imgbox.com/8a/03/zuXtntBh_o.png" width="1092"></p> 
<p>图 10 提供了各种模型的收敛曲线，明显地表明知识增强策略显著加速了扩散模型的收敛过程。值得注意的是，在训练的初始阶段，知识增强模型达到或甚至超过了具有两倍训练样本的基线模型的性能（即100M与200M、200M与400M）。 这些结果表明，知识增强策略可以显著提高模型的训练效率，使模型在早期阶段就达到更好的性能。</p> 
<p><img alt="" height="265" src="https://images2.imgbox.com/32/22/Nyn9mCbK_o.png" width="1050"></p> 
<p>为了定量衡量每种知识源带来的改进，我们计算了 ViLG-300 提示与生成的图像之间的 CLIP 分数。表 5 呈现了每种策略相对于基线的前五个具有最大性能提升的类别。可以看到不同的策略在不同的类别中带来了改进，表明它们在互补方面帮助模型吸收知识并改善文本-图像对齐。此外，我们还注意到 CLIP 不能很好地捕捉多个对象之间的关系（例如，counterfactual），因此我们将精确的用于细粒度语义控制的自动评估方法留作有价值的未来工作。 </p> 
<h3 id="D.2.%20%E6%B7%B7%E5%90%88%E5%8E%BB%E5%99%AA%E4%B8%93%E5%AE%B6%E6%B6%88%E8%9E%8D%C2%A0" style="background-color:transparent;">D.2 混合去噪专家消融 </h3> 
<p>探索训练样本或模型大小（即降噪专家的数量）对性能的影响，我们使用 400百万、10亿 和 20亿个样本进行 1 个专家的设置，如第 3.3 节所述。这对应于由 2、5 和 10 个专家训练的样本数量。图 11 说明了单个专家使用 4 亿样本和两个专家每个使用 2 亿样本时的性能几乎相同，而在训练过程中，单个专家的性能落后于两个专家。这表明将不同阶段的降噪能力分离是一种有效策略，并且合理增加 U-Net 的大小能进一步提高文本到图像模型的性能。 </p> 
<p class="img-center"><img alt="" height="383" src="https://images2.imgbox.com/cb/1d/y0KybKH7_o.png" width="729"></p> 
<p><img alt="" height="749" src="https://images2.imgbox.com/4a/b1/7DcEIHGi_o.png" width="1138"></p> 
<h3 style="background-color:transparent;">D.3 图像质量比较 </h3> 
<p>图 12 比较了 ERNIE-ViLG 2.0 和基线模型的图像细节，通过放大生成图像的小区域来进行对比。从技术上讲，ERNIE-ViLG 2.0 和 Stable Diffusion 都是使用扩散模型来生成与文本相关的图像潜在表示。然而，Stable Diffusion 只能生成 512x512 大小的图像，而 ERNIE-ViLG 2.0 可以直接输出 1024x1024 分辨率的图像。因此，ERNIE-ViLG 2.0 的放大部分比 Stable Diffusion 更清晰。至于DALL-E 2，它采用级联生成，首先使用文本生成 64x64 大小的图像，然后通过两个超分辨率模型将其放大到 1024x1024 分辨率。尽管它生成与 ERNIE-ViLG 2.0 相同分辨率的图像，但 DALL-E 2 的输出有时包含不自然的纹理，比如放大区域中的蓬松树木和雨滴。相反，我们模型的结果更具自然感和照片逼真度。</p> 
<p></p> 
<p></p> 
<h2 id="S.%20%E6%80%BB%E7%BB%93">S. 总结</h2> 
<h3 id="S.1%C2%A0%20%E4%B8%BB%E8%A6%81%E8%B4%A1%E7%8C%AE" style="background-color:transparent;">S.1 主要贡献</h3> 
<p>本文提出了 <strong>ERNIE-ViLG 2.0</strong>，一种大规模中文文本到图像的扩散模型，通过知识增强（将关键元素的细粒度文本和视觉知识融入到图像中）和去噪专家混合（在不同的去噪阶段使用不同的去噪专家）来逐步提升生成图像的质量。</p> 
<h3 id="S.2%20%E6%9E%B6%E6%9E%84%E5%92%8C%E6%96%B9%E6%B3%95%C2%A0">S.2 架构和方法 </h3> 
<p><img alt="" height="524" src="https://images2.imgbox.com/7e/7f/HjGRkOhF_o.png" width="1200"></p> 
<p><strong>ERNIE-ViLG 2.0</strong> 的整体架构如图 2 所示。 </p> 
<p><strong>知识增强扩散模型</strong>。简单的扩散模型不能区分元素的重要性（比如文本中的关键词或图像中的显著区域），并且无差别地迭代去噪过程。ERNIE-ViLG 2.0 将额外的文本和视觉知识纳入学习阶段，从而增强扩散模型对细粒度语义的感知能力。</p> 
<ul><li>文本知识增强：根据词汇分析的结果增强关键词（关键词被定义为现代汉语中的虚词（notional words），即名词、动词、形容词、数词、量词和代词）的注意力权重。</li><li>视觉知识增强：扩散模型的损失函数直接作用于图像空间，因此可以为显著的区域（例如人、树木、建筑和输入中提到的对象）分配更高的权重。</li></ul> 
<p><strong>去噪专家混合</strong>。去噪过程中，所有步骤的目标都是去噪噪声输入。但根据输入的噪声比例，这些去噪步骤的难度有所不同。理论上去噪过程不需要相同的去噪网络来处理所有步骤。</p> 
<ul><li>基于此，本文提出了 "Mixture-of-Denoising-Experts" (MoDE) 模型，利用多个专门的专家网络（去噪 U-Net）来去噪不同时间步图像。</li><li>将所有时间步均匀地分成 n 个块，每个块包含连续的时间步，分配给一个去噪专家。</li><li>通过采用专家网络来专门处理不同的去噪阶段，从而提高了模型的性能。 </li><li>当使用更多专家时，每个块包含更少的时间步骤，因此每个专家可以更好地专注于学习分配给它的特定去噪步的特征。同时，由于每个时间步只激活一个专家网络，增加专家的数量不会影响推断时的计算开销。</li></ul> 
<p></p> 
<p></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/44b2a620d894f03c141b413fbb6ba90b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">java实现pgsql自动更新创建时间与更新时间的两种方式</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c72ffd1ee50f19ea0713e88e653e6a9c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">write javaBean error, fastjson version 1.2.68, class org.springframework.web.multipart.support.Stand</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>