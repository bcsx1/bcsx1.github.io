<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>NERF&#43;&#43;: ANALYZING AND IMPROVING NEURAL RADIANCE FIELDS分析和改进神经辐射场 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="NERF&#43;&#43;: ANALYZING AND IMPROVING NEURAL RADIANCE FIELDS分析和改进神经辐射场" />
<meta property="og:description" content="目录
NERF&#43;&#43;: ANALYZING AND IMPROVING NEURAL RADIANCE FIELDS分析和改进神经辐射场
ABSTRACT
1 INTRODUCTION
2 PRELIMINARIES
3 SHAPE-RADIANCE AMBIGUITY形状-辐射模糊度
4 INVERTED SPHERE PARAMETRIZATION反向球体参数化
NERF&#43;&#43;: ANALYZING AND IMPROVING NEURAL RADIANCE FIELDS分析和改进神经辐射场 ABSTRACT 神经辐射场(NeRF)为各种捕捉设置实现了令人印象深刻的视图合成结果，包括有界场景的360°捕捉360◦ capture of bounded scenes以及有界和无界场景的前向捕捉forward-facing capture of bounded and unbounded scenes。NeRF将表示视图不变不透明度view-invariant opacity和视图相关颜色体积的view-dependent color volumes多层感知器(MLPs)拟合到一组训练图像，并基于体积绘制技术对新视图进行采样。在这份技术报告中，我们首先讨论了辐射场及其潜在的模糊性potential ambiguities，即形状辐射率的模糊性shape-radiance ambiguity，并分析了NeRF在避免这种模糊性方面的成功之处。第二，我们提出了一个参数化问题，它涉及到将NeRF应用于大规模、无边界3D场景中的360°物体特征。我们的方法在这种具有挑战性的场景中提高了视图合成的保真度。代码可在https://github.com/Kai-46/nerfplusplus.获得
1 INTRODUCTION 回忆一下你的上一个假期，在那里你拍了几张你最喜欢的地方的照片。现在在家里，你希望再次在这个特别的地方走动，如果只是虚拟的。这要求您在一个可能无限的场景中从不同的、自由放置的视点渲染同一个场景。这种新颖的视图合成任务是计算机视觉和图形学中的一个长期存在的问题(Chen &amp; Williams，1993；Debevec等人，1996年；莱沃伊和汉拉汉，1996年；Gortler等人，1996年；Shum &amp; Kang，2000年)。
最近，基于学习的方法已经导致了照片级的新颖视图合成的重大进展。特别是神经辐射场(NeRF)的方法已经引起了极大的关注(Mildenhall等人，2020)。NeRF是一个隐式的基于MLP的模型，它将5D向量(3D坐标加上2D观察方向)映射到不透明度和颜色值，通过将模型拟合到一组训练视图来计算。然后，所得到的5D函数可以用于利用传统的体绘制技术生成新的视图。
在本技术报告中，我们首先对NeRF中的潜在故障模式进行了分析，并分析了NeRF在实践中避免这些故障模式的原因。第二，我们提出了一种新的空间参数化方案novel spatial parameterization scheme，我们称之为反向球面参数化inverted sphere parameterization，它允许NeRF处理一类新的无界场景捕获captures of unbounded scenes。
图1：形状辐射模糊度（左）和无边界场景的参数化（右）。形状歧义Shape-radiance ambiguity：我们的理论分析表明，在缺乏显式或隐式正则化，一组训练图像可以独立于恢复几何(例如，不正确的场景几何Sˆ而不是正确的几何S∗)通过利用视图相关的辐射来模拟正确的几何的效果。无界场景的参数化Parameterization of unbounded scenes：使用标准参数化方案，要么只建模部分场景（红色轮廓），导致背景元素中的重要伪影，或（2）整个场景被建模（橙色轮廓），这导致由于有限的采样分辨率的细节的总体损失。
特别地，我们发现在理论上，在没有任何正则化的情况下，从一组训练图像优化5D函数会遇到不能推广到新的测试视图的临界退化解critical degenerate solutions。这种现象被封装在形状-辐射模糊度中shape-radiance ambiguity(图1，左)，其中通过适当选择每个表面点的出射2D辐射outgoing 2D radiance，可以为任意不正确的几何形状完美地拟合一组训练图像。我们的经验表明，NeRF中使用的特定MLP结构在避免这种歧义方面发挥了重要作用，产生了令人印象深刻的综合新观点synthesize novel views的能力。我们的分析为NeRF令人印象深刻的成功提供了一个新的视角。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/e348bc1a9786471323f62b44720c2dcc/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-05-16T15:40:45+08:00" />
<meta property="article:modified_time" content="2022-05-16T15:40:45+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">NERF&#43;&#43;: ANALYZING AND IMPROVING NEURAL RADIANCE FIELDS分析和改进神经辐射场</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="NERF%2B%2B%3A%20ANALYZING%20AND%20IMPROVING%20NEURAL%20RADIANCE%20FIELDS%E5%88%86%E6%9E%90%E5%92%8C%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E8%BE%90%E5%B0%84%E5%9C%BA-toc" style="margin-left:0px;"><a href="#NERF%2B%2B%3A%20ANALYZING%20AND%20IMPROVING%20NEURAL%20RADIANCE%20FIELDS%E5%88%86%E6%9E%90%E5%92%8C%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E8%BE%90%E5%B0%84%E5%9C%BA" rel="nofollow">NERF++: ANALYZING AND IMPROVING NEURAL RADIANCE FIELDS分析和改进神经辐射场</a></p> 
<p id="ABSTRACT-toc" style="margin-left:0px;"><a href="#ABSTRACT" rel="nofollow">ABSTRACT</a></p> 
<p id="1%20INTRODUCTION-toc" style="margin-left:0px;"><a href="#1%20INTRODUCTION" rel="nofollow">1 INTRODUCTION</a></p> 
<p id="2%20PRELIMINARIES-toc" style="margin-left:0px;"><a href="#2%20PRELIMINARIES" rel="nofollow">2 PRELIMINARIES</a></p> 
<p id="3%20SHAPE-RADIANCE%20AMBIGUITY%E5%BD%A2%E7%8A%B6-%E8%BE%90%E5%B0%84%E6%A8%A1%E7%B3%8A%E5%BA%A6-toc" style="margin-left:0px;"><a href="#3%20SHAPE-RADIANCE%20AMBIGUITY%E5%BD%A2%E7%8A%B6-%E8%BE%90%E5%B0%84%E6%A8%A1%E7%B3%8A%E5%BA%A6" rel="nofollow">3 SHAPE-RADIANCE AMBIGUITY形状-辐射模糊度</a></p> 
<p id="4%20INVERTED%20SPHERE%20PARAMETRIZATION%E5%8F%8D%E5%90%91%E7%90%83%E4%BD%93%E5%8F%82%E6%95%B0%E5%8C%96-toc" style="margin-left:0px;"><a href="#4%20INVERTED%20SPHERE%20PARAMETRIZATION%E5%8F%8D%E5%90%91%E7%90%83%E4%BD%93%E5%8F%82%E6%95%B0%E5%8C%96" rel="nofollow">4 INVERTED SPHERE PARAMETRIZATION反向球体参数化</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2><strong>NERF++: ANALYZING AND IMPROVING NEURAL RADIANCE FIELDS分析和改进神经辐射场</strong></h2> 
<h2 id="ABSTRACT" style="text-align:justify;"><strong>ABSTRACT</strong></h2> 
<p>神经辐射场(NeRF)为各种捕捉设置实现了令人印象深刻的视图合成结果，包括有界场景的360°捕捉<strong><u><span style="color:#0070c0;"><strong><u>360◦ capture of bounded scenes</u></strong></span></u></strong>以及有界和无界场景的前向捕捉<strong><u><span style="color:#0070c0;"><strong><u>forward-facing capture of bounded and unbounded scenes</u></strong></span></u></strong>。NeRF将表示视图不变不透明度view-invariant opacity和视图相关颜色体积的view-dependent color volumes多层感知器(MLPs)拟合到一组训练图像，并基于体积绘制技术对新视图进行采样。在这份技术报告中，我们<u><u>首先讨论了辐射场及其潜在的模糊性potential ambiguities，即形状辐射率的模糊性shape-radiance ambiguity</u></u>，<u><u>并分析了NeRF在避免这种模糊性方面的成功之处</u></u>。<u><u>第二，我们提出了一个</u></u><u><span style="color:#0000ff;"><u>参数化问题</u></span></u><u><u>，它涉及到将NeRF应用于</u></u><u><span style="color:#0000ff;"><u>大规模、无边界3D场景中的360°物体特征</u></span></u>。我们的方法在这种具有挑战性的场景中提高了视图合成的保真度。代码可在<u><span style="color:#0000ff;"><u>https://github.com/Kai-46/nerfplusplus.</u></span></u>获得</p> 
<h2 id="1%20INTRODUCTION" style="text-align:justify;"><strong>1 INTRODUCTION</strong></h2> 
<p style="margin-left:.0001pt;text-align:justify;">回忆一下你的上一个假期，在那里你拍了几张你最喜欢的地方的照片。现在在家里，你希望再次在这个特别的地方走动，如果只是虚拟的。这要求您在一个可能无限的场景中从不同的、自由放置的视点渲染同一个场景。这种新颖的视图合成任务是计算机视觉和图形学中的一个长期存在的问题(Chen &amp; Williams，1993；Debevec等人，1996年；莱沃伊和汉拉汉，1996年；Gortler等人，1996年；Shum &amp; Kang，2000年)。</p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p style="margin-left:.0001pt;text-align:justify;">最近，基于学习的方法已经导致了照片级的新颖视图合成的重大进展。特别是神经辐射场(NeRF)的方法已经引起了极大的关注(Mildenhall等人，2020)。NeRF是一个隐式的基于MLP的模型，它将5D向量(3D坐标加上2D观察方向)映射到不透明度和颜色值，通过将模型拟合到一组训练视图来计算。然后，所得到的5D函数可以用于利用传统的体绘制技术生成新的视图。</p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p><u><u>在本技术报告中，我们首先对NeRF中的潜在故障模式进行了分析，并分析了NeRF在实践中避免这些故障模式的原因。第二，我们提出了一种新的</u></u><u><span style="color:#0000ff;"><u>空间参数化方案</u></span></u><u><u>novel spatial parameterization scheme，我们称之为</u></u><strong><u><span style="color:#0070c0;"><strong><u>反向球面参数化</u></strong></span></u></strong><u><u>inverted sphere parameterization，它允许NeRF处理一类</u></u><u><span style="color:#0000ff;"><u>新的无界场景捕获</u></span></u><u><u>captures of unbounded scenes。</u></u></p> 
<p><img alt="" height="370" src="https://images2.imgbox.com/38/60/hcZJFNwl_o.png" width="830"></p> 
<p>图1：形状辐射模糊度（左）和无边界场景的参数化（右）。形状歧义<span style="color:#0000ff;">Shape</span><span style="color:#0000ff;">-</span><span style="color:#0000ff;">radiance ambiguity</span>：我们的理论分析表明，<u><u>在缺乏显式或隐式正则化，一组训练图像可以独立于恢复几何</u></u>(例如，不正确的场景几何Sˆ而不是正确的几何S∗)通过利用视图相关的辐射来模拟正确的几何的效果。无界场景的参数化<span style="color:#0000ff;">Parameterization of unbounded scenes</span>：使用标准参数化方案，要么<u><u>只建模部分场景（红色轮廓），导致背景元素中的重要伪影</u></u>，或（2）<u><u>整个场景被建模（橙色轮廓），这导致由于有限的采样分辨率的细节的总体损失。</u></u></p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p>特别地，我们发现<u><u>在理论上，在没有任何正则化的情况下，从一组训练图像优化5D函数会遇到不能推广到新的测试视图的</u></u><u><span style="color:#0000ff;"><u>临界退化解</u></span></u>critical degenerate solutions。这种现象被封装在<span style="color:#0000ff;">形状-辐射模糊度</span>中shape-radiance ambiguity(图1，左)，其中<u><u>通过适当选择每个表面点的出射2D辐射outgoing 2D radiance，可以为任意不正确的几何形状完美地拟合一组训练图像</u></u>。我们的经验表明，<u><span style="color:#0000ff;"><u>NeRF中使用的特定MLP结构在避免这种歧义方面发挥了重要作用</u></span></u>，产生了令人印象深刻的综合新观点synthesize novel views的能力。我们的分析为NeRF令人印象深刻的成功提供了一个新的视角。</p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p>我们还解决了一个<u><span style="color:#0000ff;"><u>空间参数化问题</u></span></u>，该问题出现在具有挑战性的场景中，包括<u><span style="color:#0000ff;"><u>在无界环境中围绕物体进行360°捕捉</u></span></u>(图1，右侧)。<u><u>对于</u></u><u><u>360◦ captures</u></u><u><u>，NeRF假设整个场景可以打包到一个有界的体积中</u></u>，这对于<u><span style="color:#0000ff;"><u>大规模场景</u></span></u>来说是有问题的:<u><u>要么我们将场景的一小部分装进体积中，并对其进行详细采样，但完全无法捕捉背景元素；或者，我们将整个场景放入体积中，由于有限的采样分辨率，到处都缺少细节</u></u>。我们提出了一种简单而有效的解决方案，该方案分别<u><span style="color:#0000ff;"><u>对前景和背景进行建模</u></span></u>，<u><u>利用反向球体场景参数化inverted sphere scene parameterization对无界3D背景内容进行建模</u></u>解决了挑战。我们展示了坦克和寺庙数据集Tanks and Temples dataset(Knapitsch等人，2017年)和余等人(2016年)的光场数据集的真实世界捕捉的定量和定性结果。</p> 
<p><u><span style="color:#0000ff;"><u>总之，我们提出了一个关于NeRF如何设法解决形状-辐射模糊的分析，以及一个在</u></span></u><u><span style="color:#0000ff;"><u>360◦ captures</u></span></u><u><span style="color:#0000ff;"><u>情况下对无界场景参数化的补救措施。</u></span></u></p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<h2 id="2%20PRELIMINARIES" style="text-align:justify;"><strong>2 PRELIMINARIES</strong></h2> 
<p>给定静态场景的设定的多视图图像，NeRF重建表示软形状soft shape的不透明度场opacity field σ，以及表示依赖于视图的表面纹理的辐射场c。σ和c都隐式表示为多层感知器(MLPs)；不透明度场作为3D位置x ∈ R3的函数来计算，并且辐射场由3D位置和观察方向d ∈ S2(即，单位3向量的集合)来参数化。因此，我们<u><u>使用σ(x)来表示作为位置函数的不透明度，使用c(x，d)来表示作为位置和观察方向函数的辐射度。</u></u></p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p>理想情况下，<u><u>σ应在不透明材料的地面真实表面位置达到峰值</u></u>，在这种情况下，<u><u>c降低到表面光场</u></u>surface light field(Wood等人，2000)。给定n个训练图像，NeRF使用随机梯度下降通过最小化地面真实观察图像I和从相同视点处的σ和c渲染的预测图像I(σ，c)之间的差异来优化σ和c:<img alt="" height="188" src="https://images2.imgbox.com/71/44/3LLO2Qfk_o.png" width="828"></p> 
<p style="margin-left:0pt;">为了补偿网络的<u><span style="color:#0000ff;"><u>光谱偏差</u></span></u>spectral bias并合成更清晰的图像，NeRF使<u><span style="color:#0000ff;"><u>用位置编码γ将x和d映射到它们的傅立叶特征</u></span></u>(Tancik等人，2020年):</p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="50" src="https://images2.imgbox.com/bc/a6/XfTHaUO1_o.png" width="828"></p> 
<p>其中<u><span style="color:#0000ff;"><u>k是指定傅立叶特征向量的维度的超参数</u></span></u>。</p> 
<h2 id="3%20SHAPE-RADIANCE%20AMBIGUITY%E5%BD%A2%E7%8A%B6-%E8%BE%90%E5%B0%84%E6%A8%A1%E7%B3%8A%E5%BA%A6" style="text-align:justify;"><strong>3 SHAPE-RADIANCE AMBIGUITY</strong><strong>形状-辐射模糊度</strong></h2> 
<p>NeRF对依赖于视图的外观进行建模的能力导致3D形状和辐射度之间的固有模糊性ambiguity，在没有正则化的情况下，这可能允许退化的解决方案。<u><span style="color:#0000ff;"><u>对于任意的、不正确的形状，可以表明存在一族辐射场，其完美地解释了训练图像，但是其对于新颖的测试视图的概括较差。</u></span></u></p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p>为了说明这种模糊性，想象对于给定的场景，我们<u><span style="color:#0000ff;"><u>将几何图形表示为一个单位球</u></span></u>。换句话说，让我们将NeRF的<u><u>不透明度场在单位球表面固定为1，在其他地方固定为0</u></u>。然后，对于每个<u><span style="color:#0000ff;"><u>训练图像</u></span></u>中的每个像素，<u><span style="color:#0000ff;"><u>我们将穿过该像素的光线与球体相交，并将交点处的辐射值(沿着光线方向)定义为该像素的颜色</u></span></u>。这种人工构建的解决方案是一种有效的NeRF重建，与输入图像完全吻合。然而，这种解决方案合成新视图的能力非常有限:<u><u>精确地生成这样的视图需要在每个表面点上重建任意复杂的视图相关函数</u></u>。<u><u>该模型不太可能精确地内插这样一个复杂的函数，除非训练视图非常密集</u></u>，如在传统的光场渲染工作中(Buehler等人，2001；莱沃伊和汉拉汉，1996年；Gortler等人，1996年)。这种<u><u>形状-辐射模糊性如图2所示。</u></u></p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="197" src="https://images2.imgbox.com/94/39/cCnYc6Wg_o.png" width="828"></p> 
<p> </p> 
<p>图2:为了证明形状-辐射模糊性shape-radiance ambiguity，我们在合成数据集上预训练NeRF，其中<span style="color:#0000ff;">不透明度场σ被优化以模拟不正确的3D形状(单位球体，而不是推土机形状)</span>，而<span style="color:#0000ff;">辐射场c被优化以将训练射线与球体的交点和视图方向映射到它们的像素颜色。</span>在本例中，我们使用3个MLP层来模拟视点相关的效果(参见图3中的MLP结构)，<u><u>并适合50个视点随机分布在一个半球上的合成训练图像</u></u>。<u><u>产生的不正确的解决方案很好地解释了训练图像(左边的两个图像)，但是未能推广到新的测试视图(右边的两个图像)</u></u>。</p> 
<p style="margin-left:0pt;"><strong>形状辐射歧义是指根据给出的训练视图，最后训练出的辐射场，不是正确的挖掘机的形状，而是可以拟合训练集的形状如球形，但是，对于测试集，该拟合出的错误辐射场不能正确生成图片。</strong></p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p>NeRF为什么能避免这样的退化解degenerate solutions？我们假设两个相关因素拯救了NeRF:</p> 
<ol><li><u><u>不正确的几何形状迫使辐射场具有更高的内在复杂性(即，更高的频率)</u></u>，</li><li>而相反， <u><u>NeRF的特定MLP结构隐含地编码了表面反射之前的平滑BRDF。</u></u></li></ol> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p>因素1:<u><u>当σ偏离正确的形状时，c通常必须变成相对于d的高频函数，以重构输入图像。对于正确的形状，表面光场通常会平滑得多(事实上，对于朗伯材质是恒定的)。不正确的形状所需的更高的复杂性更难以用有限容量的MLP来表示。</u></u></p> 
<p></p> 
<p>因素2:特别是，<u><u>NeRF的特定MLP结构编码了一个隐含的先验信息，有利于平滑的表面反射函数，其中c在任何给定的表面点x相对于d是平滑的</u></u>。如图3所示，这种MLP结构不对称地处理场景位置x和观察方向d，d被注入到靠近MLP末端的网络中，这意味着<u><u>在视图相关效果的创建中涉及较少的MLP参数以及较少的非线性激活</u></u>。此外，<u><u>用于对观察方向进行编码的傅立叶特征仅由低频分量组成</u></u>，即，用于对d和x进行编码的γ4()和γ10()(见等式3)。换句话说，<u><u>对于固定的x，辐射度c(x，d)相对于d具有有限的表现力。</u></u><img alt="" height="447" src="https://images2.imgbox.com/9d/ba/yruU6Khp_o.png" width="829"></p> 
<p>图3:用于建模辐射亮度c的NeRF MLP的结构</p> 
<p></p> 
<p>NeRF的特色MLP比普通的对称MLP效果更好</p> 
<p>为了验证这一假设，我们进行了一项实验，<u><u>用一个普通的MLP来表示c，它对称地对待x和d，即接受两者作为第一层的输入，并用γ10()进行编码，以消除网络结构中出现的涉及观察方向的任何隐含先验。如果我们用c的这个替代模型从头开始训练NeRF，我们观察到与NeRF的特殊MLP相比，测试图像质量下降</u></u>，如图4和表1所示。这一结果与我们的假设相一致，即<u><span style="color:#0000ff;"><u>NeRF辐射c的MLP模型中反射率的隐式正则化有助于恢复正确的解。</u></span></u></p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="365" src="https://images2.imgbox.com/5b/81/f77qlEFP_o.png" width="830"></p> 
<p>图4：在DTU场景中(Jensen等人，2014；里格勒和科尔顿，2020)，该图显示了用普通MLP替换NeRF的辐射场c模型的效果（同时保持σ的结构相同，并从头开始训练两个场）。普通的MLP损害了NeRF概括到新观点的能力。</p> 
<p style="margin-left:0pt;"><img alt="" height="77" src="https://images2.imgbox.com/9c/ff/8FMpzjh6_o.png" width="829"></p> 
<p> </p> 
<p>表1：在DTU场景上(Jensen等人，2014)，用普通的MLP取代NeRF的MLP显著减少了对新视图的泛化。我们使用与Riegler&amp;Koltun（2020）相同的数据分割。左边的数字是插值interpolation，右边的数字是外推extrapolation。他们在背景被掩盖的完整图像上进行评估。</p> 
<p> </p> 
<h2 id="4%20INVERTED%20SPHERE%20PARAMETRIZATION%E5%8F%8D%E5%90%91%E7%90%83%E4%BD%93%E5%8F%82%E6%95%B0%E5%8C%96" style="text-align:justify;"><strong>4 INVERTED SPHERE PARAMETRIZATION反向球体参数化</strong></h2> 
<p>等式2中的体绘制公式在欧几里德深度上积分。<strong><u><span style="color:#0000ff;"><strong><u>当真实场景深度的动态范围很小时</u></strong></span></u></strong><u><span style="color:#0000ff;"><u>，可以用有限数量的样本在数值上很好地近似积分。</u></span></u>然而，<u><u>对于室外，</u></u><u><u>360◦ captures </u></u><u><u>以附近物体为中心，同时观察周围环境，动态深度范围可以非常大，作为背景(建筑物、山脉、云等。)可以任意远。</u></u><u><span style="color:#0000ff;"><u>如此高的动态深度范围在NeRF的体积场景表示中导致了严重的分辨率问题</u></span></u>，因为为了合成照片般逼真的图像，等式2在前景和背景区域都需要足够的分辨率，这<u><u>很难通过根据3D空间的欧几里德参数化简单地采样点来实现</u></u>。</p> 
<p style="margin-left:0pt;"><img alt="" height="225" src="https://images2.imgbox.com/31/9b/BuWMVPvX_o.png" width="830"></p> 
<p> </p> 
<p>图5：对于360◦捕获的无限场景，NeRF的空间参数化要么只建模场景的一部分，导致背景元素(a)中的重要工件，要么建模整个场景，并由于有限的采样分辨率(b).而遭受整体细节损失</p> 
<p style="margin-left:0pt;">图5展示了场景覆盖和捕捉细节之间的折衷tradeoff。在一个更受限制的场景中，所有相机都面向将相机与场景内容分开的平面，<u><u>NeRF通过将欧几里得空间的子集(即，camera’s view frustum)投影映射到归一化的设备坐标(NDC) (McReynolds &amp; Blythe，2005)，并在该NDC空间中积分来解决这个分辨率问题。</u></u>然而，这种NDC参数化也从根本上限制了可能的视点，因为它未能覆盖参考视图截锥reference view frustum外部的空间。</p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p>我们通过<u><span style="color:#0000ff;"><u>简化自由视图合成</u></span></u><u><span style="color:#0000ff;"><u>facilitates free view</u></span></u><u> </u><u><span style="color:#0000ff;"><u>synthesis</u></span></u><u><span style="color:#0000ff;"><u>的</u></span></u><strong><u><span style="color:#0070c0;"><strong><u>反向球体参数化</u></strong></span></u></strong><u><span style="color:#0000ff;"><u> inverted</u></span></u><u> </u><u><span style="color:#0000ff;"><u>sphere parameterization</u></span></u>来解决这一限制。在我们的表示中，我们<u><span style="color:#0000ff;"><u>首先将场景空间划分为两个体积，一个</u></span></u><strong><u><span style="color:#0070c0;"><strong><u>内部单位球体</u></strong></span></u></strong><u><span style="color:#0000ff;"><u>和一个由覆盖内部体积补集的反向球体表示的</u></span></u><strong><u><span style="color:#0070c0;"><strong><u>外部体积</u></strong></span></u></strong>outer volume(参见图6中的说明和图7中以这种方式建模的场景的真实世界示例)。<u><span style="color:#0000ff;"><u>内部体积包含前景和所有摄像机，而外部体积包含环境的剩余部分。</u></span></u></p> 
<p style="margin-left:.0001pt;text-align:center;"><img alt="" height="391" src="https://images2.imgbox.com/82/de/tT1vSc23_o.png" width="489"></p> 
<p> </p> 
<p style="text-align:center;">图6: <span style="color:#0000ff;">NeRF++对单位球内外的场景内容应用了不同的参数化。</span></p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p style="margin-left:0pt;">这两个卷volumes是用两个独立的神经模型制作的。要渲染光线的颜色，需要单独进行光线投射raycast，然后进行最终合成。<u><u>内部NeRF不需要重新参数化，因为场景的这一部分被很好地限制住了。对于外部NeRF，我们应用了一个</u></u><strong><u><span style="color:#0070c0;"><strong><u>反向球体参数化</u></strong></span></u></strong><u><u>inverted sphere parametrization。</u></u></p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="135" src="https://images2.imgbox.com/d0/46/MncEv92e_o.png" width="828"></p> 
<p> </p> 
<p style="margin-left:0pt;">这<u><u>不仅提高了数值的稳定性</u></u>，而且考虑到了<u><u>更远的对象应该获得更低的分辨率这一事实</u></u>。我们可以<u><u>直接光线投射这个4D有界体(只有3个自由度)来渲染相机光线的颜色</u></u>。注意，前景和背景的合成相当于打破了等式2中的积分 分为两部分，integration inside the inner and outer volumes。特别是，考虑到<u><u>射线r = o + td被单位球面分割成两段:第一段，t ∈ (0，t′)在球面内；在第二种情况下，t∈(t′，∞)在球面之外。</u></u>我们可以重写等式2中的体绘制积分为</p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="262" src="https://images2.imgbox.com/d5/2d/wTEnnQra_o.png" width="828"></p> 
<p> </p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="402" src="https://images2.imgbox.com/ea/79/bXJjl2Q3_o.png" width="829"></p> 
<p> </p> 
<p style="margin-left:0pt;">外体积的倒球面参数化具有直观的物理解释。它可以<u><u>用一个虚拟摄像机来观察，它的像平面是场景原点的单位球面</u></u>。因此，3D点(x，y，z)被投影到图像平面上的像素(x′，y′，z′)，而项1/r ∈ (0，1)用作该点的(逆)深度或视差。从这个角度来看，<u><u>仅适用于前向捕捉的NDC参数化与我们的表示相关，因为它使用虚拟针孔相机而不是球形投影表面。</u></u>在这个意义上，我们的反向球体参数化与在最近的视图合成工作中提出的多球体图像(由嵌套的同心球体组成的场景表示，根据从球体中心的反向深度采样)的概念有关(Attal等人，2020；Broxton等人，2020年)。</p> 
<p> </p> 
<p> </p> 
<p> </p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/93841548eb807648d8fe003fd977efb9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">IntelliJ IDEA插件的Jrebel激活踩坑【内网离线使用】</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7b8a79dd7eca7d7a72b273a4150b03f6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Doris 分区与分桶</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>