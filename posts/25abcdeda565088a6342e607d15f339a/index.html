<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>darknet下训练yolov3过程 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="darknet下训练yolov3过程" />
<meta property="og:description" content="1、首先得安装cygwin：
参考链接：https://blog.csdn.net/chunleixiahe/article/details/55666792
2、下载darknet：
链接：https://github.com/pjreddie/darknet.git
3、修改Makefile：
PU=1 # 是否打开GPU CUDNN=1 # 是否打开cudnn OPENCV=0　# 是否打开opencv OPENMP=0 DEBUG=1　# 是否进行debugARCH= -gencode arch=compute_61,code=compute_61　# 根据GPU计算能力选择对应数值（GTX 1080Ti:61、Tesla K80:37）官网查看：https://developer.nvidia.com/cuda-gpus...NVCC=/usr/local/cuda-9.0/bin/nvcc #修改nvcc路径...ifeq ($(GPU), 1)　#修改cuda路径--黄色部分，若不需要更改删除黄色部分即可COMMON&#43;= -DGPU -I/usr/local/cuda-9.0/includeCFLAGS&#43;= -DGPULDFLAGS&#43;= -L/usr/local/cuda-9.0/lib64 -lcuda -lcudart -lcublas -lcurandendif 打开cygwin程序，输入以下：
cd D:\python\darknet make 4、建立VOC2007数据集：参考我的前一篇文章
5、生成2007_train.txt、2007_test.txt和2007_val.txt：
建立my_label.py:
import xml.etree.ElementTree as ET import pickle import os from os import listdir, getcwd from os.path import join sets=[(&#39;2007&#39;, &#39;train&#39;), (&#39;2007&#39;, &#39;val&#39;), (&#39;2007&#39;, &#39;test&#39;)] classes = [&#34;cat&#34;]#修改成自己的类别 def convert(size, box): dw = 1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/25abcdeda565088a6342e607d15f339a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-04-22T15:30:12+08:00" />
<meta property="article:modified_time" content="2020-04-22T15:30:12+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">darknet下训练yolov3过程</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>1、首先得安装cygwin：</p> 
<p>参考链接：<a href="https://blog.csdn.net/chunleixiahe/article/details/55666792">https://blog.csdn.net/chunleixiahe/article/details/55666792</a></p> 
<p>2、下载darknet：</p> 
<p>链接：<a href="https://github.com/pjreddie/darknet.git">https://github.com/pjreddie/darknet.git</a></p> 
<p>3、修改Makefile：</p> 
<pre><code>PU=1 　　　# 是否打开GPU
CUDNN=1 　　# 是否打开cudnn
OPENCV=0　　# 是否打开opencv
OPENMP=0
DEBUG=1　　 # 是否进行debugARCH= -gencode arch=compute_61,code=compute_61　　# 根据GPU计算能力选择对应数值（GTX 1080Ti:61、Tesla K80:37）官网查看：https://developer.nvidia.com/cuda-gpus...NVCC=/usr/local/cuda-9.0/bin/nvcc 　　#修改nvcc路径...ifeq ($(GPU), 1)　　　　　　　#修改cuda路径--黄色部分，若不需要更改删除黄色部分即可COMMON+= -DGPU -I/usr/local/cuda-9.0/includeCFLAGS+= -DGPULDFLAGS+= -L/usr/local/cuda-9.0/lib64 -lcuda -lcudart -lcublas -lcurandendif</code></pre> 
<p>打开cygwin程序，输入以下：</p> 
<pre><code class="language-cpp">cd D:\python\darknet
make</code></pre> 
<p>4、建立VOC2007数据集：参考我的前一篇文章</p> 
<p>5、生成2007_train.txt、2007_test.txt和2007_val.txt：</p> 
<p>建立my_label.py:</p> 
<pre><code>import xml.etree.ElementTree as ET
import pickle
import os
from os import listdir, getcwd
from os.path import join

sets=[('2007', 'train'), ('2007', 'val'), ('2007', 'test')]

classes = ["cat"]#修改成自己的类别


def convert(size, box):
    dw = 1./(size[0])
    dh = 1./(size[1])
    x = (box[0] + box[1])/2.0 - 1
    y = (box[2] + box[3])/2.0 - 1
    w = box[1] - box[0]
    h = box[3] - box[2]
    x = x*dw
    w = w*dw
    y = y*dh
    h = h*dh
    return (x,y,w,h)

def convert_annotation(year, image_id):
    in_file = open('VOCdevkit/VOC%s/Annotations/%s.xml'%(year, image_id))
    out_file = open('VOCdevkit/VOC%s/labels/%s.txt'%(year, image_id), 'w')
    tree=ET.parse(in_file)
    root = tree.getroot()
    size = root.find('size')
    w = int(size.find('width').text)
    h = int(size.find('height').text)

    for obj in root.iter('object'):
        difficult = obj.find('difficult').text
        cls = obj.find('name').text
        if cls not in classes or int(difficult)==1:
            continue
        cls_id = classes.index(cls)
        xmlbox = obj.find('bndbox')
        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text))
        bb = convert((w,h), b)
        out_file.write(str(cls_id) + " " + " ".join([str(a) for a in bb]) + '\n')

wd = getcwd()

for year, image_set in sets:
    if not os.path.exists('VOCdevkit/VOC%s/labels/'%(year)):
        os.makedirs('VOCdevkit/VOC%s/labels/'%(year))
    image_ids = open('VOCdevkit/VOC%s/ImageSets/Main/%s.txt'%(year, image_set)).read().strip().split()
    list_file = open('%s_%s.txt'%(year, image_set), 'w')
    for image_id in image_ids:
        list_file.write('%s/VOCdevkit/VOC%s/JPEGImages/%s.jpg\n'%(wd, year, image_id))
        convert_annotation(year, image_id)
    list_file.close()

os.system("cat 2007_train.txt 2007_val.txt 2012_train.txt 2012_val.txt &gt; train.txt")
os.system("cat 2007_train.txt 2007_val.txt 2007_test.txt 2012_train.txt 2012_val.txt &gt; train.all.txt")

</code></pre> 
<p>或者修改scripts/vac_label.py文件</p> 
<p>将生成的2007_train.txt、2007_test.txt和2007_val.txt的“2007_"的去掉，将train.txt放入新建的mydata文件中。</p> 
<p>6、将darknet/cfg文件下的voc.data和yolov3-voc.cfg复制到darknet目录下：</p> 
<p>修改：</p> 
<p>voc.data:</p> 
<pre><code>classes=1
train=D:/python/darknet/mydata/train.txt
valid=D:/python/darknet/mydata/test.txt
names=D:/python/darknet/mydata/voc.names
backup=D:/python/darknet/backup</code></pre> 
<p>yolov3-voc.cfg:</p> 
<pre><code>[convolutional]
size=1
stride=1
pad=1
filters=18#修改
activation=linear

[yolo]
mask = 0,1,2
anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
classes=1#修改
num=9
jitter=.3
ignore_thresh = .5
truth_thresh = 1
random=0#修改看显存，1或者0</code></pre> 
<p>有三处yolo需要修改。</p> 
<pre><code>batch=64　　　　　　   # 一批训练样本的样本数量，每batch个样本更新一次参数
subdivisions=32　　   # 它会让你的每一个batch不是一下子都丢到网络里。而是分成subdivision对应数字的份数，一份一份的跑完后，在一起打包算作完成一次iteration
width=416　　　　     # 只可以设置成32的倍数
height=416　　　　    # 只可以设置成32的倍数

channels=3　　　　　　 # 若为灰度图，则chennels=1，另外还需修改/scr/data.c文件中的load_data_detection函数；若为RGB则 channels=3 ，无需修改/scr/data.c文件

momentum=0.9　　      # 最优化方法的动量参数，这个值影响着梯度下降到最优值得速度 
decay=0.0005　　   　 # 权重衰减正则项，防止过拟合
angle=0　　　　    　　# 通过旋转角度来生成更多训练样本
saturation = 1.5　　  # 通过调整饱和度来生成更多训练样本
exposure = 1.5　　    # 通过调整曝光量来生成更多训练样本
hue=.1　　　　　     　# 通过调整色调来生成更多训练样本


learning_rate=0.001        # 学习率, 刚开始训练时, 以 0.01 ~ 0.001 为宜, 一定轮数过后,逐渐减缓。
burn_in=1000　　　      　　 # 在迭代次数小于burn_in时，其学习率的更新有一种方式，大于burn_in时，才采用policy的更新方式
max_batches = 50200  　　   # 训练步数
policy=steps　　　　　 　　   # 学习率调整的策略
steps=40000,45000     　    # 开始衰减的步数
scales=.1,.1　　　　 　　   　# 在第40000和第45000次迭代时，学习率衰减10倍
...
[convolutional]——YOLO层前一层卷积层
...
filters=24 　　　　　　 　　  # 每一个[yolo]层前的最后一个卷积层中的 filters=num(yolo层个数)*(classes+5)
...

[yolo]
mask = 6,7,8
anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326　　#如果想修改默认anchors数值，使用k-means即可；
classes=3  　　　　　　# 修改为自己的类别数
num=9　　　　　　　   　# 每个grid cell预测几个box,和anchors的数量一致。调大num后训练时Obj趋近0的话可以尝试调大object_scale
jitter=.3　　　　　　　# 利用数据抖动产生更多数据, jitter是crop的参数, jitter=.3，就是在0~0.3中进行crop
ignore_thresh = .5   # 决定是否需要计算IOU误差的参数，大于thresh，IOU误差不会夹在cost function中
truth_thresh = 1
random=1　　　　　　　  # 如果为1，每次迭代图片大小随机从320到608，步长为32，如果为0，每次训练大小与输入大小一致
...</code></pre> 
<p>7、下载预训练模型：</p> 
<p>链接：<code>https:</code><code>//pjreddie.com/media/files/darknet53.conv.74</code></p> 
<p><code>8、新建voc.names:</code></p> 
<p>输入自己的类别：</p> 
<pre><code>cat</code></pre> 
<p>放入mydata文件目录下</p> 
<p> </p> 
<p><code>9、训练：</code></p> 
<pre><code>./darknet detector train voc.data yolov3-voc.cfg ./mydata/darknet53.conv.74


</code></pre> 
<p>10、测试：</p> 
<pre><code>./darknet detector test voc.data yolov3-voc.cfg backup/yolov3-voc_final.weights mydata/cat2.jpg</code></pre> 
<p>11、C++调用代码：</p> 
<pre><code>#include &lt;fstream&gt;
#include &lt;sstream&gt;
#include &lt;iostream&gt;
#include &lt;io.h&gt;
#include &lt;opencv2/dnn.hpp&gt;
#include &lt;opencv2/imgproc.hpp&gt;
#include &lt;opencv2/highgui.hpp&gt;
#include&lt;vector&gt;

using namespace std;
using namespace cv;
using namespace dnn;

vector&lt;string&gt; classes;

vector&lt;String&gt; getOutputsNames(Net&amp;net)
{
	static vector&lt;String&gt; names;
	if (names.empty())
	{
		//Get the indices of the output layers, i.e. the layers with unconnected outputs
		vector&lt;int&gt; outLayers = net.getUnconnectedOutLayers();

		//get the names of all the layers in the network
		vector&lt;String&gt; layersNames = net.getLayerNames();

		// Get the names of the output layers in names
		names.resize(outLayers.size());
		for (size_t i = 0; i &lt; outLayers.size(); ++i)
			names[i] = layersNames[outLayers[i] - 1];
	}
	return names;
}
void drawPred(int classId, float conf, int left, int top, int right, int bottom, Mat&amp; frame)
{
	//Draw a rectangle displaying the bounding box
	rectangle(frame, Point(left, top), Point(right, bottom), Scalar(255, 178, 50), 3);

	//Get the label for the class name and its confidence
	string label = format("%.5f", conf);
	if (!classes.empty())
	{
		CV_Assert(classId &lt; (int)classes.size());
		label = classes[classId] + ":" + label;
	}

	//Display the label at the top of the bounding box
	int baseLine;
	Size labelSize = getTextSize(label, FONT_HERSHEY_SIMPLEX, 0.5, 1, &amp;baseLine);
	top = max(top, labelSize.height);
	rectangle(frame, Point(left, top - round(1.5*labelSize.height)), Point(left + round(1.5*labelSize.width), top + baseLine), Scalar(255, 255, 255), FILLED);
	putText(frame, label, Point(left, top), FONT_HERSHEY_SIMPLEX, 0.75, Scalar(0, 0, 0), 1);
}
void postprocess(Mat&amp; frame, const vector&lt;Mat&gt;&amp; outs, float confThreshold, float nmsThreshold)
{
	vector&lt;int&gt; classIds;
	vector&lt;float&gt; confidences;
	vector&lt;Rect&gt; boxes;

	for (size_t i = 0; i &lt; outs.size(); ++i)
	{
		// Scan through all the bounding boxes output from the network and keep only the
		// ones with high confidence scores. Assign the box's class label as the class
		// with the highest score for the box.
		float* data = (float*)outs[i].data;
		for (int j = 0; j &lt; outs[i].rows; ++j, data += outs[i].cols)
		{
			Mat scores = outs[i].row(j).colRange(5, outs[i].cols);
			Point classIdPoint;
			double confidence;
			// Get the value and location of the maximum score
			minMaxLoc(scores, 0, &amp;confidence, 0, &amp;classIdPoint);
			if (confidence &gt; confThreshold)
			{
				int centerX = (int)(data[0] * frame.cols);
				int centerY = (int)(data[1] * frame.rows);
				int width = (int)(data[2] * frame.cols);
				int height = (int)(data[3] * frame.rows);
				int left = centerX - width / 2;
				int top = centerY - height / 2;

				classIds.push_back(classIdPoint.x);
				confidences.push_back((float)confidence);
				boxes.push_back(Rect(left, top, width, height));
			}
		}
	}

	// Perform non maximum suppression to eliminate redundant overlapping boxes with
	// lower confidences
	vector&lt;int&gt; indices;
	NMSBoxes(boxes, confidences, confThreshold, nmsThreshold, indices);
	for (size_t i = 0; i &lt; indices.size(); ++i)
	{
		int idx = indices[i];
		Rect box = boxes[idx];
		drawPred(classIds[idx], confidences[idx], box.x, box.y,
			box.x + box.width, box.y + box.height, frame);
	}
}

int main()
{
	string names_file = "D:\\python\\darknet\\mydata\\voc.names";
	String model_def = "D:\\python\\darknet\\yolov3-voc.cfg";
	String weights = "D:\\python\\darknet\\backup\\yolov3-voc_final.weights";

	int in_w, in_h;
	double thresh = 0.1;
	double nms_thresh = 0.02;
	in_w = in_h = 416;

	string img_path = "D:\\python\\darknet\\mydata\\cat2.jpg";

	//read names

	ifstream ifs(names_file.c_str());
	string line;
	while (getline(ifs, line)) classes.push_back(line);

	//init model
	Net net = readNetFromDarknet(model_def, weights);
	net.setPreferableBackend(DNN_BACKEND_OPENCV);
	net.setPreferableTarget(DNN_TARGET_CPU);

	//read image and forward

	Mat frame, blob;
	if ((_access(img_path.c_str(), 0)) == -1)
	{
		cerr &lt;&lt; "file: " &lt;&lt; img_path.c_str() &lt;&lt; " not exist" &lt;&lt; endl;
		return -1;
	}
	frame = imread(img_path);
	// Create a 4D blob from a frame.

	blobFromImage(frame, blob, 1 / 255.0, Size(in_w, in_h), Scalar(), true, false);

	vector&lt;Mat&gt; mat_blob;
	imagesFromBlob(blob, mat_blob);

	//Sets the input to the network
	net.setInput(blob);

	// Runs the forward pass to get output of the output layers
	vector&lt;Mat&gt; outs;
	net.forward(outs, getOutputsNames(net));

	postprocess(frame, outs, thresh, nms_thresh);

	vector&lt;double&gt; layersTimes;
	double freq = getTickFrequency() / 1000;
	double t = net.getPerfProfile(layersTimes) / freq;
	string label = format("Inference time for a frame : %.2f ms", t);
	putText(frame, label, Point(0, 15), FONT_HERSHEY_SIMPLEX, 0.5, Scalar(0, 0, 255));

	imshow("res", frame);

	waitKey(0);
}
</code></pre> 
<p>遇到问题解决：</p> 
<p><a href="https://blog.csdn.net/weixin_41931110/article/details/89360225?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-10&amp;utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-10">https://blog.csdn.net/weixin_41931110/article/details/89360225?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-10&amp;utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-10</a></p> 
<p>训练过程中的参数含义</p> 
<p><img alt="" src="https://images2.imgbox.com/6d/67/bRKo7nZT_o.png"></p> 
<p>       其中 Region 82/94/106表示的是 3个不同feature map的 detector。对于每一个detector共有 subdivisions行数据，每一个行表示的当前batch中 batch/subdivisions个样本的计算结果。 含义分别是： Avg IOU: 当前 batch/subdivisions个样本的平均IOU值（-nan表示当前detector没有预测的物体，只要三个detector不全为-nan就没问题），Class: 是某类物体的概率， Obj: 存在物体的概率， No obj: 不存在物体的概率（这里由于yolo首先预测是否存在物体，同时给出每一个物体的概率），0.5R表示IOU值为0.5的召回率，0.75R表示IOU阈值为0.75时的召回率， count: 表示当前 batch/subdivisions个样本中包含物体的个数。</p> 
<p>       黄色部分表示的当前的iteration(按batch计算),  第二个是当前的 loss， 第三个是平均loss，第四个是当前的学习率，</p> 
<p>第五个是当前batch训练花费的时间，第6个是当前训练过的总图像数目。</p> 
<p> </p> 
<p> </p> 
<p>另一种C++代码：</p> 
<pre><code>#include&lt;stdlib.h&gt;
#include&lt;Windows.h&gt;

#include &lt;fstream&gt;
#include &lt;sstream&gt;
#include &lt;iostream&gt;

#include&lt;opencv2\opencv.hpp&gt;
#include&lt;opencv2\dnn.hpp&gt;
#include&lt;opencv2\imgproc.hpp&gt;

using namespace cv;
using namespace std;
using namespace dnn;

vector&lt;string&gt; classes;

vector&lt;String&gt; getOutputsNames(Net&amp; net)
{
	static vector&lt;String&gt; names;
	if (names.empty())
	{
		//返回加载模型中所有层的输入和输出形状(shape)
		vector&lt;int&gt; outLayers = net.getUnconnectedOutLayers();

		//get the names of all the layers in the network
		vector&lt;String&gt; layersNames = net.getLayerNames();

		// Get the names of the output layers in names
		names.resize(outLayers.size());
		for (size_t i = 0; i &lt; outLayers.size(); ++i)
			names[i] = layersNames[outLayers[i] - 1];
	}
	return names;
}

// Draw the predicted bounding box 绘出框
void drawPred(int classId, float conf, int left, int top, int right, int bottom, Mat&amp; frame)
{
	//Draw a rectangle displaying the bounding box 绘制矩形
	rectangle(frame, Point(left, top), Point(right, bottom), Scalar(0, 0, 255));

	//Get the label for the class name and its confidence
	string label = format("%.2f", conf);//分类标签及其置信度
	//若存在类别标签，读取对应的标签
	if (!classes.empty())
	{
		CV_Assert(classId &lt; (int)classes.size());
		label = classes[classId] + ":" + label;
	}

	//Display the label at the top of the bounding box
	int baseLine;
	Size labelSize = getTextSize(label, FONT_HERSHEY_SIMPLEX, 0.5, 1, &amp;baseLine);
	top = max(top, labelSize.height);
	//绘制框上文字
	putText(frame, label, Point(left, top), FONT_HERSHEY_SIMPLEX, 0.5, Scalar(255, 255, 255));
}

// Remove the bounding boxes with low confidence using non-maxima suppression
void postprocess(Mat&amp; frame, const vector&lt;Mat&gt;&amp; outs, double confThreshold, double nmsThreshold)
{
	vector&lt;int&gt; classIds;
	vector&lt;float&gt; confidences;
	vector&lt;Rect&gt; boxes;

	for (size_t i = 0; i &lt; outs.size(); ++i)
	{
		// Scan through all the bounding boxes output from the network and keep only the
		// ones with high confidence scores. Assign the box's class label as the class
		// with the highest score for the box.
		float* data = (float*)outs[i].data;
		for (int j = 0; j &lt; outs[i].rows; ++j, data += outs[i].cols)
		{
			int a = outs[i].cols;//中心坐标+框的宽高+置信度+分为各个类别分数=2+2+1+1
			int b = outs[i].rows;//框的个数1083
			Mat scores = outs[i].row(j).colRange(5, outs[i].cols);//取当前框的第六列到最后一列，即该框被分为80个类别，各个类别的评分
			Point classIdPoint;
			double confidence;
			// Get the value and location of the maximum score
			minMaxLoc(scores, 0, &amp;confidence, 0, &amp;classIdPoint);//找出最大评分的类别
			if (confidence &gt; confThreshold)//置信度阈值
			{
				int centerX = (int)(data[0] * frame.cols);
				int centerY = (int)(data[1] * frame.rows);
				int width = (int)(data[2] * frame.cols);
				int height = (int)(data[3] * frame.rows);
				int left = centerX - width / 2;
				int top = centerY - height / 2;

				classIds.push_back(classIdPoint.x);
				confidences.push_back((float)confidence);
				boxes.push_back(Rect(left, top, width, height));
			}
		}
	}

	// Perform non maximum suppression to eliminate redundant overlapping boxes with
	// lower confidences
	vector&lt;int&gt; indices;
	NMSBoxes(boxes, confidences, confThreshold, nmsThreshold, indices);//框、置信度、置信度阈值、非极大值抑制阈值、指标（输出）
	for (size_t i = 0; i &lt; indices.size(); ++i)
	{
		int idx = indices[i];//框序号
		Rect box = boxes[idx];//框的坐标（矩形区域）
		//drawPred(classIds[idx], confidences[idx], box.x, box.y,
			//box.x + box.width, box.y + box.height, frame);
		//rectangle(frame, Rect(box.x, box.y, box.x + box.width, box.y + box.height), Scalar(0, 0, 255), 1.0);
		rectangle(frame, Rect(box.x, box.y, box.width, box.height), Scalar(0, 0, 255), 1.0);
	}

	imwrite("./output.jpg", frame);
}

int main() {

	//Mat src = imread("");
	//Mat src = imread("D://DeepLearningCshape//workspace//data//test1.jpg");
	//imshow("Src.jpg", src);

	/*Mat gray;
	cvtColor(src,gray,6);
	imshow("gray.jpg", gray);*/

	const char* darknet_weight_name = "D://python//darknet//yolov3.weights";
	const char* darknet_cfg_name = "D://python//darknet//cfg//yolov3-voc.cfg";
	const char* darknet_data_name = "D://python//darknet//data//voc.names";

	classes.push_back("aquaculture");

	dnn::Net net_darknet = readNetFromDarknet(darknet_cfg_name, darknet_weight_name);
	if (net_darknet.empty()) {
		cout &lt;&lt; "Can't load the net" &lt;&lt; endl;
		return -1;
	}
	cout &lt;&lt; "Load the net successfully..." &lt;&lt; endl;
	//dnn::blobFromImage


	/*要求网络在支持的地方使用特定的计算后端
	*如果opencv是用intel的推理引擎库编译的，那么dnn_backend_default意味着dnn_backend_interrusion_引擎
	*否则等于dnn_backend_opencv。*/

	net_darknet.setPreferableBackend(DNN_BACKEND_OPENCV);
	//要求网络对特定目标设备进行计算
	net_darknet.setPreferableTarget(DNN_TARGET_CPU);

	Mat testimage = imread("D://python//darknet//mydata//cat2.jpg");
	imshow("srcimage.jpg", testimage);

	//std::vector&lt;Mat&gt; frames;
	//frames.push_back(testimage);

	/*Mat inputBlob = blobFromImage(testimage,1.0, Size(500, 860), Scalar(), true, false);
	imshow("blobimage.jpg", inputBlob);*/

	Mat inputBlob;
	blobFromImage(testimage, inputBlob, 1 / 255.0, Size(416, 416), Scalar(), true, false);  //不知道为什么这个Mat不能显示，存储数据不对
	//imshow("blobimage.jpg", inputBlob);
	//net_darknet.setInput(inputBlob, "data"); 
	net_darknet.setInput(inputBlob);

	//4.检测和显示
	//获得“dectection_out"的输出
	vector&lt;Mat&gt; outs;
	net_darknet.forward(outs, getOutputsNames(net_darknet));

	//阈值置信度和nms阈值
	postprocess(testimage, outs, 0.000000001, 0.9);
	
	waitKey();
	Sleep(1000); //1000微妙，为1s
	return 0;
}
</code></pre> 
<p>opencv4.2调用模型不成功原因：<a href="https://blog.csdn.net/avideointerfaces/article/details/90475694?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase">https://blog.csdn.net/avideointerfaces/article/details/90475694?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase</a></p> 
<p> </p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b23b016909420bfcf59f0861a64b7fb4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【opencv学习】相位滤波程序编写中遇到的许多小问题</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/98bd30808442ed1915d9c62a146b81db/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">告别矩阵繁琐运算，让c&#43;&#43;程序助你轻松求矩阵的逆</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>