<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>å´æ©è¾¾æ·±åº¦å­¦ä¹ å­¦ä¹ ç¬”è®°â€”â€”C4W3â€”â€”ç›®æ ‡æ£€æµ‹â€”â€”ä½œä¸šâ€”â€”è‡ªåŠ¨é©¾é©¶ä¹‹è½¦è¾†æ£€æµ‹ - ç¼–ç¨‹éšæƒ³</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="å´æ©è¾¾æ·±åº¦å­¦ä¹ å­¦ä¹ ç¬”è®°â€”â€”C4W3â€”â€”ç›®æ ‡æ£€æµ‹â€”â€”ä½œä¸šâ€”â€”è‡ªåŠ¨é©¾é©¶ä¹‹è½¦è¾†æ£€æµ‹" />
<meta property="og:description" content="è¿™é‡Œä¸»è¦æ¢³ç†ä¸€ä¸‹ä½œä¸šçš„ä¸»è¦å†…å®¹å’Œæ€è·¯ï¼Œå®Œæ•´ä½œä¸šæ–‡ä»¶å¯å‚è€ƒ:
https://github.com/pandenghuang/Andrew-Ng-Deep-Learning-notes/tree/master/assignments/C4W3/Assignment/Car_detection_for_Autonomous_Driving
ä½œä¸šå®Œæ•´æˆªå›¾ï¼Œå‚è€ƒæœ¬æ–‡ç»“å°¾ï¼šä½œä¸šå®Œæ•´æˆªå›¾ã€‚
Autonomous driving - Car detectionï¼ˆè‡ªåŠ¨é©¾é©¶ä¹‹è½¦è¾†æ£€æµ‹ï¼‰ Welcome to your week 3 programming assignment. You will learn about object detection using the very powerful YOLO model. Many of the ideas in this notebook are described in the two YOLO papers: Redmon et al., 2016 (https://arxiv.org/abs/1506.02640) and Redmon and Farhadi, 2016 (https://arxiv.org/abs/1612.08242).
You will learn to:
Use object detection on a car detection datasetDeal with bounding boxes ...
1 - Problem Statementï¼ˆé—®é¢˜é™ˆè¿°ï¼‰ You are working on a self-driving car." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/11da48e7beb70dc849c7ea77e50b4c64/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-02-10T16:23:50+08:00" />
<meta property="article:modified_time" content="2021-02-10T16:23:50+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="ç¼–ç¨‹éšæƒ³" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">ç¼–ç¨‹éšæƒ³</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">å´æ©è¾¾æ·±åº¦å­¦ä¹ å­¦ä¹ ç¬”è®°â€”â€”C4W3â€”â€”ç›®æ ‡æ£€æµ‹â€”â€”ä½œä¸šâ€”â€”è‡ªåŠ¨é©¾é©¶ä¹‹è½¦è¾†æ£€æµ‹</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>è¿™é‡Œä¸»è¦æ¢³ç†ä¸€ä¸‹ä½œä¸šçš„ä¸»è¦å†…å®¹å’Œæ€è·¯ï¼Œå®Œæ•´ä½œä¸šæ–‡ä»¶å¯å‚è€ƒ:</p> 
<p><a href="https://github.com/pandenghuang/Andrew-Ng-Deep-Learning-notes/tree/master/assignments/C4W3/Assignment/Car_detection_for_Autonomous_Driving">https://github.com/pandenghuang/Andrew-Ng-Deep-Learning-notes/tree/master/assignments/C4W3/Assignment/Car_detection_for_Autonomous_Driving</a></p> 
<p>ä½œä¸šå®Œæ•´æˆªå›¾ï¼Œå‚è€ƒæœ¬æ–‡ç»“å°¾ï¼šä½œä¸šå®Œæ•´æˆªå›¾ã€‚</p> 
<h2 id="Autonomous-driving---Car-detection">Autonomous driving - Car detectionï¼ˆè‡ªåŠ¨é©¾é©¶ä¹‹è½¦è¾†æ£€æµ‹ï¼‰</h2> 
<p>Welcome to your week 3 programming assignment. You will learn about object detection using the very powerful YOLO model. Many of the ideas in this notebook are described in the two YOLO papers: Redmon et al., 2016 (<a href="https://arxiv.org/abs/1506.02640" rel="nofollow">https://arxiv.org/abs/1506.02640</a>) and Redmon and Farhadi, 2016 (<a href="https://arxiv.org/abs/1612.08242" rel="nofollow">https://arxiv.org/abs/1612.08242</a>).</p> 
<p><strong>You will learn to</strong>:</p> 
<ul><li>Use object detection on a car detection dataset</li><li>Deal with bounding boxes</li></ul> 
<p>...</p> 
<h3 id="1---Problem-Statement">1 - Problem Statementï¼ˆé—®é¢˜é™ˆè¿°ï¼‰</h3> 
<p>You are working on a self-driving car. As a critical component of this project, you'd like to first build a car detection system. To collect data, you've mounted a camera to the hood (meaning the front) of the car, which takes pictures of the road ahead every few seconds while you drive around.</p> 
<p>You've gathered all these images into a folder and have labelled them by drawing bounding boxes around every car you found. Here's an example of what your bounding boxes look like.</p> 
<p>If you have 80 classes that you want YOLO to recognize, you can represent the class labelÂ ğ‘cÂ either as an integer from 1 to 80, or as an 80-dimensional vector (with 80 numbers) one component of which is 1 and the rest of which are 0. The video lectures had used the latter representation; in this notebook, we will use both representations, depending on which is more convenient for a particular step.</p> 
<p>In this exercise, you will learn how YOLO works, then apply it to car detection. Because the YOLO model is very computationally expensive to train, we will load pre-trained weights for you to use.</p> 
<h3 id="2---YOLO">2 - YOLOï¼ˆYOLOç®—æ³•ï¼‰</h3> 
<p>YOLO ("you only look once") is a popular algoritm because it achieves high accuracy while also being able to run in real-time. This algorithm "only looks once" at the image in the sense that it requires only one forward propagation pass through the network to make predictions. After non-max suppression, it then outputs recognized objects together with the bounding boxes.</p> 
<h4 id="2.1---Model-details">2.1 - Model detailsï¼ˆæ¨¡å‹è¯¦è¿°ï¼‰</h4> 
<p>First things to know:</p> 
<ul><li>TheÂ <strong>input</strong>Â is a batch of images of shape (m, 608, 608, 3)</li><li>TheÂ <strong>output</strong>Â is a list of bounding boxes along with the recognized classes. Each bounding box is represented by 6 numbersÂ (ğ‘ğ‘,ğ‘ğ‘¥,ğ‘ğ‘¦,ğ‘â„,ğ‘ğ‘¤,ğ‘) as explained above. If you expandÂ ğ‘Â into an 80-dimensional vector, each bounding box is then represented by 85 numbers.</li></ul> 
<p>We will use 5 anchor boxes. So you can think of the YOLO architecture as the following: IMAGE (m, 608, 608, 3) -&gt; DEEP CNN -&gt; ENCODING (m, 19, 19, 5, 85).</p> 
<p>Lets look in greater detail at what this encoding represents.</p> 
<p><img alt="" height="613" src="https://images2.imgbox.com/a3/42/rDN70Cmw_o.png" width="1126"></p> 
<p><u><strong>Figure 2</strong></u>:Â <strong>Encoding architecture for YOLO</strong></p> 
<p>If the center/midpoint of an object falls into a grid cell, that grid cell is responsible for detecting that object.</p> 
<p>...</p> 
<h4 id="2.2---Filtering-with-a-threshold-on-class-scores">2.2 - Filtering with a threshold on class scoresï¼ˆç­›é€‰ç›®æ ‡ç±»ï¼‰</h4> 
<p>You are going to apply a first filter by thresholding. You would like to get rid of any box for which the class "score" is less than a chosen threshold.</p> 
<p>The model gives you a total of 19x19x5x85 numbers, with each box described by 85 numbers. It'll be convenient to rearrange the (19,19,5,85) (or (19,19,425)) dimensional tensor into the following variables:</p> 
<p>...</p> 
<h4 id="2.3---Non-max-suppression">2.3 - Non-max suppressionï¼ˆéæå¤§å€¼æŠ‘åˆ¶ï¼‰</h4> 
<p>Even after filtering by thresholding over the classes scores, you still end up a lot of overlapping boxes. A second filter for selecting the right boxes is called non-maximum suppression (NMS).</p> 
<p>...</p> 
<h4 id="2.4-Wrapping-up-the-filtering">2.4 Wrapping up the filteringï¼ˆä½¿ç”¨æ»¤æ³¢å™¨ï¼‰</h4> 
<p>It's time to implement a function taking the output of the deep CNN (the 19x19x5x85 dimensional encoding) and filtering through all the boxes using the functions you've just implemented.</p> 
<p>...</p> 
<p><strong>Summary for YOLO</strong>:ï¼ˆYOLOç®—æ³•å°ç»“ï¼‰<br> - Input image (608, 608, 3)<br> - The input image goes through a CNN, resulting in a (19,19,5,85) dimensional output.<br> - After flattening the last two dimensions, the output is a volume of shape (19, 19, 425):</p> 
<ul><li>Each cell in a 19x19 grid over the input image gives 425 numbers.</li><li>425 = 5 x 85 because each cell contains predictions for 5 boxes, corresponding to 5 anchor boxes, as seen in lecture.</li><li>85 = 5 + 80 where 5 is becauseÂ (ğ‘ğ‘,ğ‘ğ‘¥,ğ‘ğ‘¦,ğ‘â„,ğ‘ğ‘¤) has 5 numbers, and and 80 is the number of classes we'd like to detect</li></ul> 
<p>- You then select only few boxes based on:</p> 
<ul><li>Score-thresholding: throw away boxes that have detected a class with a score less than the threshold</li><li>Non-max suppression: Compute the Intersection over Union and avoid selecting overlapping boxes</li></ul> 
<p>- This gives you YOLO's final output.</p> 
<p>...</p> 
<h3 id="3---Test-YOLO-pretrained-model-on-images">3 - Test YOLO pretrained model on imagesï¼ˆä½¿ç”¨å›¾ç‰‡æµ‹è¯•YOLOé¢„è®­ç»ƒæ¨¡å‹ï¼‰</h3> 
<p>In this part, you are going to use a pretrained model and test it on the car detection dataset. As usual, you start byÂ <strong>creating a session to start your graph</strong>. Run the following cell.</p> 
<p>...</p> 
<h4 id="3.1---Defining-classes,-anchors-and-image-shape.">3.1 - Defining classes, anchors and image shape.ï¼ˆå®šä¹‰ç±»ã€é”šæ¡†å’Œå›¾åƒå°ºå¯¸ï¼‰</h4> 
<p>Recall that we are trying to detect 80 classes, and are using 5 anchor boxes. We have gathered the information about the 80 classes and 5 boxes in two files "coco_classes.txt" and "yolo_anchors.txt". Let's load these quantities into the model by running the next cell.</p> 
<p>The car detection dataset has 720x1280 images, which we've pre-processed into 608x608 images.</p> 
<p>...</p> 
<h4 id="3.2---Loading-a-pretrained-model">3.2 - Loading a pretrained modelï¼ˆåŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼‰</h4> 
<p>Training a YOLO model takes a very long time and requires a fairly large dataset of labelled bounding boxes for a large range of target classes. You are going to load an existing pretrained Keras YOLO model stored in "yolo.h5". (These weights come from the official YOLO website, and were converted using a function written by Allan Zelener. References are at the end of this notebook. Technically, these are the parameters from the "YOLOv2" model, but we will more simply refer to it as "YOLO" in this notebook.) Run the cell below to load the model from this file.</p> 
<p>...</p> 
<h4 id="3.3---Convert-output-of-the-model-to-usable-bounding-box-tensors">3.3 - Convert output of the model to usable bounding box tensorsï¼ˆå°†æ¨¡å‹è¾“å‡ºè½¬æ¢ä¸ºè¾¹ç•Œæ¡†å¼ é‡ï¼‰</h4> 
<p>The output ofÂ <code>yolo_model</code>Â is a (m, 19, 19, 5, 85) tensor that needs to pass through non-trivial processing and conversion. The following cell does that for you.</p> 
<p>...</p> 
<h4 id="3.4---Filtering-boxes">3.4 - Filtering boxesï¼ˆç­›é€‰è¾¹ç•Œæ¡†ï¼‰</h4> 
<p><code>yolo_outputs</code>Â gave you all the predicted boxes ofÂ <code>yolo_model</code>Â in the correct format.</p> 
<p>...</p> 
<h4 id="3.5---Run-the-graph-on-an-image" style="margin-left:0px;"><strong><span style="color:#000000;">3.5 - Run the graph on an imageï¼ˆåœ¨å›¾åƒä¸Šç”»å‡ºè¾¹ç•Œæ¡†ï¼‰</span></strong></h4> 
<p style="margin-left:0px;"><span style="color:#000000;">Let the fun begin. You have created a (<code>sess</code>) graph that can be summarized as follows:</span></p> 
<ol><li><span style="color:#800080;">yolo_model.inputÂ </span>is given toÂ <code>yolo_model</code>. The model is used to compute the outputÂ <span style="color:#800080;">yolo_model.output</span></li><li><span style="color:#800080;">yolo_model.outputÂ </span>is processed byÂ <code>yolo_head</code>. It gives youÂ <span style="color:#800080;">yolo_outputs</span></li><li><span style="color:#800080;">yolo_outputsÂ </span>goes through a filtering function,Â <code>yolo_eval</code>. It outputs your predictions:Â <span style="color:#800080;">scores, boxes, classes</span></li></ol> 
<p><span style="color:#800080;">...</span></p> 
<p><strong>What you should remember</strong>: - YOLO is a state-of-the-art object detection model that is fast and accurate<br> - It runs an input image through a CNN which outputs a 19x19x5x85 dimensional volume.<br> - The encoding can be seen as a grid where each of the 19x19 cells contains information about 5 boxes.<br> - You filter through all the boxes using non-max suppression. Specifically:</p> 
<ul><li>- Score thresholding on the probability of detecting a class to keep only accurate (high probability) boxes</li><li>- Intersection over Union (IoU) thresholding to eliminate overlapping boxes</li></ul> 
<p>- Because training a YOLO model from randomly initialized weights is non-trivial and requires a large dataset as well as lot of computation, we used previously trained model parameters in this exercise. If you wish, you can also try fine-tuning the YOLO model with your own dataset, though this would be a fairly non-trivial exercise.ã€‚</p> 
<p><strong>References</strong>: The ideas presented in this notebook came primarily from the two YOLO papers. The implementation here also took significant inspiration and used many components from Allan Zelener's github repository. The pretrained weights used in this exercise came from the official YOLO website.</p> 
<ul><li>Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi -Â <a href="https://arxiv.org/abs/1506.02640" rel="nofollow">You Only Look Once: Unified, Real-Time Object Detection</a>Â (2015)</li><li>Joseph Redmon, Ali Farhadi -Â <a href="https://arxiv.org/abs/1612.08242" rel="nofollow">YOLO9000: Better, Faster, Stronger</a>Â (2016)</li><li>Allan Zelener -Â <a href="https://github.com/allanzelener/YAD2K">YAD2K: Yet Another Darknet 2 Keras</a></li><li>The official YOLO website (<a href="https://pjreddie.com/darknet/yolo/" rel="nofollow">https://pjreddie.com/darknet/yolo/</a>)</li></ul> 
<h2>ä½œä¸šå®Œæ•´æˆªå›¾ï¼š</h2> 
<p>Â </p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9693f902e7b251356583429d0088c097/" rel="prev">
			<span class="pager__subtitle">Â«&thinsp;Previous</span>
			<p class="pager__title">æ·±å…¥ç†è§£javascriptåŸå‹å’Œé—­åŒ…ï¼ˆ19ï¼‰â€”â€”è¡¥å……ï¼šthis</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d0675c945c66efa67899f398f36baf5c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;Â»</span>
			<p class="pager__title">Qt Designer UI ä¸­çš„è®¾è®¡æ¨¡å¼</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 ç¼–ç¨‹éšæƒ³.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>