<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>PySpark基础入门（1）：基础概念＋环境搭建 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="PySpark基础入门（1）：基础概念＋环境搭建" />
<meta property="og:description" content="目录
Spark基础入门
spark基础概念
spark架构
Spark环境搭建
local模式
Standalone 模式
Spark On YARN 模式
PySpark开发环境搭建
Python On Spark 执行原理 更好的阅读体验：PySpark基础入门（1）：基础概念＋环境搭建 - 掘金 (juejin.cn)
Spark基础入门 版本：Spark3.2.0
特性：完善了对Pandas API的支持
spark基础概念 Apache Spark是用于大规模数据处理的统一分析引擎 Spark 的核心数据结构：弹性分布式数据集（RDD），支持在大规模集群中的内存计算Spark 借鉴了 MapReduce 思想发展而来，保留了其分布式并行计算的优点并改进了其明显的缺陷。让中间数据存储在内存中提高了运行速度、并提供丰富的操作数据的API提高了开发速度 如何理解“统一分析引擎”？ spark可以对任意类型的数据进行自定义计算，比如说结构化、半结构化、非结构化等各种类型的数据结构；spark支持使用多种语言，如Python、Java、Scala、R以及SQL语言去开发应用程序计算数据 spark和hadoop的对比
在计算层面，Spark相比较MR（MapReduce）有巨大的性能优势Spark仅做计算，而Hadoop生态圈不仅有计算（MR）也有存储（HDFS）和资源管理调度（YARN），HDFS和YARN仍是许多大数据体系的核心架构Spark处理数据与MapReduce处理数据的不同点： Spark处理数据时，可以将中间处理结果数据存储到内存中Spark 提供了非常丰富的算子(API), 可以做到复杂任务在一个Spark 程序中完成 *Hadoop的基于进程的计算和Spark基于线程方式优缺点
Hadoop中的MR中每个map/reduce task都是一个java进程方式运行，好处在于进程之间是互相独立的，每个task独享进程资源，没有互相干扰，监控方便，但是问题在于task之间不方便共享数据，执行效率比较低。比如多个map task读取不同数据源文件需要将数据源加载到每个map task中，造成重复加载和浪费内存。而基于线程的方式计算是为了数据共享和提高执行效率，Spark采用了线程的最小的执行单位，但缺点是线程之间会有资源竞争
spark的特点
速度快：Spark支持内存计算，并且通过DAG（有向无环图）执行引擎支持无环数据流通用性强： 在 Spark 的基础上，Spark 还提供了包括Spark SQL、Spark Streaming、MLib 及GraphX在内的多个工具库，我们可以在一个应用中无缝地使用这些工具库Spark 支持多种运行方式，包括在 Hadoop 和 Mesos 上，也支持 Standalone的独立运行模式，同时也可以运行在云Kubernetes上（2.3之后）Spark 支持从HDFS、HBase、Cassandra 及 Kafka 等多种途径获取数据 spark框架模块
Spark Core：Spark的核心，Spark核心功能均由Spark Core模块提供，是Spark运行的基础。Spark Core以RDD为数据抽象，提供Python、Java、Scala、R语言的API，可以编程进行海量离线数据批处理计算。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/118b82c0e3aa2f35497408f07a3ea8f6/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-01T19:11:05+08:00" />
<meta property="article:modified_time" content="2023-05-01T19:11:05+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">PySpark基础入门（1）：基础概念＋环境搭建</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="Spark%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8-toc" style="margin-left:40px;"><a href="#Spark%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8" rel="nofollow">Spark基础入门</a></p> 
<p id="spark%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5-toc" style="margin-left:80px;"><a href="#spark%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5" rel="nofollow">spark基础概念</a></p> 
<p id="spark%E6%9E%B6%E6%9E%84-toc" style="margin-left:80px;"><a href="#spark%E6%9E%B6%E6%9E%84" rel="nofollow">spark架构</a></p> 
<p id="Spark%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA-toc" style="margin-left:40px;"><a href="#Spark%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA" rel="nofollow">Spark环境搭建</a></p> 
<p id="local%E6%A8%A1%E5%BC%8F-toc" style="margin-left:80px;"><a href="#local%E6%A8%A1%E5%BC%8F" rel="nofollow">local模式</a></p> 
<p id="Standalone%20%E6%A8%A1%E5%BC%8F-toc" style="margin-left:80px;"><a href="#Standalone%20%E6%A8%A1%E5%BC%8F" rel="nofollow">Standalone 模式</a></p> 
<p id="Spark%20On%20YARN%20%E6%A8%A1%E5%BC%8F-toc" style="margin-left:80px;"><a href="#Spark%20On%20YARN%20%E6%A8%A1%E5%BC%8F" rel="nofollow">Spark On YARN 模式</a></p> 
<p id="PySpark%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA-toc" style="margin-left:80px;"><a href="#PySpark%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA" rel="nofollow">PySpark开发环境搭建</a></p> 
<p id="Python%20On%20Spark%20%E6%89%A7%E8%A1%8C%E5%8E%9F%E7%90%86-toc" style="margin-left:80px;"><a href="#Python%20On%20Spark%20%E6%89%A7%E8%A1%8C%E5%8E%9F%E7%90%86" rel="nofollow">Python On Spark 执行原理</a> </p> 
<p> </p> 
<p>更好的阅读体验：<a href="https://juejin.cn/post/7228162548901904440" rel="nofollow" title="PySpark基础入门（1）：基础概念＋环境搭建 - 掘金 (juejin.cn)">PySpark基础入门（1）：基础概念＋环境搭建 - 掘金 (juejin.cn)</a></p> 
<p> </p> 
<h3 id="Spark%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8">Spark基础入门</h3> 
<p>版本：Spark3.2.0</p> 
<p>特性：完善了对Pandas API的支持</p> 
<h4 id="spark%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5">spark基础概念</h4> 
<ul><li>Apache Spark是用于大规模数据处理的统一分析引擎</li></ul> 
<ul><li> 
  <ul><li>Spark 的核心数据结构：弹性分布式数据集（RDD），支持在大规模集群中的内存计算</li><li>Spark 借鉴了 MapReduce 思想发展而来，保留了其<strong>分布式并行计算</strong>的优点并改进了其明显的缺陷。让<strong>中间数据存储在内存中提高了运行速度</strong>、并提供丰富的操作数据的API提高了开发速度</li></ul></li></ul> 
<ul><li>如何理解“统一分析引擎”？</li></ul> 
<ul><li> 
  <ul><li>spark可以对<strong>任意类型的数据</strong>进行自定义计算，比如说结构化、半结构化、非结构化等各种类型的数据结构；</li><li>spark支持使用多种语言，如Python、Java、Scala、R以及SQL语言去开发应用程序计算数据</li></ul></li></ul> 
<p>spark和hadoop的对比</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/d4/a2/WhV2tDLF_o.png"></p> 
<ul><li>在计算层面，Spark相比较MR（MapReduce）有巨大的性能优势</li><li><strong>Spark仅做计算</strong>，而Hadoop生态圈不仅有计算（MR）也有存储（HDFS）和资源管理调度（YARN），HDFS和YARN仍是许多大数据体系的核心架构</li><li>Spark处理数据与MapReduce处理数据的不同点：</li></ul> 
<ul><li> 
  <ul><li>Spark处理数据时，可以将中间处理结果数据存储到内存中</li><li>Spark 提供了非常丰富的算子(API), 可以做到复杂任务在一个Spark 程序中完成</li></ul></li></ul> 
<p>*Hadoop的基于进程的计算和Spark基于线程方式优缺点</p> 
<p>Hadoop中的MR中每个map/reduce task都是一个java进程方式运行，好处在于进程之间是互相独立的，每个task独享进程资源，没有互相干扰，监控方便，但是问题在于task之间不方便共享数据，执行效率比较低。比如多个map task读取不同数据源文件需要将数据源加载到每个map task中，造成重复加载和浪费内存。而基于线程的方式计算是为了数据共享和提高执行效率，Spark采用了线程的最小的执行单位，但缺点是线程之间会有资源竞争</p> 
<p>spark的特点</p> 
<ol><li>速度快：Spark支持内存计算，并且通过DAG（有向无环图）执行引擎支持无环数据流</li><li>通用性强：</li></ol> 
<ol><li> 
  <ol><li>在 Spark 的基础上，Spark 还提供了包括Spark SQL、Spark Streaming、MLib 及GraphX在内的多个工具库，我们可以在一个应用中无缝地使用这些工具库</li><li>Spark 支持多种运行方式，包括在 Hadoop 和 Mesos 上，也支持 Standalone的独立运行模式，同时也可以运行在云Kubernetes上（2.3之后）</li><li>Spark 支持从HDFS、HBase、Cassandra 及 Kafka 等多种途径获取数据</li></ol></li></ol> 
<p>spark框架模块</p> 
<p><strong>Spark Core</strong>：Spark的核心，Spark核心功能均由Spark Core模块提供，是Spark运行的基础。Spark Core以<strong>RDD</strong>为数据抽象，提供Python、Java、Scala、R语言的API，可以编程进行海量离线数据批处理计算。</p> 
<p><strong>SparkSQL</strong>：基于SparkCore之上，提供结构化数据的处理模块。SparkSQL支持以SQL语言对数据进行处理，SparkSQL本身针对离线计算场景。同时基于SparkSQL，Spark提供了<strong>StructuredStreaming</strong>模块，可以以SparkSQL为基础，进行<strong>数据的流式计算</strong>。</p> 
<p><strong>SparkStreaming</strong>：以SparkCore为基础，提供数据的流式计算功能。</p> 
<p><strong>MLlib</strong>：以SparkCore为基础，进行<strong>机器学习</strong>计算，内置了大量的机器学习库和API算法等。方便用户以分布式计算的模式进行机器学习计算。</p> 
<p><strong>GraphX</strong>：以SparkCore为基础，进行<strong>图计算</strong>，提供了大量的图计算API，方便用于以分布式计算模式进行图计算。</p> 
<p>spark运行模式</p> 
<ol><li>本地模式（单机）：本地模式就是以一个<strong>独立的进程</strong>，通过其<strong>内部的多个线程</strong>来模拟整个Spark运行时环境</li><li>Standalone模式（集群）：Spark中的各个角色以独立进程的形式存在，并组成Spark集群环境</li><li>Hadoop YARN模式（集群）：Spark中的各个角色运行在YARN的容器内部，并组成Spark集群环境</li><li>Kubernetes模式（容器集群）：Spark中的各个角色运行在Kubernetes的容器内部，并组成Spark集群环境</li></ol> 
<h4 id="spark%E6%9E%B6%E6%9E%84">spark架构</h4> 
<p>类比Yarn架构：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/39/5a/iSkLotm5_o.png"></p> 
<p>YARN主要有4类角色：</p> 
<ul><li>资源管理层面</li></ul> 
<ul><li> 
  <ul><li>集群资源管理者（Master）：ResourceManager</li><li>单机资源管理者（Worker）：NodeManager</li></ul></li></ul> 
<ul><li>任务计算层面</li></ul> 
<ul><li> 
  <ul><li>单任务管理者（Master）：ApplicationMaster</li><li>单任务执行者（Worker）：Task（容器内计算框架的工作角色）</li></ul></li></ul> 
<p><strong>spark架构也由4类角色组成：</strong></p> 
<ul><li>资源管理：</li></ul> 
<ul><li> 
  <ul><li>Master：管理整个集群的资源</li><li>Worker：管理单个服务器的资源</li></ul></li></ul> 
<ul><li>任务计算：</li></ul> 
<ul><li> 
  <ul><li>Driver：管理单个Spark任务运行时的工作（单任务管理者）</li><li>Exector：单个任务运行时的工作者（单任务执行者）</li></ul></li></ul> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/7f/10/fOisohw9_o.png"></p> 
<h3 id="Spark%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA">Spark环境搭建</h3> 
<p>首先安装spark，在安装spark之前需要安装anaconda</p> 
<p>可以到清华大学镜像源下载：</p> 
<p><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/" rel="nofollow" title="Index of /anaconda/archive/ | 清华大学开源软件镜像站 | Tsinghua Open Source Mirror">Index of /anaconda/archive/ | 清华大学开源软件镜像站 | Tsinghua Open Source Mirror</a></p> 
<p>也可以到官网下载：</p> 
<p><a href="https://www.anaconda.com/download#downloads" rel="nofollow" title="Free Download | Anaconda">Free Download | Anaconda</a></p> 
<p>以镜像源为例：</p> 
<p>由于使用python3.8，anaconda下载的版本是：<code>Anaconda3-2021.05-Linux-x86_64.sh</code></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/c9/05/cGITpyhB_o.png"></p> 
<p>下载完成后上传到linux服务器上</p> 
<p>然后通过<code>sh 安装包路径/Anaconda3-2021.05-Linux-x86_64.sh</code>来进行安装</p> 
<p>安装完成后创建<code>pyspark</code>环境：<code>conda create -n pyspark python=3.8</code><br> 然后可以通过<code>conda activate pyspark</code>激活当前环境</p> 
<p>然后需要在虚拟环境中安装jieba包：<code>pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple</code></p> 
<p>jieba包是Python中一个常用的中文分词库，它的作用是将中文文本进行分词处理</p> 
<hr> 
<p>常用的conda指令如下：</p> 
<p>禁止激活默认base环境：</p> 
<p><code>conda config --set auto_activate_base false</code></p> 
<p>创建环境：<code>conda create -n env_name</code></p> 
<p>查看所有环境：<code>conda info --envs</code></p> 
<p>查看当前环境中安装的所有包：<code>conda list</code></p> 
<p>查看当前环境中安装的某一个包的信息：<code>conda list --show &lt;package_name&gt;</code></p> 
<p>删除一个环境：<code>conda remove -n env_name --all</code></p> 
<p>激活环境：<code>conda activate airflow</code></p> 
<p>退出当前环境：<code>conda deactivate</code></p> 
<hr> 
<p>安装完anaconda之后，进行spark的安装：</p> 
<ol><li>下载安装包（3.2版本）：<a href="https://dlcdn.apache.org/spark/" rel="nofollow" title="Index of /spark">Index of /spark</a></li><li>解压安装包到对应的路径：<code>tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /opt/module/</code></li><li>安装路径名太长，可以通过<code>mv</code>来改名：<code>mv spark-3.2.0-bin-hadoop3.2 spark</code></li><li>配置环境变量：还是在<code>my_env.sh</code>中： <p class="img-center"><img alt="" src="https://images2.imgbox.com/69/c7/YZyabJYZ_o.png"></p> </li></ol> 
<p>其中<code>JAVA_HOME</code>和<code>HADOOP_HOME</code>在安装Hadoop的时候就已经配置过了</p> 
<p><code>PYSPARK_PYTHON</code>配置python的执行器，即我们安装的anaconda环境</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/ae/9b/ac6nggeZ_o.png"></p> 
<p>这里需要注意HADOOP_CONF_DIR和HADOOP_HOME的区别：</p> 
<p>HADOOP_CONF_DIR环境变量是Hadoop的配置目录，它指向Hadoop的配置文件所在的目录。在Hadoop中，有许多配置文件，例如<strong>core-site.xml、hdfs-site.xml、mapred-site.xml</strong>等。这些配置文件中包含了Hadoop集群的各种配置信息，例如HDFS的副本数量、块大小、NameNode和DataNode的地址等。当Hadoop启动时，它会读取这些配置文件并使用其中的配置信息。</p> 
<p>如果想更改或使用这些配置信息，则可以使用HADOOP_CONF_DIR环境变量来指定这些文件所在的目录</p> 
<p>由于spark在运行时可以采用spark on yarn的模式，需要读取yarn-site.xml，所以这个路径需要配置；</p> 
<p>而HADOOP_HOME是hadoop的安装路径；</p> 
<h4 id="local%E6%A8%A1%E5%BC%8F">local模式</h4> 
<p>启动一个JVM Process<strong>进程</strong>(一个进程里面有多个线程)，执行任务Task</p> 
<p>Local模式可以限制模拟Spark集群环境的线程数量, 即Local[N] 或 Local[*]</p> 
<p>其中N代表可以使用N个线程，每个线程拥有一个cpu core。如果不指定N，则默认是1个线程（该线程有1个core）。 通常Cpu有几个Core，就指定几个线程，最大化利用计算能力</p> 
<p><strong>需要注意的是， Local模式只能运行一个Spark程序, 如果执行多个Spark程序, 那就是由多个相互独立的Local进程在执行</strong></p> 
<p>local模式运行</p> 
<p>1.<code>bin/pyspark</code>：提供一个交互式的 Python解释器环境, 在这里面可以写普通python代码, 以及spark代码</p> 
<p>运行界面如下：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/13/c2/8nFLzrrX_o.png"></p> 
<ul><li>SparkContext是Spark的核心组件之一，它是与<strong>Spark集群通信的主要入口点</strong>。SparkContext负责<strong>与集群管理器通信，以便在集群上启动应用程序</strong>。它还负责将应用程序的代码分发到集群中的各个节点，并将数据分发到这些节点上。在Spark 2.0之前，SparkContext是与RDD编程交互的主要入口点。</li><li>SparkSession是Spark 2.0中引入的新概念。它是一个新的切入点，用于访问所有Spark功能。它提供了一种以较少数量的构造与各种spark功能交互的方法。它还提供了许多新功能，例如DataFrame和Dataset API，这些API使得使用Spark更加容易和直观。</li><li>4040端口：每一个Spark程序在运行的时候, 会绑定到<strong>Driver所在机器</strong>的4040端口上；如果4040端口被占用, 会顺延到4041 ... 4042...</li></ul> 
<p>打开<code>ip:4040</code>，可以看到监控页面：</p> 
<p>由于是local模式，只有一个<code>driver</code></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/ce/41/6YyX9Zi2_o.png"></p> 
<hr> 
<p>2.<code>bin/spark-shell</code>：使用scala语言，仅作了解</p> 
<p>3.<code>bin/spark-submit</code>：提交指定的Spark代码到Spark环境中运行</p> 
<p>使用示例代码：<code>bin/spark-submit /home/wuhaoyi/module/spark/examples/src/main/python/pi.py 10</code>（10是参数值）</p> 
<p>结果如下：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/ee/8d/mdo7lfcn_o.png"></p> 
<p>pyspark/spark-shell/spark-submit 对比</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/6a/b2/EVkgflCF_o.png"></p> 
<h4 id="Standalone%20%E6%A8%A1%E5%BC%8F">Standalone 模式</h4> 
<p>StandAlone 是完整的Spark运行环境：</p> 
<p>Master角色以Master进程存在；Worker角色以Worker进程存在；Driver和Executor运行于Worker进程内, 由Worker提供资源供给它们运行</p> 
<p><strong>StandAlone集群的三种进程：</strong></p> 
<ul><li>主节点Master进程：Master角色, 管理整个集群资源，并托管运行各个任务的Driver</li><li>从节点Workers： Worker角色, 管理每个机器的资源，分配对应的资源来运行Executor(Task)；</li><li>历史服务器HistoryServer(可选)：Spark Application运行完成以后，<strong>保存事件日志数据至HDFS</strong>，启动HistoryServer可以查看应用运行相关信息</li></ul> 
<p>StandAlone集群搭建</p> 
<p>采用三台Linux虚拟机，都需要安装anaconda环境</p> 
<p>需要配置的文件如下（每台机器都需要配置）：</p> 
<p>①<code>workers</code>：配置三个worker节点</p> 
<pre><code># A Spark Worker will be started on each of the machines listed below.
slave1
master
slave3
</code></pre> 
<p>②<code>spark-env.sh</code>：</p> 
<pre><code># 设置JAVA安装目录
JAVA_HOME=/usr/java/default

# Hadoop相关
# HADOOP配置文件目录，读取HDFS上文件和运行YARN集群
HADOOP_CONF_DIR=/home/wuhaoyi/module/hadoop/etc/hadoop
YARN_CONF_DIR=/home/wuhaoyi/module/hadoop/etc/hadoop

# master相关
# 告知Spark的master运行在哪个机器上
export SPARK_MASTER_HOST=slave1
# 告知spark master的通讯端口
export SPARK_MASTER_PORT=7077
# 告知spark master的 webui端口
SPARK_MASTER_WEBUI_PORT=8080

# worker相关
# worker cpu可用核数
SPARK_WORKER_CORES=56
# worker可用内存
SPARK_WORKER_MEMORY=100g
# worker的工作通讯地址
SPARK_WORKER_PORT=7078
# worker的 webui地址
SPARK_WORKER_WEBUI_PORT=8081

# 设置历史服务器
# 将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中
SPARK_HISTORY_OPTS="-Dspark.history.fs.logDirectory=hdfs://slave1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true"
</code></pre> 
<p>上面的sparklog文件夹需要自己创建</p> 
<p>③<code>spark-default.conf</code>：</p> 
<pre><code># # 开启spark的日志记录功能
spark.eventLog.enabled  true
# # 设置spark日志记录的路径
spark.eventLog.dir       hdfs://slave1:8020/sparklog/
# # 设置spark日志是否启动压缩
spark.eventLog.compress         true
</code></pre> 
<p>集群启动</p> 
<p>启动历史服务器：<code>sbin/start-history-server.sh</code></p> 
<p>jps名称为HistoryServer</p> 
<p>启动所有master和worker：<code>sbin/start-all.sh</code></p> 
<p>关闭所有master和worker：<code>sbin/stop-all.sh</code></p> 
<p>启动当前节点上的master/worker：<code>sbin/start-master.sh</code> <code>sbin/start-worker.sh</code></p> 
<p>关闭当前节点上的master/worker：<code>sbin/stop-master.sh</code> <code>sbin/stop-worker.sh</code></p> 
<p>启动集群后可以查看Master的WEB UI：<code>http://10.245.150.47:8080/</code></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/44/18/Slii5x1I_o.png"></p> 
<p>还可以查看历史服务器：<code>http://10.245.150.47:18080/</code></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/8a/2b/VjRlbmjN_o.png"></p> 
<p>点击<code>App ID</code>可以查看spark程序运行的细记录</p> 
<p>连接到StandAlone集群</p> 
<p><code>--master spark://ip地址:7077</code>(7077就是配置的master的通讯地址)</p> 
<p>示例：<code>bin/pyspark --master spark://slave1:7077</code>、</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/ac/17/am2C9UGF_o.png"></p> 
<p>Spark应用架构</p> 
<p>向spark中提交程序：<code>bin/spark-submit --master spark://slave1:7077 /home/wuhaoyi/module/spark/examples/src/main/python/pi.py 10</code></p> 
<p>查看程序运行情况：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/02/58/pVWQXvsy_o.png"></p> 
<p>可以看到Spark Application运行到集群上时，由两部分组成：<strong>Driver Program和Executors</strong></p> 
<p>1.Driver Program</p> 
<ul><li>相当于AppMaster，整个应用管理者，负责应用中所有Job的调度执行;</li><li>运行JVM Process，运行程序的MAIN函数，必须创建SparkContext上下文对象；</li><li>一个SparkApplication仅有一个；</li></ul> 
<p>2.Executors</p> 
<ul><li>相当于一个线程池，运行JVM Process，其中有很多线程，每个线程运行一个Task任务，一个Task任务运行需要1 Core CPU，所以可以认为Executor中线程数就等于CPU Core核数；</li><li>一个Spark Application可以有多个，可以设置个数和资源信息；</li></ul> 
<p>*程序提交运行的全过程</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/46/dc/yqSDn9c5_o.png"></p> 
<ol><li>用户程序创建 SparkContext 时，新创建的 SparkContext 实例会连接到 ClusterManager。 Cluster Manager 会根据用户提交时设置的 CPU 和内存等信息为本次提交分配计算资源，启动 Executor。</li><li><strong>Driver会将用户程序划分为不同的执行阶段Stage</strong>，每个执行阶段Stage由<strong>一组完全相同Task组成</strong>，这些Task分别作用于待处理数据的不同分区。在阶段划分完成和Task创建后， <strong>Driver会向Executor发送 Task</strong>；</li><li>Executor在接收到Task后，会下载Task的运行时依赖，在准备好Task的执行环境后，会开始执行Task，并且将Task的运行状态汇报给Driver；</li><li>Driver会根据收到的Task的运行状态来处理不同的状态更新。 Task分为两种：一种是Shuffle Map Task，它实现数据的重新洗牌，洗牌的结果保存到Executor 所在节点的文件系统中；另外一种是Result Task，它负责生成结果数据；</li><li>Driver 会不断地调用Task，将Task发送到Executor执行，在所有的Task 都正确执行或者超过执行次数的限制仍然没有执行成功时停止；</li></ol> 
<p>Spark程序运行层次结构</p> 
<ol><li>在一个Spark Application中，包含多个Job； <p class="img-center"><img alt="" src="https://images2.imgbox.com/10/2d/qSfXK9wO_o.png"></p> </li><li>每个Job由多个<strong>Stage</strong>组成，每个Job按照DAG图来执行 <p class="img-center"><img alt="" src="https://images2.imgbox.com/6a/23/VLyCuC1D_o.png"></p> </li><li>每个Stage中包含<strong>多个Task任务</strong>，每个Task以线程Thread方式执行，需要1Core CPU <p class="img-center"><img alt="" src="https://images2.imgbox.com/42/2d/fmHwO55H_o.png"></p> </li></ol> 
<p><strong>下面对Spark Application程序运行时三个核心概念进行说明：</strong></p> 
<ol><li>Job：由多个 Task 的并行计算部分组成，一般 Spark 中的action 操作（如 save、collect），会生成一个 Job</li><li>Stage：Job 的组成单位，一个 Job 会切分成多个 Stage，Stage 彼此之间相互依赖顺序执行，而每个 Stage 是多个 Task 的集合，类似 map 和 reduce stage</li><li>Task：被分配到各个 Executor 的单位工作内容，它是<strong>Spark 中的最小执行单位</strong>，一般来说<strong>有多少个 Paritition</strong>（物理层面的概念，即分支<strong>可以理解为将数据划分成不同部分并行处理</strong>），<strong>就会有多少个 Task</strong>，每个 Task 只会处理单一分支上的数据</li></ol> 
<h4 id="Spark%20On%20YARN%20%E6%A8%A1%E5%BC%8F">Spark On YARN 模式</h4> 
<p>本质：</p> 
<p><strong>Master角色由YARN的ResourceManager担任</strong></p> 
<p><strong>Worker角色由YARN的NodeManager担任</strong></p> 
<p>配置过程：</p> 
<ul><li>配置好Hadoop集群</li><li>配置环境变量：<code>HADOOP_CONF_DIR</code>；以便spark运行时读取配置文件相关信息： <p class="img-center"><img alt="" src="https://images2.imgbox.com/50/d0/LqG3hfsS_o.png"></p> </li></ul> 
<p>连接到YARN中</p> 
<pre><code>bin/pyspark --master yarn --deploy-mode client|cluster
# --deploy-mode 选项是指定部署模式, 默认是 客户端模式
# client就是客户端模式
# cluster就是集群模式
# --deploy-mode 仅可以用在YARN模式下
</code></pre> 
<p>注意： pyspark 和 spark-shell 无法运行 cluster模式；</p> 
<pre><code>bin/spark-submit --master yarn --deploy-mode client|cluster /xxx/xxx/xxx.py 参数
</code></pre> 
<p>spark-submit可以运行cluster模式</p> 
<p>两种<strong>DeployMode</strong>的区别</p> 
<p>Driver运行的位置不同：</p> 
<ul><li>Cluster模式即:<strong>Driver运行在YARN容器内部</strong>, 和ApplicationMaster在同一个容器内</li><li>Client模式即:Driver运行在客户端进程中, 比如Driver运行在spark-submit程序的进程中</li></ul> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/f6/42/kDjFiHeW_o.png"></p> 
<p><strong>Cluster模式：</strong></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/cb/5a/Sdvm42Ry_o.png"></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/0d/1f/eywzbyPe_o.png"></p> 
<p><strong>Client模式：</strong></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/80/05/8o8om5Vv_o.png"></p> 
<p>两种DeployMode的使用场景</p> 
<p><strong>Client模式</strong>：学习测试时使用，生产不推荐(要用也可以,性能略低,稳定性略低)</p> 
<p>1.Driver运行在Client上,和集群的通信成本高</p> 
<p>2.Driver输出结果会在客户端显示</p> 
<p><strong>Cluster模式</strong>：生产环境中使用该模式</p> 
<p>1.Driver程序在YARN集群中，和集群的通信成本低</p> 
<p>2.Driver输出结果不能在客户端显示</p> 
<p>3.该模式下Driver运行ApplicattionMaster这个节点上,由Yarn管理，如果出现问题，yarn会重启ApplicattionMaster(Driver)</p> 
<p>两种DeployMode的详细运行流程</p> 
<p><strong>Client：</strong></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/c8/0e/meICbiHu_o.png"></p> 
<p>1）Driver在任务提交的本地机器上运行，Driver启动后会和ResourceManager通讯申请启动ApplicationMaster；</p> 
<p>2）随后ResourceManager分配Container，在合适的NodeManager上启动ApplicationMaster，此时的</p> 
<p><strong>ApplicationMaster的功能相当于一个ExecutorLaucher</strong>，只负责<strong>向ResourceManager申请Executor内存</strong>；</p> 
<p>ApplicationMaster负责Executor的启动</p> 
<p>3）ResourceManager接到ApplicationMaster的资源申请后会分配Container，然后ApplicationMaster在资源分配指定的NodeManager上启动Executor进程；</p> 
<p>4）<strong>Executor进程启动后会向Driver反向注册</strong>，Executor全部注册完成后<strong>Driver开始执行main函数</strong>；</p> 
<p>5）之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分Stage，每个Stage生成对应的TaskSet，之后将Task分发到各个Executor上执行。</p> 
<p>在Client模式下，由于Driver运行在本地机器上，所以spark任务的调度是由本地机器完成的，所以通讯效率会比较低</p> 
<p><strong>Cluster：</strong></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/cd/c3/QHR7X8S4_o.png"></p> 
<p>1）任务提交后会和ResourceManager通讯申请启动ApplicationMaster</p> 
<p>2）随后ResourceManager分配Container，在合适的NodeManager上启动ApplicationMaster，此时的</p> 
<p><strong>ApplicationMaster就是Driver</strong></p> 
<p>3）Driver启动后向ResourceManager申请Executor内存，ResourceManager接到ApplicationMaster的资源申请后会分配Container,然后在合适的NodeManager上启动Executor进程</p> 
<p>4）Executor进程启动后会向Driver反向注册</p> 
<p>5）Executor全部注册完成后Driver开始执行main函数，之后执行到Action算子时，触发一个job，并根据宽依赖开始划分stage，每个stage生成对应的taskSet，之后将task分发到各个Executor上执行</p> 
<h4 id="PySpark%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA">PySpark开发环境搭建</h4> 
<p>PySpark：是Spark官方提供的一个<strong>Python类库</strong>, 内置了完全的Spark API, 可以通过PySpark类库来编写Spark应用程序,并将其提交到Spark集群中运行.</p> 
<p>环境搭建步骤：</p> 
<p>1、安装Windows anaconda环境：</p> 
<p>下载地址：<a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/" rel="nofollow" title="Index of /anaconda/archive/ | 清华大学开源软件镜像站 | Tsinghua Open Source Mirror">Index of /anaconda/archive/ | 清华大学开源软件镜像站 | Tsinghua Open Source Mirror</a></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/80/e7/cQij9K1r_o.png"></p> 
<p>下载时候直接安装即可，安装过程中可以自行指定路径，其余没有需要勾选的内容</p> 
<p>安装完成后打开打开 <code>Anaconda Prompt</code> 程序</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/51/4f/QDF1zK83_o.png"></p> 
<p>出现<code>base</code>说明安装成功：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/61/59/p88ZBlgE_o.png"></p> 
<p>2、配置国内镜像源：</p> 
<p>打开<code>Anaconda Prompt</code></p> 
<p>输入：<code>conda config --set show_channel_urls yes</code></p> 
<p>这个设置的作用是在安装包时显示包的安装来源</p> 
<p>然后找到<code>C:\Users\用户名.condarc</code>文件，用以下内容替换文件中原有的内容：</p> 
<pre><code>channels:
  - defaults
show_channel_urls: true
default_channels:
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2
custom_channels:
  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
</code></pre> 
<p>3、创建虚拟环境：</p> 
<pre><code># 创建虚拟环境 pyspark, 基于Python 3.8
conda create -n pyspark python=3.8

# 切换到虚拟环境内
conda activate pyspark

# 在虚拟环境内安装包
pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple 
</code></pre> 
<p>4、安装pyspark：<code>pip install pyspark -i https://pypi.tuna.tsinghua.edu.cn/simple</code></p> 
<p>5、在Windows中配置Hadoop补丁文件:</p> 
<ul><li>将文件夹内bin内的hadoop.dll复制到: C:\Windows\System32里面去</li><li>配置HADOOP_HOME环境变量指向 hadoop补丁文件夹的路径</li></ul> 
<p>下载地址：<br><a href="https://gitcode.net/mirrors/cdarlint/winutils?utm_source=csdn_github_accelerator" rel="nofollow" title="mirrors / cdarlint / winutils ·  GitCode">mirrors / cdarlint / winutils · GitCode</a></p> 
<p>或者：</p> 
<p><a href="https://github.com/steveloughran/winutils" title="GitHub - steveloughran/winutils: Windows binaries for Hadoop versions (built from the git commit ID used for the ASF relase)">GitHub - steveloughran/winutils: Windows binaries for Hadoop versions (built from the git commit ID used for the ASF relase)</a></p> 
<p>所需文件内容如下：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/48/45/aRlrpAXA_o.png"></p> 
<p>6、在pycharm中配置本地解释器</p> 
<p>File-&gt;Settings-&gt;Python Interpreter</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/91/d8/ZWWlF7l6_o.png"></p> 
<p>点击Add Interpreter，选择Conda Interpreter：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/de/0a/Fw8RzipH_o.png"></p> 
<p>然后会自动加载conda中已经创建好的环境，如果没有的话可以选择右上角的<code>Load Environments</code>手动加载；</p> 
<p>之后选择pyspark：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/10/2a/wrqx2w4q_o.png"></p> 
<p>点击OK即可；</p> 
<p>7、通过SSH配置Linux解释器</p> 
<p>本地解释器在性能上会慢一些，而且一些比较耗内存地操作无法完成，所以配置linux解释器：</p> 
<h4 id="Python%20On%20Spark%20%E6%89%A7%E8%A1%8C%E5%8E%9F%E7%90%86">Python On Spark 执行原理</h4> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/ef/e6/vUQJcUA1_o.png"></p> 
<p>PySpark宗旨是在不破坏Spark已有的运行时架构，在Spark架构外层包装一层Python API，借助<strong>Py4j</strong>实现Python和Java的交互，进而实现通过Python编写Spark应用程序</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/f8/32/5wEYrKLx_o.png"></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/568f25c27b86634f5882931c3dddeddb/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">更改docker数据目录</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a1c1cf70332233e625fee7764eff20f1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">使用C语言创建登录系统</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>