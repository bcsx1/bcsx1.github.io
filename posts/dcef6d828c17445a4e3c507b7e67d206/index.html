<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>tensorflow中3种常用图片数据的训练方式及性能分析 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="tensorflow中3种常用图片数据的训练方式及性能分析" />
<meta property="og:description" content="摘要：
本文对比了：原图、tobytes、gfile三种方式的图片数据的训练文件大小、训练时长、GPU利用率等指标，通过实际数据分析这3种训练方式的优缺点；明确指出gfile训练方式具有最佳的空间占用和GPU利用率；行文中强调了初学者容易犯错的tf.decode_raw 、 tf.image.decode_jpeg、tf.image.convert_image_dtype、tf.image.resize_images函数的使用方法；旨在减少大家在相关问题上为此付出的检索时间
目录
1、引入
1.1、tensorflow中3种常用的图片数据形式
方式一（原图）
方式二（tobytes写入）
方式三（gfile写入）
1.2、TFRecords介绍
1.3、训练环境
1.4、数据
2、数据准备
2.1、方式一（原图）数据准备
2.2、方式二（tobytes写入）数据准备
2.3、方式三（gfile写入）数据准备
2.4、数据准备总结
3、训练评估
3.1、训练介绍
3.2、方式一（原图）数据读取和训练时长分析
3.3、方式二（tobtytes写入）数据读取和训练时长分析
3.5、训练评估总结
4、总结
1、引入 1.1、tensorflow中3种常用的图片数据形式 方式一（原图） 使用cv2.imread将图片数据直接读入内存队列中，训练时读取队列元素（初学者使用较多）。
ps：使用opencv读取图像，直接返回numpy.ndarray 对象，通道顺序为BGR ，注意是BGR，通道值默认范围0-255。
方式二（tobytes写入） 使用PIL.Image的tobytes()方法写入tfrecords文件中，训练时读取tfrecords（网上教程使用较多）。
ps：tobytes()方法返回一个使用标准“raw”编码器生成的包含像素数据的字符串。
方式三（gfile写入） 使用tf.gfile.FastGFile的read方法读取图片，写入tfrecords文件中，训练时读取tfrecords（tensorflow官方使用，例如：models-master\models-master\research\slim\datasets\download_and_convert_cifar10.py）。
ps：read方法默认以字符串返回整个文件的内容，如果参数是rb，按照字节返回。
1.2、TFRecords介绍 TFRecords可以允许你讲任意的数据转换为TensorFlow所支持的格式， 这种方法可以使TensorFlow的数据集更容易与网络应用架构相匹配。这种建议的方法就是使用TFRecords文件，TFRecords文件包含了[tf.train.Example 协议内存块(protocol buffer)](协议内存块包含了字段[Features]。你可以写一段代码获取你的数据， 将数据填入到Example协议内存块(protocol buffer)，将协议内存块序列化为一个字符串， 并且通过[tf.python_io.TFRecordWriter class]写入到TFRecords文件。
TFRecords文件格式在图像识别中有很好的使用,其可以将二进制数据和标签数据(训练的类别标签)数据存储在同一个文件中,它可以在模型进行训练之前通过预处理步骤将图像转换为TFRecords格式,此格式最大的优点实践每幅输入图像和与之关联的标签放在同一个文件中.TFRecords文件是一种二进制文件,其不对数据进行压缩,所以可以被快速加载到内存中.格式不支持随机访问，因此它适合于大量的数据流，但不适用于快速分片或其他非连续存取。
1.3、训练环境 CPU：Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz
GPU：Tesla V100
MEN：250G
硬盘：固态硬盘
1.4、数据 数据内容是2款不同型号的ONU图片，已经分好训练集 train 和验证集 val
du -h
查看数据的目录结构
201M ./603_663_img/train/F603" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/dcef6d828c17445a4e3c507b7e67d206/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-07-21T00:09:19+08:00" />
<meta property="article:modified_time" content="2018-07-21T00:09:19+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">tensorflow中3种常用图片数据的训练方式及性能分析</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <blockquote> 
 <p><strong>摘要：</strong></p> 
 <p><strong>本文对比了：原图、tobytes、gfile三种方式的图片数据的训练文件大小、训练时长、GPU利用率等指标，通过实际数据分析这3种训练方式的优缺点；明确指出gfile训练方式具有最佳的空间占用和GPU利用率；行文中强调了初学者容易犯错的tf.decode_raw 、 tf.image.decode_jpeg、tf.image.convert_image_dtype、tf.image.resize_images函数的使用方法；旨在减少大家在相关问题上为此付出的检索时间</strong></p> 
</blockquote> 
<p> </p> 
<p><strong>目录</strong></p> 
<p id="0%E3%80%81%E5%BC%95%E5%85%A5-toc" style="margin-left:0px;"><a href="#0%E3%80%81%E5%BC%95%E5%85%A5" rel="nofollow">1、引入</a></p> 
<p id="tensorflow%E8%AE%AD%E7%BB%83%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%843%E7%A7%8D%E5%9B%BE%E7%89%87%E6%95%B0%E6%8D%AE%E5%BD%A2%E5%BC%8F%EF%BC%9A-toc" style="margin-left:40px;"><a href="#tensorflow%E8%AE%AD%E7%BB%83%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%843%E7%A7%8D%E5%9B%BE%E7%89%87%E6%95%B0%E6%8D%AE%E5%BD%A2%E5%BC%8F%EF%BC%9A" rel="nofollow">1.1、tensorflow中3种常用的图片数据形式</a></p> 
<p id="%E6%96%B9%E5%BC%8F%E4%B8%80%EF%BC%88%E5%8E%9F%E5%9B%BE%EF%BC%89%EF%BC%9A-toc" style="margin-left:80px;"><a href="#%E6%96%B9%E5%BC%8F%E4%B8%80%EF%BC%88%E5%8E%9F%E5%9B%BE%EF%BC%89%EF%BC%9A" rel="nofollow">方式一（原图）</a></p> 
<p id="%E6%96%B9%E5%BC%8F%E4%BA%8C%EF%BC%88tobytes%E5%86%99%E5%85%A5%EF%BC%89%EF%BC%9A-toc" style="margin-left:80px;"><a href="#%E6%96%B9%E5%BC%8F%E4%BA%8C%EF%BC%88tobytes%E5%86%99%E5%85%A5%EF%BC%89%EF%BC%9A" rel="nofollow">方式二（tobytes写入）</a></p> 
<p id="%E6%96%B9%E5%BC%8F%E4%B8%89%EF%BC%88gfile%E5%86%99%E5%85%A5%EF%BC%89%EF%BC%9A-toc" style="margin-left:80px;"><a href="#%E6%96%B9%E5%BC%8F%E4%B8%89%EF%BC%88gfile%E5%86%99%E5%85%A5%EF%BC%89%EF%BC%9A" rel="nofollow">方式三（gfile写入）</a></p> 
<p id="1.2%E3%80%81TFRecords%E4%BB%8B%E7%BB%8D%EF%BC%9A%20%C2%A0-toc" style="margin-left:40px;"><a href="#1.2%E3%80%81TFRecords%E4%BB%8B%E7%BB%8D%EF%BC%9A%20%C2%A0" rel="nofollow">1.2、TFRecords介绍</a></p> 
<p id="1.3%E3%80%81%E8%AE%AD%E7%BB%83%E7%8E%AF%E5%A2%83%EF%BC%9A-toc" style="margin-left:40px;"><a href="#1.3%E3%80%81%E8%AE%AD%E7%BB%83%E7%8E%AF%E5%A2%83%EF%BC%9A" rel="nofollow">1.3、训练环境</a></p> 
<p id="1.4%E3%80%81%E6%95%B0%E6%8D%AE%EF%BC%9A-toc" style="margin-left:40px;"><a href="#1.4%E3%80%81%E6%95%B0%E6%8D%AE%EF%BC%9A" rel="nofollow">1.4、数据</a></p> 
<p id="2%E3%80%81%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87-toc" style="margin-left:0px;"><a href="#2%E3%80%81%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87" rel="nofollow">2、数据准备</a></p> 
<p id="2.1%E3%80%81%E6%96%B9%E5%BC%8F%E4%B8%80%EF%BC%88%E5%8E%9F%E5%9B%BE%EF%BC%89%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87%EF%BC%9A-toc" style="margin-left:40px;"><a href="#2.1%E3%80%81%E6%96%B9%E5%BC%8F%E4%B8%80%EF%BC%88%E5%8E%9F%E5%9B%BE%EF%BC%89%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87%EF%BC%9A" rel="nofollow">2.1、方式一（原图）数据准备</a></p> 
<p id="2.2%E3%80%81%E6%96%B9%E5%BC%8F%E4%BA%8C%EF%BC%88tobytes%E5%86%99%E5%85%A5%EF%BC%89%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87%EF%BC%9A-toc" style="margin-left:40px;"><a href="#2.2%E3%80%81%E6%96%B9%E5%BC%8F%E4%BA%8C%EF%BC%88tobytes%E5%86%99%E5%85%A5%EF%BC%89%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87%EF%BC%9A" rel="nofollow">2.2、方式二（tobytes写入）数据准备</a></p> 
<p id="2.3%E3%80%81%E6%96%B9%E5%BC%8F%E4%B8%89%EF%BC%88gfile%E5%86%99%E5%85%A5%EF%BC%89%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87-toc" style="margin-left:40px;"><a href="#2.3%E3%80%81%E6%96%B9%E5%BC%8F%E4%B8%89%EF%BC%88gfile%E5%86%99%E5%85%A5%EF%BC%89%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87" rel="nofollow">2.3、方式三（gfile写入）数据准备</a></p> 
<p id="2.4%E3%80%81%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87%E6%80%BB%E7%BB%93%EF%BC%9A-toc" style="margin-left:40px;"><a href="#2.4%E3%80%81%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87%E6%80%BB%E7%BB%93%EF%BC%9A" rel="nofollow">2.4、数据准备总结</a></p> 
<p id="3%E3%80%81%E8%AE%AD%E7%BB%83%E8%AF%84%E4%BC%B0-toc" style="margin-left:0px;"><a href="#3%E3%80%81%E8%AE%AD%E7%BB%83%E8%AF%84%E4%BC%B0" rel="nofollow">3、训练评估</a></p> 
<p id="3.1%E3%80%81%E8%AE%AD%E7%BB%83%E4%BB%8B%E7%BB%8D%EF%BC%9A-toc" style="margin-left:40px;"><a href="#3.1%E3%80%81%E8%AE%AD%E7%BB%83%E4%BB%8B%E7%BB%8D%EF%BC%9A" rel="nofollow">3.1、训练介绍</a></p> 
<p id="3.2%E3%80%81%E6%96%B9%E5%BC%8F%E4%B8%80%EF%BC%88%E5%8E%9F%E5%9B%BE%EF%BC%89%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E5%92%8C%E8%AE%AD%E7%BB%83%E6%97%B6%E9%95%BF%E5%88%86%E6%9E%90%EF%BC%9A-toc" style="margin-left:40px;"><a href="#3.2%E3%80%81%E6%96%B9%E5%BC%8F%E4%B8%80%EF%BC%88%E5%8E%9F%E5%9B%BE%EF%BC%89%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E5%92%8C%E8%AE%AD%E7%BB%83%E6%97%B6%E9%95%BF%E5%88%86%E6%9E%90%EF%BC%9A" rel="nofollow">3.2、方式一（原图）数据读取和训练时长分析</a></p> 
<p id="3.3%E3%80%81%E6%96%B9%E5%BC%8F%E4%BA%8C%EF%BC%88tobtytes%E5%86%99%E5%85%A5%EF%BC%89%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E5%92%8C%E8%AE%AD%E7%BB%83%E6%97%B6%E9%95%BF%E5%88%86%E6%9E%90%EF%BC%9A-toc" style="margin-left:40px;"><a href="#3.3%E3%80%81%E6%96%B9%E5%BC%8F%E4%BA%8C%EF%BC%88tobtytes%E5%86%99%E5%85%A5%EF%BC%89%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E5%92%8C%E8%AE%AD%E7%BB%83%E6%97%B6%E9%95%BF%E5%88%86%E6%9E%90%EF%BC%9A" rel="nofollow">3.3、方式二（tobtytes写入）数据读取和训练时长分析</a></p> 
<p id="3.5%E3%80%81%E8%AE%AD%E7%BB%83%E8%AF%84%E4%BC%B0%E6%80%BB%E7%BB%93-toc" style="margin-left:40px;"><a href="#3.5%E3%80%81%E8%AE%AD%E7%BB%83%E8%AF%84%E4%BC%B0%E6%80%BB%E7%BB%93" rel="nofollow">3.5、训练评估总结</a></p> 
<p id="4%E3%80%81%E6%80%BB%E7%BB%93%EF%BC%9A-toc" style="margin-left:0px;"><a href="#4%E3%80%81%E6%80%BB%E7%BB%93%EF%BC%9A" rel="nofollow">4、总结</a></p> 
<hr> 
<h2 id="0%E3%80%81%E5%BC%95%E5%85%A5">1、引入</h2> 
<h3 id="tensorflow%E8%AE%AD%E7%BB%83%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%843%E7%A7%8D%E5%9B%BE%E7%89%87%E6%95%B0%E6%8D%AE%E5%BD%A2%E5%BC%8F%EF%BC%9A">1.1、tensorflow中3种常用的图片数据形式</h3> 
<h4 id="%E6%96%B9%E5%BC%8F%E4%B8%80%EF%BC%88%E5%8E%9F%E5%9B%BE%EF%BC%89%EF%BC%9A">方式一（原图）</h4> 
<p>使用cv2.imread将图片数据直接读入内存队列中，训练时读取队列元素（初学者使用较多）。</p> 
<p>ps：使用opencv读取图像，直接返回numpy.ndarray 对象，通道顺序为<strong>BGR</strong> ，<em>注意是BGR</em>，通道值默认范围0-255。</p> 
<h4 id="%E6%96%B9%E5%BC%8F%E4%BA%8C%EF%BC%88tobytes%E5%86%99%E5%85%A5%EF%BC%89%EF%BC%9A">方式二（tobytes写入）</h4> 
<p>使用PIL.Image的tobytes()方法写入tfrecords文件中，训练时读取tfrecords（网上教程使用较多）。</p> 
<p>ps：tobytes()方法返回一个使用标准“raw”编码器生成的包含像素数据的字符串。</p> 
<h4 id="%E6%96%B9%E5%BC%8F%E4%B8%89%EF%BC%88gfile%E5%86%99%E5%85%A5%EF%BC%89%EF%BC%9A">方式三（gfile写入）</h4> 
<p>使用tf.gfile.FastGFile的read方法读取图片，写入tfrecords文件中，训练时读取tfrecords（tensorflow官方使用，例如：models-master\models-master\research\slim\datasets\download_and_convert_cifar10.py）。</p> 
<p>ps：read方法默认以字符串返回整个文件的内容，如果参数是<code>rb</code>，按照字节返回。</p> 
<h3 id="1.2%E3%80%81TFRecords%E4%BB%8B%E7%BB%8D%EF%BC%9A%20%C2%A0">1.2、TFRecords介绍</h3> 
<p>TFRecords可以允许你讲任意的数据转换为TensorFlow所支持的格式， 这种方法可以使TensorFlow的数据集更容易与网络应用架构相匹配。这种建议的方法就是使用TFRecords文件，TFRecords文件包含了[<code>tf.train.Example</code> 协议内存块(protocol buffer)](协议内存块包含了字段[<code>Features</code>]。你可以写一段代码获取你的数据， 将数据填入到<code>Example</code>协议内存块(protocol buffer)，将协议内存块序列化为一个字符串， 并且通过[<code>tf.python_io.TFRecordWriter</code> class]写入到TFRecords文件。</p> 
<p>TFRecords文件格式在图像识别中有很好的使用,其可以将二进制数据和标签数据(训练的类别标签)数据存储在同一个文件中,它可以在模型进行训练之前通过预处理步骤将图像转换为TFRecords格式,此格式最大的优点实践每幅输入图像和与之关联的标签放在同一个文件中.TFRecords文件是一种二进制文件,其不对数据进行压缩,所以可以被快速加载到内存中.格式不支持随机访问，因此它适合于大量的数据流，但不适用于快速分片或其他非连续存取。</p> 
<h3 id="1.3%E3%80%81%E8%AE%AD%E7%BB%83%E7%8E%AF%E5%A2%83%EF%BC%9A">1.3、训练环境</h3> 
<p>CPU：Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz</p> 
<p>GPU：Tesla V100</p> 
<p>MEN：250G</p> 
<p>硬盘：固态硬盘</p> 
<h3 id="1.4%E3%80%81%E6%95%B0%E6%8D%AE%EF%BC%9A">1.4、数据</h3> 
<p>数据内容是2款不同型号的ONU图片，已经分好训练集 train 和验证集 val</p> 
<blockquote> 
 <p>du -h</p> 
</blockquote> 
<p>查看数据的目录结构</p> 
<blockquote> 
 <p>201M    ./603_663_img/train/F603<br> 235M    ./603_663_img/train/F663N<br> 435M    ./603_663_img/train<br> 102M    ./603_663_img/val/F603<br> 87M     ./603_663_img/val/F663N<br> 188M    ./603_663_img/val<br> 623M    ./603_663_img<br> 623M    .</p> 
</blockquote> 
<blockquote> 
 <p>ls -lR|grep "^-"| wc -l</p> 
</blockquote> 
<p>查看数据文件个数</p> 
<blockquote> 
 <p>3562</p> 
</blockquote> 
<h2 id="2%E3%80%81%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87">2、数据准备</h2> 
<h3 id="2.1%E3%80%81%E6%96%B9%E5%BC%8F%E4%B8%80%EF%BC%88%E5%8E%9F%E5%9B%BE%EF%BC%89%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87%EF%BC%9A">2.1、方式一（原图）数据准备</h3> 
<p>通过代码直接读取目录下的数据，无需转化，图片数量<strong>3562张</strong>，大小为<strong>623M。</strong></p> 
<h3 id="2.2%E3%80%81%E6%96%B9%E5%BC%8F%E4%BA%8C%EF%BC%88tobytes%E5%86%99%E5%85%A5%EF%BC%89%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87%EF%BC%9A">2.2、方式二（tobytes写入）数据准备</h3> 
<p>使用tobytes()写入tfrecords中，创建训练和验证数据集，关键代码：</p> 
<pre class="has"><code class="language-python">
tfrecords_path='603_663_train_raw.tfrecord'
#tfrecords_path='603_663_val_raw.tfrecord'
writer = tf.python_io.TFRecordWriter(tfrecords_path)
for line in lines:
  with tf.Session('') as sess:
    img = Image.open(os.path.join(base_path,line[0]))
    width = img.size[0]
    height = img.size[1]
    img_raw = img.tobytes()              #将图片转化为原生bytes
    example = tf.train.Example(features=tf.train.Features(feature={
    "image/class/label": tf.train.Feature(int64_list=tf.train.Int64List(value=[int(line[1])])),
    'image/width': tf.train.Feature(int64_list=tf.train.Int64List(value=[int(width)])),
    'image/height': tf.train.Feature(int64_list=tf.train.Int64List(value=[int(height)])),
    'image/raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw]))
    }))
    writer.write(example.SerializeToString())  #序列化为字符串
writer.close()</code></pre> 
<p>文件大小</p> 
<blockquote> 
 <p>total 6.8G<br> -rw-r--r-- 1 root root 4.8G Jul 21 00:45 603_663_train_raw.tfrecord<br> -rw-r--r-- 1 root root 2.1G Jul 21 00:45 603_663_val_raw.tfrecord</p> 
</blockquote> 
<p>生成的数据文件 <strong>6.8G </strong>约为原数据 <strong>623M</strong> 的 <strong>11.1倍！</strong></p> 
<h3 id="2.3%E3%80%81%E6%96%B9%E5%BC%8F%E4%B8%89%EF%BC%88gfile%E5%86%99%E5%85%A5%EF%BC%89%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87">2.3、方式三（gfile写入）数据准备</h3> 
<p>使用tf.gfile.FastGFile的read方法读取图片，写入tfrecords文件中，创建训练和验证数据集，关键代码：</p> 
<pre class="has"><code class="language-python">tfrecords_path='603_663_train_gfile.tfrecord'
#tfrecords_path='603_663_val_gfile.tfrecord'
writer = tf.python_io.TFRecordWriter(tfrecords_path)
for line in lines:
  decode_jpeg_data = tf.placeholder(dtype=tf.string)
  decode_jpeg = tf.image.decode_jpeg(decode_jpeg_data, channels=3)
  with tf.Session('') as sess:
    image_data = tf.gfile.FastGFile(os.path.join(base_path, line[0]), 'rb').read()
    image = sess.run(decode_jpeg, feed_dict={decode_jpeg_data: image_data})
    height, width = image.shape[0], image.shape[1]
    example = tf.train.Example(features=tf.train.Features(feature={
    "image/class/label": tf.train.Feature(int64_list=tf.train.Int64List(value=[int(line[1])])),
    'image/width': tf.train.Feature(int64_list=tf.train.Int64List(value=[int(width)])),
    'image/height': tf.train.Feature(int64_list=tf.train.Int64List(value=[int(height)])),
    'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_data]))
    }))
    writer.write(example.SerializeToString())  #序列化为字符串
writer.close()</code></pre> 
<p>文件大小</p> 
<blockquote> 
 <p>总用量 617M<br> -rw-rw-r-- 1 hadoop hadoop 431M 7月  21 00:26 603_663_train_g.tfrecord<br> -rw-rw-r-- 1 hadoop hadoop 186M 7月  21 00:29 603_663_val_g.tfrecord</p> 
</blockquote> 
<p>生成的数据文件 <strong>617M</strong><strong> </strong>比原数据 <strong>623M</strong> 还小！</p> 
<p>ps：tensorflow官方数据准备</p> 
<h3 id="2.4%E3%80%81%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87%E6%80%BB%E7%BB%93%EF%BC%9A">2.4、数据准备总结</h3> 
<p>数据占用大小排序  <strong>方式三（gfile写入）  &lt;  方式一（原图） &lt; 方式二（tobtytes写入）；</strong></p> 
<p>方式三（gfile写入） 与 方式一（原图）空间占用大小相似；</p> 
<p>方式二（tobtytes写入） 空间占用超过其他两种方式<strong>十倍</strong>；</p> 
<h2 id="3%E3%80%81%E8%AE%AD%E7%BB%83%E8%AF%84%E4%BC%B0">3、训练评估</h2> 
<h3 id="3.1%E3%80%81%E8%AE%AD%E7%BB%83%E4%BB%8B%E7%BB%8D%EF%BC%9A">3.1、训练介绍</h3> 
<p>分类算法：Inception_Resnet_v2</p> 
<p>源码在tensorflow的model项目中  models-master\research\slim\nets\inception_resnet_v2.py。</p> 
<p>训练方式：train集合中每张图片在训练是都经过随机数据增强，以保证每张训练图片都不同，每10个epoch，用验证集进行一次 accuracy验证并导出对应pb文件，训练脚本参考 models-master\research\slim\train_image_classifier.py 进行对应修改。或可以参考： <a href="https://blog.csdn.net/wlzard/article/details/77689311">https://blog.csdn.net/wlzard/article/details/77689311 </a></p> 
<p>评估方式：每个batch为32张图片，记录每个batch读入图片之前和完成loss值run后的时间差duration。</p> 
<p> </p> 
<h3 id="3.2%E3%80%81%E6%96%B9%E5%BC%8F%E4%B8%80%EF%BC%88%E5%8E%9F%E5%9B%BE%EF%BC%89%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E5%92%8C%E8%AE%AD%E7%BB%83%E6%97%B6%E9%95%BF%E5%88%86%E6%9E%90%EF%BC%9A">3.2、方式一（原图）数据读取和训练时长分析</h3> 
<p>方式一（原图）数据读取：通过load_img函数将训练集图片地址读入内存：</p> 
<pre class="has"><code class="language-python">def load_img():
    train_path = 'train/'

    X_train = []
    y_train = []

    class_list = os.listdir(train_path)
    for c in range(len(class_list)):
        # 训练数据
        img_list = os.listdir(train_path + class_list[c])
        # 每一个分类随机获取TEST_DATA_SAMPLE_SIZE张作为测试，避免tensorflow显存OOM，维持数据均衡
        sample_size = len(img_list) if len(img_list) &lt;= TRAIN_DATA_SAMPLE_SIZE else TRAIN_DATA_SAMPLE_SIZE
        train_img_list_random = random.sample(img_list, sample_size)
        for img_name in train_img_list_random:
            img_path = train_path + class_list[c] + '/' + img_name
            # 训练数据不直接加载，训练时加载并做数据增强
            X_train.append(img_path)
            y_train.append(c)

    X_train = np.array(X_train)
    y_train = np.array(y_train)

    return X_train, y_train</code></pre> 
<p>训练时，构造load_augmentation_data函数从对应目录读取图片（并进行随机数据增强）。</p> 
<pre class="has"><code class="language-python">def load_augmentation_data(X_train_array):
    X_train = []
    for img_path in X_train_array:
        img = cv2.imread(img_path)
        img = random_rotation(img)
        img = random_exposure(img)
        img = cv2.resize(img, (default_image_size, default_image_size))
        # 归一化
        img = img / 255.0
        X_train.append(img)

    return np.array(X_train)</code></pre> 
<p> 通过duration计算每个batch的训练时长</p> 
<pre class="has"><code class="language-python">            np.random.shuffle(train_indicies)
            # make sure we iterate over the dataset once
            for i in range(int(math.ceil(Xd_num / BATCH_SIZE)) - 1):
                # generate indicies for the batch
                start_idx = (i * BATCH_SIZE) % Xd_num
                idx = train_indicies[start_idx:start_idx + BATCH_SIZE]
                start_time = time.time()
                X_train_batch = load_augmentation_data(X_train[idx])
                X_rs = np.reshape(X_train_batch, [BATCH_SIZE, image_size_H, image_size_W, num_channels])
                # create a feed dictionary for this batch
                feed_dict = {X: X_rs,
                             y: y_train[idx],
                             k_prob: keep_prob,
                             is_training: True}
                
                _, loss_value, step = sess.run([train_step, loss, global_step], feed_dict=feed_dict)
                duration = time.time()-start_time</code></pre> 
<p>随机查看第573个epoch，每个batch的训练时长</p> 
<blockquote> 
 <p>=epoch : 573 ==========<br> ==shuffle training data==<br> ===epoch : 573 ,batch : 0 ,training loss : 0.000939467, duration : 6.990  ===<br> ===epoch : 573 ,batch : 1 ,training loss : 0.00018375, duration : 5.298  ===<br> ===epoch : 573 ,batch : 2 ,training loss : 0.00446536, duration : 3.874  ===<br> ===epoch : 573 ,batch : 3 ,training loss : 0.00122624, duration : 6.115  ===<br> ===epoch : 573 ,batch : 4 ,training loss : 4.27897e-05, duration : 5.215  ===<br> ===epoch : 573 ,batch : 5 ,training loss : 0.000973509, duration : 7.537  ===<br> ===epoch : 573 ,batch : 6 ,training loss : 0.00018321, duration : 4.449  ===<br> ===epoch : 573 ,batch : 7 ,training loss : 0.000185303, duration : 5.799  ===<br> ===epoch : 573 ,batch : 8 ,training loss : 0.000451507, duration : 4.623  ===<br> ===epoch : 573 ,batch : 9 ,training loss : 0.00191352, duration : 6.497  ===<br> ===epoch : 573 ,batch : 10 ,training loss : 0.000114542, duration : 7.248  ===<br> ===epoch : 573 ,batch : 11 ,training loss : 0.000165993, duration : 4.750  ===<br> ===epoch : 573 ,batch : 12 ,training loss : 0.000519277, duration : 5.391  ===<br> ===epoch : 573 ,batch : 13 ,training loss : 0.000214111, duration : 3.950  ===<br> ===epoch : 573 ,batch : 14 ,training loss : 0.000279504, duration : 4.913  ===<br> ===epoch : 573 ,batch : 15 ,training loss : 0.000331382, duration : 7.457  ===<br> ===epoch : 573 ,batch : 16 ,training loss : 0.000179745, duration : 6.576  ===<br> ===epoch : 573 ,batch : 17 ,training loss : 0.000352278, duration : 5.640  ===<br> ===epoch : 573 ,batch : 18 ,training loss : 0.000146118, duration : 6.106  ===<br> ===epoch : 573 ,batch : 19 ,training loss : 0.000144974, duration : 5.775  ===<br> ===epoch : 573 ,batch : 20 ,training loss : 0.000233854, duration : 5.554  ===<br> ===epoch : 573 ,batch : 21 ,training loss : 0.00214951, duration : 6.736  ===<br> ===epoch : 573 ,batch : 22 ,training loss : 0.000471303, duration : 6.563  ===<br> ===epoch : 573 ,batch : 23 ,training loss : 0.0403984, duration : 4.057  ===<br> ===epoch : 573 ,batch : 24 ,training loss : 0.000338226, duration : 5.766  ===<br> ===epoch : 573 ,batch : 25 ,training loss : 0.000138452, duration : 4.761  ===<br> ===epoch : 573 ,batch : 26 ,training loss : 0.000272711, duration : 5.982  ===<br> ===epoch : 573 ,batch : 27 ,training loss : 8.50688e-05, duration : 6.151  ===<br> ===epoch : 573 ,batch : 28 ,training loss : 0.00122387, duration : 4.286  ===<br> ===epoch : 573 ,batch : 29 ,training loss : 0.000393897, duration : 6.040  ===<br> ===epoch : 573 ,batch : 30 ,training loss : 0.0108629, duration : 4.741  ===<br> ===epoch : 573 ,batch : 31 ,training loss : 0.000192479, duration : 4.712  ===<br> ===epoch : 573 ,batch : 32 ,training loss : 8.31924e-05, duration : 4.378  ===<br> ===epoch : 573 ,batch : 33 ,training loss : 0.000232057, duration : 5.263  ===<br> ===epoch : 573 ,batch : 34 ,training loss : 0.000416539, duration : 6.058  ===<br> ===epoch : 573 ,batch : 35 ,training loss : 0.000387605, duration : 6.199  ===<br> ===epoch : 573 ,batch : 36 ,training loss : 0.000366599, duration : 6.499  ===<br> ===epoch : 573 ,batch : 37 ,training loss : 0.00061386, duration : 5.950  ===<br> ===epoch : 573 ,batch : 38 ,training loss : 0.000292077, duration : 4.811  ===<br> ===epoch : 573 ,batch : 39 ,training loss : 0.000189406, duration : 6.279  ===<br> ===epoch : 573 ,batch : 40 ,training loss : 0.000306261, duration : 4.495  ===<br> ===epoch : 573 ,batch : 41 ,training loss : 0.000369673, duration : 6.078  ===<br> ===epoch : 573 ,batch : 42 ,training loss : 0.000378717, duration : 7.124  ===<br> ===epoch : 573 ,batch : 43 ,training loss : 0.000119711, duration : 4.739  ===<br> ===epoch : 573 ,batch : 44 ,training loss : 0.000523766, duration : 3.970  ===<br> ===epoch : 573 ,batch : 45 ,training loss : 0.000165045, duration : 6.820  ===<br> ===epoch : 573 ,batch : 46 ,training loss : 0.000131299, duration : 6.152  ===<br> ===epoch : 573 ,batch : 47 ,training loss : 0.000267241, duration : 5.596  ===<br> ===epoch : 573 ,batch : 48 ,training loss : 0.000307097, duration : 5.661  ===<br> ===epoch : 573 ,batch : 49 ,training loss : 7.79769e-05, duration : 6.451  ===<br> ===epoch : 573 ,batch : 50 ,training loss : 0.000377345, duration : 3.622  ===<br> ===epoch : 573 ,batch : 51 ,training loss : 0.000201783, duration : 5.403  ===<br> ===epoch : 573 ,batch : 52 ,training loss : 0.000388394, duration : 4.989  ===<br> ===epoch : 573 ,batch : 53 ,training loss : 0.00268338, duration : 8.330  ===<br> ===epoch : 573 ,batch : 54 ,training loss : 0.000297159, duration : 3.592  ===<br> ===epoch : 573 ,batch : 55 ,training loss : 0.00193538, duration : 5.746  ===<br> ===epoch : 573 ,batch : 56 ,training loss : 0.000743068, duration : 5.976  ===<br> ===epoch : 573 ,batch : 57 ,training loss : 0.000149464, duration : 5.746  ===<br> ===epoch : 573 ,batch : 58 ,training loss : 0.000302717, duration : 6.536  ===<br> ===epoch : 573 ,batch : 59 ,training loss : 0.000622811, duration : 7.743  ===<br> ===epoch : 573 ,batch : 60 ,training loss : 0.00157522, duration : 6.933  ===<br> ===epoch : 573 ,batch : 61 ,training loss : 0.00044104, duration : 6.160  ===</p> 
</blockquote> 
<p>训练时长分析：平均每个batch需要<strong>5.9s，</strong>通过nvidia-smi命令观察GPU使用情况可以发现，GPU的利用率长时间为0%，</p> 
<p><img alt="" class="has" height="54" src="https://images2.imgbox.com/4f/7f/HvMbciWa_o.png" width="653"></p> 
<p>可以推断，opencv从硬盘读取图片并进行解码、数据增强等操作，并未使用GPU进行加速，仅在运行tensorflow相关操作时利用GPU进行加速，想加速训练，应该优化代码，尽量提升GPU利用率。</p> 
<h3 id="3.3%E3%80%81%E6%96%B9%E5%BC%8F%E4%BA%8C%EF%BC%88tobtytes%E5%86%99%E5%85%A5%EF%BC%89%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E5%92%8C%E8%AE%AD%E7%BB%83%E6%97%B6%E9%95%BF%E5%88%86%E6%9E%90%EF%BC%9A">3.3、方式二（<strong>tobtytes写入</strong>）数据读取和训练时长分析</h3> 
<p>方式二（tobytes写入）数据读取：利用tensorflow的队列自动生成数据</p> 
<p>ps：参考 tensorflow读取数据-tfrecord格式 <a href="https://blog.csdn.net/happyhorizion/article/details/77894055">https://blog.csdn.net/happyhorizion/article/details/77894055</a></p> 
<p><strong>特别注意：</strong></p> 
<p>1、使用 <strong>tobytes</strong> 方法写入tfrecords中，需对应使用 <strong>tf.decode_raw</strong> 方法进行解码；</p> 
<p>2、使用<strong>tf.image.convert_image_dtype </strong>进行归一化操作，必须保证输入的每位像素都是整数。要特别注意<strong> tf.image.resize_images </strong>操作的顺序，<strong>先归一化，再resize</strong>，resize中不同参数返回的图片像素值可能不是整型。笔者曾经忽略了这个细节，花费了大量时间用于调试。</p> 
<p>ps： 参考 Tensorflow中图像处理函数(图像大小调整)  <a href="https://blog.csdn.net/zSean/article/details/76383100">https://blog.csdn.net/zSean/article/details/76383100</a></p> 
<pre class="has"><code class="language-python">def pre_process_img(image):
    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)
    image = tf.image.random_brightness(image, max_delta=32. / 255)
    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)
    image = tf.image.random_hue(image, max_delta=0.2)
    image = tf.contrib.image.rotate(image,  np.random.randint(-10, 10))
    return image


def read_and_decode(filename, batch_size, capacity, min_after_dequeue):
    filename_queue = tf.train.string_input_producer([filename],shuffle=False) #读入流中
    reader = tf.TFRecordReader()
    _, serialized_example = reader.read(filename_queue)   #返回文件名和文件
    features = tf.parse_single_example(serialized_example,
                                       features={
                                           'image/class/label': tf.FixedLenFeature([], tf.int64),
                                           'image/raw': tf.FixedLenFeature([], tf.string),
                                           'image/width': tf.FixedLenFeature([], tf.int64),
                                           'image/height':tf.FixedLenFeature([], tf.int64),
                                       })  #取出包含image和label的feature对象

    image = tf.decode_raw(features['image/raw'], tf.uint8)
    width = tf.cast(features['image/width'], tf.int32)
    height = tf.cast(features['image/height'], tf.int32)
    image = tf.reshape(image,[height,width,3])   
    
    image = tf.image.convert_image_dtype(image, dtype=tf.float32)
    image = tf.image.resize_images(image, [default_image_size, default_image_size])
    image = pre_process_img(image)

    label = tf.cast(features['image/class/label'], tf.int32)
    # 生成batch
    batch_image, batch_label = tf.train.shuffle_batch([image, label], batch_size=batch_size,
                                                      capacity=capacity, min_after_dequeue=min_after_dequeue)
    return batch_image, batch_label</code></pre> 
<p>通过sess.run([X_train_batch, Y_train_label])生成训练数据后，再送入神经网络进行训练。</p> 
<p>通过duration计算每个batch的训练时长。</p> 
<pre class="has"><code class="language-python">            b_num = int(math.ceil(FLAGS.TRAIN_NUM_EPOCH / FLAGS.BATCH_SIZE)) - 1
            for i in range(b_num):
                start_time = time.time()
                train_batch,train_label = sess.run([X_train_batch, Y_train_label])
                feed_dict = {X: train_batch,
                             y: train_label,
                             is_training: True}                           	
                _, loss_value, step,acc = sess.run([train_step, loss, global_step,accuracy], feed_dict=feed_dict)
                duration = time.time()-start_time</code></pre> 
<p>随机查看第739个epoch，每个batch的训练时长</p> 
<blockquote> 
 <p>===epoch : 739 ,batch : 0 ,training loss : 0.000458688 , accuracy : 1.000000, duration : 0.560  ===<br> ===epoch : 739 ,batch : 1 ,training loss : 0.000252981 , accuracy : 1.000000, duration : 0.591  ===<br> ===epoch : 739 ,batch : 2 ,training loss : 0.000421034 , accuracy : 1.000000, duration : 0.578  ===<br> ===epoch : 739 ,batch : 3 ,training loss : 0.000621131 , accuracy : 1.000000, duration : 0.633  ===<br> ===epoch : 739 ,batch : 4 ,training loss : 0.000471209 , accuracy : 1.000000, duration : 0.559  ===<br> ===epoch : 739 ,batch : 5 ,training loss : 0.00109829 , accuracy : 1.000000, duration : 0.578  ===<br> ===epoch : 739 ,batch : 6 ,training loss : 0.000799854 , accuracy : 1.000000, duration : 0.606  ===<br> ===epoch : 739 ,batch : 7 ,training loss : 0.00094151 , accuracy : 1.000000, duration : 0.569  ===<br> ===epoch : 739 ,batch : 8 ,training loss : 0.00357334 , accuracy : 1.000000, duration : 0.561  ===<br> ===epoch : 739 ,batch : 9 ,training loss : 0.000717244 , accuracy : 1.000000, duration : 0.581  ===<br> ===epoch : 739 ,batch : 10 ,training loss : 0.00162466 , accuracy : 1.000000, duration : 0.563  ===<br> ===epoch : 739 ,batch : 11 ,training loss : 0.00216218 , accuracy : 1.000000, duration : 0.591  ===<br> ===epoch : 739 ,batch : 12 ,training loss : 0.00305078 , accuracy : 1.000000, duration : 0.578  ===<br> ===epoch : 739 ,batch : 13 ,training loss : 0.00485827 , accuracy : 1.000000, duration : 0.570  ===<br> ===epoch : 739 ,batch : 14 ,training loss : 0.000763111 , accuracy : 1.000000, duration : 0.612  ===<br> ===epoch : 739 ,batch : 15 ,training loss : 0.0012173 , accuracy : 1.000000, duration : 0.559  ===<br> ===epoch : 739 ,batch : 16 ,training loss : 0.000912588 , accuracy : 1.000000, duration : 0.590  ===<br> ===epoch : 739 ,batch : 17 ,training loss : 0.000284491 , accuracy : 1.000000, duration : 0.591  ===<br> ===epoch : 739 ,batch : 18 ,training loss : 0.000173526 , accuracy : 1.000000, duration : 0.595  ===<br> ===epoch : 739 ,batch : 19 ,training loss : 0.000246254 , accuracy : 1.000000, duration : 0.563  ===<br> ===epoch : 739 ,batch : 20 ,training loss : 0.000206566 , accuracy : 1.000000, duration : 0.596  ===<br> ===epoch : 739 ,batch : 21 ,training loss : 0.000116531 , accuracy : 1.000000, duration : 0.564  ===<br> ===epoch : 739 ,batch : 22 ,training loss : 8.58229e-05 , accuracy : 1.000000, duration : 0.596  ===<br> ===epoch : 739 ,batch : 23 ,training loss : 0.000143532 , accuracy : 1.000000, duration : 0.634  ===<br> ===epoch : 739 ,batch : 24 ,training loss : 0.000129403 , accuracy : 1.000000, duration : 0.577  ===<br> ===epoch : 739 ,batch : 25 ,training loss : 4.88722e-05 , accuracy : 1.000000, duration : 0.557  ===<br> ===epoch : 739 ,batch : 26 ,training loss : 0.000311686 , accuracy : 1.000000, duration : 0.593  ===<br> ===epoch : 739 ,batch : 27 ,training loss : 0.000107129 , accuracy : 1.000000, duration : 0.577  ===<br> ===epoch : 739 ,batch : 28 ,training loss : 8.82945e-05 , accuracy : 1.000000, duration : 0.567  ===<br> ===epoch : 739 ,batch : 29 ,training loss : 5.18051e-05 , accuracy : 1.000000, duration : 0.572  ===<br> ===epoch : 739 ,batch : 30 ,training loss : 4.80934e-05 , accuracy : 1.000000, duration : 0.634  ===<br> ===epoch : 739 ,batch : 31 ,training loss : 0.000163095 , accuracy : 1.000000, duration : 0.575  ===<br> ===epoch : 739 ,batch : 32 ,training loss : 0.00010896 , accuracy : 1.000000, duration : 0.552  ===<br> ===epoch : 739 ,batch : 33 ,training loss : 0.000137048 , accuracy : 1.000000, duration : 0.629  ===<br> ===epoch : 739 ,batch : 34 ,training loss : 0.00027466 , accuracy : 1.000000, duration : 0.595  ===<br> ===epoch : 739 ,batch : 35 ,training loss : 0.000142379 , accuracy : 1.000000, duration : 0.570  ===<br> ===epoch : 739 ,batch : 36 ,training loss : 0.000336119 , accuracy : 1.000000, duration : 0.573  ===<br> ===epoch : 739 ,batch : 37 ,training loss : 0.00034926 , accuracy : 1.000000, duration : 0.558  ===<br> ===epoch : 739 ,batch : 38 ,training loss : 0.000100725 , accuracy : 1.000000, duration : 0.629  ===<br> ===epoch : 739 ,batch : 39 ,training loss : 0.0015204 , accuracy : 1.000000, duration : 0.556  ===<br> ===epoch : 739 ,batch : 40 ,training loss : 0.000176535 , accuracy : 1.000000, duration : 0.571  ===<br> ===epoch : 739 ,batch : 41 ,training loss : 0.000625736 , accuracy : 1.000000, duration : 0.570  ===<br> ===epoch : 739 ,batch : 42 ,training loss : 0.000519971 , accuracy : 1.000000, duration : 0.556  ===<br> ===epoch : 739 ,batch : 43 ,training loss : 0.00245568 , accuracy : 1.000000, duration : 0.555  ===<br> ===epoch : 739 ,batch : 44 ,training loss : 0.000495821 , accuracy : 1.000000, duration : 0.634  ===<br> ===epoch : 739 ,batch : 45 ,training loss : 0.000780712 , accuracy : 1.000000, duration : 0.581  ===<br> ===epoch : 739 ,batch : 46 ,training loss : 0.00104021 , accuracy : 1.000000, duration : 0.595  ===<br> ===epoch : 739 ,batch : 47 ,training loss : 0.000877889 , accuracy : 1.000000, duration : 0.580  ===<br> ===epoch : 739 ,batch : 48 ,training loss : 0.00239627 , accuracy : 1.000000, duration : 0.569  ===<br> ===epoch : 739 ,batch : 49 ,training loss : 0.00187364 , accuracy : 1.000000, duration : 0.563  ===<br> ===epoch : 739 ,batch : 50 ,training loss : 0.00190735 , accuracy : 1.000000, duration : 0.594  ===<br> ===epoch : 739 ,batch : 51 ,training loss : 0.00623525 , accuracy : 1.000000, duration : 0.574  ===<br> ===epoch : 739 ,batch : 52 ,training loss : 0.00365224 , accuracy : 1.000000, duration : 0.594  ===<br> ===epoch : 739 ,batch : 53 ,training loss : 0.00609043 , accuracy : 1.000000, duration : 0.561  ===<br> ===epoch : 739 ,batch : 54 ,training loss : 0.000788014 , accuracy : 1.000000, duration : 0.566  ===<br> ===epoch : 739 ,batch : 55 ,training loss : 0.000209338 , accuracy : 1.000000, duration : 0.625  ===<br> ===epoch : 739 ,batch : 56 ,training loss : 3.40514e-05 , accuracy : 1.000000, duration : 0.596  ===<br> ===epoch : 739 ,batch : 57 ,training loss : 0.000287742 , accuracy : 1.000000, duration : 0.574  ===<br> ===epoch : 739 ,batch : 58 ,training loss : 6.50318e-05 , accuracy : 1.000000, duration : 0.567  ===<br> ===epoch : 739 ,batch : 59 ,training loss : 0.000213395 , accuracy : 1.000000, duration : 0.558  ===<br> ===epoch : 739 ,batch : 60 ,training loss : 5.59622e-05 , accuracy : 1.000000, duration : 0.590  ===<br> ===epoch : 739 ,batch : 61 ,training loss : 0.000133713 , accuracy : 1.000000, duration : 0.563  ===</p> 
</blockquote> 
<p>训练时长分析：平均每个batch需要<strong>0.58s，</strong>通过nvidia-smi命令观察GPU使用情况可以发现，GPU利用率长时间稳定于较高水平</p> 
<p><img alt="" class="has" height="96" src="https://images2.imgbox.com/b2/0f/6k3bQhfa_o.png" width="630"></p> 
<p>GPU得到有效利用，可以提升训练效率，降低训练时长。</p> 
<p>3.4、方式三（gfile写入） 数据读 取和训练时长分析：</p> 
<p>方式三（gfile写入）数据读取：利用tensorflow的队列自动生成数据</p> 
<p>注意：这里对应的解码方式为 <strong>tf.image.decode_jpeg</strong> 方法，区别于方式二</p> 
<pre class="has"><code class="language-python">def pre_process_img(image):
    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)
    image = tf.image.random_brightness(image, max_delta=32. / 255)
    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)
    image = tf.image.random_hue(image, max_delta=0.2)
    image = tf.contrib.image.rotate(image,  np.random.randint(-10, 10))
    return image


def read_and_decode(filename, batch_size, capacity, min_after_dequeue):
    filename_queue = tf.train.string_input_producer([filename],shuffle=False) #读入流中
    reader = tf.TFRecordReader()
    _, serialized_example = reader.read(filename_queue)   #返回文件名和文件
    features = tf.parse_single_example(serialized_example,
                                       features={
                                           'image/class/label': tf.FixedLenFeature([], tf.int64),
                                           'image/encoded': tf.FixedLenFeature([], tf.string),
                                           'image/width': tf.FixedLenFeature([], tf.int64),
                                           'image/height':tf.FixedLenFeature([], tf.int64),
                                       })  #取出包含image和label的feature对象

    image = tf.image.decode_jpeg(features['image/encoded'])
    width = tf.cast(features['image/width'], tf.int32)
    height = tf.cast(features['image/height'], tf.int32)
    image = tf.reshape(image,[height,width,3])   
    
    image = tf.image.convert_image_dtype(image, dtype=tf.float32)
    image = tf.image.resize_images(image, [default_image_size, default_image_size])
    image = pre_process_img(image)

    label = tf.cast(features['image/class/label'], tf.int32)
    # 生成batch
    batch_image, batch_label = tf.train.shuffle_batch([image, label], batch_size=batch_size,
                                                      capacity=capacity, min_after_dequeue=min_after_dequeue)
    return batch_image, batch_label</code></pre> 
<p> 训练脚本同方式二相同，不在赘述</p> 
<p>随机查看第200个epoch，每个batch的训练时长</p> 
<blockquote> 
 <p>===epoch : 200 ,batch : 0 ,training loss : 9.96804e-06 , accuracy : 1.000000, duration : 0.617  ===<br> ===epoch : 200 ,batch : 1 ,training loss : 2.79766e-06 , accuracy : 1.000000, duration : 0.652  ===<br> ===epoch : 200 ,batch : 2 ,training loss : 7.1747e-06 , accuracy : 1.000000, duration : 0.628  ===<br> ===epoch : 200 ,batch : 3 ,training loss : 2.34188e-05 , accuracy : 1.000000, duration : 0.671  ===<br> ===epoch : 200 ,batch : 4 ,training loss : 3.16147e-05 , accuracy : 1.000000, duration : 0.728  ===<br> ===epoch : 200 ,batch : 5 ,training loss : 0.000155467 , accuracy : 1.000000, duration : 0.645  ===<br> ===epoch : 200 ,batch : 6 ,training loss : 0.000191888 , accuracy : 1.000000, duration : 0.671  ===<br> ===epoch : 200 ,batch : 7 ,training loss : 3.72732e-05 , accuracy : 1.000000, duration : 0.599  ===<br> ===epoch : 200 ,batch : 8 ,training loss : 7.74464e-06 , accuracy : 1.000000, duration : 0.623  ===<br> ===epoch : 200 ,batch : 9 ,training loss : 1.76095e-05 , accuracy : 1.000000, duration : 0.610  ===<br> ===epoch : 200 ,batch : 10 ,training loss : 8.60901e-06 , accuracy : 1.000000, duration : 0.604  ===<br> ===epoch : 200 ,batch : 11 ,training loss : 0.000154941 , accuracy : 1.000000, duration : 0.620  ===<br> ===epoch : 200 ,batch : 12 ,training loss : 0.00011795 , accuracy : 1.000000, duration : 0.600  ===<br> ===epoch : 200 ,batch : 13 ,training loss : 4.72731e-06 , accuracy : 1.000000, duration : 0.647  ===<br> ===epoch : 200 ,batch : 14 ,training loss : 6.96867e-05 , accuracy : 1.000000, duration : 0.638  ===<br> ===epoch : 200 ,batch : 15 ,training loss : 2.29104e-06 , accuracy : 1.000000, duration : 0.665  ===<br> ===epoch : 200 ,batch : 16 ,training loss : 3.4905e-06 , accuracy : 1.000000, duration : 0.617  ===<br> ===epoch : 200 ,batch : 17 ,training loss : 3.55389e-06 , accuracy : 1.000000, duration : 0.696  ===<br> ===epoch : 200 ,batch : 18 ,training loss : 6.92515e-06 , accuracy : 1.000000, duration : 0.686  ===<br> ===epoch : 200 ,batch : 19 ,training loss : 6.75381e-06 , accuracy : 1.000000, duration : 0.645  ===<br> ===epoch : 200 ,batch : 20 ,training loss : 2.99511e-06 , accuracy : 1.000000, duration : 0.718  ===<br> ===epoch : 200 ,batch : 21 ,training loss : 3.66562e-06 , accuracy : 1.000000, duration : 0.596  ===<br> ===epoch : 200 ,batch : 22 ,training loss : 8.90663e-06 , accuracy : 1.000000, duration : 0.684  ===<br> ===epoch : 200 ,batch : 23 ,training loss : 4.35108e-06 , accuracy : 1.000000, duration : 0.635  ===<br> ===epoch : 200 ,batch : 24 ,training loss : 1.28055e-05 , accuracy : 1.000000, duration : 0.613  ===<br> ===epoch : 200 ,batch : 25 ,training loss : 2.93427e-05 , accuracy : 1.000000, duration : 0.686  ===<br> ===epoch : 200 ,batch : 26 ,training loss : 2.97213e-05 , accuracy : 1.000000, duration : 0.621  ===<br> ===epoch : 200 ,batch : 27 ,training loss : 3.35554e-05 , accuracy : 1.000000, duration : 0.627  ===<br> ===epoch : 200 ,batch : 28 ,training loss : 8.73921e-06 , accuracy : 1.000000, duration : 0.640  ===<br> ===epoch : 200 ,batch : 29 ,training loss : 8.87718e-06 , accuracy : 1.000000, duration : 0.633  ===<br> ===epoch : 200 ,batch : 30 ,training loss : 1.19316e-05 , accuracy : 1.000000, duration : 0.730  ===<br> ===epoch : 200 ,batch : 31 ,training loss : 7.80038e-06 , accuracy : 1.000000, duration : 0.639  ===<br> ===epoch : 200 ,batch : 32 ,training loss : 3.0789e-05 , accuracy : 1.000000, duration : 0.639  ===<br> ===epoch : 200 ,batch : 33 ,training loss : 3.27821e-06 , accuracy : 1.000000, duration : 0.635  ===<br> ===epoch : 200 ,batch : 34 ,training loss : 3.6002e-05 , accuracy : 1.000000, duration : 0.641  ===<br> ===epoch : 200 ,batch : 35 ,training loss : 1.35211e-05 , accuracy : 1.000000, duration : 0.635  ===<br> ===epoch : 200 ,batch : 36 ,training loss : 5.59903e-06 , accuracy : 1.000000, duration : 0.733  ===<br> ===epoch : 200 ,batch : 37 ,training loss : 7.75578e-06 , accuracy : 1.000000, duration : 0.649  ===<br> ===epoch : 200 ,batch : 38 ,training loss : 5.64479e-05 , accuracy : 1.000000, duration : 0.622  ===<br> ===epoch : 200 ,batch : 39 ,training loss : 4.32871e-06 , accuracy : 1.000000, duration : 0.664  ===<br> ===epoch : 200 ,batch : 40 ,training loss : 1.12876e-06 , accuracy : 1.000000, duration : 0.674  ===<br> ===epoch : 200 ,batch : 41 ,training loss : 0.000441137 , accuracy : 1.000000, duration : 0.638  ===<br> ===epoch : 200 ,batch : 42 ,training loss : 1.77718e-05 , accuracy : 1.000000, duration : 0.634  ===<br> ===epoch : 200 ,batch : 43 ,training loss : 2.89471e-05 , accuracy : 1.000000, duration : 0.659  ===<br> ===epoch : 200 ,batch : 44 ,training loss : 8.2659e-06 , accuracy : 1.000000, duration : 0.641  ===<br> ===epoch : 200 ,batch : 45 ,training loss : 1.32167e-05 , accuracy : 1.000000, duration : 0.641  ===<br> ===epoch : 200 ,batch : 46 ,training loss : 2.93152e-05 , accuracy : 1.000000, duration : 0.704  ===<br> ===epoch : 200 ,batch : 47 ,training loss : 1.20351e-05 , accuracy : 1.000000, duration : 0.588  ===<br> ===epoch : 200 ,batch : 48 ,training loss : 0.000137681 , accuracy : 1.000000, duration : 0.731  ===<br> ===epoch : 200 ,batch : 49 ,training loss : 5.31589e-06 , accuracy : 1.000000, duration : 0.638  ===<br> ===epoch : 200 ,batch : 50 ,training loss : 2.68441e-05 , accuracy : 1.000000, duration : 0.597  ===<br> ===epoch : 200 ,batch : 51 ,training loss : 7.99048e-06 , accuracy : 1.000000, duration : 0.590  ===<br> ===epoch : 200 ,batch : 52 ,training loss : 3.29691e-05 , accuracy : 1.000000, duration : 0.650  ===<br> ===epoch : 200 ,batch : 53 ,training loss : 1.87034e-05 , accuracy : 1.000000, duration : 0.681  ===<br> ===epoch : 200 ,batch : 54 ,training loss : 1.56731e-05 , accuracy : 1.000000, duration : 0.609  ===<br> ===epoch : 200 ,batch : 55 ,training loss : 9.93123e-06 , accuracy : 1.000000, duration : 0.720  ===<br> ===epoch : 200 ,batch : 56 ,training loss : 1.89678e-05 , accuracy : 1.000000, duration : 0.630  ===<br> ===epoch : 200 ,batch : 57 ,training loss : 1.54174e-05 , accuracy : 1.000000, duration : 0.625  ===<br> ===epoch : 200 ,batch : 58 ,training loss : 7.36479e-06 , accuracy : 1.000000, duration : 0.642  ===<br> ===epoch : 200 ,batch : 59 ,training loss : 9.15997e-06 , accuracy : 1.000000, duration : 0.705  ===<br> ===epoch : 200 ,batch : 60 ,training loss : 1.20092e-05 , accuracy : 1.000000, duration : 0.716  ===<br> ===epoch : 200 ,batch : 61 ,training loss : 3.4272e-06 , accuracy : 1.000000, duration : 0.684  ===</p> 
</blockquote> 
<p>训练时长分析：平均每个batch需要<strong>0.64s</strong></p> 
<p> </p> 
<h3 id="3.5%E3%80%81%E8%AE%AD%E7%BB%83%E8%AF%84%E4%BC%B0%E6%80%BB%E7%BB%93">3.5、训练评估总结</h3> 
<p>根据统计情况，平均每batch（32张）训练时长排序： <strong>方式二（tobtytes写入） &lt;  方式三（gfile写入）  &lt;  方式一（原图） ； </strong></p> 
<p>根据统计情况，方式二（tobtytes写入）较 方式三（gfile写入） 可以缩短10%训练时长；</p> 
<p>方式一（原图）训练时长超过其余两种方式<strong>十倍</strong>；</p> 
<h2 id="4%E3%80%81%E6%80%BB%E7%BB%93%EF%BC%9A">4、总结</h2> 
<table cellspacing="0"><tbody><tr><td style="background-color:#ffc000;vertical-align:middle;width:188pt;"><span style="color:#000000;">　</span></td><td style="background-color:#ffc000;vertical-align:middle;width:174pt;"><span style="color:#000000;">每batch训练时长（单位：秒）</span></td><td style="background-color:#ffc000;vertical-align:middle;width:213pt;"><span style="color:#000000;">train和val集的总数据大小（单位MB）</span></td></tr><tr><td style="vertical-align:middle;"><span style="color:#000000;">方式三（gfile写入tfrecords）</span></td><td style="vertical-align:middle;"><span style="color:#000000;">0.64</span></td><td style="vertical-align:middle;"><span style="color:#000000;">617</span></td></tr><tr><td style="vertical-align:middle;"><span style="color:#000000;">方式二（tobtytes写入tfrecords）</span></td><td style="vertical-align:middle;"><span style="color:#000000;">0.58</span></td><td style="vertical-align:middle;"><span style="color:#000000;">637952</span></td></tr><tr><td style="vertical-align:middle;"><span style="color:#000000;">方式一（原图）</span></td><td style="vertical-align:middle;"><span style="color:#000000;">6.4</span></td><td style="vertical-align:middle;"><span style="color:#000000;">623</span></td></tr></tbody></table> 
<p>总体来说，在相似硬件配置条件下</p> 
<p>方式三（<span style="color:#000000;">gfile写入tfrecords</span>）达到了空间和性能的最佳配比，也是tf官方采用的方式，推荐在大多数场景中使用；</p> 
<p>方式二（<span style="color:#000000;">tobtytes写入tfrecords</span>）适合追求最短训练时长；</p> 
<p>方式一（原图）浪费了GPU的性能，不推荐使用；</p> 
<p> </p> 
<p> </p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5fcc1f87ea23a1eadf79eb00b9f8b925/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Android Studio无法识别Genymotion设备的解决方法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/3632bd336d506cfbc3a606f2e41e5333/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">数据结构中一些常用的算法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>