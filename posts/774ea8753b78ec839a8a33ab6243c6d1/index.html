<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>神经网络与深度学习（八）网络优化与正则化（3）不同优化算法比较 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="神经网络与深度学习（八）网络优化与正则化（3）不同优化算法比较" />
<meta property="og:description" content="目录
7.3 不同优化算法的比较分析
7.3.1 优化算法的实验设定
7.3.1.1 2D可视化实验 7.3.1.2 简单拟合实验 7.3.2 学习率调整
7.3.2.1 AdaGrad算法 7.3.2.2 RMSprop算法 7.3.3 梯度估计修正 7.3.3.1 动量法 7.3.3.2 Adam算法 7.3.4 不同优化器的3D可视化对比 7.3.4.1 构建一个三维空间中的被优化函数 【选做题】
参考资料
7.3 不同优化算法的比较分析 除了批大小对模型收敛速度的影响外，学习率和梯度估计也是影响神经网络优化的重要因素。神经网络优化中常用的优化方法也主要是如下两方面的改进，包括：
学习率调整：主要通过自适应地调整学习率使得优化更稳定。这类算法主要有AdaGrad、RMSprop、AdaDelta算法等。梯度估计修正：主要通过修正每次迭代时估计的梯度方向来加快收敛速度。这类算法主要有动量法、Nesterov加速梯度方法等。 除上述方法外，本节还会介绍综合学习率调整和梯度估计修正的优化算法，如Adam算法。
7.3.1 优化算法的实验设定 为了更好地对比不同的优化算法，我们准备两个实验：第一个是2D可视化实验。第二个是简单拟合实验。首先介绍下这两个实验的任务设定。
7.3.1.1 2D可视化实验 为了更好地展示不同优化算法的能力对比，我们选择一个二维空间中的凸函数，然后用不同的优化算法来寻找最优解，并可视化梯度下降过程的轨迹。被优化函数 选择Sphere函数作为被优化函数，并对比它们的优化效果。Sphere函数的定义为：
其中，表示逐元素平方。Sphere函数有全局的最优点。
这里为了展示方便，我们使用二维的输入并略微修改Sphere函数，定义，并根据梯度下降公式计算对的偏导
其中⊙表示逐元素积。
将被优化函数实现为OptimizedFunction算子，其forward方法是Sphere函数的前向计算，backward方法则计算被优化函数对xx的偏导。代码实现如下：
from nndl.op import Op import torch import numpy as np from matplotlib import pyplot as plt class OptimizedFunction(Op): def __init__(self, w): super(OptimizedFunction, self).__init__() self.w = w self." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/774ea8753b78ec839a8a33ab6243c6d1/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-12-10T09:56:28+08:00" />
<meta property="article:modified_time" content="2022-12-10T09:56:28+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">神经网络与深度学习（八）网络优化与正则化（3）不同优化算法比较</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="7.3%20%E4%B8%8D%E5%90%8C%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E7%9A%84%E6%AF%94%E8%BE%83%E5%88%86%E6%9E%90-toc" style="margin-left:0px;"><a href="#7.3%20%E4%B8%8D%E5%90%8C%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E7%9A%84%E6%AF%94%E8%BE%83%E5%88%86%E6%9E%90" rel="nofollow">7.3 不同优化算法的比较分析</a></p> 
<p id="7.3.1%20%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E7%9A%84%E5%AE%9E%E9%AA%8C%E8%AE%BE%E5%AE%9A-toc" style="margin-left:40px;"><a href="#7.3.1%20%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E7%9A%84%E5%AE%9E%E9%AA%8C%E8%AE%BE%E5%AE%9A" rel="nofollow">7.3.1 优化算法的实验设定</a></p> 
<p id="7.3.1.1%202D%E5%8F%AF%E8%A7%86%E5%8C%96%E5%AE%9E%E9%AA%8C%C2%A0-toc" style="margin-left:80px;"><a href="#7.3.1.1%202D%E5%8F%AF%E8%A7%86%E5%8C%96%E5%AE%9E%E9%AA%8C%C2%A0" rel="nofollow">7.3.1.1 2D可视化实验 </a></p> 
<p id="7.3.1.2%20%E7%AE%80%E5%8D%95%E6%8B%9F%E5%90%88%E5%AE%9E%E9%AA%8C%C2%A0-toc" style="margin-left:80px;"><a href="#7.3.1.2%20%E7%AE%80%E5%8D%95%E6%8B%9F%E5%90%88%E5%AE%9E%E9%AA%8C%C2%A0" rel="nofollow">7.3.1.2 简单拟合实验 </a></p> 
<p id="732-学习率调整-toc" style="margin-left:40px;"><a href="#732-%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4" rel="nofollow">7.3.2 学习率调整</a></p> 
<p id="7.3.2.1%20AdaGrad%E7%AE%97%E6%B3%95%C2%A0-toc" style="margin-left:80px;"><a href="#7.3.2.1%20AdaGrad%E7%AE%97%E6%B3%95%C2%A0" rel="nofollow">7.3.2.1 AdaGrad算法 </a></p> 
<p id="7.3.2.2%20RMSprop%E7%AE%97%E6%B3%95%C2%A0-toc" style="margin-left:80px;"><a href="#7.3.2.2%20RMSprop%E7%AE%97%E6%B3%95%C2%A0" rel="nofollow">7.3.2.2 RMSprop算法 </a></p> 
<p id="7.3.3%20%E6%A2%AF%E5%BA%A6%E4%BC%B0%E8%AE%A1%E4%BF%AE%E6%AD%A3%C2%A0-toc" style="margin-left:40px;"><a href="#7.3.3%20%E6%A2%AF%E5%BA%A6%E4%BC%B0%E8%AE%A1%E4%BF%AE%E6%AD%A3%C2%A0" rel="nofollow">7.3.3 梯度估计修正 </a></p> 
<p id="7.3.3.1%20%E5%8A%A8%E9%87%8F%E6%B3%95%C2%A0-toc" style="margin-left:80px;"><a href="#7.3.3.1%20%E5%8A%A8%E9%87%8F%E6%B3%95%C2%A0" rel="nofollow">7.3.3.1 动量法 </a></p> 
<p id="7.3.3.2%20Adam%E7%AE%97%E6%B3%95%C2%A0-toc" style="margin-left:80px;"><a href="#7.3.3.2%20Adam%E7%AE%97%E6%B3%95%C2%A0" rel="nofollow">7.3.3.2 Adam算法 </a></p> 
<p id="7.3.4%20%E4%B8%8D%E5%90%8C%E4%BC%98%E5%8C%96%E5%99%A8%E7%9A%843D%E5%8F%AF%E8%A7%86%E5%8C%96%E5%AF%B9%E6%AF%94%C2%A0-toc" style="margin-left:40px;"><a href="#7.3.4%20%E4%B8%8D%E5%90%8C%E4%BC%98%E5%8C%96%E5%99%A8%E7%9A%843D%E5%8F%AF%E8%A7%86%E5%8C%96%E5%AF%B9%E6%AF%94%C2%A0" rel="nofollow">7.3.4 不同优化器的3D可视化对比 </a></p> 
<p id="7.3.4.1%20%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E4%B8%89%E7%BB%B4%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%9A%84%E8%A2%AB%E4%BC%98%E5%8C%96%E5%87%BD%E6%95%B0%C2%A0-toc" style="margin-left:80px;"><a href="#7.3.4.1%20%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E4%B8%89%E7%BB%B4%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%9A%84%E8%A2%AB%E4%BC%98%E5%8C%96%E5%87%BD%E6%95%B0%C2%A0" rel="nofollow">7.3.4.1 构建一个三维空间中的被优化函数 </a></p> 
<p id="%E3%80%90%E9%80%89%E5%81%9A%E9%A2%98%E3%80%91-toc" style="margin-left:0px;"><a href="#%E3%80%90%E9%80%89%E5%81%9A%E9%A2%98%E3%80%91" rel="nofollow">【选做题】</a></p> 
<p id="%C2%A0%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99-toc" style="margin-left:0px;"><a href="#%C2%A0%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99" rel="nofollow"> 参考资料</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="7.3%20%E4%B8%8D%E5%90%8C%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E7%9A%84%E6%AF%94%E8%BE%83%E5%88%86%E6%9E%90">7.3 不同优化算法的比较分析</h2> 
<p>除了批大小对模型收敛速度的影响外，学习率和梯度估计也是影响神经网络优化的重要因素。神经网络优化中常用的优化方法也主要是如下两方面的改进，包括：</p> 
<ul><li>学习率调整：主要通过自适应地调整学习率使得优化更稳定。这类算法主要有AdaGrad、RMSprop、AdaDelta算法等。</li><li>梯度估计修正：主要通过修正每次迭代时估计的梯度方向来加快收敛速度。这类算法主要有动量法、Nesterov加速梯度方法等。</li></ul> 
<p>除上述方法外，本节还会介绍综合学习率调整和梯度估计修正的优化算法，如Adam算法。</p> 
<h3 id="7.3.1%20%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E7%9A%84%E5%AE%9E%E9%AA%8C%E8%AE%BE%E5%AE%9A">7.3.1 优化算法的实验设定</h3> 
<p>为了更好地对比不同的优化算法，我们准备两个实验：第一个是2D可视化实验。第二个是简单拟合实验。首先介绍下这两个实验的任务设定。</p> 
<h4 id="7.3.1.1%202D%E5%8F%AF%E8%A7%86%E5%8C%96%E5%AE%9E%E9%AA%8C%C2%A0">7.3.1.1 2D可视化实验 </h4> 
<p>为了更好地展示不同优化算法的能力对比，我们选择一个二维空间中的凸函数，然后用不同的优化算法来寻找最优解，并可视化梯度下降过程的轨迹。<strong>被优化函数</strong> 选择Sphere函数作为被优化函数，并对比它们的优化效果。Sphere函数的定义为：</p> 
<p style="text-align:center;"><img alt="sphere(x)=\sum_{d=1}^{D}x_{d}^{2} =x^{2}" class="mathcode" src="https://images2.imgbox.com/fa/98/LkqU2iT7_o.png"></p> 
<p>其中<img alt="x\in \mathbb{R} ^{D}" class="mathcode" src="https://images2.imgbox.com/98/7a/8R523mCM_o.png">，<img alt="x^{2}" class="mathcode" src="https://images2.imgbox.com/2d/6b/rdul1ulw_o.png">表示逐元素平方。Sphere函数有全局的最优点<img alt="x^{*}=0" class="mathcode" src="https://images2.imgbox.com/54/c2/gAAPJxgr_o.png">。</p> 
<p>这里为了展示方便，我们使用二维的输入并略微修改Sphere函数，定义<img alt="sphere(x)=w^{T}x^{2}" class="mathcode" src="https://images2.imgbox.com/e5/8e/0ymSdT6O_o.png">，并根据梯度下降公式计算对<img alt="x" class="mathcode" src="https://images2.imgbox.com/c1/f2/h6FL7wVq_o.png">的偏导</p> 
<p style="text-align:center;"> <img alt="\frac{\partial sphere(x)}{\partial x} =2w\odot x" class="mathcode" src="https://images2.imgbox.com/c3/f1/A6b6cxaa_o.png"></p> 
<p>其中⊙表示逐元素积。</p> 
<p>将被优化函数实现为OptimizedFunction算子，其forward方法是Sphere函数的前向计算，backward方法则计算被优化函数对xx的偏导。代码实现如下：</p> 
<pre><code class="language-python">from nndl.op import Op
import torch
import numpy as np
from matplotlib import pyplot as plt


class OptimizedFunction(Op):
    def __init__(self, w):
        super(OptimizedFunction, self).__init__()
        self.w = w
        self.params = {'x': 0}
        self.grads = {'x': 0}

    def forward(self, x):
        self.params['x'] = x
        return torch.matmul(self.w.T, torch.tensor(torch.square(self.params['x']), dtype=torch.float32))

    def backward(self):
        self.grads['x'] = 2 * torch.multiply(self.w.T, self.params['x'])</code></pre> 
<p><strong>小批量梯度下降优化器</strong> 复用3.1.4.3节定义的梯度下降优化器SimpleBatchGD。按照梯度下降的梯度更新公式<img alt="\theta _{t}\longleftarrow \theta _{t-1}-\alpha g_{t}" class="mathcode" src="https://images2.imgbox.com/f1/70/CZRyhaLu_o.png">进行梯度更新。</p> 
<p><strong>训练函数</strong> 定义一个简易的训练函数，记录梯度下降过程中每轮的参数<img alt="x" class="mathcode" src="https://images2.imgbox.com/fc/d4/fOErD9TP_o.png">和损失。代码实现如下：</p> 
<pre><code class="language-python">import copy
def train_f(model, optimizer, x_init, epoch):

    x = x_init
    all_x = []
    losses = []
    for i in range(epoch):
        all_x.append(copy.copy(x.numpy()))
        loss = model(x)
        losses.append(loss)
        model.backward()
        optimizer.step()
        x = model.params['x']
        print(all_x)
    return torch.tensor(all_x), losses</code></pre> 
<p><strong>可视化函数</strong> 定义一个Visualization类，用于绘制<img alt="x" class="mathcode" src="https://images2.imgbox.com/a8/c5/a3HWzwWF_o.png">的更新轨迹。代码实现如下：</p> 
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt


class Visualization(object):
    def __init__(self):
        """
        初始化可视化类
        """
        # 只画出参数x1和x2在区间[-5, 5]的曲线部分
        x1 = np.arange(-5, 5, 0.1)
        x2 = np.arange(-5, 5, 0.1)
        x1, x2 = np.meshgrid(x1, x2)
        self.init_x = torch.tensor([x1, x2])

    def plot_2d(self, model, x, fig_name):
        """
        可视化参数更新轨迹
        """
        fig, ax = plt.subplots(figsize=(10, 6))
        cp = ax.contourf(self.init_x[0], self.init_x[1], model(self.init_x.transpose(0, 1)),
                         colors=['#e4007f', '#f19ec2', '#e86096', '#eb7aaa', '#f6c8dc', '#f5f5f5', '#000000'])
        c = ax.contour(self.init_x[0], self.init_x[1], model(self.init_x.transpose(0, 1)), colors='black')
        cbar = fig.colorbar(cp)
        ax.plot(x[:, 0], x[:, 1], '-o', color='#000000')
        ax.plot(0, 'r*', markersize=18, color='#fefefe')

        ax.set_xlabel('$x1$')
        ax.set_ylabel('$x2$')

        ax.set_xlim((-2, 5))
        ax.set_ylim((-2, 5))
        plt.savefig(fig_name)</code></pre> 
<p>定义train_and_plot_f函数，调用train_f和Visualization，训练模型并可视化参数更新轨迹。代码实现如下：</p> 
<pre><code class="language-python">import numpy as np


def train_and_plot_f(model, optimizer, epoch, fig_name):
    """
    训练模型并可视化参数更新轨迹
    """
    # 设置x的初始值
    x_init = torch.tensor([3, 4], dtype=torch.float32)
    print('x1 initiate: {}, x2 initiate: {}'.format(x_init[0].numpy(), x_init[1].numpy()))
    x, losses = train_f(model, optimizer, x_init, epoch)
    print(x)
    losses = np.array(losses)

    # 展示x1、x2的更新轨迹
    vis = Visualization()
    vis.plot_2d(model, x, fig_name)</code></pre> 
<p><strong>模型训练与可视化</strong> 指定Sphere函数中ww的值，实例化被优化函数，通过小批量梯度下降法更新参数，并可视化<img alt="x" class="mathcode" src="https://images2.imgbox.com/e0/da/iIqTGgWw_o.png">的更新轨迹。</p> 
<pre><code class="language-python">from nndl.opitimizer import SimpleBatchGD

# 固定随机种子
torch.manual_seed(0)
w = torch.tensor([0.2, 2])
model = OptimizedFunction(w)
opt = SimpleBatchGD(init_lr=0.2, model=model)
train_and_plot_f(model, opt, epoch=20, fig_name='opti-vis-para.pdf')</code></pre> 
<p>运行结果：</p> 
<pre><code class="language-python">x1 initiate: 3.0, x2 initiate: 4.0</code></pre> 
<p class="img-center"><img alt="" height="431" src="https://images2.imgbox.com/4f/a2/nQoksAxc_o.png" width="658"></p> 
<p>输出图中不同颜色代表<img alt="f(x_{1},x_{2})" class="mathcode" src="https://images2.imgbox.com/a1/9a/wflJz1Ix_o.png">的值，具体数值可以参考图右侧的对应表，比如深粉色区域代表<img alt="f(x_{1},x_{2})" class="mathcode" src="https://images2.imgbox.com/b5/94/m64Yog2S_o.png">在0～8之间，不同颜色间黑色的曲线是等值线，代表落在该线上的点对应的<img alt="f(x_{1},x_{2})" class="mathcode" src="https://images2.imgbox.com/42/fc/neL3KoWm_o.png">的值都相同。</p> 
<h4 id="7.3.1.2%20%E7%AE%80%E5%8D%95%E6%8B%9F%E5%90%88%E5%AE%9E%E9%AA%8C%C2%A0">7.3.1.2 简单拟合实验 </h4> 
<p>除了2D可视化实验外，我们还设计一个简单的拟合任务，然后对比不同的优化算法。</p> 
<p>这里我们随机生成一组数据作为数据样本，再构建一个简单的单层前馈神经网络，用于前向计算。</p> 
<p><strong>数据集构建</strong> 通过paddle.randn随机生成一些训练数据<img alt="X" class="mathcode" src="https://images2.imgbox.com/54/98/7MOnWdiB_o.png">，并根据一个预定义函数<img alt="y=0.5\times x_{1}+0.8\times x_{2}+0.01\times noise" class="mathcode" src="https://images2.imgbox.com/78/85/YV0afjd0_o.png">计算得到<img alt="y" class="mathcode" src="https://images2.imgbox.com/e3/d0/i322uoIt_o.png">，再将<img alt="X" class="mathcode" src="https://images2.imgbox.com/56/b4/Fmq5PsNl_o.png">和<img alt="y" class="mathcode" src="https://images2.imgbox.com/5a/d8/q68G0Lp8_o.png">拼接起来得到训练样本。代码实现如下：</p> 
<pre><code class="language-python"># 固定随机种子
torch.manual_seed(0)
# 随机生成shape为（1000，2）的训练数据
X = torch.randn([1000, 2])
w = torch.tensor([0.5, 0.8])
w = torch.unsqueeze(w, dim=1)
noise = 0.01 * torch.rand([1000])
noise = torch.unsqueeze(noise, dim=1)
# 计算y
y = torch.matmul(X, w) + noise
# 打印X, y样本
print('X: ', X[0].numpy())
print('y: ', y[0].numpy())

# X，y组成训练样本数据
data = torch.concat((X, y), dim=1)
print('input data shape: ', data.shape)
print('data: ', data[0].numpy())</code></pre> 
<p>运行结果：</p> 
<p class="img-center"><img alt="" height="141" src="https://images2.imgbox.com/ab/ea/avhRxhPo_o.png" width="431"></p> 
<p><strong>模型构建</strong> 定义单层前馈神经网络，<img alt="X\in \mathbb{R} ^{N\times D}" class="mathcode" src="https://images2.imgbox.com/9c/d0/yP1oz4xf_o.png">为网络输入, <img alt="w\in \mathbb{R} ^{D}" class="mathcode" src="https://images2.imgbox.com/63/b5/yhEO4FXM_o.png">是网络的权重矩阵，<img alt="b\in \mathbb{R}" class="mathcode" src="https://images2.imgbox.com/d0/32/wcVDIYvR_o.png">为偏置。</p> 
<p style="text-align:center;"><img alt="y=Xw+b\in \mathbb{R} ^{K\times 1}" class="mathcode" src="https://images2.imgbox.com/9f/01/u49Q27Kv_o.png"></p> 
<p>其中<img alt="K" class="mathcode" src="https://images2.imgbox.com/cd/7c/itehwdLr_o.png">代表一个批次中的样本数量，<img alt="D" class="mathcode" src="https://images2.imgbox.com/37/cb/sYazjLh8_o.png">为单层网络的输入特征维度。</p> 
<p><strong>损失函数</strong> 使用均方误差作为训练时的损失函数，计算损失函数关于参数<img alt="w" class="mathcode" src="https://images2.imgbox.com/1f/c4/c9A7P8SQ_o.png">和<img alt="b" class="mathcode" src="https://images2.imgbox.com/9c/f5/g1R6Dqjo_o.png">的偏导数。定义均方误差损失函数的计算方法为</p> 
<p style="text-align:center;"><img alt="L=\frac{1}{2K} \sum_{k=1}^{K} (y^{(k)}-z^{(k)})^{2}" class="mathcode" src="https://images2.imgbox.com/2e/31/ANF5vGis_o.png"></p> 
<p>其中<img alt="z^{(k)}" class="mathcode" src="https://images2.imgbox.com/c7/1d/o3gdthhG_o.png">是网络对第<img alt="k" class="mathcode" src="https://images2.imgbox.com/40/03/U4rjP9ND_o.png">个样本的预测值。根据损失函数关于参数的偏导公式，得到<img alt="L(\cdot )" class="mathcode" src="https://images2.imgbox.com/ed/ef/0cd4KYt4_o.png">对于参数<img alt="w" class="mathcode" src="https://images2.imgbox.com/a9/16/axHJPrVE_o.png">和<img alt="b" class="mathcode" src="https://images2.imgbox.com/37/06/ki1vBIvo_o.png">的偏导数，</p> 
<p style="text-align:center;"><img alt="\frac{\partial L}{\partial w} =\frac{1}{K}\sum_{k=1}^{K} x^{(k)}(z^{(k)}-y^{(k)})=\frac{1}{K}X^{T}(z-y)" class="mathcode" src="https://images2.imgbox.com/cf/18/F4f2UQev_o.png"></p> 
<p style="text-align:center;"><img alt="\frac{\partial L}{\partial b} =\frac{1}{K}\sum_{k=1}^{K} (z^{(k)}-y^{(k)})=\frac{1}{K}1^{T}(z-y)" class="mathcode" src="https://images2.imgbox.com/c9/18/MhW7YeqK_o.png"></p> 
<p>定义Linear算子，实现一个线性层的前向和反向计算。代码实现如下：</p> 
<pre><code class="language-python">class Linear(Op):
    def __init__(self, input_size, weight_init=torch.randn, bias_init=torch.zeros):
        super(Linear, self).__init__()
        self.params = {}
        self.params['W'] = weight_init(size=[input_size, 1])
        self.params['b'] = bias_init(size=[1])
        self.inputs = None
        self.grads = {}

    def forward(self, inputs):
        self.inputs = inputs
        self.outputs = torch.matmul(self.inputs, self.params['W']) + self.params['b']
        return self.outputs

    def backward(self, labels):
        K = self.inputs.shape[0]
        self.grads['W'] = 1. / K * torch.matmul(self.inputs.T, (self.outputs - labels))
        self.grads['b'] = 1. / K * torch.sum(self.outputs - labels, dim=0)</code></pre> 
<p><strong>笔记</strong></p> 
<p>这里backward函数中实现的梯度并不是forward函数对应的梯度，而是最终损失关于参数的梯度．由于这里的梯度是手动计算的，所以直接给出了最终的梯度。</p> 
<p><strong>训练函数</strong> 在准备好样本数据和网络以后，复用优化器SimpleBatchGD类，使用小批量梯度下降来进行简单的拟合实验。</p> 
<p>这里我们重新定义模型训练train函数。主要以下两点原因：</p> 
<ul><li>在一般的随机梯度下降中要在每回合迭代开始之前随机打乱训练数据的顺序，再按批大小进行分组。这里为了保证每次运行结果一致以便更好地对比不同的优化算法，这里不再随机打乱数据。</li><li>与RunnerV2中的训练函数相比，这里使用小批量梯度下降。而与RunnerV3中的训练函数相比，又通过继承优化器基类Optimizer实现不同的优化器。</li></ul> 
<p>模型训练train函数的代码实现如下：</p> 
<pre><code class="language-python">def train(data, num_epochs, batch_size, model, calculate_loss, optimizer, verbose=False):

    # 记录每个回合损失的变化
    epoch_loss = []
    # 记录每次迭代损失的变化
    iter_loss = []
    N = len(data)
    for epoch_id in range(num_epochs):
        # np.random.shuffle(data) #不再随机打乱数据
        # 将训练数据进行拆分，每个mini_batch包含batch_size条的数据
        mini_batches = [data[i:i+batch_size] for i in range(0, N, batch_size)]
        for iter_id, mini_batch in enumerate(mini_batches):
            # data中前两个分量为X
            inputs = mini_batch[:, :-1]
            # data中最后一个分量为y
            labels = mini_batch[:, -1:]
            # 前向计算
            outputs = model(inputs)
            # 计算损失
            loss = calculate_loss(outputs, labels).numpy()
            # 计算梯度
            model.backward(labels)
            # 梯度更新
            optimizer.step()
            iter_loss.append(loss)
        # verbose = True 则打印当前回合的损失
        if verbose:
            print('Epoch {:3d}, loss = {:.4f}'.format(epoch_id, np.mean(iter_loss)))
        epoch_loss.append(np.mean(iter_loss))
    return iter_loss, epoch_loss</code></pre> 
<p><strong>优化过程可视化</strong> 定义plot_loss函数，用于绘制损失函数变化趋势。代码实现如下：</p> 
<pre><code class="language-python">def plot_loss(iter_loss, epoch_loss, fig_name):
    plt.figure(figsize=(10, 4))
    ax1 = plt.subplot(121)
    ax1.plot(iter_loss, color='#191970')
    plt.title('iteration loss')
    ax2 = plt.subplot(122)
    ax2.plot(epoch_loss, color='#2F4F4F')
    plt.title('epoch loss')
    plt.savefig(fig_name)
    plt.show()</code></pre> 
<p>对于使用不同优化器的模型训练，保存每一个回合损失的更新情况，并绘制出损失函数的变化趋势，以此验证模型是否收敛。定义train_and_plot函数，调用train和plot_loss函数，训练并展示每个回合和每次迭代(Iteration)的损失变化情况。在模型训练时，使用paddle.nn.MSELoss()计算均方误差。代码实现如下：</p> 
<pre><code class="language-python">import torch.nn as nn
def train_and_plot(optimizer, fig_name):
    # 定义均方差损失
    mse = nn.MSELoss()
    iter_loss, epoch_loss = train(data, num_epochs=30, batch_size=64, model=model, calculate_loss=mse, optimizer=optimizer)
    plot_loss(iter_loss, epoch_loss, fig_name)</code></pre> 
<p>训练网络并可视化损失函数的变化趋势。代码实现如下：</p> 
<pre><code class="language-python"># 固定随机种子
torch.manual_seed(0)
# 定义网络结构
model = Linear(2)
# 定义优化器
opt = SimpleBatchGD(init_lr=0.01, model=model)
train_and_plot(opt, 'opti-loss.pdf')
</code></pre> 
<p>运行结果：</p> 
<p class="img-center"><img alt="" height="277" src="https://images2.imgbox.com/5f/ab/qvVDfNtu_o.png" width="638"></p> 
<p>从输出结果看，loss在不断减小，模型逐渐收敛。</p> 
<p><strong>对比，验证正确性</strong></p> 
<pre><code class="language-python">torch.manual_seed(0)
 
x = data[0, :-1].unsqueeze(0)
y = data[0, -1].unsqueeze(0)
 
model1 = Linear(2)
print('model1 parameter W: ', model1.params['W'].numpy())
opt1 = SimpleBatchGD(init_lr=0.01, model=model1)
output1 = model1(x)
 
model2 = nn.Linear(2, 1)
model2.weight=torch.nn.Parameter(torch.tensor(model1.params['W'].T))
print('model2 parameter W: ', model2.state_dict()['weight'].numpy())
output2 = model2(x)
 
model1.backward(y)
opt1.step()
print('model1 parameter W after train step: ', model1.params['W'].numpy())
 
opt2 = torch.optim.SGD(lr=0.01, params=model2.parameters())
loss = torch.nn.functional.mse_loss(output2, y) / 2
loss.backward()
opt2.step()
opt2.zero_grad()
print('model2 parameter W after train step: ', model2.state_dict()['weight'].numpy())
</code></pre> 
<p>运行结果：</p> 
<p class="img-center"><img alt="" height="200" src="https://images2.imgbox.com/49/97/oes28tE9_o.png" width="648"></p> 
<p>从输出结果看，在一次梯度更新后，两个模型的参数值保持一致，证明优化器实现正确。</p> 
<h3 id="732-学习率调整">7.3.2 学习率调整</h3> 
<p>学习率是神经网络优化时的重要超参数。在梯度下降法中，学习率αα的取值非常关键，如果取值过大就不会收敛，如果过小则收敛速度太慢。</p> 
<p>常用的学习率调整方法包括如下几种方法：</p> 
<ul><li>学习率衰减：如分段常数衰减（Piecewise Constant Decay）、余弦衰减（Cosine Decay）等；</li><li>学习率预热：如逐渐预热(Gradual Warmup) 等；</li><li>周期性学习率调整：如循环学习率等；</li><li>自适应调整学习率的方法：如AdaGrad、RMSprop、AdaDelta等。自适应学习率方法可以针对每个参数设置不同的学习率。</li></ul> 
<h4 id="7.3.2.1%20AdaGrad%E7%AE%97%E6%B3%95%C2%A0">7.3.2.1 AdaGrad算法 </h4> 
<p>AdaGrad算法（Adaptive Gradient Algorithm，自适应梯度算法)是借鉴<img alt="\ell _{2}" class="mathcode" src="https://images2.imgbox.com/c4/eb/CmzJgJbZ_o.png">正则化的思想，每次迭代时自适应地调整每个参数的学习率。在第<img alt="t" class="mathcode" src="https://images2.imgbox.com/34/9f/DO022wa0_o.png">次迭代时，先计算每个参数梯度平方的累计值。</p> 
<p style="text-align:center;"><img alt="G_{t}=\sum_{\tau =1}^{t} g_{\tau }\odot g_{\tau }" class="mathcode" src="https://images2.imgbox.com/a5/d1/2dmTbFn9_o.png"></p> 
<p>其中⊙为按元素乘积，<img alt="g_{\tau }\in \mathbb{R} ^{\left | \theta \right | }" class="mathcode" src="https://images2.imgbox.com/c9/cf/cqkEdHOy_o.png">是第<img alt="\tau" class="mathcode" src="https://images2.imgbox.com/6d/9b/bDMjE7Ql_o.png">次迭代时的梯度。</p> 
<p style="text-align:center;"><img alt="\bigtriangleup \theta _{t}=-\frac{\alpha }{\sqrt{G_{t}+\epsilon } } \odot g_{t}" class="mathcode" src="https://images2.imgbox.com/93/34/f85vY1sY_o.png"></p> 
<p>其中<img alt="\alpha" class="mathcode" src="https://images2.imgbox.com/06/0c/lgksnVQd_o.png">是初始的学习率，<img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/e0/b5/E0jHBl0e_o.png">是为了保持数值稳定性而设置的非常小的常数，一般取值<img alt="e^{-7}" class="mathcode" src="https://images2.imgbox.com/9e/e1/PdSsO6WP_o.png">到<img alt="e^{-10}" class="mathcode" src="https://images2.imgbox.com/18/c4/kHIeNhdy_o.png">。此外，这里的开平方、除、加运算都是按元素进行的操作。</p> 
<p><strong>构建优化器</strong> 定义Adagrad类，继承Optimizer类。定义step函数调用adagrad进行参数更新。代码实现如下：</p> 
<pre><code class="language-python">class Adagrad(Optimizer):
    def __init__(self, init_lr, model, epsilon):
        super(Adagrad, self).__init__(init_lr=init_lr, model=model)
        self.G = {}
        for key in self.model.params.keys():
            self.G[key] = 0
        self.epsilon = epsilon

    def adagrad(self, x, gradient_x, G, init_lr):
        G += gradient_x ** 2
        x -= init_lr / torch.sqrt(G + self.epsilon) * gradient_x
        return x, G

    def step(self):
        for key in self.model.params.keys():
            self.model.params[key], self.G[key] = self.adagrad(self.model.params[key],
                                                               self.model.grads[key],
                                                               self.G[key],
                                                               self.init_lr)</code></pre> 
<p><strong>2D可视化实验</strong> 使用被优化函数展示Adagrad算法的参数更新轨迹。代码实现如下：</p> 
<pre><code class="language-python"># 固定随机种子
torch.manual_seed(0)
w = torch.tensor([0.2, 2])
model = OptimizedFunction(w)
opt = Adagrad(init_lr=0.5, model=model, epsilon=1e-7)
train_and_plot_f(model, opt, epoch=50, fig_name='opti-vis-para2.pdf')</code></pre> 
<p>运行结果：</p> 
<p class="img-center"><img alt="" height="420" src="https://images2.imgbox.com/d8/e1/jGecBhbD_o.png" width="636"></p> 
<p>从输出结果看，AdaGrad算法在前几个回合更新时参数更新幅度较大，随着回合数增加，学习率逐渐缩小，参数更新幅度逐渐缩小。在AdaGrad算法中，如果某个参数的偏导数累积比较大，其学习率相对较小。相反，如果其偏导数累积较小，其学习率相对较大。但整体随着迭代次数的增加，学习率逐渐缩小。该算法的缺点是在经过一定次数的迭代依然没有找到最优点时，由于这时的学习率已经非常小，很难再继续找到最优点。</p> 
<p><strong>简单拟合实验</strong> 训练单层线性网络，验证损失是否收敛。代码实现如下：</p> 
<pre><code class="language-python"># 固定随机种子
torch.manual_seed(0)
# 定义网络结构
model = Linear(2)
# 定义优化器
opt = Adagrad(init_lr=0.1, model=model, epsilon=1e-7)
train_and_plot(opt, 'opti-loss2.pdf')
</code></pre> 
<p>运行结果：</p> 
<p class="img-center"><img alt="" height="461" src="https://images2.imgbox.com/8c/0f/dJb6FXF8_o.png" width="1067"></p> 
<h4 id="7.3.2.2%20RMSprop%E7%AE%97%E6%B3%95%C2%A0">7.3.2.2 RMSprop算法 </h4> 
<p>RMSprop算法是一种自适应学习率的方法，可以在有些情况下避免AdaGrad算法中学习率不断单调下降以至于过早衰减的缺点。</p> 
<p>RMSprop算法首先计算每次迭代梯度平方<img alt="g_{t}^{2}" class="mathcode" src="https://images2.imgbox.com/23/db/PQ7drNuu_o.png">的加权移动平均</p> 
<p style="text-align:center;"><img alt="G_{t}=\beta G_{t-1}+(1-\beta )g_{t}\odot g_{t}" class="mathcode" src="https://images2.imgbox.com/15/a1/VhpaNYPR_o.png"></p> 
<p>其中<img alt="\beta" class="mathcode" src="https://images2.imgbox.com/50/ed/CyL22TRD_o.png">为衰减率，一般取值为0.9。</p> 
<p>RMSprop算法的参数更新差值为：</p> 
<p style="text-align:center;"><img alt="\bigtriangleup \theta _{t}=-\frac{\alpha }{\sqrt{G_{t}+\epsilon } } \odot g_{t}" class="mathcode" src="https://images2.imgbox.com/9e/72/Wbb0uP3D_o.png"></p> 
<p>其中<img alt="\alpha" class="mathcode" src="https://images2.imgbox.com/84/4d/8V1lpgHy_o.png">是初始的学习率，比如0.001。RMSprop算法和AdaGrad算法的区别在于RMSprop算法中<img alt="G_{t}" class="mathcode" src="https://images2.imgbox.com/12/e5/BRSsEw0b_o.png">的计算由累积方式变成了加权移动平均。在迭代过程中，每个参数的学习率并不是呈衰减趋势，既可以变小也可以变大。</p> 
<p><strong>构建优化器</strong> 定义RMSprop类，继承Optimizer类。定义step函数调用rmsprop更新参数。代码实现如下：</p> 
<pre><code class="language-python">class RMSprop(Optimizer):
    def __init__(self, init_lr, model, beta, epsilon):
        """
        RMSprop优化器初始化
        输入：
            - init_lr：初始学习率
            - model：模型，model.params存储模型参数值
            - beta：衰减率
            - epsilon：保持数值稳定性而设置的常数
        """
        super(RMSprop, self).__init__(init_lr=init_lr, model=model)
        self.G = {}
        for key in self.model.params.keys():
            self.G[key] = 0
        self.beta = beta
        self.epsilon = epsilon

    def rmsprop(self, x, gradient_x, G, init_lr):
        """
        rmsprop算法更新参数，G为迭代梯度平方的加权移动平均
        """
        G = self.beta * G + (1 - self.beta) * gradient_x ** 2
        x -= init_lr / torch.sqrt(G + self.epsilon) * gradient_x
        return x, G

    def step(self):
        """参数更新"""
        for key in self.model.params.keys():
            self.model.params[key], self.G[key] = self.rmsprop(self.model.params[key],
                                                               self.model.grads[key],
                                                               self.G[key],
                                                               self.init_lr)
</code></pre> 
<p><strong>2D可视化实验</strong> 使用被优化函数展示RMSprop算法的参数更新轨迹。代码实现如下：</p> 
<pre><code class="language-python"># 固定随机种子
torch.manual_seed(0)
w = torch.tensor([0.2, 2])
model = OptimizedFunction(w)
opt = RMSprop(init_lr=0.1, model=model, beta=0.9, epsilon=1e-7)
train_and_plot_f(model, opt, epoch=50, fig_name='opti-vis-para3.pdf')</code></pre> 
<p>运行结果：</p> 
<p class="img-center"><img alt="" height="429" src="https://images2.imgbox.com/70/06/uE9CBLeA_o.png" width="641"></p> 
<p><strong>简单拟合实验</strong> 训练单层线性网络，进行简单的拟合实验。代码实现如下：</p> 
<pre><code class="language-python"># 固定随机种子
torch.manual_seed(0)
# 定义网络结构
model = Linear(2)
# 定义优化器
opt = RMSprop(init_lr=0.1, model=model, beta=0.9, epsilon=1e-7)
train_and_plot(opt, 'opti-loss3.pdf')
</code></pre> 
<p>运行结果：</p> 
<p class="img-center"><img alt="" height="276" src="https://images2.imgbox.com/29/6f/rDrkVNTP_o.png" width="644"></p> 
<h3 id="7.3.3%20%E6%A2%AF%E5%BA%A6%E4%BC%B0%E8%AE%A1%E4%BF%AE%E6%AD%A3%C2%A0">7.3.3 梯度估计修正 </h3> 
<p>除了调整学习率之外，还可以进行梯度估计修正。在小批量梯度下降法中，由于每次迭代的样本具有一定的随机性，因此每次迭代的梯度估计和整个训练集上的最优梯度并不一致。如果每次选取样本数量比较小，损失会呈振荡的方式下降。</p> 
<p>一种有效地缓解梯度估计随机性的方式是通过使用最近一段时间内的平均梯度来代替当前时刻的随机梯度来作为参数更新的方向，从而提高优化速度。</p> 
<h4 id="7.3.3.1%20%E5%8A%A8%E9%87%8F%E6%B3%95%C2%A0">7.3.3.1 动量法 </h4> 
<p>动量法（Momentum Method）是用之前积累动量来替代真正的梯度。每次迭代的梯度可以看作加速度。</p> 
<p>在第<img alt="t" class="mathcode" src="https://images2.imgbox.com/9f/2c/s0enDXL4_o.png">次迭代时，计算负梯度的“加权移动平均”作为参数的更新方向，</p> 
<p style="text-align:center;"><img alt="\bigtriangleup \theta _{t}=\rho \bigtriangleup \theta _{t-1}-\alpha g_{t}=-\alpha \sum_{\tau =1}^{t}\rho ^{t-\tau }g_{\tau }" class="mathcode" src="https://images2.imgbox.com/21/f4/fKA36UOq_o.png"></p> 
<p>其中<img alt="\rho" class="mathcode" src="https://images2.imgbox.com/60/c5/xxXZm4pv_o.png">为动量因子，通常设为0.9，<img alt="\alpha" class="mathcode" src="https://images2.imgbox.com/c6/0e/Bh19mkaV_o.png">为学习率。</p> 
<p>这样，每个参数的实际更新差值取决于最近一段时间内梯度的加权平均值。当某个参数在最近一段时间内的梯度方向不一致时，其真实的参数更新幅度变小。相反，当某个参数在最近一段时间内的梯度方向都一致时，其真实的参数更新幅度变大，起到加速作用。一般而言，在迭代初期，梯度方向都比较一致，动量法会起到加速作用，可以更快地到达最优点。在迭代后期，梯度方向会不一致，在收敛值附近振荡，动量法会起到减速作用，增加稳定性。从某种角度来说，当前梯度叠加上部分的上次梯度，一定程度上可以近似看作二阶梯度。</p> 
<p><strong>构建优化器</strong> 定义Momentum类，继承Optimizer类。定义step函数调用momentum进行参数更新。代码实现如下：</p> 
<pre><code class="language-python">class Momentum(Optimizer):
    def __init__(self, init_lr, model, rho):
        """
        Momentum优化器初始化
        输入：
            - init_lr：初始学习率
            - model：模型，model.params存储模型参数值
            - rho：动量因子
        """
        super(Momentum, self).__init__(init_lr=init_lr, model=model)
        self.delta_x = {}
        for key in self.model.params.keys():
            self.delta_x[key] = 0
        self.rho = rho

    def momentum(self, x, gradient_x, delta_x, init_lr):
        """
        momentum算法更新参数，delta_x为梯度的加权移动平均
        """
        delta_x = self.rho * delta_x - init_lr * gradient_x
        x += delta_x
        return x, delta_x

    def step(self):
        """参数更新"""
        for key in self.model.params.keys():
            self.model.params[key], self.delta_x[key] = self.momentum(self.model.params[key],
                                                                      self.model.grads[key],
                                                                      self.delta_x[key],
                                                                      self.init_lr)</code></pre> 
<p><strong>2D可视化实验</strong> 使用被优化函数展示Momentum算法的参数更新轨迹。</p> 
<p></p> 
<pre><code class="language-python"># 固定随机种子
torch.manual_seed(0)
w = torch.tensor([0.2, 2])
model = OptimizedFunction(w)
opt = Momentum(init_lr=0.01, model=model, rho=0.9)
train_and_plot_f(model, opt, epoch=50, fig_name='opti-vis-para4.pdf')</code></pre> 
<p>运行结果：</p> 
<p class="img-center"><img alt="" height="422" src="https://images2.imgbox.com/1d/fb/4RtqJieR_o.png" width="632"></p> 
<p>从输出结果看，在模型训练初期，梯度方向比较一致，参数更新幅度逐渐增大，起加速作用；在迭代后期，参数更新幅度减小，在收敛值附近振荡。</p> 
<p><strong>简单拟合实验</strong> 训练单层线性网络，进行简单的拟合实验。代码实现如下：</p> 
<pre><code class="language-python"># 固定随机种子
torch.manual_seed(0)

# 定义网络结构
model = Linear(2)
# 定义优化器
opt = Momentum(init_lr=0.01, model=model, rho=0.9)
train_and_plot(opt, 'opti-loss4.pdf')

</code></pre> 
<p>运行结果：</p> 
<p class="img-center"><img alt="" height="273" src="https://images2.imgbox.com/fe/d6/rTbGkXvc_o.png" width="626"></p> 
<h4 id="7.3.3.2%20Adam%E7%AE%97%E6%B3%95%C2%A0">7.3.3.2 Adam算法 </h4> 
<p>Adam算法（Adaptive Moment Estimation Algorithm，自适应矩估计算法）可以看作动量法和RMSprop算法的结合，不但使用动量作为参数更新方向，而且可以自适应调整学习率。</p> 
<p>Adam算法一方面计算梯度平方<img alt="g_{t}^{2}" class="mathcode" src="https://images2.imgbox.com/b7/1e/IN40n5pj_o.png">的加权移动平均（和RMSprop算法类似），另一方面计算梯度<img alt="g_{t}" class="mathcode" src="https://images2.imgbox.com/50/25/nF5JQMix_o.png">的加权移动平均（和动量法类似）。</p> 
<p style="text-align:center;"><img alt="M_{t}=\beta _{1}M_{t-1}+(1-\beta_{1})g_{t}" class="mathcode" src="https://images2.imgbox.com/5f/b6/uZIbbJRT_o.png"></p> 
<p style="text-align:center;"><img alt="G_{t}=\beta _{2}G_{t-1}+(1-\beta_{2})g_{t} \odot g_{t}" class="mathcode" src="https://images2.imgbox.com/f1/1d/47HIG6L7_o.png"></p> 
<p>其中<img alt="\beta _{1}" class="mathcode" src="https://images2.imgbox.com/f8/5d/cQ8D5Ti9_o.png">和<img alt="\beta _{2}" class="mathcode" src="https://images2.imgbox.com/d7/13/vwUJbnSq_o.png">分别为两个移动平均的衰减率，通常取值为<img alt="\beta _1" class="mathcode" src="https://images2.imgbox.com/be/0b/1sIuiSP3_o.png">=0.9,<img alt="\beta _2" class="mathcode" src="https://images2.imgbox.com/5e/e6/72nIZuX1_o.png">=0.99。我们可以把<img alt="M_t" class="mathcode" src="https://images2.imgbox.com/a9/55/E81I9IDm_o.png">和<img alt="G_t" class="mathcode" src="https://images2.imgbox.com/76/42/kCG8MQHG_o.png">分别看作梯度的均值(一阶矩)和未减去均值的方差(二阶矩)。</p> 
<p>假设<img alt="M_0" class="mathcode" src="https://images2.imgbox.com/6f/76/FhCSrxQ8_o.png">=0,<img alt="G_0" class="mathcode" src="https://images2.imgbox.com/f2/ca/lsITuvmG_o.png">=0，那么在迭代初期<img alt="M_t" class="mathcode" src="https://images2.imgbox.com/70/28/P5GIF4W7_o.png">和<img alt="G_t" class="mathcode" src="https://images2.imgbox.com/1a/50/dvPDMWTf_o.png">的值会比真实的均值和方差要小。特别是当<img alt="\beta _1" class="mathcode" src="https://images2.imgbox.com/73/06/fzLmH2ZJ_o.png">和<img alt="\beta _2" class="mathcode" src="https://images2.imgbox.com/ee/08/x5tt17my_o.png">都接近于1时，偏差会很大。因此，需要对偏差进行修正。</p> 
<p style="text-align:center;"><img alt="\hat{M}_{t}=\frac{M_{t}}{1-\beta _{1}^{t}}" class="mathcode" src="https://images2.imgbox.com/1f/0c/f8Ek6tzz_o.png"></p> 
<p style="text-align:center;"><img alt="\hat{G}_{t}=\frac{G_{t}}{1-\beta _{2}^{t}}" class="mathcode" src="https://images2.imgbox.com/4c/24/d76ZaGgz_o.png"></p> 
<p>Adam算法的参数更新差值为</p> 
<p style="text-align:center;"><img alt="\bigtriangleup \theta_{t}=-\frac{\alpha }{\sqrt{\hat{G}_{t}+\epsilon } } \hat{M}_{t}" class="mathcode" src="https://images2.imgbox.com/f0/18/ZNnhJP5z_o.png"></p> 
<p>其中学习率<img alt="\alpha" class="mathcode" src="https://images2.imgbox.com/86/12/J2F2alkN_o.png">通常设为0.001，并且也可以进行衰减，比如<img alt="a_{t}=\frac{a_{0}}{\sqrt{t} }" class="mathcode" src="https://images2.imgbox.com/29/9d/oyQ8kbMO_o.png">。</p> 
<p><strong>构建优化器</strong> 定义Adam类，继承Optimizer类。定义step函数调用adam函数更新参数。代码实现如下：</p> 
<pre><code class="language-python">class Adam(Optimizer):
    def __init__(self, init_lr, model, beta1, beta2, epsilon):
        """
        Adam优化器初始化
        输入：
            - init_lr：初始学习率
            - model：模型，model.params存储模型参数值
            - beta1, beta2：移动平均的衰减率
            - epsilon：保持数值稳定性而设置的常数
        """
        super(Adam, self).__init__(init_lr=init_lr, model=model)
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.M, self.G = {}, {}
        for key in self.model.params.keys():
            self.M[key] = 0
            self.G[key] = 0
        self.t = 1

    def adam(self, x, gradient_x, G, M, t, init_lr):
        """
        adam算法更新参数
        输入：
            - x：参数
            - G：梯度平方的加权移动平均
            - M：梯度的加权移动平均
            - t：迭代次数
            - init_lr：初始学习率
        """
        M = self.beta1 * M + (1 - self.beta1) * gradient_x
        G = self.beta2 * G + (1 - self.beta2) * gradient_x ** 2
        M_hat = M / (1 - self.beta1 ** t)
        G_hat = G / (1 - self.beta2 ** t)
        t += 1
        x -= init_lr / torch.sqrt(G_hat + self.epsilon) * M_hat
        return x, G, M, t

    def step(self):
        """参数更新"""
        for key in self.model.params.keys():
            self.model.params[key], self.G[key], self.M[key], self.t = self.adam(self.model.params[key],
                                                                                 self.model.grads[key],
                                                                                 self.G[key],
                                                                                 self.M[key],
                                                                                 self.t,
                                                                                 self.init_lr)

</code></pre> 
<p><strong>2D可视化实验</strong> 使用被优化函数展示Adam算法的参数更新轨迹。代码实现如下：</p> 
<pre><code class="language-python"># 固定随机种子
torch.manual_seed(0)
w = torch.tensor([0.2, 2])
model = OptimizedFunction(w)
opt = Adam(init_lr=0.2, model=model, beta1=0.9, beta2=0.99, epsilon=1e-7)
train_and_plot_f(model, opt, epoch=20, fig_name='opti-vis-para5.pdf')</code></pre> 
<p>运行结果：</p> 
<p class="img-center"><img alt="" height="444" src="https://images2.imgbox.com/d2/30/YJXuPTqB_o.png" width="653"></p> 
<p>从输出结果看，Adam算法可以自适应调整学习率，参数更新更加平稳。</p> 
<p><strong>简单拟合实验</strong> 训练单层线性网络，进行简单的拟合实验。代码实现如下：</p> 
<pre><code class="language-python"># 固定随机种子
torch.manual_seed(0)
# 定义网络结构
model = Linear(2)
# 定义优化器
opt = Adam(init_lr=0.1, model=model, beta1=0.9, beta2=0.99, epsilon=1e-7)
train_and_plot(opt, 'opti-loss5.pdf')
</code></pre> 
<p>运行结果：</p> 
<p class="img-center"><img alt="" height="268" src="https://images2.imgbox.com/96/a4/rWCCqfKd_o.png" width="630"></p> 
<h3 id="7.3.4%20%E4%B8%8D%E5%90%8C%E4%BC%98%E5%8C%96%E5%99%A8%E7%9A%843D%E5%8F%AF%E8%A7%86%E5%8C%96%E5%AF%B9%E6%AF%94%C2%A0">7.3.4 不同优化器的3D可视化对比 </h3> 
<h4 id="7.3.4.1%20%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E4%B8%89%E7%BB%B4%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%9A%84%E8%A2%AB%E4%BC%98%E5%8C%96%E5%87%BD%E6%95%B0%C2%A0">7.3.4.1 构建一个三维空间中的被优化函数 </h4> 
<p>定义OptimizedFunction3D算子，表示被优化函数<img alt="f(x)=x\left [ 0 \right ]^{2}+ x\left [ 1 \right ]^{2}+x\left [ 1 \right ]^{3}+x\left [ 0 \right ]\ast x\left [ 1 \right ]" class="mathcode" src="https://images2.imgbox.com/db/7d/kuTL7QR0_o.png">，其中<img alt="x\left [ 0 \right ]" class="mathcode" src="https://images2.imgbox.com/38/58/wxbaZhua_o.png">, <img alt="x\left [ 1 \right ]" class="mathcode" src="https://images2.imgbox.com/6a/b7/uviRBEy4_o.png">代表两个参数。该函数在(0,0)处存在鞍点，即一个既不是极大值点也不是极小值点的临界点。希望训练过程中，优化算法可以使参数离开鞍点，向模型最优解收敛。代码实现如下： </p> 
<pre><code class="language-python">import torch
import numpy as np
import copy
from matplotlib import pyplot as plt
from matplotlib import animation
from itertools import zip_longest
 
 
class Op(object):
    def __init__(self):
        pass
 
    def __call__(self, inputs):
        return self.forward(inputs)
 
    # 输入：张量inputs
    # 输出：张量outputs
    def forward(self, inputs):
        # return outputs
        raise NotImplementedError
 
    # 输入：最终输出对outputs的梯度outputs_grads
    # 输出：最终输出对inputs的梯度inputs_grads
    def backward(self, outputs_grads):
        # return inputs_grads
        raise NotImplementedError
 
 
class Optimizer(object):  # 优化器基类
    def __init__(self, init_lr, model):
        """
        优化器类初始化
        """
        # 初始化学习率，用于参数更新的计算
        self.init_lr = init_lr
        # 指定优化器需要优化的模型
        self.model = model
 
    def step(self):
        """
        定义每次迭代如何更新参数
        """
        pass
 
 
class SimpleBatchGD(Optimizer):
    def __init__(self, init_lr, model):
        super(SimpleBatchGD, self).__init__(init_lr=init_lr, model=model)
 
    def step(self):
        # 参数更新
        if isinstance(self.model.params, dict):
            for key in self.model.params.keys():
                self.model.params[key] = self.model.params[key] - self.init_lr * self.model.grads[key]
 
 
class Adagrad(Optimizer):
    def __init__(self, init_lr, model, epsilon):
        """
        Adagrad 优化器初始化
        输入：
            - init_lr： 初始学习率 - model：模型，model.params存储模型参数值  - epsilon：保持数值稳定性而设置的非常小的常数
        """
        super(Adagrad, self).__init__(init_lr=init_lr, model=model)
        self.G = {}
        for key in self.model.params.keys():
            self.G[key] = 0
        self.epsilon = epsilon
 
    def adagrad(self, x, gradient_x, G, init_lr):
        """
        adagrad算法更新参数，G为参数梯度平方的累计值。
        """
        G += gradient_x ** 2
        x -= init_lr / torch.sqrt(G + self.epsilon) * gradient_x
        return x, G
 
    def step(self):
        """
        参数更新
        """
        for key in self.model.params.keys():
            self.model.params[key], self.G[key] = self.adagrad(self.model.params[key],
                                                               self.model.grads[key],
                                                               self.G[key],
                                                               self.init_lr)
 
 
class RMSprop(Optimizer):
    def __init__(self, init_lr, model, beta, epsilon):
        """
        RMSprop优化器初始化
        输入：
            - init_lr：初始学习率
            - model：模型，model.params存储模型参数值
            - beta：衰减率
            - epsilon：保持数值稳定性而设置的常数
        """
        super(RMSprop, self).__init__(init_lr=init_lr, model=model)
        self.G = {}
        for key in self.model.params.keys():
            self.G[key] = 0
        self.beta = beta
        self.epsilon = epsilon
 
    def rmsprop(self, x, gradient_x, G, init_lr):
        """
        rmsprop算法更新参数，G为迭代梯度平方的加权移动平均
        """
        G = self.beta * G + (1 - self.beta) * gradient_x ** 2
        x -= init_lr / torch.sqrt(G + self.epsilon) * gradient_x
        return x, G
 
    def step(self):
        """参数更新"""
        for key in self.model.params.keys():
            self.model.params[key], self.G[key] = self.rmsprop(self.model.params[key],
                                                               self.model.grads[key],
                                                               self.G[key],
                                                               self.init_lr)
 
 
class Momentum(Optimizer):
    def __init__(self, init_lr, model, rho):
        """
        Momentum优化器初始化
        输入：
            - init_lr：初始学习率
            - model：模型，model.params存储模型参数值
            - rho：动量因子
        """
        super(Momentum, self).__init__(init_lr=init_lr, model=model)
        self.delta_x = {}
        for key in self.model.params.keys():
            self.delta_x[key] = 0
        self.rho = rho
 
    def momentum(self, x, gradient_x, delta_x, init_lr):
        """
        momentum算法更新参数，delta_x为梯度的加权移动平均
        """
        delta_x = self.rho * delta_x - init_lr * gradient_x
        x += delta_x
        return x, delta_x
 
    def step(self):
        """参数更新"""
        for key in self.model.params.keys():
            self.model.params[key], self.delta_x[key] = self.momentum(self.model.params[key],
                                                                      self.model.grads[key],
                                                                      self.delta_x[key],
                                                                      self.init_lr)
 
 
class Adam(Optimizer):
    def __init__(self, init_lr, model, beta1, beta2, epsilon):
        """
        Adam优化器初始化
        输入：
            - init_lr：初始学习率
            - model：模型，model.params存储模型参数值
            - beta1, beta2：移动平均的衰减率
            - epsilon：保持数值稳定性而设置的常数
        """
        super(Adam, self).__init__(init_lr=init_lr, model=model)
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.M, self.G = {}, {}
        for key in self.model.params.keys():
            self.M[key] = 0
            self.G[key] = 0
        self.t = 1
 
    def adam(self, x, gradient_x, G, M, t, init_lr):
        """
        adam算法更新参数
        输入：
            - x：参数
            - G：梯度平方的加权移动平均
            - M：梯度的加权移动平均
            - t：迭代次数
            - init_lr：初始学习率
        """
        M = self.beta1 * M + (1 - self.beta1) * gradient_x
        G = self.beta2 * G + (1 - self.beta2) * gradient_x ** 2
        M_hat = M / (1 - self.beta1 ** t)
        G_hat = G / (1 - self.beta2 ** t)
        t += 1
        x -= init_lr / torch.sqrt(G_hat + self.epsilon) * M_hat
        return x, G, M, t
 
    def step(self):
        """参数更新"""
        for key in self.model.params.keys():
            self.model.params[key], self.G[key], self.M[key], self.t = self.adam(self.model.params[key],
                                                                                 self.model.grads[key],
                                                                                 self.G[key],
                                                                                 self.M[key],
                                                                                 self.t,
                                                                                 self.init_lr)
 
 
class OptimizedFunction3D(Op):
    def __init__(self):
        super(OptimizedFunction3D, self).__init__()
        self.params = {'x': 0}
        self.grads = {'x': 0}
 
    def forward(self, x):
        self.params['x'] = x
        return x[0] ** 2 + x[1] ** 2 + x[1] ** 3 + x[0] * x[1]
 
    def backward(self):
        x = self.params['x']
        gradient1 = 2 * x[0] + x[1]
        gradient2 = 2 * x[1] + 3 * x[1] ** 2 + x[0]
        grad1 = torch.Tensor([gradient1])
        grad2 = torch.Tensor([gradient2])
        self.grads['x'] = torch.cat([grad1, grad2])
 
 
class Visualization3D(animation.FuncAnimation):
    """    绘制动态图像，可视化参数更新轨迹    """
 
    def __init__(self, *xy_values, z_values, labels=[], colors=[], fig, ax, interval=600, blit=True, **kwargs):
        """
        初始化3d可视化类
        输入：
            xy_values：三维中x,y维度的值
            z_values：三维中z维度的值
            labels：每个参数更新轨迹的标签
            colors：每个轨迹的颜色
            interval：帧之间的延迟（以毫秒为单位）
            blit：是否优化绘图
        """
        self.fig = fig
        self.ax = ax
        self.xy_values = xy_values
        self.z_values = z_values
 
        frames = max(xy_value.shape[0] for xy_value in xy_values)
        self.lines = [ax.plot([], [], [], label=label, color=color, lw=2)[0]
                      for _, label, color in zip_longest(xy_values, labels, colors)]
        super(Visualization3D, self).__init__(fig, self.animate, init_func=self.init_animation, frames=frames,
                                              interval=interval, blit=blit, **kwargs)
 
    def init_animation(self):
        # 数值初始化
        for line in self.lines:
            line.set_data([], [])
        return self.lines
 
    def animate(self, i):
        # 将x,y,z三个数据传入，绘制三维图像
        for line, xy_value, z_value in zip(self.lines, self.xy_values, self.z_values):
            line.set_data(xy_value[:i, 0], xy_value[:i, 1])
            line.set_3d_properties(z_value[:i])
        return self.lines
 
 
def train_f(model, optimizer, x_init, epoch):
    x = x_init
    all_x = []
    losses = []
    for i in range(epoch):
        all_x.append(copy.deepcopy(x.numpy())) 
        loss = model(x)
        losses.append(loss)
        model.backward()
        optimizer.step()
        x = model.params['x']
    return torch.Tensor(np.array(all_x)), losses
 
 
# 构建5个模型，分别配备不同的优化器
model1 = OptimizedFunction3D()
opt_gd = SimpleBatchGD(init_lr=0.01, model=model1)
 
model2 = OptimizedFunction3D()
opt_adagrad = Adagrad(init_lr=0.5, model=model2, epsilon=1e-7)
 
model3 = OptimizedFunction3D()
opt_rmsprop = RMSprop(init_lr=0.1, model=model3, beta=0.9, epsilon=1e-7)
 
model4 = OptimizedFunction3D()
opt_momentum = Momentum(init_lr=0.01, model=model4, rho=0.9)
 
model5 = OptimizedFunction3D()
opt_adam = Adam(init_lr=0.1, model=model5, beta1=0.9, beta2=0.99, epsilon=1e-7)
 
models = [model1, model2, model3, model4, model5]
opts = [opt_gd, opt_adagrad, opt_rmsprop, opt_momentum, opt_adam]
 
x_all_opts = []
z_all_opts = []
 
# 使用不同优化器训练
 
for model, opt in zip(models, opts):
    x_init = torch.FloatTensor([2, 3])
    x_one_opt, z_one_opt = train_f(model, opt, x_init, 150)  # epoch
    # 保存参数值
    x_all_opts.append(x_one_opt.numpy())
    z_all_opts.append(np.squeeze(z_one_opt))
 

x1 = np.arange(-3, 3, 0.1)
x2 = np.arange(-3, 3, 0.1)
x1, x2 = np.meshgrid(x1, x2)
init_x = torch.Tensor(np.array([x1, x2]))
 
model = OptimizedFunction3D()
 
# 绘制 f_3d函数 的 三维图像
fig = plt.figure()
ax = plt.axes(projection='3d')
X = init_x[0].numpy()
Y = init_x[1].numpy()
Z = model(init_x).numpy()  # 改为 model(init_x).numpy() David 2022.12.4
ax.plot_surface(X, Y, Z, cmap='rainbow')
 
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('f(x1,x2)')
 
labels = ['SGD', 'AdaGrad', 'RMSprop', 'Momentum', 'Adam']
colors = ['#f6373c', '#f6f237', '#45f637', '#37f0f6', '#000000']
 
animator = Visualization3D(*x_all_opts, z_values=z_all_opts, labels=labels, colors=colors, fig=fig, ax=ax)
ax.legend(loc='upper left')
 
plt.show()
animator.save('animation.gif')  
</code></pre> 
<p>运行结果：</p> 
<p class="img-center"><img alt="" height="415" src="https://images2.imgbox.com/89/42/rCBKiy50_o.png" width="446"></p> 
<p class="img-center"><img alt="" height="422" src="https://images2.imgbox.com/80/8c/TamXXuNN_o.png" width="482"></p> 
<p>从输出结果看，对于我们构建的函数，有些优化器如Momentum在参数更新时成功逃离鞍点，其他优化器在本次实验中收敛到鞍点处没有成功逃离。但这并不证明Momentum优化器是最好的优化器，在模型训练时使用哪种优化器，还要结合具体的场景和数据具体分析。</p> 
<h2 id="%E3%80%90%E9%80%89%E5%81%9A%E9%A2%98%E3%80%91">【选做题】</h2> 
<p><strong>1、编程实现下面的动画</strong></p> 
<p><strong>2、在动画中增加Adam</strong></p> 
<p><strong>3、分析解释这个动画</strong></p> 
<pre><code class="language-python">import torch
import numpy as np
import copy
from matplotlib import pyplot as plt
from matplotlib import animation
from itertools import zip_longest
from matplotlib import cm


class Op(object):
    def __init__(self):
        pass

    def __call__(self, inputs):
        return self.forward(inputs)

    # 输入：张量inputs
    # 输出：张量outputs
    def forward(self, inputs):
        # return outputs
        raise NotImplementedError

    # 输入：最终输出对outputs的梯度outputs_grads
    # 输出：最终输出对inputs的梯度inputs_grads
    def backward(self, outputs_grads):
        # return inputs_grads
        raise NotImplementedError


class Optimizer(object):  # 优化器基类
    def __init__(self, init_lr, model):
        """
        优化器类初始化
        """
        # 初始化学习率，用于参数更新的计算
        self.init_lr = init_lr
        # 指定优化器需要优化的模型
        self.model = model

    def step(self):
        """
        定义每次迭代如何更新参数
        """
        pass


class SimpleBatchGD(Optimizer):
    def __init__(self, init_lr, model):
        super(SimpleBatchGD, self).__init__(init_lr=init_lr, model=model)

    def step(self):
        # 参数更新
        if isinstance(self.model.params, dict):
            for key in self.model.params.keys():
                self.model.params[key] = self.model.params[key] - self.init_lr * self.model.grads[key]


class Adagrad(Optimizer):
    def __init__(self, init_lr, model, epsilon):
        """
        Adagrad 优化器初始化
        输入：
            - init_lr： 初始学习率 - model：模型，model.params存储模型参数值  - epsilon：保持数值稳定性而设置的非常小的常数
        """
        super(Adagrad, self).__init__(init_lr=init_lr, model=model)
        self.G = {}
        for key in self.model.params.keys():
            self.G[key] = 0
        self.epsilon = epsilon

    def adagrad(self, x, gradient_x, G, init_lr):
        """
        adagrad算法更新参数，G为参数梯度平方的累计值。
        """
        G += gradient_x ** 2
        x -= init_lr / torch.sqrt(G + self.epsilon) * gradient_x
        return x, G

    def step(self):
        """
        参数更新
        """
        for key in self.model.params.keys():
            self.model.params[key], self.G[key] = self.adagrad(self.model.params[key],
                                                               self.model.grads[key],
                                                               self.G[key],
                                                               self.init_lr)


class RMSprop(Optimizer):
    def __init__(self, init_lr, model, beta, epsilon):
        """
        RMSprop优化器初始化
        输入：
            - init_lr：初始学习率
            - model：模型，model.params存储模型参数值
            - beta：衰减率
            - epsilon：保持数值稳定性而设置的常数
        """
        super(RMSprop, self).__init__(init_lr=init_lr, model=model)
        self.G = {}
        for key in self.model.params.keys():
            self.G[key] = 0
        self.beta = beta
        self.epsilon = epsilon

    def rmsprop(self, x, gradient_x, G, init_lr):
        """
        rmsprop算法更新参数，G为迭代梯度平方的加权移动平均
        """
        G = self.beta * G + (1 - self.beta) * gradient_x ** 2
        x -= init_lr / torch.sqrt(G + self.epsilon) * gradient_x
        return x, G

    def step(self):
        """参数更新"""
        for key in self.model.params.keys():
            self.model.params[key], self.G[key] = self.rmsprop(self.model.params[key],
                                                               self.model.grads[key],
                                                               self.G[key],
                                                               self.init_lr)


class Momentum(Optimizer):
    def __init__(self, init_lr, model, rho):
        """
        Momentum优化器初始化
        输入：
            - init_lr：初始学习率
            - model：模型，model.params存储模型参数值
            - rho：动量因子
        """
        super(Momentum, self).__init__(init_lr=init_lr, model=model)
        self.delta_x = {}
        for key in self.model.params.keys():
            self.delta_x[key] = 0
        self.rho = rho

    def momentum(self, x, gradient_x, delta_x, init_lr):
        """
        momentum算法更新参数，delta_x为梯度的加权移动平均
        """
        delta_x = self.rho * delta_x - init_lr * gradient_x
        x += delta_x
        return x, delta_x

    def step(self):
        """参数更新"""
        for key in self.model.params.keys():
            self.model.params[key], self.delta_x[key] = self.momentum(self.model.params[key],
                                                                      self.model.grads[key],
                                                                      self.delta_x[key],
                                                                      self.init_lr)


class Adam(Optimizer):
    def __init__(self, init_lr, model, beta1, beta2, epsilon):
        """
        Adam优化器初始化
        输入：
            - init_lr：初始学习率
            - model：模型，model.params存储模型参数值
            - beta1, beta2：移动平均的衰减率
            - epsilon：保持数值稳定性而设置的常数
        """
        super(Adam, self).__init__(init_lr=init_lr, model=model)
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.M, self.G = {}, {}
        for key in self.model.params.keys():
            self.M[key] = 0
            self.G[key] = 0
        self.t = 1

    def adam(self, x, gradient_x, G, M, t, init_lr):
        """
        adam算法更新参数
        输入：
            - x：参数
            - G：梯度平方的加权移动平均
            - M：梯度的加权移动平均
            - t：迭代次数
            - init_lr：初始学习率
        """
        M = self.beta1 * M + (1 - self.beta1) * gradient_x
        G = self.beta2 * G + (1 - self.beta2) * gradient_x ** 2
        M_hat = M / (1 - self.beta1 ** t)
        G_hat = G / (1 - self.beta2 ** t)
        t += 1
        x -= init_lr / torch.sqrt(G_hat + self.epsilon) * M_hat
        return x, G, M, t

    def step(self):
        """参数更新"""
        for key in self.model.params.keys():
            self.model.params[key], self.G[key], self.M[key], self.t = self.adam(self.model.params[key],
                                                                                 self.model.grads[key],
                                                                                 self.G[key],
                                                                                 self.M[key],
                                                                                 self.t,
                                                                                 self.init_lr)


class OptimizedFunction3D(Op):
    def __init__(self):
        super(OptimizedFunction3D, self).__init__()
        self.params = {'x': 0}
        self.grads = {'x': 0}

    def forward(self, x):
        self.params['x'] = x
        return - x[0] * x[0] / 2 + x[1] * x[1] / 1  # x[0] ** 2 + x[1] ** 2 + x[1] ** 3 + x[0] * x[1]

    def backward(self):
        x = self.params['x']
        gradient1 = - 2 * x[0] / 2
        gradient2 = 2 * x[1] / 1
        grad1 = torch.Tensor([gradient1])
        grad2 = torch.Tensor([gradient2])
        self.grads['x'] = torch.cat([grad1, grad2])


class Visualization3D(animation.FuncAnimation):
    """    绘制动态图像，可视化参数更新轨迹    """

    def __init__(self, *xy_values, z_values, labels=[], colors=[], fig, ax, interval=100, blit=True, **kwargs):
        """
        初始化3d可视化类
        输入：
            xy_values：三维中x,y维度的值
            z_values：三维中z维度的值
            labels：每个参数更新轨迹的标签
            colors：每个轨迹的颜色
            interval：帧之间的延迟（以毫秒为单位）
            blit：是否优化绘图
        """
        self.fig = fig
        self.ax = ax
        self.xy_values = xy_values
        self.z_values = z_values

        frames = max(xy_value.shape[0] for xy_value in xy_values)
        # , marker = 'o'
        self.lines = [ax.plot([], [], [], label=label, color=color, lw=2)[0]
                      for _, label, color in zip_longest(xy_values, labels, colors)]
        print(self.lines)
        super(Visualization3D, self).__init__(fig, self.animate, init_func=self.init_animation, frames=frames,
                                              interval=interval, blit=blit, **kwargs)

    def init_animation(self):
        # 数值初始化
        for line in self.lines:
            line.set_data([], [])
            # line.set_3d_properties(np.asarray([]))  # 源程序中有这一行，加上会报错。 Edit by David 2022.12.4
        return self.lines

    def animate(self, i):
        # 将x,y,z三个数据传入，绘制三维图像
        for line, xy_value, z_value in zip(self.lines, self.xy_values, self.z_values):
            line.set_data(xy_value[:i, 0], xy_value[:i, 1])
            line.set_3d_properties(z_value[:i])
        return self.lines


def train_f(model, optimizer, x_init, epoch):
    x = x_init
    all_x = []
    losses = []
    for i in range(epoch):
        all_x.append(copy.deepcopy(x.numpy()))  # 浅拷贝 改为 深拷贝, 否则List的原值会被改变。 Edit by David 2022.12.4.
        loss = model(x)
        losses.append(loss)
        model.backward()
        optimizer.step()
        x = model.params['x']
    return torch.Tensor(np.array(all_x)), losses


# 构建5个模型，分别配备不同的优化器
model1 = OptimizedFunction3D()
opt_gd = SimpleBatchGD(init_lr=0.05, model=model1)

model2 = OptimizedFunction3D()
opt_adagrad = Adagrad(init_lr=0.05, model=model2, epsilon=1e-7)

model3 = OptimizedFunction3D()
opt_rmsprop = RMSprop(init_lr=0.05, model=model3, beta=0.9, epsilon=1e-7)

model4 = OptimizedFunction3D()
opt_momentum = Momentum(init_lr=0.05, model=model4, rho=0.9)

model5 = OptimizedFunction3D()
opt_adam = Adam(init_lr=0.05, model=model5, beta1=0.9, beta2=0.99, epsilon=1e-7)

models = [model5, model2, model3, model4, model1]
opts = [opt_adam, opt_adagrad, opt_rmsprop, opt_momentum, opt_gd]

x_all_opts = []
z_all_opts = []

# 使用不同优化器训练

for model, opt in zip(models, opts):
    x_init = torch.FloatTensor([0.00001, 0.5])
    x_one_opt, z_one_opt = train_f(model, opt, x_init, 100)  # epoch
    # 保存参数值
    x_all_opts.append(x_one_opt.numpy())
    z_all_opts.append(np.squeeze(z_one_opt))

# 使用numpy.meshgrid生成x1,x2矩阵，矩阵的每一行为[-3, 3]，以0.1为间隔的数值
x1 = np.arange(-1, 2, 0.01)
x2 = np.arange(-1, 1, 0.05)
x1, x2 = np.meshgrid(x1, x2)
init_x = torch.Tensor(np.array([x1, x2]))

model = OptimizedFunction3D()

# 绘制 f_3d函数 的 三维图像
fig = plt.figure()
ax = plt.axes(projection='3d')
X = init_x[0].numpy()
Y = init_x[1].numpy()
Z = model(init_x).numpy()  # 改为 model(init_x).numpy() David 2022.12.4
surf = ax.plot_surface(X, Y, Z, edgecolor='grey', cmap=cm.coolwarm)
# fig.colorbar(surf, shrink=0.5, aspect=1)
ax.set_zlim(-3, 2)
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('f(x1,x2)')

labels = ['Adam', 'AdaGrad', 'RMSprop', 'Momentum', 'SGD']
colors = ['#8B0000', '#0000FF', '#000000', '#008B00', '#FF0000']

animator = Visualization3D(*x_all_opts, z_values=z_all_opts, labels=labels, colors=colors, fig=fig, ax=ax)
ax.legend(loc='upper right')

plt.show()

</code></pre> 
<p>运行结果：</p> 
<p class="img-center"><img alt="" height="363" src="https://images2.imgbox.com/6c/bc/ihJ3C1jF_o.png" width="411"></p> 
<p> </p> 
<h2 id="%C2%A0%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"> 参考资料</h2> 
<p><a href="https://cs231n.github.io/neural-networks-3/" rel="nofollow" title="CS231n Convolutional Neural Networks for Visual Recognition">CS231n Convolutional Neural Networks for Visual Recognition</a></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/97df0a668ab40ba2545571208524e743/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">⚡文件工具类⚡</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d58ba36c0f4d77cde01edb2c61294a40/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">koa项目</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>