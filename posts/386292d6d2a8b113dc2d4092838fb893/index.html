<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>（2020，StyleGAN2）分析和提高 StyleGAN 的图像质量 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="（2020，StyleGAN2）分析和提高 StyleGAN 的图像质量" />
<meta property="og:description" content="Analyzing and Improving the Image Quality of StyleGAN
公众号：EDPJ
目录
0. 摘要
1. 简介
2. 去除归一化伪影
2.1 重新审视生成器架构
2.2 重新审视实例归一化
3. 图像质量和生成器平滑度 3.1 惰性正则化（Lazy regularization）
3.2 路径长度正则化
4. 重新审视渐进式增长
4.1 替代的网络架构 4.2 分辨率使用
5. 图像到隐空间的投影
5.1 生成图像的属性
6. 结论与未来工作
参考
S. 总结
S.1 核心思想
S.2 网络结构
S.3 其他贡献
0. 摘要 基于样式的 GAN 架构 (StyleGAN) 在数据驱动的无条件生成图像建模中产生了最先进的结果。 我们揭示并分析了它的几个特征伪影（artifacts），并提出了模型架构和训练方法的更改以解决这些问题。 特别是，我们重新设计了生成器归一化（normalization），重新审视渐进式增长（progressive growing），并对生成器进行正则化（regularization），以鼓励在从隐编码到图像的映射中进行良好调节。 除了提高图像质量之外，这种路径长度正则化器还带来了额外的好处，即生成器变得更容易反转（invert）。 这使得将生成的图像可靠地溯源到特定网络成为可能。 我们进一步可视化生成器如何很好地利用其输出分辨率，并确定容量问题，促使我们训练更大的模型以进一步提高质量。 总的来说，我们改进的模型在现有分布质量指标和感知图像质量方面重新定义了无条件图像建模的最新技术水平。
1. 简介 许多观察者已经注意到 StyleGAN 生成的图像中的特征伪影。 我们确定了这些伪影的两个原因，并描述了消除它们的架构和训练方法的变化。
首先，我们调查了常见的类似小斑点的伪影的来源，并发现生成器创建它们是为了规避其体系结构中的设计缺陷。 在第 2 节中，我们重新设计了生成器中使用的规范化，它删除了伪影。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/386292d6d2a8b113dc2d4092838fb893/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-22T17:28:25+08:00" />
<meta property="article:modified_time" content="2023-06-22T17:28:25+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">（2020，StyleGAN2）分析和提高 StyleGAN 的图像质量</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p style="text-align:center;"><strong>Analyzing and Improving the Image Quality of StyleGAN</strong></p> 
<p style="text-align:center;">公众号：EDPJ</p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="-toc" style="margin-left:0px;"><a href="#0.%20%E6%91%98%E8%A6%81" rel="nofollow">0. 摘要</a></p> 
<p id="1.%20%E7%AE%80%E4%BB%8B-toc" style="margin-left:0px;"><a href="#1.%20%E7%AE%80%E4%BB%8B" rel="nofollow">1. 简介</a></p> 
<p id="2.%20%E5%8E%BB%E9%99%A4%E5%BD%92%E4%B8%80%E5%8C%96%E4%BC%AA%E5%BD%B1-toc" style="margin-left:0px;"><a href="#2.%20%E5%8E%BB%E9%99%A4%E5%BD%92%E4%B8%80%E5%8C%96%E4%BC%AA%E5%BD%B1" rel="nofollow">2. 去除归一化伪影</a></p> 
<p id="2.1%20%E9%87%8D%E6%96%B0%E5%AE%A1%E8%A7%86%E7%94%9F%E6%88%90%E5%99%A8%E6%9E%B6%E6%9E%84-toc" style="margin-left:40px;"><a href="#2.1%20%E9%87%8D%E6%96%B0%E5%AE%A1%E8%A7%86%E7%94%9F%E6%88%90%E5%99%A8%E6%9E%B6%E6%9E%84" rel="nofollow">2.1 重新审视生成器架构</a></p> 
<p id="2.2%20%E9%87%8D%E6%96%B0%E5%AE%A1%E8%A7%86%E5%AE%9E%E4%BE%8B%E5%BD%92%E4%B8%80%E5%8C%96-toc" style="margin-left:40px;"><a href="#2.2%20%E9%87%8D%E6%96%B0%E5%AE%A1%E8%A7%86%E5%AE%9E%E4%BE%8B%E5%BD%92%E4%B8%80%E5%8C%96" rel="nofollow">2.2 重新审视实例归一化</a></p> 
<p id="3.%20%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E5%92%8C%E7%94%9F%E6%88%90%E5%99%A8%E5%B9%B3%E6%BB%91%E5%BA%A6%C2%A0-toc" style="margin-left:0px;"><a href="#3.%20%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E5%92%8C%E7%94%9F%E6%88%90%E5%99%A8%E5%B9%B3%E6%BB%91%E5%BA%A6%C2%A0" rel="nofollow">3. 图像质量和生成器平滑度 </a></p> 
<p id="3.1%20%E6%83%B0%E6%80%A7%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88Lazy%20regularization%EF%BC%89-toc" style="margin-left:40px;"><a href="#3.1%20%E6%83%B0%E6%80%A7%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88Lazy%20regularization%EF%BC%89" rel="nofollow">3.1 惰性正则化（Lazy regularization）</a></p> 
<p id="3.2%20%E8%B7%AF%E5%BE%84%E9%95%BF%E5%BA%A6%E6%AD%A3%E5%88%99%E5%8C%96-toc" style="margin-left:40px;"><a href="#3.2%20%E8%B7%AF%E5%BE%84%E9%95%BF%E5%BA%A6%E6%AD%A3%E5%88%99%E5%8C%96" rel="nofollow">3.2 路径长度正则化</a></p> 
<p id="4.%20%E9%87%8D%E6%96%B0%E5%AE%A1%E8%A7%86%E6%B8%90%E8%BF%9B%E5%BC%8F%E5%A2%9E%E9%95%BF-toc" style="margin-left:0px;"><a href="#4.%20%E9%87%8D%E6%96%B0%E5%AE%A1%E8%A7%86%E6%B8%90%E8%BF%9B%E5%BC%8F%E5%A2%9E%E9%95%BF" rel="nofollow">4. 重新审视渐进式增长</a></p> 
<p id="4.1%20%E6%9B%BF%E4%BB%A3%E7%9A%84%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%C2%A0-toc" style="margin-left:40px;"><a href="#4.1%20%E6%9B%BF%E4%BB%A3%E7%9A%84%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%C2%A0" rel="nofollow">4.1 替代的网络架构 </a></p> 
<p id="4.2%20%E5%88%86%E8%BE%A8%E7%8E%87%E4%BD%BF%E7%94%A8-toc" style="margin-left:40px;"><a href="#4.2%20%E5%88%86%E8%BE%A8%E7%8E%87%E4%BD%BF%E7%94%A8" rel="nofollow">4.2 分辨率使用</a></p> 
<p id="5.%20%E5%9B%BE%E5%83%8F%E5%88%B0%E9%9A%90%E7%A9%BA%E9%97%B4%E7%9A%84%E6%8A%95%E5%BD%B1-toc" style="margin-left:0px;"><a href="#5.%20%E5%9B%BE%E5%83%8F%E5%88%B0%E9%9A%90%E7%A9%BA%E9%97%B4%E7%9A%84%E6%8A%95%E5%BD%B1" rel="nofollow">5. 图像到隐空间的投影</a></p> 
<p id="5.1%20%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E7%9A%84%E5%B1%9E%E6%80%A7-toc" style="margin-left:40px;"><a href="#5.1%20%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E7%9A%84%E5%B1%9E%E6%80%A7" rel="nofollow">5.1 生成图像的属性</a></p> 
<p id="6.%20%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B7%A5%E4%BD%9C-toc" style="margin-left:0px;"><a href="#6.%20%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B7%A5%E4%BD%9C" rel="nofollow">6. 结论与未来工作</a></p> 
<p id="%E5%8F%82%E8%80%83-toc" style="margin-left:0px;"><a href="#%E5%8F%82%E8%80%83" rel="nofollow">参考</a></p> 
<p id="S.%20%E6%80%BB%E7%BB%93-toc" style="margin-left:0px;"><a href="#S.%20%E6%80%BB%E7%BB%93" rel="nofollow">S. 总结</a></p> 
<p id="S.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-toc" style="margin-left:40px;"><a href="#S.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3" rel="nofollow">S.1 核心思想</a></p> 
<p id="S.2%20%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-toc" style="margin-left:40px;"><a href="#S.2%20%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84" rel="nofollow">S.2 网络结构</a></p> 
<p id="S.3%20%E5%85%B6%E4%BB%96%E8%B4%A1%E7%8C%AE-toc" style="margin-left:40px;"><a href="#S.3%20%E5%85%B6%E4%BB%96%E8%B4%A1%E7%8C%AE" rel="nofollow">S.3 其他贡献</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="0.%20%E6%91%98%E8%A6%81">0. 摘要</h2> 
<p>基于样式的 GAN 架构 (StyleGAN) 在数据驱动的无条件生成图像建模中产生了最先进的结果。 我们揭示并分析了它的几个特征伪影（artifacts），并提出了模型架构和训练方法的更改以解决这些问题。 特别是，我们重新设计了生成器归一化（normalization），重新审视渐进式增长（progressive growing），并对生成器进行正则化（regularization），以鼓励在从隐编码到图像的映射中进行良好调节。 除了提高图像质量之外，这种路径长度正则化器还带来了额外的好处，即生成器变得更容易反转（invert）。 这使得将生成的图像可靠地溯源到特定网络成为可能。 我们进一步可视化生成器如何很好地利用其输出分辨率，并确定容量问题，促使我们训练更大的模型以进一步提高质量。 总的来说，我们改进的模型在现有分布质量指标和感知图像质量方面重新定义了无条件图像建模的最新技术水平。</p> 
<h2 id="1.%20%E7%AE%80%E4%BB%8B">1. 简介</h2> 
<p>许多观察者已经注意到 StyleGAN 生成的图像中的特征伪影。 我们确定了这些伪影的两个原因，并描述了消除它们的架构和训练方法的变化。</p> 
<p>首先，我们调查了常见的类似小斑点的伪影的来源，并发现生成器创建它们是为了规避其体系结构中的设计缺陷。 在第 2 节中，我们重新设计了生成器中使用的规范化，它删除了伪影。</p> 
<p>其次，我们分析了与渐进式增长相关的伪影，这些伪影在稳定高分辨率 GAN 训练方面非常成功。 我们提出了一种实现相同目标的替代设计——训练从关注低分辨率图像开始，然后逐渐将注意力转移到越来越高的分辨率——在训练期间不改变网络拓扑。 这种新设计还使我们能够推断生成图像的有效分辨率，结果证明它低于预期，从而促使容量增加（第 4 节）。</p> 
<p>使用生成式方法生成的图像质量的定量分析仍然是一个具有挑战性的话题。FID 测量 InceptionV3 分类器高维特征空间中两个分布的密度差异。 Precision and Recall (P&amp;R) 通过明确量化生成的与训练数据相似的图像的百分比和可以生成的训练数据的百分比来提供额外的可见性。 我们使用这些指标来量化改进。</p> 
<p>FID 和 P&amp;R 都基于分类器网络，这些网络最近被证明专注于纹理而不是外形，因此，指标无法准确捕捉图像质量的所有方面。感知路径长度 (PPL) 度量最初是作为一种估计隐空间插值质量的方法引入的，它与外形的一致性和稳定性相关。 在此基础上，我们对合成网络进行正则化以支持平滑映射（第 3 节）并实现质量的明显提高。 为了抵消其计算开销，我们还建议降低执行所有正则化的频率，观察到这可以在不影响有效性的情况下完成。</p> 
<p>最后，我们发现与原始 StyleGAN 相比，新的路径长度正则化 StyleGAN2 生成器将图像投影到潜在空间 W 的效果要好得多。 这使得将生成的图像溯源变得更加容易（第 5 节）。</p> 
<h2 id="2.%20%E5%8E%BB%E9%99%A4%E5%BD%92%E4%B8%80%E5%8C%96%E4%BC%AA%E5%BD%B1">2. 去除归一化伪影</h2> 
<p><img alt="" height="291" src="https://images2.imgbox.com/cc/0d/AWztD1SE_o.png" width="1154"></p> 
<p>我们首先观察到，大多数由 StyleGAN 生成的图像都表现出类似于水滴的特征性斑点状伪影。 如图 1 所示，虽然液滴在最终图像中可能不明显，但它在生成器的中间特征图中很明显（在极少数情况下，可能占图像的 0.1%，液滴丢失，导致图像严重损坏。 详见附录 A）。 异常开始出现在 64×64 分辨率附近，存在于所有特征图中，并在更高的分辨率下逐渐变得更强。 这种一致伪像的存在令人费解，因为鉴别器应该能够检测到它。</p> 
<p>我们将问题确定为 AdaIN 操作，该操作分别对每个特征映射的均值和方差进行归一化，从而可能破坏基于特征相对于彼此的大小而发现的任何信息。 我们假设伪影是生成器故意将信号强度信息偷偷通过实例归一化的结果：通过创建一个强大的局部尖峰来控制统计数据，生成器可以在其他地方随意缩放信号。 我们的假设得到以下发现的支持：当归一化步骤从生成器中移除时，如下文详述，伪影完全消失。</p> 
<h3 id="2.1%20%E9%87%8D%E6%96%B0%E5%AE%A1%E8%A7%86%E7%94%9F%E6%88%90%E5%99%A8%E6%9E%B6%E6%9E%84">2.1 重新审视生成器架构</h3> 
<p class="img-center"><img alt="" height="710" src="https://images2.imgbox.com/05/5b/MMgj55tc_o.png" width="1137"></p> 
<p>我们将首先修改 StyleGAN 生成器的几个细节，以更好地促进我们重新设计的归一化。这些更改本身在质量指标方面具有中性或小的积极影响。</p> 
<ul><li>图 2a 显示了原始的 StyleGAN 合成网络 g</li><li>图 2b 中，我们显示权重和偏差，并将 AdaIN 操作分解为两个部分：归一化（normalization）和调制（modulation），将图扩展到完整的细节。 这使我们能够重新绘制概念性的灰色框，以便每个框指示一种样式处于活动状态的网络部分（即 “style block”）。 有趣的是，最初的 StyleGAN 在 style block 中应用了偏差和噪声，导致它们的影响与当前风格的幅度成反比。</li><li>我们观察到，通过将这些操作移到 style block 之外（它们对归一化数据进行操作），可以获得更可预测的结果。 此外，我们注意到，在此更改之后，标准化和调制仅对标准差（std）进行操作就足够了（即不需要均值 mean）。 也可以安全地移除对常量输入使用的偏差、噪声和归一化，而不会产生明显的缺陷。 此变体如图 2c 所示，并作为我们重新设计的归一化的起点。 </li></ul> 
<h3 id="2.2%20%E9%87%8D%E6%96%B0%E5%AE%A1%E8%A7%86%E5%AE%9E%E4%BE%8B%E5%BD%92%E4%B8%80%E5%8C%96">2.2 重新审视实例归一化</h3> 
<p>StyleGAN 的主要优势之一是能够通过风格混合来控制生成的图像，即在推理时将不同的隐编码 w 馈送到不同的层。 在实践中，风格调制可能会将某些特征图放大一个数量级或更多。 为了使风格混合起作用，我们必须在每个样本的基础上明确地抵消这种放大——否则后续层将无法以有意义的方式对数据进行操作。 </p> 
<p>如果我们愿意牺牲特定比例的控制，我们可以简单地删除归一化，从而删除伪影并稍微改善 FID。 我们现在将提出一个更好的替代方案，在保留完全可控性的同时去除伪影。 主要思想是基于输入特征图的预期统计数据进行归一化，但没有明确的强制。</p> 
<p>回想一下，图 2c 中的 style block 由调制、卷积和归一化组成。 让我们首先考虑调制后跟卷积的效果。 调制根据输入的样式缩放卷积的每个输入特征图，也可以通过缩放卷积权重来实现：</p> 
<p class="img-center"><img alt="" height="33" src="https://images2.imgbox.com/66/5d/ENhYDGO9_o.png" width="355"></p> 
<p>其中 w 和 w' 分别是原始和调制权重，s_i 是对应于第 i 个输入特征图的尺度，j 和 k 分别列举输出特征图和卷积的空间索引。</p> 
<p>现在，实例归一化的目的是从本质上消除 s 从卷积输出特征图的统计数据中的影响。 我们观察到这个目标可以更直接地实现。 让我们假设输入激活是独立同分布的、具有单位标准偏差的随机变量。 在调制和卷积之后，输出激活的标准差为</p> 
<p class="img-center"><img alt="" height="66" src="https://images2.imgbox.com/73/fc/4M1ljopG_o.png" width="365"></p> 
<p>即，输出由相应权重的 L2 范数缩放。 随后的归一化旨在将输出恢复到单位标准偏差。 根据等式 2，如果我们按 1 / σ_ j 缩放（“demodulate”）每个输出特征图 j，就可以实现这一点。 或者，我们可以再次将其加入（除）到卷积权重中： </p> 
<p class="img-center"><img alt="" height="69" src="https://images2.imgbox.com/b5/bc/MHokJHtY_o.png" width="422"></p> 
<p>其中 ε 是一个小常数，以避免数值问题。</p> 
<p>我们现在已经将整个样式块加入到单个卷积层，其权重根据 s 使用等式 1 和 3 进行调整（图 2d）。 与实例归一化相比，我们的解调技术较弱，因为它基于信号的统计假设而不是特征图的实际内容。 类似的统计分析已广泛用于现代网络初始化器，但我们不知道它以前被用作数据相关规范化的替代品。 我们的解调还与权重归一化相关，权重归一化执行与重新参数化权重张量的一部分相同的计算。 之前的工作已经确定权重归一化在 GAN 训练的背景下是有益的。</p> 
<p class="img-center"><img alt="" height="568" src="https://images2.imgbox.com/ba/ae/6ZOB15Kg_o.png" width="548"></p> 
<p><img alt="" height="411" src="https://images2.imgbox.com/13/b1/rddIcb7W_o.png" width="1200">我们的新设计消除了特征伪影（图 3），同时保留了完全可控性，如随附视频中所示。 FID 基本不受影响（表 1，A、B 行），但 precision 和 recall 有显着转变。 我们认为这通常是可取的，因为可以通过截断将 recall 换成 precision，而反之则不然。 在实践中，我们的设计可以使用分组卷积有效地实现，如附录 B 中详述。为了避免必须考虑等式 3 中的激活函数，我们缩放我们的激活函数，以便它们保留预期的信号方差。 </p> 
<h2 id="3.%20%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E5%92%8C%E7%94%9F%E6%88%90%E5%99%A8%E5%B9%B3%E6%BB%91%E5%BA%A6%C2%A0">3. 图像质量和生成器平滑度 </h2> 
<p>虽然 FID 或 precision 和 recall (P&amp;R) 等 GAN 指标成功地捕获了生成器的许多方面，但它们在图像质量方面仍然存在一些盲点。 例如，请参阅补充材料中的图 3 和图 4，它们对比了具有相同 FID 和 P&amp;R 分数但总体质量明显不同的生成器。</p> 
<p>我们认为，造成明显不一致的关键在于特征空间的特定选择，而不是 FID 或 P&amp;R 的基础。 最近发现，使用 ImageNet 训练的分类器倾向于更多地基于纹理而不是外形做出决策，而人类则非常关注外形。 这在我们的上下文中是相关的，因为 FID 和 P&amp;R 分别使用来自 InceptionV3 和 VGG-16 的高级特征，它们以这种方式训练，因此预计会偏向于纹理检测。 因此，具有例如强壮猫的纹理的图像可能看起来比人类观察者认为的更相似，因此部分地折衷了基于密度的度量（FID）和流形覆盖度量（P＆R）。</p> 
<p>我们观察到感知图像质量和感知路径长度 (perceptual path length，PPL) 之间的相关性，这是最初引入的一种度量，在隐空间中的小扰动下，通过测量生成图像之间的平均 LPIPS 距离来量化从隐空间到输出图像的映射的平滑度。 再次参考补充材料中的图 3 和图 4，较小的 PPL（更平滑的生成器映射）似乎与更高的整体图像质量相关，而其他指标对变化视而不见。</p> 
<p class="img-center"><img alt="" height="343" src="https://images2.imgbox.com/6f/c4/FDzBMxeI_o.png" width="548"></p> 
<p>图 4 通过 LSUN CAT 上的每张图像的 PPL 分数更仔细地检查了这种相关性，通过对 w∼f(z) 周围的隐空间进行采样来计算。 低分确实表明图像质量高，反之亦然。</p> 
<p><img alt="" height="881" src="https://images2.imgbox.com/26/36/SIi0D451_o.png" width="1200"></p> 
<p>图 5a 显示了相应的直方图并揭示了分布的长尾。 模型的总体 PPL 只是这些每张图像 PPL 分数的预期值。 我们为整个图像计算 PPL，而不是 Karras 等人使用的较小的中央裁剪。</p> 
<p>低 PPL 与图像质量相关的原因并不是很明显。 我们假设在训练过程中，由于鉴别器对损坏的图像进行惩罚，因此生成器改进的最直接方法是有效地拉伸产生良好图像的隐空间区域。 这将导致低质量图像被挤压到快速变化的小隐空间区域。 虽然这在短期内提高了平均输出质量，但累积的失真会损害训练动态，从而影响最终图像质量。 </p> 
<p>显然，我们不能简单地鼓励最小 PPL，因为这会引导生成器朝着零 recall 的退化解决方案发展。 相反，我们将描述一种新的正则化器，旨在实现没有这个缺点的更平滑的生成器映射。 由于该正则化项计算起来有些昂贵，我们首先描述适用于任何正则化技术的一般优化。 </p> 
<h3 id="3.1%20%E6%83%B0%E6%80%A7%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88Lazy%20regularization%EF%BC%89">3.1 惰性正则化（Lazy regularization）</h3> 
<p>通常，主要损失函数（例如，logistic loss）和正则化项（例如，R1）被写为单个表达式，因此同时进行优化。 我们观察到正则化项的计算频率低于主损失函数，因此大大降低了它们的计算成本和整体内存使用量。 表 1，C 行显示，当每 16 个 minibatch 只执行一次 R1 正则化时，不会造成任何伤害，我们也对新的正则化器采用相同的策略。 附录 B 给出了实现细节。 </p> 
<h3 id="3.2%20%E8%B7%AF%E5%BE%84%E9%95%BF%E5%BA%A6%E6%AD%A3%E5%88%99%E5%8C%96">3.2 路径长度正则化</h3> 
<p>我们希望鼓励 W 中的固定大小步长导致图像中的非零的、固定幅度变化。 我们可以通过进入图像空间中的随机方向并观察相应的 w 梯度来凭经验测量与这个理想的偏差。 无论 w 或图像空间方向如何，这些梯度应该接近相等，表明从隐空间到图像空间的映射是条件良好的。</p> 
<p>在单个 w ∈ W 处，生成器映射 g(w) : W → Y 的局部度量缩放属性由雅可比矩阵 J_w = ∂g(w)/∂w 表示。 出于无论方向如何都保留向量预期长度的愿望，我们将正则化器表示为</p> 
<p class="img-center"><img alt="" height="38" src="https://images2.imgbox.com/fd/5b/XFvKaEli_o.png" width="412"></p> 
<p>其中 y 是像素强度呈正态分布的随机图像，w ∼ f(z)，其中 z 呈正态分布。 我们在附录 C 中表明，在高维度中，当 J_w 在任何 w 处正交（达到全局范围）时，此先验最小化。 正交矩阵保留长度并且不会沿任何维度引入压缩。</p> 
<p>为了避免雅可比矩阵的显式计算，我们使用恒等式 <img alt="\mathop J\nolimits_w^T y = \mathop \nabla \nolimits_w (g(w) \cdot y)" class="mathcode" src="https://images2.imgbox.com/c3/59/SOMvWNpm_o.png">，使用标准反向传播可以有效地计算它。 常数 a 在优化期间动态设置为长度 <img alt="\mathop {\left\| {\mathop J\nolimits_w^T y} \right\|}\nolimits_2" class="mathcode" src="https://images2.imgbox.com/db/f6/ROkoNwrj_o.png"> 的长期指数移动平均值，允许优化自行找到合适的全局尺度。 </p> 
<p>我们的正则化器与 Odena 等人提出的雅可比 Clamping 正则化器密切相关。 实际差异包括，我们分析地计算乘积 <img alt="\mathop J\nolimits_w^T y" class="mathcode" src="https://images2.imgbox.com/1e/06/P2s1nQ6I_o.png">，而他们使用有限差分来估计 Jw 和 Z ∋ ∼ N(0, I)。 应该注意的是，生成器的谱归一化仅约束最大的奇异值，对其他值没有约束，因此不一定会导致更好的调节。 我们发现，在我们的贡献之外启用谱归一化（或代替它们）总是会损害 FID，如附录 E 中详述。</p> 
<p>在实践中，我们注意到路径长度正则化导致更可靠和一致的行为模型，使架构探索更容易。 我们还观察到更平滑的生成器更容易反转（第 5 节）。 图 5b 显示路径长度正则化明显收紧了每幅图像 PPL 分数的分布，而没有将 mode 推至零。 然而，表 1 的 D 行指向在结构不如 FFHQ 的数据集中 FID 和 PPL 之间的权衡。</p> 
<h2 id="4.%20%E9%87%8D%E6%96%B0%E5%AE%A1%E8%A7%86%E6%B8%90%E8%BF%9B%E5%BC%8F%E5%A2%9E%E9%95%BF">4. 重新审视渐进式增长</h2> 
<p class="img-center"><img alt="" height="360" src="https://images2.imgbox.com/f2/9a/IJrNl9Ud_o.png" width="548"></p> 
<p>渐进式增长在稳定高分辨率图像合成方面非常成功，但它会导致其自身的特征伪影。 关键问题是逐渐增长的生成器似乎对细节有很强的位置偏好； 随附的视频显示，当牙齿或眼睛等特征应该在图像上平滑移动时，它们可能会在跳到下一个偏好位置之前停留在原地。 图 6 显示了一个相关的伪影（虽然头部姿态改变，但是蓝线在图中的相对位置并没有改变）。 我们认为问题在于，在渐进式增长中，每个分辨率都会暂时用作输出分辨率，迫使它生成最大频率细节，然后导致经过训练的网络在中间层中具有过高的频率，从而导致移位不变性。 附录 A 显示了一个示例。 这些问题促使我们寻找一种替代方案，以保留渐进式增长的好处而没有缺点。 </p> 
<h3 id="4.1%20%E6%9B%BF%E4%BB%A3%E7%9A%84%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%C2%A0">4.1 替代的网络架构 </h3> 
<p>虽然 StyleGAN 在生成器（合成网络）和鉴别器中使用简单的前馈设计，但仍有大量工作致力于研究更好的网络架构。 跳跃连接（skip connection）、残差网络（residual networks）和分层方法（hierarchical methods）在生成方法的背景下也被证明非常成功。 因此，我们决定重新评估 StyleGAN 的网络设计，并寻找一种无需渐进式增长即可生成高质量图像的架构。</p> 
<p><img alt="" height="698" src="https://images2.imgbox.com/01/e1/wcWm99Pn_o.png" width="723"></p> 
<p>图 7a 显示了 MSG-GAN，它使用多个 skip connection 连接生成器和鉴别器的匹配分辨率。 MSG-GAN 生成器被修改为输出 mipmap 而不是图像，并为每个真实图像计算类似的 representation。 在图 7b 中，我们通过对不同分辨率对应的 RGB 输出的贡献进行上采样和求和来简化此设计。 在鉴别器中，我们类似地向鉴别器的每个分辨率块提供下采样图像。 我们在所有上采样和下采样操作中使用双线性过滤。 在图 7c 中，我们进一步修改设计以使用 residual connection（在残差网络架构中，添加两条路径会导致信号方差加倍，我们通过乘以 1 / 2^(1/2) 来抵消。 这对我们的网络至关重要，而在分类 resnets 中，该问题通常被 batch normalization 隐藏）。该设计类似于 LAPGAN，但没有 Denton 等人使用的每分辨率鉴别器。</p> 
<p class="img-center"><img alt="" height="346" src="https://images2.imgbox.com/e7/55/HyCjBL60_o.png" width="549"></p> 
<p>表 2 比较了三个生成器和三个鉴别器架构：StyleGAN 中使用的原始前馈网络、跳跃连接和残差网络，所有这些都经过训练而没有渐进式增长。9 种组合中的每一种都提供了 FID 和 PPL。 我们可以看到两大趋势：生成器中的跳过连接在所有配置中显着改善 PPL，而残差鉴别器网络显然有利于 FID。 后者可能并不令人惊讶，因为鉴别器的结构类似于已知残差架构的有用的分类器。 然而，残差架构在生成器中是有害的——唯一的例外是 LSUN CAR 中的 FID，此时两个网络都是残差的。 </p> 
<p>对于本文的其余部分，我们使用跳跃生成器和残差鉴别器，而不使用渐进式增长。 这对应于表 1 中的配置 E，它显着改善了 FID 和 PPL。</p> 
<h3 id="4.2%20%E5%88%86%E8%BE%A8%E7%8E%87%E4%BD%BF%E7%94%A8">4.2 分辨率使用</h3> 
<p>我们希望保留的渐进式增长的关键方面是生成器最初将关注低分辨率特征，然后慢慢将注意力转移到更精细的细节上。 图 7 中的架构使生成器可以首先输出不受高分辨率层影响的低分辨率图像，然后随着训练的进行将焦点转移到高分辨率层。 由于这不是以任何方式强制执行的，因此生成器只有在有益时才会这样做。 为了分析实践中的行为，我们需要量化生成器在训练过程中对特定分辨率的依赖程度。  </p> 
<p>由于跳跃生成器（图 7b）通过显式地对来自多个分辨率的 RGB 值求和来形成图像，我们可以通过测量相应层对最终图像的贡献程度来估计相应层的相对重要性。 在图 8a 中，我们将每个 tRGB 层产生的像素值的标准差绘制为训练时间的函数。 我们计算 w 的 1024 个随机样本的标准差，并对这些值进行归一化，使它们总和为 100%。</p> 
<p>在训练开始时，我们可以看到新的跳跃生成器的行为类似于渐进式增长——现在无需更改网络拓扑即可实现。 因此，期望最高分辨率在训练结束时占主导地位是合理的。 然而，该图显示这在实践中并未发生，这表明生成器可能无法“充分利用”目标分辨率。 为了验证这一点，我们手动检查了生成的图像，并注意到它们通常缺少训练数据中存在的一些像素级细节——这些图像可以被描述为 512x512 张图像的锐化版本，而不是真正的 1024x1024 张图像。</p> 
<p class="img-center"><img alt="" height="479" src="https://images2.imgbox.com/2e/5f/JzEgYt4j_o.png" width="545"></p> 
<p>这使我们假设我们的网络存在容量问题，我们通过将两个网络的最高分辨率层中的特征图数量加倍来测试这一点。我们将分辨率 64x64 –1024x1024 中的特征图数量加倍，同时保持网络的其他部分不变。 这使生成器中的可训练参数总数增加了 22% (25M-30M)，鉴别器中的可训练参数总数增加了 21% (24M-29M)。这使行为更符合预期：图 8b 显示最高分辨率层的贡献显着增加，表 1 的 F 行显示 FID 和 recall 显着提高。 最后一行显示基线 StyleGAN 也受益于额外的容量，但其质量仍远低于 StyleGAN2。 </p> 
<p class="img-center"><img alt="" height="218" src="https://images2.imgbox.com/56/a7/XeAacEwU_o.png" width="552"></p> 
<p>表 3 在四个 LSUN 类别中比较了 StyleGAN 和 StyleGAN2，再次显示了 FID 的明显改进和 PPL 的显着进步。 进一步增加尺寸可能会提供额外的好处。     </p> 
<h2 id="5.%20%E5%9B%BE%E5%83%8F%E5%88%B0%E9%9A%90%E7%A9%BA%E9%97%B4%E7%9A%84%E6%8A%95%E5%BD%B1">5. 图像到隐空间的投影</h2> 
<p>逆转合成网络 g 是一个有趣的问题，有很多应用。 在隐特征空间中操作给定图像需要首先为其找到匹配的隐编码 w。 先前的研究表明，如果为生成器的每一层选择一个单独的、而不是找到一个共同的隐编码 w，结果会有所改善。 在早期的编码器实现中使用了相同的方法。 虽然以这种方式扩展隐空间可以找到与给定图像更接近的匹配，但它还可以投影应该没有 latent representation 的任意图像。 相反，我们专注于在原始的、未扩展的隐空间中寻找隐编码，因为这些对应于生成器可能生成的图像。 </p> 
<p>我们的投影方法在两个方面不同于以前的方法。</p> 
<ul><li>首先，为了更全面地探索隐空间，我们在优化过程中向隐编码添加了斜降噪声（ramped-down）。</li><li>其次，我们还优化了 StyleGAN 生成器的随机噪声输入，对其进行正则化以确保它们最终不会携带相干信号。 正则化基于强制噪声图的自相关系数与多个尺度上的单位高斯噪声相匹配。 我们的投影方法的详细信息可以在附录 D 中找到。</li></ul> 
<h3 id="5.1%20%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E7%9A%84%E5%B1%9E%E6%80%A7">5.1 生成图像的属性</h3> 
<p>检测操纵或生成的图像是一项非常重要的任务。 目前，基于分类器的方法可以非常可靠地检测生成的图像，无论它们的确切来源如何。 然而，鉴于生成方法的快速进步，这种情况可能不会持久。 除了假图像的一般检测外，我们还可以考虑问题的一种更极限的形式：能够追溯假图像的特定来源。 使用 StyleGAN，这相当于检查是否存在用于合成待检图像的 w ∈ W。</p> 
<p>我们通过计算原始图像和重新合成图像之间的 LPIPS 距离，来衡量投影的成功程度</p> 
<p class="img-center"><img alt="" height="29" src="https://images2.imgbox.com/19/1e/ELohdDXf_o.png" width="211"></p> 
<p>其中 x 是正在分析的图像，并且 ∼g^(−1) 表示近似投影操作。</p> 
<p class="img-center"><img alt="" height="483" src="https://images2.imgbox.com/69/8c/BgRu3HKB_o.png" width="547"></p> 
<p><img alt="" height="458" src="https://images2.imgbox.com/b6/9f/HWKp9nL6_o.png" width="1200"></p> 
<p>图 10 显示了使用原始 StyleGAN 和 StyleGAN2 的 LSUN CAR 和 FFHQ 数据集的这些距离的直方图，图 9 显示了示例投影。 使用 StyleGAN2 生成的图像可以很好地投影到 W 中，以至于它们几乎可以明确地溯源生成网络。 然而，对于原始的 StyleGAN，即使在技术上应该可以找到匹配的隐编码，但从 W 到图像的映射似乎过于复杂，以至于无法在实践中可靠地成功。 我们发现令人鼓舞的是，即使图像质量显着提高，StyleGAN2 也使溯源变得更加容易。</p> 
<h2 id="6.%20%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B7%A5%E4%BD%9C">6. 结论与未来工作</h2> 
<p>我们已经确定并修复了 StyleGAN 中的几个图像质量问题，进一步提高了质量，并大大提高了几个数据集中的最新技术水平。 在某些情况下，可以更清楚地看到动态中的改进，如随附视频中所示。 附录 A 包括使用我们的方法可获得的结果的更多示例。 尽管质量有所提高，但 StyleGAN2 使将生成的图像溯源变得更加容易。</p> 
<p>训练成绩也有所提高。 在 1024x1024 分辨率下，原始 StyleGAN（表 1 中的配置 A）在配备 8 个 Tesla V100 GPU 的 NVIDIA DGX-1 上以每秒 37 张图像的速度训练，而我们的配置 E 以 每秒 61 张图像的速度训练，快了 40%。 由于权重解调、惰性正则化和代码优化，大部分加速来自简化的数据流。 StyleGAN2（配置 F，更大的网络）以 31 img/s 的速度训练，因此训练成本仅比原始 StyleGAN 略高。 FFHQ 的总训练时间为 9 天，LSUN CAR 为 13 天。 </p> 
<p>整个项目，包括所有探索，消耗了 132 兆瓦时的电力，其中 0.68 兆瓦时用于训练最终的 FFHQ 模型。 总的来说，我们使用了大约 51 个单 GPU 年的计算（Volta 级 GPU）。 附录 F 中提供了更详细的讨论。</p> 
<p>将来，研究路径长度正则化的进一步改进可能会富有成果，例如，通过用数据驱动的特征空间度量替换像素空间 L2 距离。 考虑到 GAN 的实际部署，我们认为找到减少训练数据需求的新方法非常重要。 这在无法获取数万个训练样本以及包含大量内在变异的数据集的应用中尤为重要。</p> 
<h2 id="%E5%8F%82%E8%80%83">参考</h2> 
<p>Karras, Tero, et al. "Analyzing and improving the image quality of stylegan." <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 2020.</p> 
<h2 id="S.%20%E6%80%BB%E7%BB%93">S. 总结</h2> 
<h3 id="S.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3">S.1 核心思想</h3> 
<p>由 StyleGAN 生成的图像存在伪影，作者发现这是由生成网络的中的 AdaIN 引起的。通过修改生成器的网络结构，把 AdaIN 转化为调制解调操作，该问题被解决。</p> 
<h3 id="S.2%20%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84">S.2 网络结构</h3> 
<p class="img-center"><img alt="" height="710" src="https://images2.imgbox.com/c0/7e/nFuv0bXu_o.png" width="1137"></p> 
<p>生成器结构更改的过程如图 2 所示，a → d 显示的是从 AdaIN 转化为对风格进行调制（公式 1）和解调（公式 3）的过程。 </p> 
<p class="img-center"><img alt="" height="33" src="https://images2.imgbox.com/1c/e3/rKsQKLrh_o.png" width="355"></p> 
<p class="img-center"><img alt="" height="69" src="https://images2.imgbox.com/c3/d5/FcCxtnR0_o.png" width="422"></p> 
<h3 id="S.3%20%E5%85%B6%E4%BB%96%E8%B4%A1%E7%8C%AE">S.3 其他贡献</h3> 
<p><strong>惰性正则化</strong>。损失函数通常为主要损失函数加正则化项的形式，在实现时，可以在多次优化主要损失函数的同时只优化一次正则化项。该方法不会对结果造成影响，但是可以大大降低计算成本和整体内存使用量。</p> 
<p class="img-center"><img alt="" height="38" src="https://images2.imgbox.com/7e/cf/x50UAcIa_o.png" width="412"></p> 
<p><strong>路径长度正则化</strong>。正则化项如公式 4 所示，其中 W 和 Y 分别表示隐空间和图像空间，J 表示雅克比矩阵。当图像 y 的隐空间任一点对各个方向（属性）的梯度相等时，该正则化项取最小值。</p> 
<p>基于 II2S 一文可知，当属性相互解耦时，隐空间的多元分布近似于一个超球体，在球体表面，任一点对各个方向（属性）的梯度相等。</p> 
<p>基于 StyleGAN 一文可知，属性解耦程度越高，隐空间越不扭曲，感知路径长度越短。</p> 
<p><strong>替代渐进式增长</strong>。在渐进式增长中，每个分辨率都会暂时用作输出分辨率，迫使它生成最大频率细节，然后导致经过训练的网络在中间层中具有过高的频率，从而导致移位不变性（例如，虽然头部姿态改变，但是牙齿在图中的相对位置并没有改变）。</p> 
<p><img alt="" height="698" src="https://images2.imgbox.com/1c/6a/OaWEdHhc_o.png" width="723">如图 7 所示，可以使用 skip connection 和 residual networks 替代渐进式增长，从而生成高质量图像。</p> 
<p class="img-center"><img alt="" height="483" src="https://images2.imgbox.com/05/3b/zvpXs4E4_o.png" width="547"></p> 
<p><strong>图像溯源</strong>。如图 10 所示，对于图像与其投影图像的 LPIPS 距离分布，真实图像的整体数值大于生成图像的整体数值。相比于 StyleGAN，经由 StyleGAN2 生成的图像的差异更明显，更适合进行溯源。</p> 
<p class="img-center"><img alt="" height="29" src="https://images2.imgbox.com/17/72/gdVfCUE2_o.png" width="211"></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/747cae1af05f3f8e443ddc35631898c3/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">轻量化CNN模型整理—MobileNet，ShuffleNet，GhostNet</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/33c47a73fa15c3d9d67728172dd57d13/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">奇怪代码-c&#43;&#43;可变参数实现</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>