<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>DETR[sub-branch for Vit] - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="DETR[sub-branch for Vit]" />
<meta property="og:description" content="目录
End to End Object Detection with Transformers 基本思想
核心图
位置信息初始化query向量
论文模型细节架构图
然后就是预测头head部分了
这里有一个值得关注的地方
End to End Object Detection with Transformers 第一次把transformer用在object detection上面
Because of DETR,Transformer is getting hot again. people think transformer is just a backbone,backbone is not directly suitable for Downstream tasks.
基本思想 Vit like Bert ,just have the encoder part of the transformer,but Detr have both of encoder module and decoder module.
DETR没有套用Swin窗口分层的形式，它就是一个直接的transformer
First step,get patch work and positional embedding work ready(then every patch is a time vector),just like vit work（vit just use the encoder of the transformer）" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/f3dce4ee1cd366a65e32f766056df061/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-06T01:21:39+08:00" />
<meta property="article:modified_time" content="2022-04-06T01:21:39+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">DETR[sub-branch for Vit]</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="-toc" style="margin-left:0px;"></p> 
<p id="End%20to%20End%20Object%20Detection%20with%20Transformers%C2%A0-toc" style="margin-left:0px;"><a href="#End%20to%20End%20Object%20Detection%20with%20Transformers%C2%A0" rel="nofollow">End to End Object Detection with Transformers </a></p> 
<p id="%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3-toc" style="margin-left:0px;"><a href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3" rel="nofollow">基本思想</a></p> 
<p id="%E6%A0%B8%E5%BF%83%E5%9B%BE-toc" style="margin-left:0px;"><a href="#%E6%A0%B8%E5%BF%83%E5%9B%BE" rel="nofollow">核心图</a></p> 
<p id="%E4%BD%8D%E7%BD%AE%E4%BF%A1%E6%81%AF%E5%88%9D%E5%A7%8B%E5%8C%96query%E5%90%91%E9%87%8F-toc" style="margin-left:0px;"><a href="#%E4%BD%8D%E7%BD%AE%E4%BF%A1%E6%81%AF%E5%88%9D%E5%A7%8B%E5%8C%96query%E5%90%91%E9%87%8F" rel="nofollow">位置信息初始化query向量</a></p> 
<p id="%E8%AE%BA%E6%96%87%E6%A8%A1%E5%9E%8B%E7%BB%86%E8%8A%82%E6%9E%B6%E6%9E%84%E5%9B%BE-toc" style="margin-left:0px;"><a href="#%E8%AE%BA%E6%96%87%E6%A8%A1%E5%9E%8B%E7%BB%86%E8%8A%82%E6%9E%B6%E6%9E%84%E5%9B%BE" rel="nofollow">论文模型细节架构图</a></p> 
<p id="%C2%A0%E7%84%B6%E5%90%8E%E5%B0%B1%E6%98%AF%E9%A2%84%E6%B5%8B%E5%A4%B4head%E9%83%A8%E5%88%86%E4%BA%86-toc" style="margin-left:0px;"><a href="#%C2%A0%E7%84%B6%E5%90%8E%E5%B0%B1%E6%98%AF%E9%A2%84%E6%B5%8B%E5%A4%B4head%E9%83%A8%E5%88%86%E4%BA%86" rel="nofollow"> 然后就是预测头head部分了</a></p> 
<p id="%E8%BF%99%E9%87%8C%E6%9C%89%E4%B8%80%E4%B8%AA%E5%80%BC%E5%BE%97%E5%85%B3%E6%B3%A8%E7%9A%84%E5%9C%B0%E6%96%B9-toc" style="margin-left:40px;"><a href="#%E8%BF%99%E9%87%8C%E6%9C%89%E4%B8%80%E4%B8%AA%E5%80%BC%E5%BE%97%E5%85%B3%E6%B3%A8%E7%9A%84%E5%9C%B0%E6%96%B9" rel="nofollow">这里有一个值得关注的地方</a></p> 
<p id="-toc" style="margin-left:40px;"></p> 
<p id="-toc" style="margin-left:0px;"></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="End%20to%20End%20Object%20Detection%20with%20Transformers%C2%A0"><strong>End to End Object Detection with Transformers </strong></h2> 
<p><strong>第一次把transformer用在object detection上面</strong></p> 
<p></p> 
<p><img alt="" height="584" src="https://images2.imgbox.com/c6/76/39DrBPSr_o.png" width="987"></p> 
<p><strong><span style="background-color:#ed7976;">Because of DETR,Transformer is getting hot again. </span></strong></p> 
<p><strong><span style="background-color:#ed7976;">people think transformer is just a backbone,backbone is not directly suitable for Downstream tasks.</span></strong></p> 
<h2 id="%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3">基本思想</h2> 
<p><img alt="" height="610" src="https://images2.imgbox.com/6a/2b/MV3o2Ujv_o.png" width="1178"></p> 
<p>Vit like Bert ,just have the encoder part of the transformer,<strong>but Detr have both of encoder module and decoder module.</strong></p> 
<p>DETR没有套用Swin窗口分层的形式，它就是一个直接的transformer</p> 
<p>First step,get patch work and positional embedding work ready(then every patch is a time vector),just like vit work（vit just use the encoder of the transformer）</p> 
<p>那么问题来了，我们得到编码输出之后，怎么样能输出一个预测结果呢？</p> 
<p>是这样的，一张图像当中大家想象一下最多能有多少个物体呢？一般来说是几个或者十几个（常规来说），再多也不能100多个吧，所以说在这个DETR当中它设了一个限值（这个值可以根据自己任务需求来更改），如果没有特别需求按人家的限制就行。</p> 
<p class="img-center"><img alt="" height="120" src="https://images2.imgbox.com/a4/fa/wqChO5EX_o.png" width="204"></p> 
<p>encoder出来编排好的特征，我就预测100个坐标框。（什么意思呢）</p> 
<p><img alt="" height="222" src="https://images2.imgbox.com/bf/b9/Sp9RbJDX_o.png" width="1137"></p> 
<p></p> 
<p>这里两只鸭子就是我们的正样本，剩下的98个预测框都是非物体。</p> 
<p>之前的transformer咱们说过可以做机器翻译，因为transformer是（seq2seq+attention）</p> 
<p><img alt="" height="231" src="https://images2.imgbox.com/53/05/VZTYXk7i_o.png" width="322"></p> 
<p></p> 
<p>encoder把特征做好，decoder一个词一个词的翻译，decoder先预测第一个词是什么，基于第一个词去预测第二个词是什么，再基于第二个词预测第三个词，是一种串行结构，今天这个decoder的预测的100个坐标框是并行的不是串行的，同一时间一口气全部预测出来就完事儿了。</p> 
<p>今天咱们这个预测与之类似但又不完全相同，</p> 
<p><img alt="" height="253" src="https://images2.imgbox.com/d8/bd/PZrEREbK_o.png" width="653"></p> 
<p><strong><span style="background-color:#ed7976;">假设我们拿到100个向量 ，每一个向量都可以预测一个坐标框和分类的一个概率值，这不就是我们的检测吗，一个bounding-box回归和一个cls的分类。</span></strong></p> 
<p><strong><span style="background-color:#ed7976;">预测的时候把非物体再过滤掉，只框出找到的目标物体。</span></strong></p> 
<h2 id="%E6%A0%B8%E5%BF%83%E5%9B%BE">核心图</h2> 
<p><img alt="" height="500" src="https://images2.imgbox.com/52/17/VcjP0jps_o.png" width="1193"></p> 
<p></p> 
<p><strong><span style="background-color:#fbd4d0;">Backbone+p-e+encoder:</span></strong>Vit就多了一个token【cls】分类用的向量（vit也是模仿的bert），DETR就用不到cls了，只是用transformer的encoder做一个特征提取.<strong>//这部分除了没有cls几乎和vit一模一样</strong></p> 
<p class="img-center"><img alt="" height="170" src="https://images2.imgbox.com/53/ac/K1aLy50y_o.png" width="254"></p> 
<p><strong><span style="background-color:#fbd4d0;"> Decoder:  Decoder初始化会initialize 100 个向量，这100个向量要使用encoder</span></strong>生成出来的特征决定每个向量该怎么进行重构，所以transformer-decoder作用是学习这100个向量，这些向量学好了后，每一个向量对应的坐标框所对应的cls结果可以出来。</p> 
<h2 id="%E4%BD%8D%E7%BD%AE%E4%BF%A1%E6%81%AF%E5%88%9D%E5%A7%8B%E5%8C%96query%E5%90%91%E9%87%8F"><span style="background-color:#ed7976;">位置信息初始化query向量</span></h2> 
<p><span style="color:#ed7976;"><strong>所以DETR最核心的是什么？</strong></span><strong>Object queries，让它学会怎么从原始特征找到物体的位置。</strong></p> 
<p><strong>通俗的解释：</strong></p> 
<p><img alt="" height="394" src="https://images2.imgbox.com/6a/26/kGzUzTeL_o.png" width="999"></p> 
<p>相当于q1,q2，...qm都去问问k1v1,k2v2,...kn,vn这样下去</p> 
<p><strong>q1,q2是并行的还是串行的？</strong></p> 
<p><strong>这个与一般的机器翻译不一样的，一般nlp里面decoder一般用来生成对话，客服机器人，mt</strong></p> 
<p><strong>一般是先有第一个预测结果，然后其会被当成第二次的输入，依次类推···//传统nlp当中的处理方式</strong></p> 
<p class="img-center"><img alt="" height="164" src="https://images2.imgbox.com/b4/90/9tK0sDMY_o.png" width="258"></p> 
<p><strong> DETR就不需要这个架构了，把传统的decoder改了，把原来串联的结构改为了并联。</strong></p> 
<p><strong>并联的意思——（图中4个query都<span style="color:#ed7976;">一起去问</span>encoder那个是我的目标），这100个框不是说先有第一个第二个第三个，而是100个框一次全部出来，这是在Decoder当中做的比较有特点的地方。</strong></p> 
<p></p> 
<p><strong>decoder输出的向量连上FFN（就是全连接层），bbox就是预测4个值，Cls预测80个值就完事了（针对coco数据集）。所以核心是把decoder输出的向量（1，2，3，4）给做好，再用全连接层输出来最终我想要的实际位置以及其对应的类别。</strong></p> 
<p><strong>annotation:</strong>Feed Forward Network(<em>FFN</em>,前馈神经网络)</p> 
<p><strong>提个问题，咱们前面不用transformer作为encoder行不行，直接连个cnn行不行？</strong></p> 
<p class="img-center"><img alt="" height="169" src="https://images2.imgbox.com/3e/ec/xzXPJrXP_o.png" width="319"></p> 
<p> 拿一个cnn做一个特征提取不也行吗前面为什么非要用transformer作为encoder？</p> 
<p>看看论文怎么评价的，</p> 
<p><img alt="" height="346" src="https://images2.imgbox.com/af/b4/NiHzLDu4_o.png" width="714"></p> 
<p> 比如告诉解码器，物体在那个地方，红色是物体，蓝色就是背景，凑合看看就行了。</p> 
<p class="img-center"><img alt="" height="155" src="https://images2.imgbox.com/d8/41/9inCUnxX_o.png" width="135"></p> 
<p> 论文当中反复强调一件事情，transformer比cnn好的一个地方在于能让我们的模型更知道我们关注的物体它所在的一个趋势在哪，并且呢遮挡现象也没问题。</p> 
<p>这里decoder框出四头牛，剩下97个框重在参与。</p> 
<h2 id="%E8%AE%BA%E6%96%87%E6%A8%A1%E5%9E%8B%E7%BB%86%E8%8A%82%E6%9E%B6%E6%9E%84%E5%9B%BE"><span style="background-color:#ed7976;">论文模型细节架构图</span></h2> 
<p><strong><span style="background-color:#fbd4d0;">网友（李teacher）自己的分析：未证明，比较通俗的理解（也算一种观点了）</span></strong></p> 
<p><img alt="" height="704" src="https://images2.imgbox.com/4f/63/7Qo3wiEy_o.png" width="1200"></p> 
<p class="img-center"><img alt="" height="589" src="https://images2.imgbox.com/64/37/SwRcGuHz_o.png" width="539"></p> 
<p>   首先蓝色方框框出来的这块没有用到Encoder的信息，自己在玩儿（Multi-Head Self-attention），（以下李teacher的观点，非严密证明）</p> 
<p class="img-center"><img alt="" height="292" src="https://images2.imgbox.com/dc/5f/KZpToN2l_o.png" width="269"></p> 
<p>(q1,k1,v1),(q2,k2,v2)，...,（q100,k100,v100）<strong><span style="background-color:#ed7976;">第一步他们自己先做一个多头自注意力机制，先分好初始化向量的底盘</span></strong>， 咱一会儿别重了，其工作意义是把100个初始向量query分一分（这里query,value,key都一样），分别把100个query整合一下，把初始向量准备好。</p> 
<p class="img-center"><img alt="" height="401" src="https://images2.imgbox.com/46/45/22ICCIGm_o.png" width="550"></p> 
<p> </p> 
<p style="text-align:center;">第二步， 多头注意力（不是自注意力），没有Self什么意思呢，由encoder提供（k,v）pair特征,由第一步做完multi-head self-attention 整合的输出作为q，通过这个q去做运算，基于算出的关系（相似度）来重构query，使咱们q做的更好一些，所以整个生命周期都在为这个Query向量服务（一开始 是100个随机初始向量），最后把重构的100个query通过两个FFN去输出Class与Bounding Box</p> 
<p style="text-align:center;"></p> 
<p></p> 
<p>传统的transformer-decoder里面是有mask机制的，17年的《Attention is all you need》，也就是第一个向量要去做attention的时候，后面的向量都要被mask掉，因为机器翻译预测部分你是不知道后面的东西的（虽然attention是能获得全局的，但这样有作弊嫌疑，这样学习到的东西可能适合训练集的预测，在测试集上面表现就可能不好了），而DETR里面就是不需要MASK的，直接往前跑（以前transformer做文本比较多，所以需要Mask机制）</p> 
<p class="img-center"><img alt="" height="142" src="https://images2.imgbox.com/6e/cf/kLFbQ7eG_o.png" width="158"></p> 
<h2 id="%C2%A0%E7%84%B6%E5%90%8E%E5%B0%B1%E6%98%AF%E9%A2%84%E6%B5%8B%E5%A4%B4head%E9%83%A8%E5%88%86%E4%BA%86"><span style="background-color:#ed7976;"> 然后就是预测头head部分了</span></h2> 
<h2 id="%E2%80%8B"><img alt="" height="697" src="https://images2.imgbox.com/34/57/5KZ8zt1c_o.png" width="1070"></h2> 
<p>在17年deepsort论文里面做目标跟踪，这里面需要做匹配，因为你要看下一帧目标和上一帧目标是不是一个目标，所以有匹配这个东西。</p> 
<p><a href="https://www.bilibili.com/video/BV16K4y1X7Ph?spm_id_from=333.999.0.0" rel="nofollow" title="14-4: 匈牙利算法 Hungarian Algorithm_哔哩哔哩_bilibili">14-4: 匈牙利算法 Hungarian Algorithm_哔哩哔哩_bilibili</a></p> 
<p><img alt="" height="34" src="https://images2.imgbox.com/a1/73/CEVvxkxy_o.png" width="520"></p> 
<p>100个里面选那两个作为最后的预测框呢？ 选两个能让Ground-Truth与其之间的损失最小，这个损失可以是iou损失,bounding box，cls损失，可以是多个损失融合在一起，让我们的loss最小就完事儿了。<strong>目标追踪算法基本都会匈牙利匹配算法</strong></p> 
<h3 id="%E8%BF%99%E9%87%8C%E6%9C%89%E4%B8%80%E4%B8%AA%E5%80%BC%E5%BE%97%E5%85%B3%E6%B3%A8%E7%9A%84%E5%9C%B0%E6%96%B9">这里有一个值得关注的地方</h3> 
<p><img alt="" height="539" src="https://images2.imgbox.com/18/1e/1LuTfufE_o.png" width="1042"></p> 
<h3></h3> 
<p></p> 
<p> </p> 
<p> </p> 
<p></p> 
<h2></h2> 
<p></p> 
<p></p> 
<p></p> 
<p><span style="color:#ed7976;"><strong>Reference//视频看第一个和第二个就可以了，第一个感性认识，第二个实打实的理性认识</strong></span></p> 
<p><a href="https://www.bilibili.com/video/BV1eL411V7TP?spm_id_from=333.337.search-card.all.click" rel="nofollow" title="干货！2022讲得最清晰的【Transformer核心项目DETR目标检测训练】整体网络架构与注意力机制的作用方法分析解读！transformer模型/深度学习_哔哩哔哩_bilibili">干货！2022讲得最清晰的【Transformer核心项目DETR目标检测训练】整体网络架构与注意力机制的作用方法分析解读！transformer模型/深度学习_哔哩哔哩_bilibili</a></p> 
<p><a href="https://www.bilibili.com/video/BV1T64y1D7NY?spm_id_from=333.337.search-card.all.click" rel="nofollow" title="[论文简析]DETR: End-to-End Object Detection with Transfromers[2005.12872]_哔哩哔哩_bilibili">[论文简析]DETR: End-to-End Object Detection with Transfromers[2005.12872]_哔哩哔哩_bilibili</a></p> 
<p><a href="https://www.bilibili.com/video/BV1Gt4y197hn/?spm_id_from=333.788.recommend_more_video.3" rel="nofollow" title="【CODE】Facebook 最新DETR（基于Transformer）目标检测算法实战_哔哩哔哩_bilibili">【CODE】Facebook 最新DETR（基于Transformer）目标检测算法实战_哔哩哔哩_bilibili</a></p> 
<p><a href="https://www.cnblogs.com/BlairGrowing/p/15023137.html" rel="nofollow" title="网络前置任务（Pretext task）和下游任务（downstream tasks） - Learner- - 博客园">网络前置任务（Pretext task）和下游任务（downstream tasks） - Learner- - 博客园</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/146065711" rel="nofollow" title="【论文笔记】从Transformer到DETR - 知乎">【论文笔记】从Transformer到DETR - 知乎</a></p> 
<p><a href="https://www.bilibili.com/video/av506204429" rel="nofollow" title="vision transformer(detr,vit,deit,swin_哔哩哔哩_bilibili">vision transformer(detr,vit,deit,swin_哔哩哔哩_bilibili</a> //This dude have a good summary of those models</p> 
<p><a href="https://blog.csdn.net/qq_41917697/article/details/123020245" title="目标检测之DETR_球场书生的博客-CSDN博客">目标检测之DETR_球场书生的博客-CSDN博客</a> //More points of view</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8342d3fb2e087fbe9ded8188eb923314/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">CTF-show-爆破</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b79a7e7963659d33c3826286091a9d11/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【eMMC】简介与协议浅析</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>