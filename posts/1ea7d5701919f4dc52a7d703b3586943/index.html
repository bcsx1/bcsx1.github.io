<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>YOLO系列详解：YOLOv1、YOLOv2、YOLOv3、YOLOv4、YOLOv5、YOLOv6、YOLOv7 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="YOLO系列详解：YOLOv1、YOLOv2、YOLOv3、YOLOv4、YOLOv5、YOLOv6、YOLOv7" />
<meta property="og:description" content="一、前言 YOLO系列是one-stage且是基于深度学习的回归方法，而R-CNN、Fast-RCNN、Faster-RCNN等是two-stage且是基于深度学习的分类方法。
YOLO官网：GitHub - pjreddie/darknet: Convolutional Neural Networks
1.1 YOLO vs Faster R-CNN 1、统一网络：YOLO没有显示求取region proposal的过程。Faster R-CNN中尽管RPN与fast rcnn共享卷积层，但是在模型训练过程中，需要反复训练RPN网络和fast rcnn网络。相对于R-CNN系列的&#34;看两眼&#34;(候选框提取与分类)，YOLO只需要Look Once.
2、YOLO统一为一个回归问题，而Faster R-CNN将检测结果分为两部分求解：物体类别（分类问题）、物体位置即bounding box（回归问题）。
二、YOLOv1 论文地址：https://arxiv.org/abs/1506.02640
官方代码：GitHub - pjreddie/darknet: Convolutional Neural Networks
YOLOv1的核心思想：
YOLOv1的核心思想就是利用整张图作为网络的输入，直接在输出层回归bounding box的位置和bounding box所属的类别。Faster RCNN中也直接用整张图作为输入，但是Faster-RCNN整体还是采用了RCNN那种 proposal&#43;classifier的思想，只不过是将提取proposal的步骤放在CNN中实现了,而YOLOv1则采用直接回归的思路。 2.1、实现方法 将一幅图像分成SxS个网格(grid cell)，如果某个object的中心落在这个网格中，则这个网格就负责预测这个object。 每个网格要预测B个bounding box，每个bounding box除了要回归自身的位置之外，还要附带预测一个confidence值。这个confidence代表了所预测的box中含有object的置信度和这个box预测的有多准两重信息，其值是这样计算的： 该表达式含义：如果有object落在一个grid cell里，则第一项取1，否则取0。 第二项是预测的bounding box和实际的groundtruth之间的IoU值。 每个bounding box要预测(x, y, w, h)和confidence共5个值，每个网格还要预测一个类别信息，记为C类。则SxS个网格，每个网格要预测B个bounding box还要预测C个categories。输出就是S x S x (5*B&#43;C)的一个tensor。 注意：class信息是针对每个网格的，confidence信息是针对每个bounding box的。
举例说明： 在PASCAL VOC中，图像输入为448x448像素，取S=7，B=2，一共有20个类别(C=20)。则输出就是7x7x(2x5&#43;20)的一个tensor。整个网络结构如下图所示：
在test的时候，每个网格预测的class信息和bounding box预测的confidence信息相乘，就得到每个bounding box的class-specific confidence score，得到每个box的class-specific confidence score以后，设置阈值，滤掉得分低的boxes，对保留的boxes进行NMS处理，就得到最终的检测结果。 该表达式含义：等式左边第一项就是每个网格预测的类别信息，第二三项就是每个bounding box预测的confidence。这个乘积即encode了预测的box属于某一类的概率，也有该box准确度的信息。 注意： 由于输出层为全连接层，因此在检测时，YOLOv1模型的输入只支持与训练图像相同的输入分辨率。虽然每个格子可以预测B个bounding box，但是最终只选择IOU最高的bounding box作为物体检测输出，即每个格子最多只预测出一个物体。当物体占画面比例较小，如图像中包含畜群或鸟群时，每个格子包含多个物体，但却只能检测出其中一个。 简单的概括就是：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/1ea7d5701919f4dc52a7d703b3586943/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-02-22T09:28:02+08:00" />
<meta property="article:modified_time" content="2023-02-22T09:28:02+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">YOLO系列详解：YOLOv1、YOLOv2、YOLOv3、YOLOv4、YOLOv5、YOLOv6、YOLOv7</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>一、前言</h2> 
<p>YOLO系列是one-stage且是基于深度学习的回归方法，而R-CNN、Fast-RCNN、Faster-RCNN等是two-stage且是基于深度学习的分类方法。</p> 
<p>YOLO官网：<a href="https://github.com/pjreddie/darknet" title="GitHub - pjreddie/darknet: Convolutional Neural Networks">GitHub - pjreddie/darknet: Convolutional Neural Networks</a></p> 
<h3>1.1 YOLO vs Faster R-CNN</h3> 
<p>1、统一网络：YOLO没有显示求取region proposal的过程。Faster R-CNN中尽管RPN与fast rcnn共享卷积层，但是在模型训练过程中，需要反复训练RPN网络和fast rcnn网络。相对于R-CNN系列的"看两眼"(候选框提取与分类)，YOLO只需要Look Once.</p> 
<p>2、YOLO统一为一个回归问题，而Faster R-CNN将检测结果分为两部分求解：物体类别（分类问题）、物体位置即bounding box（回归问题）。</p> 
<h3>二、YOLOv1</h3> 
<p>论文地址：<a href="https://arxiv.org/abs/1506.02640" rel="nofollow" title="https://arxiv.org/abs/1506.02640">https://arxiv.org/abs/1506.02640</a></p> 
<p>官方代码：<a href="https://github.com/pjreddie/darknet" title="GitHub - pjreddie/darknet: Convolutional Neural Networks">GitHub - pjreddie/darknet: Convolutional Neural Networks</a></p> 
<p>YOLOv1的核心思想：</p> 
<ul><li>YOLOv1的核心思想就是利用整张图作为网络的输入，直接在输出层回归bounding box的位置和bounding box所属的类别。</li><li>Faster RCNN中也直接用整张图作为输入，但是Faster-RCNN整体还是采用了RCNN那种 proposal+classifier的思想，只不过是将提取proposal的步骤放在CNN中实现了,而YOLOv1则采用直接回归的思路。</li></ul> 
<h4>2.1、实现方法</h4> 
<ul><li>将一幅图像分成SxS个网格(grid cell)，如果某个object的中心落在这个网格中，则这个网格就负责预测这个object。</li></ul> 
<p style="text-align:center;"><img alt="" height="561" src="https://images2.imgbox.com/d9/df/EajK6O5H_o.png" width="896"></p> 
<ul><li>每个网格要预测B个bounding box，每个bounding box除了要回归自身的位置之外，还要附带预测一个confidence值。这个confidence代表了所预测的box中含有object的置信度和这个box预测的有多准两重信息，其值是这样计算的：</li></ul> 
<p style="text-align:center;"><img alt="" height="95" src="https://images2.imgbox.com/dc/39/fGFLuBe5_o.png" width="433"></p> 
<p>        该表达式含义：如果有object落在一个grid cell里，则第一项取1，否则取0。 第二项是预测的bounding box和实际的groundtruth之间的IoU值。 </p> 
<ul><li>每个bounding box要预测(x, y, w, h)和confidence共5个值，每个网格还要预测一个类别信息，记为C类。则SxS个网格，每个网格要预测B个bounding box还要预测C个categories。输出就是S x S x (5*B+C)的一个tensor。</li></ul> 
<p><strong>       注意：class信息是针对每个网格的，confidence信息是针对每个bounding box的。</strong></p> 
<h4>举例说明：</h4> 
<p>在PASCAL VOC中，图像输入为448x448像素，取S=7，B=2，一共有20个类别(C=20)。则输出就是7x7x(2x5+20)的一个tensor。整个网络结构如下图所示：</p> 
<p style="text-align:center;"><img alt="" height="579" src="https://images2.imgbox.com/30/22/kxh3ICEJ_o.png" width="1200"></p> 
<ul><li>在test的时候，每个网格预测的class信息和bounding box预测的confidence信息相乘，就得到每个bounding box的class-specific confidence score，得到每个box的class-specific confidence score以后，设置阈值，滤掉得分低的boxes，对保留的boxes进行NMS处理，就得到最终的检测结果。</li></ul> 
<p><img alt="" height="114" src="https://images2.imgbox.com/ec/ff/32pmgWeY_o.png" width="1200"></p> 
<p>       该表达式含义：等式左边第一项就是每个网格预测的类别信息，第二三项就是每个bounding box预测的confidence。这个乘积即encode了预测的box属于某一类的概率，也有该box准确度的信息。 </p> 
<h4><strong>注意：</strong></h4> 
<ol><li><strong>由于输出层为全连接层，因此在检测时，YOLOv1模型的输入只支持与训练图像相同的输入分辨率。</strong></li><li><strong>虽然每个格子可以预测B个bounding box，但是最终只选择IOU最高的bounding box作为物体检测输出，即每个格子最多只预测出一个物体。当物体占画面比例较小，如图像中包含畜群或鸟群时，每个格子包含多个物体，但却只能检测出其中一个。</strong></li></ol> 
<p><strong>简单的概括就是：</strong></p> 
<ul><li>给个一个输入图像，首先将图像划分成7*7的网格；</li><li>对于每个网格，我们都预测2个边框（包括每个边框是目标的置信度以及每个边框区域在多个类别上的概率）；</li><li>根据上一步可以预测出7*7*2个目标窗口，然后根据阈值去除可能性比较低的目标窗口，最后NMS去除冗余窗口即可。</li></ul> 
<h3>2.2、损失函数</h3> 
<p>每个grid有30维，这30维中，8维是回归box的坐标，2维是box的confidence，还有20维是类别。其中坐标的x,y用对应网格的offset归一化到0-1之间，w,h用图像的width和height归一化到0-1之间。在实现中，最主要的就是怎么设计损失函数，让这个三个方面得到很好的平衡。作者简单粗暴的全部采用了<strong>sum-squared error loss</strong>来做这件事。</p> 
<p><strong>这种做法存在以下几个问题：</strong></p> 
<ul><li>第一，8维的localization error和20维的classification error同等重要显然是不合理的；</li><li>第二，如果一个网格中没有object（一幅图中这种网格很多），那么就会将这些网格中的box的confidence push到0，相比于较少的有object的网格，这种做法是overpowering的，这会导致网络不稳定甚至发散。</li></ul> 
<p><strong>解决方法</strong>：</p> 
<ul><li>更重视8维的坐标预测，给这些损失前面赋予更大的loss weight；</li><li>对没有object的box的confidence loss，赋予小的loss weight；</li><li>有object的box的confidence loss和类别的loss的loss weight正常取1。</li></ul> 
<p>YOLOv1在对不同大小的box预测中，相比于大box预测偏一点，小box预测偏一点肯定更不能被忍受的。而<strong>sum-square error loss</strong>中对同样的偏移loss是一样。为了缓和这个问题，作者用了一个比较取巧的办法，就是将box的width和height取平方根代替原本的height和width。这个参考下面的图很容易理解，小box的横轴值较小，发生偏移时，反应到y轴上相比大box要大。(也是个近似逼近方式)</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/3c/8a/xvEzT1Ve_o.png"></p> 
<p>一个网格预测多个box，希望每个box predictor专门负责预测某个object。具体做法就是看当前预测的box与ground truth box中哪个IoU大，就负责哪个。这种做法称作box predictor的specialization。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/d4/79/JZJXec83_o.png"></p> 
<p>在YOLOv1的损失函数中：</p> 
<ul><li>只有当某个网格中有object的时候才对classification error进行惩罚。</li><li>只有当某个box predictor对某个ground truth box负责的时候，才会对box的coordinate error进行惩罚，而对哪个ground truth box负责就看其预测值和ground truth box的IoU是不是在那个cell的所有box中最大。</li></ul> 
<p><strong>注：</strong></p> 
<ul><li>YOLOv1方法模型训练依赖于物体识别标注数据，因此，对于非常规的物体形状或比例，YOLOv1的检测效果并不理想。</li><li>YOLOv1采用了多个下采样层，网络学到的物体特征并不精细，因此也会影响检测效果。</li><li>YOLOv1的loss函数中，大物体IOU误差和小物体IOU误差对网络训练中loss贡献值接近（虽然采用求平方根方式，但没有根本解决问题）。因此，对于小物体，小的IOU误差也会对网络优化过程造成很大的影响，从而降低了物体检测的定位准确性。</li></ul> 
<h4>YOLO的缺点</h4> 
<ul><li>YOLO对相互靠的很近的物体和很小的群体检测效果不好，这是因为一个网格中只预测了两个框，并且只属于一类；</li><li>同一类物体出现的新的不常见的长宽比和其他情况时，泛化能力偏弱；</li><li>由于损失函数的问题，定位误差是影响检测效果的主要原因。尤其是大小物体的处理上，还有待加强。</li></ul> 
<h3>三、YOLOv2</h3> 
<p>论文地址：<a href="https://arxiv.org/abs/1612.08242" rel="nofollow" title="https://arxiv.org/abs/1612.08242">https://arxiv.org/abs/1612.08242</a></p> 
<p>官方代码：<a href="http://pjreddie.com/darknet/yolo/" rel="nofollow" title="YOLO: Real-Time Object Detection">YOLO: Real-Time Object Detection</a></p> 
<h4>3.1 YOLOv2介绍</h4> 
<p>YOLOv2相对v1版本，在继续保持处理速度的基础上，从预测<strong>更准确（Better）</strong>，<strong>速度更快（Faster）</strong>，<strong>识别对象更多（Stronger）</strong>这三个方面进行了改进。其中识别更多对象也就是扩展到能够检测9000种不同对象，称之为<strong>YOLO9000</strong>。</p> 
<p>文章提出了一种新的<strong>训练方法–联合训练算法</strong>，这种算法可以把这两种的数据集混合到一起。使用一种分层的观点对物体进行分类，用巨量的<strong>分类数据集数据来扩充检测数据集</strong>，从而把两种不同的数据集混合起来。联合训练算法的基本思路就是：同时在检测数据集和分类数据集上训练物体检测器（Object Detectors ），<strong>用检测数据集的数据学习物体的准确位置，用分类数据集的数据来增加分类的类别量、提升健壮性。</strong></p> 
<p>YOLO9000就是使用联合训练算法训练出来的，他拥有9000类的分类信息，这些分类信息学习自ImageNet分类数据集，而物体位置检测则学习自COCO检测数据集。</p> 
<p>YOLOv1有很多缺点，作者希望改进的方向是:<strong>改善recall，提升定位的准确度</strong>，同时保持分类的准确度。目前计算机视觉的趋势是更大更深的网络，更好的性能表现通常依赖于训练更大的网络或者把多种model综合到一起，但是YOLOv2则着力于简化网络。具体的改进见下表：</p> 
<p style="text-align:center;"><img alt="" height="454" src="https://images2.imgbox.com/45/44/2d12ZCrb_o.png" width="1078"></p> 
<h3>3.2 YOLOv2的改进点</h3> 
<h4><strong>3.2.1 Batch Normalization</strong></h4> 
<p>批量归一化有助于解决反向传播过程中的梯度消失和梯度爆炸问题，降低对一些超参数（比如学习率、网络参数的大小范围、激活函数的选择）的敏感性，并且每个batch分别进行归一化的时候，起到了一定的正则化效果（YOLOv2不再使用dropout），从而能够获得更好的收敛速度和收敛效果。</p> 
<p>使用Batch Normalization对网络进行优化，让网络提高了收敛性，同时还消除了对其他形式的正则化（regularization）的依赖。通过对YOLOv2的每一个卷积层增加Batch Normalization，最终使得mAP提高了2%，同时还使model正则化。使用Batch Normalization可以从model中去掉Dropout，而不会产生过拟合。</p> 
<p>关于批规一化的更多信息可以参考：<a href="https://zhuanlan.zhihu.com/p/34879333" rel="nofollow" title="Batch Normalization原理与实战">Batch Normalization原理与实战</a></p> 
<h4><strong>3.2.2 High resolution classifier</strong></h4> 
<p>用于图像分类的训练样本很多，而标注了边框的用于训练目标检测的样本相比而言就少了很多，因为标注边框的人工成本比较高。所以目标检测模型通常都先用图像分类样本训练卷积层，提取图像特征，但这引出另一个问题，就是图像分类样本的分辨率不是很高。所以YOLOv1使用ImageNet的图像分类样本采用 224*224 作为输入，来训练CNN卷积层。然后在训练目标检测时，检测用的图像样本采用更高分辨率的 448*448 像素图像作为输入，但这样不一致的输入分辨率肯定会对模型性能有一定影响。</p> 
<p><strong>所以YOLOv2在采用 224*224 图像进行分类模型预训练后，再采用 448*448 高分辨率样本对分类模型进行微调（10个epoch）</strong>，使网络特征逐渐适应 448*448 的分辨率。然后再使用 448*448 的检测样本进行训练，缓解了分辨率突然切换造成的影响，最终通过使用高分辨率，mAP提升了4%。</p> 
<h4><strong>3.2.3 Convolution with anchor boxes</strong></h4> 
<p>YOLOv1包含有全连接层，从而能直接预测Bounding Boxes的坐标值。Faster R-CNN算法只用卷积层与Region Proposal Network来预测Anchor Box的偏移值与置信度，而不是直接预测坐标值，YOLOv2作者发现通过预测偏移量而不是坐标值能够简化问题，让神经网络学习起来更容易。</p> 
<p>借鉴Faster RCNN的做法，<strong>YOLOv2也尝试采用先验框（anchor）。在每个grid预先设定一组不同大小和宽高比的边框，来覆盖整个图像的不同位置和多种尺度</strong>，这些先验框作为预定义的候选区在神经网络中将检测其中是否存在对象，以及微调边框的位置。</p> 
<p>之前YOLOv1并没有采用先验框，并且每个grid只预测两个bounding box，也就是整个图像只有98个bounding box。YOLOv2如果每个grid采用9个先验框，总共有13*13*9=1521个先验框。所以最终YOLOv2去掉了全连接层，使用Anchor Boxes来预测 Bounding Boxes。作者去掉了网络中一个Pooling层，这让卷积层的输出能有更高的分辨率，同时对网络结构进行收缩让其运行在416*416而不是448*448。</p> 
<p>由于图片中的物体都倾向于出现在图片的中心位置，特别是那种比较大的物体，所以有一个单独位于物体中心的位置用于预测这些物体。YOLOv2的卷积层采用32这个值来下采样图片，所以通过选择416*416用作输入尺寸最终能输出一个13*13的Feature Map。使用Anchor Box会让精确度稍微下降，但用了它能让YOLOv2能预测出大于一千个框，同时recall达到88%，mAP达到69.2%。</p> 
<h4><strong>3.2.4 Dimension clusters</strong></h4> 
<p>之前Anchor Box的尺寸是手动选择的，所以尺寸还有优化的余地。YOLOv2尝试统计出更符合样本中对象尺寸的先验框，这样就可以减少网络微调先验框到实际位置的难度。YOLOv2的做法是对训练集中标注的边框进行K-means聚类分析，以寻找尽可能匹配样本的边框尺寸。如果我们用标准的欧式距离的k-means，尺寸大的框比小框产生更多的错误。因为我们的目的是提高IOU分数，这依赖于Box的大小，所以距离度量的使用：</p> 
<p style="text-align:center;"><img alt="" height="73" src="https://images2.imgbox.com/89/a4/JMawwilo_o.png" width="545"></p> 
<p> 其中，centroid是聚类时被选作中心的边框，box就是其它边框，d就是两者间的“距离”，IOU越大，“距离”越近。YOLOv2给出的聚类分析结果如下图所示，通过分析实验结果（Figure 2），在model复杂性与high recall之间权衡之后，选择聚类分类数K=5。</p> 
<p style="text-align:center;"><img alt="" height="663" src="https://images2.imgbox.com/e1/e8/RBnUV6ci_o.png" width="718"></p> 
<p>Table1是说明用K-means选择Anchor Boxes时，当Cluster IOU选择值为5时，AVG IOU的值是61，这个值要比不用聚类的方法的60.9要高。选择值为9的时候，AVG IOU更有显著提高。总之就是说明用聚类的方法是有效果的。</p> 
<p style="text-align:center;"><img alt="" height="209" src="https://images2.imgbox.com/c5/bf/tUEKa0P1_o.png" width="490"></p> 
<h4>3.2.5 Direct location prediction </h4> 
<p>用Anchor Box的方法，会让model变得不稳定，尤其是在最开始几次迭代的时候。大多数不稳定因素产生自预测Box的(x,y)位置的时候。按照之前YOLOv1的方法，网络不会预测偏移量，而是根据YOLOv1中的网格单元的位置来直接预测坐标，这就让Ground Truth的值介于0到1之间。而为了让网络的结果能落在这一范围内，网络使用一个 Logistic Activation来对于网络预测结果进行限制，让结果介于0到1之间。 网络在每一个网格单元中预测出5个Bounding Boxes，每个Bounding Boxes有五个坐标值tx，ty，tw，th，t0，他们的关系见下图。假设一个网格单元对于图片左上角的偏移量是cx，cy，Bounding Boxes Prior的宽度和高度是pw，ph，那么预测的结果见下图右面的公式：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/cf/0d/FaRld0cA_o.jpg"></p> 
<h4><strong>3.2.6 Fine-Grained Features</strong> </h4> 
<p>目标检测面临的一个问题是图像中的需要检测的目标会有大有小，输入图像经过多层网络提取特征，最后输出的特征图中（比如YOLOv2中输入416*416经过卷积网络下采样最后输出是13*13），较小的对象可能特征已经不明显甚至被忽略掉了。为了更好的检测出一些比较小的对象，最后输出的特征图需要保留一些更细节的信息。于是YOLOv2引入一种称为passthrough层的方法在特征图中保留一些细节信息。具体来说，就是在最后一个pooling之前，特征图的大小是26*26*512，将其1拆4，直接传递（passthrough）到pooling后（并且又经过一组卷积）的特征图，两者叠加到一起作为输出的特征图。</p> 
<p style="text-align:center;"><img alt="" height="645" src="https://images2.imgbox.com/a5/98/RScLM2AF_o.png" width="1077"></p> 
<p> 具体怎样将1个特征图拆成4个特征图，见下图，图中示例的是1个4*4拆成4个2*2，因为深度不变，所以没有画出来。</p> 
<p style="text-align:center;"><img alt="" height="239" src="https://images2.imgbox.com/37/44/dt8MaoBH_o.png" width="1078"></p> 
<h4><strong>3.2.7 Multi-ScaleTraining</strong></h4> 
<p>作者希望YOLOv2能健壮的运行于不同尺寸的图片之上，所以把这一想法用于训练model中。区别于之前的补全图片的尺寸的方法，YOLOv2每迭代几次都会改变网络参数。每10个Batch，网络会随机地选择一个新的图片尺寸，由于使用了下采样参数是32，所以不同的尺寸大小也选择为32的倍数{320，352…..608}，最小320*320，最大608*608，网络会自动改变尺寸，并继续训练的过程。这一政策让网络在不同的输入尺寸上都能达到一个很好的预测效果，同一网络能在不同分辨率上进行检测。当输入图片尺寸比较小的时候跑的比较快，输入图片尺寸比较大的时候精度高，所以你可以在YOLOv2的速度和精度上进行权衡。</p> 
<p><img alt="" height="452" src="https://images2.imgbox.com/be/45/Y0IGFELd_o.png" width="536"><img alt="" height="348" src="https://images2.imgbox.com/b5/21/0TvlfwLu_o.png" width="556"></p> 
<h3>3.3 YOLOv2 Faster</h3> 
<p>YOLOv1的backbone使用的是GoogleLeNet，速度比VGG-16快，YOLOv1完成一次前向过程只用8.52 billion 运算，而VGG-16要30.69billion，但是YOLOv1精度稍低于VGG-16。</p> 
<h4>3.3.1 Draknet19</h4> 
<p>YOLOv2基于一个新的分类model，有点类似与VGG。YOLOv2使用3*3filter，每次Pooling之后都增加一倍Channels的数量。YOLOv2使用Global Average Pooling，使用Batch Normilazation来让训练更稳定，加速收敛，使model规范化。最终的model–Darknet19，有19个卷积层和5个maxpooling层，处理一张图片只需要5.58 billion次运算，在ImageNet上达到72.9%top-1精确度，91.2%top-5精确度。</p> 
<p style="text-align:center;"><img alt="" height="643" src="https://images2.imgbox.com/60/93/pd9rV0wZ_o.png" width="519"></p> 
<p></p> 
<h4>3.3.2 Training for classiﬁcation</h4> 
<p>网络训练在 ImageNet 1000类分类数据集上训练了160epochs，使用随机梯度下降，初始学习率为0.1， polynomial rate decay with a power of 4, weight decay of 0.0005 and momentum of 0.9 。训练期间使用标准的数据扩大方法：随机裁剪、旋转、变换颜色（hue）、变换饱和度（saturation）， 变换曝光度（exposure shifts）。在训练时，把整个网络在更大的448*448分辨率上Fine Turnning 10个 epoches，初始学习率设置为0.001，这种网络达到达到76.5%top-1精确度，93.3%top-5精确度。</p> 
<h4>3.3.3 Training for detection</h4> 
<p>网络去掉了最后一个卷积层，而加上了三个3*3卷积层，每个卷积层有1024个Filters，每个卷积层紧接着一个1*1卷积层。对于VOC数据，网络预测出每个网格单元预测五个Bounding Boxes，每个Bounding Boxes预测5个坐标和20类，所以一共125个Filters，增加了Passthough层来获取前面层的细粒度信息，网络训练了160epoches，初始学习率0.001，数据扩大方法相同，对COCO与VOC数据集的训练对策相同。</p> 
<h3>四、YOLOv3</h3> 
<p>论文地址：https://pjreddie.com/media/files/papers/YOLOv3.pdf </p> 
<p style="text-align:center;"><img alt="" height="610" src="https://images2.imgbox.com/05/d9/RJWKcJCH_o.png" width="764"></p> 
<h3>网络结构</h3> 
<p>从<a href="https://blog.csdn.net/leviopku/article/details/82660381" title="这里">这里</a>盗了张图，这张图很好的总结了YOLOv3的结构，让我们对YOLO有更加直观的理解。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/11/6d/6M6oaWxL_o.png"></p> 
<p><strong>DBL: </strong>代码中的Darknetconv2d_BN_Leaky，是YOLOv3的基本组件，就是卷积+BN+Leaky relu。<br><strong>resn：</strong>n代表数字，有res1，res2, … ,res8等等，表示这个res_block里含有多少个res_unit。不懂resnet请戳<a href="https://blog.csdn.net/litt1e/article/details/87644012" title="这儿">这儿</a><br><strong>concat：</strong>张量拼接；将darknet中间层和后面的某一层的上采样进行拼接。拼接的操作和残差层add的操作是不一样的，拼接会扩充张量的维度，而add只是直接相加不会导致张量维度的改变。</p> 
<h4><strong>Backbone：darknet-53</strong></h4> 
<p>为了达到更好的分类效果，作者自己设计训练了darknet-53，在ImageNet数据集上实验发现这个darknet-53，的确很强，相对于ResNet-152和ResNet-101，darknet-53不仅在分类精度上差不多，计算速度还比ResNet-152和ResNet-101强多了，网络层数也比他们少，测试结果如图所示。</p> 
<p style="text-align:center;"><img alt="" height="190" src="https://images2.imgbox.com/62/0b/CmfXIPPe_o.png" width="745"></p> 
<p>darknet-53的网络结构如下图所示。YOLOv3使用了darknet-53的前面的52层（没有全连接层），YOLOv3这个网络是一个全卷积网络，大量使用残差的跳层连接，并且为了降低池化带来的梯度负面效果，作者直接摒弃了POOLing，用conv的stride来实现降采样。在这个网络结构中，使用的是步长为2的卷积来进行降采样。</p> 
<p>为了加强算法对小目标检测的精确度，YOLOv3中采用类似FPN的upsample和融合做法（最后融合了3个scale，其他两个scale的大小分别是26×26和52×52），在多个scale的feature map上做检测。</p> 
<p>作者在3条预测支路采用的也是全卷积的结构，其中最后一个卷积层的卷积核个数是255，是针对COCO数据集的80类：3*(80+4+1)=255，3表示一个grid cell包含3个bounding box，4表示框的4个坐标信息，1表示objectness score。</p> 
<p style="text-align:center;"><img alt="" height="696" src="https://images2.imgbox.com/da/f7/38u1zxV4_o.png" width="606"></p> 
<h4>output </h4> 
<p>所谓的多尺度就是来自这3条预测之路，y1,y2和y3的深度都是255，边长的规律是13:26:52。YOLOv3设定的是每个网格单元预测3个box，所以每个box需要有(x, y, w, h, confidence)五个基本参数，然后还要有80个类别的概率。所以3×(5 + 80) = 255，这个255就是这么来的。</p> 
<p style="text-align:center;"><img alt="" height="460" src="https://images2.imgbox.com/bb/f3/HIv9RZHE_o.png" width="423"></p> 
<p><strong>下面我们具体看看y1,y2,y3是如何而来的。</strong><br> 网络中作者进行了三次检测，分别是在32倍降采样，16倍降采样，8倍降采样时进行检测，这样在多尺度的feature map上检测跟SSD有点像。在网络中使用up-sample（上采样）的原因:网络越深的特征表达效果越好，比如在进行16倍降采样检测，如果直接使用第四次下采样的特征来检测，这样就使用了浅层特征，这样效果一般并不好。如果想使用32倍降采样后的特征，但深层特征的大小太小，因此YOLOv3使用了步长为2的up-sample（上采样），把32倍降采样得到的feature map的大小提升一倍，也就成了16倍降采样后的维度。同理8倍采样也是对16倍降采样的特征进行步长为2的上采样，这样就可以使用深层特征进行detection。</p> 
<p>作者通过上采样将深层特征提取，其维度是与将要融合的特征层维度相同的（channel不同）。如下图所示，85层将13×13×256的特征上采样得到26×26×256，再将其与61层的特征拼接起来得到26×26×768。为了得到channel255，还需要进行一系列的3×3，1×1卷积操作，这样既可以提高非线性程度增加泛化性能提高网络精度，又能减少参数提高实时性。52×52×255的特征也是类似的过程。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/39/cd/fL9kvPSC_o.png"></p> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/1d/27/VoXhdb1e_o.png"></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/cd/52/sUJs1mqQ_o.png"></p> 
<h4>Bounding Box</h4> 
<p><br> YOLOv3的Bounding Box由YOLOv2又做出了更好的改进。在YOLOv2和YOLOv3中，都采用了对图像中的object采用k-means聚类。feature map中的每一个cell都会预测3个边界框（bounding box） ，每个bounding box都会预测三个东西：（1）每个框的位置（4个值，中心坐标tx和ty，框的高度bh和宽度bw），（2）一个objectness prediction ，（3）N个类别，coco数据集80类，voc20类。</p> 
<p>三次检测，每次对应的感受野不同，32倍降采样的感受野最大，适合检测大的目标，所以在输入为416×416时，每个cell的三个anchor box为(116 ,90); (156 ,198); (373 ,326)。16倍适合一般大小的物体，anchor box为(30,61); (62,45); (59,119)。8倍的感受野最小，适合检测小目标，因此anchor box为(10,13); (16,30); (33,23)。所以当输入为416×416时，实际总共有（52×52+26×26+13×13）×3=10647个proposal box。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/dc/bf/5En3ONcC_o.png"></p> 
<p> 感受一下9种先验框的尺寸，下图中蓝色框为聚类得到的先验框。黄色框式ground truth，红框是对象中心点所在的网格。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/a2/1f/qFoDvdRY_o.png"></p> 
<p><strong>这里注意bounding box 与anchor box的区别：</strong><br> Bounding box它输出的是框的位置（中心坐标与宽高），confidence以及N个类别。anchor box只是一个尺度即只有宽高。</p> 
<h4>LOSS Function</h4> 
<p>YOLOv3重要改变之一：<strong>No more softmaxing the classes</strong>。YOLOv3现在对图像中检测到的对象执行多标签分类。</p> 
<p>logistic回归用于对anchor包围的部分进行一个目标性评分(objectness score)，即这块位置是目标的可能性有多大。这一步是在predict之前进行的，可以去掉不必要anchor，可以减少计算量。</p> 
<p>如果模板框不是最佳的即使它超过我们设定的阈值，我们还是不会对它进行predict。不同于Faster R-CNN的是，YOLOv3只会对1个prior进行操作，也就是那个最佳prior。而logistic回归就是用来从9个anchor priors中找到objectness score(目标存在可能性得分)最高的那一个。logistic回归就是用曲线对prior相对于 objectness score映射关系的线性建模。</p> 
<pre><code class="language-python">	lxy, lwh, lcls, lconf = ft([0]), ft([0]), ft([0]), ft([0])
    txy, twh, tcls, indices = build_targets(model, targets)#在13 26 52维度中找到大于iou阈值最适合的anchor box 作为targets
    #txy[维度(0:2),(x,y)] twh[维度(0:2),(w,h)] indices=[0,anchor索引，gi，gj]

    # Define criteria
    MSE = nn.MSELoss()
    CE = nn.CrossEntropyLoss()
    BCE = nn.BCEWithLogitsLoss()

    # Compute losses
    h = model.hyp  # hyperparameters
    bs = p[0].shape[0]  # batch size
    k = h['k'] * bs  # loss gain
    for i, pi0 in enumerate(p):  # layer i predictions, i
        b, a, gj, gi = indices[i]  # image, anchor, gridx, gridy
        tconf = torch.zeros_like(pi0[..., 0])  # conf


        # Compute losses
        if len(b):  # number of targets
            pi = pi0[b, a, gj, gi]  # predictions closest to anchors 找到p中与targets对应的数据lxy
            tconf[b, a, gj, gi] = 1  # conf
            # pi[..., 2:4] = torch.sigmoid(pi[..., 2:4])  # wh power loss (uncomment)

            lxy += (k * h['xy']) * MSE(torch.sigmoid(pi[..., 0:2]),txy[i])  # xy loss
            lwh += (k * h['wh']) * MSE(pi[..., 2:4], twh[i])  # wh yolo loss
            lcls += (k * h['cls']) * CE(pi[..., 5:], tcls[i])  # class_conf loss

        # pos_weight = ft([gp[i] / min(gp) * 4.])
        # BCE = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
        lconf += (k * h['conf']) * BCE(pi0[..., 4], tconf)  # obj_conf loss
    loss = lxy + lwh + lconf + lcls

</code></pre> 
<p>以上是一段pytorch框架描述的YOLOv3的loss_function代码。忽略恒定系数不看，以下我想着重说几点：</p> 
<ul><li>首先，YOLOv3要先build target，因为我们知道正样本是label与anchor box iou大于0.5的组成，所以我们根据label找到对应的anchor box。如何找出label中存放着[image,class,x(归一化),y,w(归一化),h],我们可以用这些坐标在对应13×13 Or 26×26 or 52×52的map中分别于9个anchor算出iou，找到符合要求的，把索引与位置记录好。用记录好的索引位置找到predict的anchor box。</li><li>xywh是由均方差来计算loss的，其中预测的xy进行sigmoid来与lable xy求差，label xy是grid cell中心点坐标，其值在0-1之间，所以predict出的xy要sigmoid。</li><li>分类用的多类别交叉熵，置信度用的二分类交叉熵。只有正样本才参与class，xywh的loss计算，负样本只参与置信度loss。</li></ul> 
<p></p> 
<h3>五、YOLOv4</h3> 
<p>YOLOv4: Optimal Speed and Accuracy of Object Detection</p> 
<p>论文：<a href="https://arxiv.org/abs/2004.10934" rel="nofollow" title="https://arxiv.org/abs/2004.10934">https://arxiv.org/abs/2004.10934</a></p> 
<p>代码：<a href="https://github.com/AlexeyAB/darknet" title="GitHub - AlexeyAB/darknet: YOLOv4 / Scaled-YOLOv4 / YOLO - Neural Networks for Object Detection (Windows and Linux version of Darknet )">GitHub - AlexeyAB/darknet: YOLOv4 / Scaled-YOLOv4 / YOLO - Neural Networks for Object Detection (Windows and Linux version of Darknet )</a></p> 
<p>YOLOv4其实是一个结合了大量前人研究技术，加以组合并进行适当创新的算法，实现了速度和精度的完美平衡。可以说有许多技巧可以提高卷积神经网络(CNN)的准确性，但是某些技巧仅适合在某些模型上运行，或者仅在某些问题上运行，或者仅在小型数据集上运行；我们来码一码这篇文章里作者都用了哪些调优手段：加权残差连接(WRC),跨阶段部分连接(CSP),跨小批量标准化(CmBN),自对抗训练(SAT),Mish激活,马赛克数据增强,CmBN,DropBlock正则化,CIoU Loss等等。经过一系列的堆料，终于实现了目前最优的实验结果：43.5％的AP(在Tesla V100上，MS COCO数据集的实时速度约为65FPS)。</p> 
<p style="text-align:center;"><img alt="" height="602" src="https://images2.imgbox.com/27/98/sLFGfatV_o.png" width="726"></p> 
<p></p> 
<h3>5.1、YOLOv4框架原理</h3> 
<p><br> 先直接上<code>YOLOv4</code>的整体原理图(来源网络)如下：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/a6/c2/zrlfjXzO_o.png"></p> 
<h4>5.1.1  CSPDarknet53</h4> 
<p>我们前面知道在YOLOv3中，特征提取网络使用的是Darknet53，而在YOLOv4中，对Darknet53做了一点改进，借鉴了CSPNet，CSPNet全称是Cross Stage Partial Networks，也就是跨阶段局部网络。CSPNet解决了其他大型卷积神经网络框架Backbone中网络优化的梯度信息重复问题，将梯度的变化从头到尾地集成到特征图中，因此减少了模型的参数量和FLOPS数值，既保证了推理速度和准确率，又减小了模型尺寸。如下图：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/a1/1a/pOfRIxDa_o.png"></p> 
<p>CSPNet实际上是基于Densnet的思想，复制基础层的特征映射图，通过dense block发送副本到下一个阶段，从而将基础层的特征映射图分离出来。这样可以有效缓解梯度消失问题(通过非常深的网络很难去反推丢失信号) ，支持特征传播，鼓励网络重用特征，从而减少网络参数数量。CSPNet思想可以和ResNet、ResNeXt和DenseNet结合，目前主要有CSPResNext50 和CSPDarknet53两种改造Backbone网络。</p> 
<p>考虑到几方面的平衡：输入网络分辨率/卷积层数量/参数数量/输出维度。一个模型的分类效果好不见得其检测效果就好，想要检测效果好需要以下几点：</p> 
<ul><li>更大的网络输入分辨率——用于检测小目标</li><li>更深的网络层——能够覆盖更大面积的感受野</li><li>更多的参数——更好的检测同一图像内不同size的目标</li></ul> 
<p>这样最终的CSPDarknet53结构就如下图：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/d1/1a/uezuGy2t_o.png"></p> 
<p><code>CSPNet</code>论文： https://arxiv.org/pdf/1911.11929v1.pdf</p> 
<p>为了增大感受野，作者还使用了<code>SPP-block</code>，使用<code>PANet</code>代替<code>FPN</code>进行参数聚合以适用于不同<code>level</code>的目标检测。</p> 
<h4> 5.1.2 SPP结构</h4> 
<p><a href="https://mp.weixin.qq.com/s?__biz=MzIzNDM2OTMzOQ%3D%3D&amp;idx=1&amp;mid=2247486628&amp;scene=21&amp;sn=8b4ed6007c4c9ab71318315966623d0d#wechat_redirect" rel="nofollow" title="SPP-Net结构">SPP-Net结构</a>我们之前也有学过，<code>SPP-Net</code>全称<code>Spatial Pyramid Pooling Networks</code>，当时主要是用来解决不同尺寸的特征图如何进入全连接层的，直接看下图，下图中对任意尺寸的特征图直接进行固定尺寸的池化，来得到固定数量的特征。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/de/7b/yL0SfUQM_o.png"></p> 
<p>如上图，以3个尺寸的池化为例，对特征图进行一个最大值池化，即一张特征图得取其最大值，得到1*d(d是特征图的维度)个特征；对特征图进行网格划分为2x2的网格，然后对每个网格进行最大值池化，那么得到4*d个特征；同样，对特征图进行网格划分为4x4个网格，对每个网格进行最大值池化，得到16*d个特征。 接着将每个池化得到的特征合起来即得到固定长度的特征个数（特征图的维度是固定的），接着就可以输入到全连接层中进行训练网络了。用到这里是为了增加感受野。</p> 
<h4>5.1.3 PAN结构</h4> 
<p><code>YOLOv4</code>使用<code>PANet</code>(<code>Path Aggregation Network</code>)代替<code>FPN</code>进行参数聚合以适用于不同<code>level</code>的目标检测, <code>PANet</code>论文中融合的时候使用的方法是<code>Addition</code>，<code>YOLOv4</code>算法将融合的方法由加法改为<code>Concatenation</code>。如下图：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/d3/69/s5dAJN2m_o.png"></p> 
<h3>5.2 BackBone训练策略</h3> 
<p>这里我们主要从数据增强，DropBlock正则化，类标签平滑方面来学习下BackBone训练策略。</p> 
<h4>5.2.1 数据增强<br> 1、CutMix</h4> 
<p>YOLOv4选择用CutMix的增强方式，CutMix的处理方式也比较简单，同样也是对一对图片做操作，简单讲就是随机生成一个裁剪框Box,裁剪掉A图的相应位置，然后用B图片相应位置的ROI放到A图中被裁剪的区域形成新的样本，ground truth标签会根据patch的面积按比例进行调整，比如0.6像狗，0.4像猫，计算损失时同样采用加权求和的方式进行求解。这里借CutMix的地方顺带说下几种类似的增强方式：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/16/27/l4ONhgcq_o.png"></p> 
<p>上图是CutMix论文中作者对几种增强方式做的对比，结果显而易见，CutMix的增强方式在三个数据集上的表现都是最优的。其中Mixup是直接求和两张图，如同附身，鬼影一样，模型很难学到准确的特征图响应分布。Cutout是直接去除图像的一个区域，这迫使模型在进行分类时不能对特定的特征过于自信。然而，图像的一部分充满了无用的信息，这是一种浪费。在CutMix中，将图像的一部分剪切并粘贴到另一个图像上,使得模型更容易区分异类。</p> 
<p>CutMix论文： https://arxiv.org/pdf/1905.04899v2.pdf</p> 
<h4><strong>2、Mosaic</strong></h4> 
<p>Yolov4的Mosaic数据增强是参考CutMix数据增强，理论上类似。区别在于Mosaic是一种将4张训练图像合并成一张进行训练的数据增强方法(而不是CutMix中的2张)。这增强了对正常背景(context)之外的对象的检测，丰富检测物体的背景。此外，每个小批包含一个大的变化图像(4倍)，因此，减少了估计均值和方差的时需要大mini-batch的要求，降低了训练成本。如下图：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/32/8a/O5Ayojjb_o.png"></p> 
<h4> 5.2.2 DropBlock正则化</h4> 
<p>正则化技术有助于避免数据科学专业人员面临的最常见的问题，即过拟合。对于正则化，已经提出了几种方法，如L1和L2正则化、Dropout、Early Stopping和数据增强。这里YOLOv4用了DropBlock正则化的方法。</p> 
<p>DropBlock方法的引入是为了克服Dropout随机丢弃特征的主要缺点，Dropout被证明是全连接网络的有效策略，但在特征空间相关的卷积层中效果不佳。DropBlock技术在称为块的相邻相关区域中丢弃特征。这样既可以实现生成更简单模型的目的，又可以在每次训练迭代中引入学习部分网络权值的概念，对权值矩阵进行补偿，从而减少过拟合。如下图：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/2f/41/AFzoZTKw_o.png"></p> 
<p><code>DropBlock</code>论文中作者最终在<code>ImageNet</code>分类任务上，使用<code>Resnet-50</code>结构，将精度提升<code>1.6%</code>个点，在<code>COCO</code>检测任务上，精度提升<code>1.6%</code>个点。</p> 
<p><code>DropBlock</code>论文： https://arxiv.org/pdf/1810.12890.pdf</p> 
<h4>5.2.3 DropBlock正则化</h4> 
<p>对于分类问题，特别是多分类问题，常常把向量转换成one-hot-vector，而one-hot带来的问题： 对于损失函数，我们需要用预测概率去拟合真实概率，而拟合one-hot的真实概率函数会带来两个问题：</p> 
<ul><li>无法保证模型的泛化能力，容易造成过拟合；</li><li>全概率和0概率鼓励所属类别和其他类别之间的差距尽可能加大，而由梯度有界可知，这种情况很难适应。会造成模型过于相信预测的类别。</li></ul> 
<p>对预测有100%的信心可能表明模型是在记忆数据，而不是在学习。标签平滑调整预测的目标上限为一个较低的值，比如0.9。它将使用这个值而不是1.0来计算损失。这个概念缓解了过度拟合。说白了，这个平滑就是一定程度缩小label中min和max的差距，label平滑可以减小过拟合。所以，适当调整label，让两端的极值往中间凑凑，可以增加泛化性能。</p> 
<h4>5.3 BackBone推理策略</h4> 
<h4>5.3.1 Mish激活函数</h4> 
<p>对激活函数的研究一直没有停止过，<code>ReLU</code>还是统治着深度学习的激活函数，不过，这种情况有可能会被<code>Mish</code>改变。<code>Mish</code>是另一个与<code>ReLU</code>和<code>Swish</code>非常相似的激活函数。正如论文所宣称的那样，<code>Mish</code>可以在不同数据集的许多深度网络中胜过它们。公式如下：</p> 
<p>                                                                                                    <img alt="\large y= x\ast tanh(ln(1+e^{x})" class="mathcode" src="https://images2.imgbox.com/30/f5/GwCot3tI_o.gif"></p> 
<p>Mish是一个平滑的曲线，平滑的激活函数允许更好的信息深入神经网络，从而得到更好的准确性和泛化；在负值的时候并不是完全截断，允许比较小的负梯度流入。实验中，随着层深的增加，ReLU激活函数精度迅速下降，而Mish激活函数在训练稳定性、平均准确率(1%-2.8%)、峰值准确率(1.2% - 3.6%)等方面都有全面的提高。如下图：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/7a/4d/3SRyks0W_o.png"></p> 
<p><code>Mish</code>论文： https://arxiv.org/pdf/1908.08681.pdf</p> 
<h4>5.3.2 MiWRC策略</h4> 
<p>MiWRC是Multi-input weighted residual connections的简称， 在BiFPN中，提出了用MiWRC来执行标尺度级重加权，添加不同尺度的特征映射。我们已经讨论了FPN和PAN作为例子。下面的图(d)显示了另一种被称为BiFPN的neck设计，根据BiFPN的论文，该设计具有更好的准确性和效率权衡。<br>  </p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/d3/24/Hr0S2tiv_o.png"></p> 
<p>上图中 (a)FPN引入自顶向下的路径，将多尺度特征从3级融合到7级(P3-P7)；(b)PANET在FPN之上增加一个额外的自下而上的路径；(c)NAS-FPN使用神经网络搜索找到一个不规则的特征拓扑网络，然后重复应用同一块拓扑结构；(d)是这里的BiFPN，具有更好的准确性和效率权衡。将该neck放到整个整个网络的连接中如下图：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/19/94/PL9eps5S_o.png"></p> 
<p>上图采用<code>EfficientNet</code>作为骨干网络，<code>BiFPN</code>作为特征网络，共享<code>class/box</code>预测网络。 基于不同的资源约束，<code>BiFPN</code>层和类/盒网层都被重复多次。</p> 
<p><code>BiFPN</code>论文： https://arxiv.org/pdf/1911.09070.pdf</p> 
<h4>5.4 检测头训练策略</h4> 
<h4>5.4.1 CIoU-loss</h4> 
<p><br> 损失函数给出了如何调整权重以降低loss。所以在我们做出错误预测的情况下，我们期望它能给我们指明前进的方向。但如果使用IoU，考虑两个预测都不与ground truth重叠，那么IoU损失函数不能告诉哪一个是更好的，或者哪个更接近ground truth。这里顺带看下常用的几种loss的形式，如下：</p> 
<p><strong>1、经典IoU loss：</strong></p> 
<p>IoU算法是使用最广泛的算法，大部分的检测算法都是使用的这个算法。</p> 
<p style="text-align:center;"><img alt="" height="82" src="https://images2.imgbox.com/ae/bd/EREXXgHs_o.png" width="251"></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/f6/b8/hJ273zPN_o.png"></p> 
<p>可以看到IOU的loss其实很简单，主要是<strong>交集/并集</strong>，但其实也存在两个问题。 </p> 
<p><img alt="" src="https://images2.imgbox.com/dd/e0/QY4OX9BI_o.png"></p> 
<p><strong>问题1：</strong>即状态1的情况，当预测框和目标框不相交时，IOU=0，无法反应两个框距离的远近，此时损失函数不可导，IOU_Loss无法优化两个框不相交的情况。</p> 
<p><strong>问题2：</strong>即状态2和状态3的情况，当两个预测框大小相同，两个IOU也相同，IOU_Loss无法区分两者相交情况的不同。</p> 
<p>因此<strong>2019</strong>年出现了GIOU_Loss来进行改进。</p> 
<p><strong>2、GIoU：Generalized IoU</strong></p> 
<p>GIoU考虑到，当检测框和真实框没有出现重叠的时候IoU的loss都是一样的，因此GIoU就加入了C检测框（C检测框是包含了检测框和真实框的最小矩形框），这样就可以解决检测框和真实框没有重叠的问题。其中，C是指能包含predict box和Ground Truth box的最小box。<br><img alt="" src="https://images2.imgbox.com/3f/ea/MXwtghD8_o.png"></p> 
<p>可以看到上图GIOU_Loss中，增加了相交尺度的衡量方式，缓解了单纯IOU_Loss时的尴尬。但为什么仅仅说缓解呢？因为还存在一种<strong>不足</strong>：</p> 
<p><img alt="" src="https://images2.imgbox.com/81/bc/dPJJegev_o.png"></p> 
<p><strong>问题</strong>：状态1、2、3都是预测框在目标框内部且预测框大小一致的情况，这时预测框和目标框的差集都是相同的，因此这三种状态的<strong>GIOU值</strong>也都是相同的，这时GIOU退化成了IOU，无法区分相对位置关系。<br> 基于这个问题，<strong>2020年</strong>的AAAI又提出了<strong>DIOU_Loss</strong>。</p> 
<p><strong>3、DIoU：Distance IoU</strong></p> 
<p>好的目标框回归函数应该考虑三个重要几何因素：<strong>重叠面积、中心点距离，长宽比。</strong>针对IOU和GIOU存在的问题，作者从两个方面进行考虑</p> 
<p><strong>(1)：如何最小化预测框和目标框之间的归一化距离？</strong></p> 
<p><strong>(2)：如何在预测框和目标框重叠时，回归的更准确？</strong></p> 
<p>针对第一个问题，提出了DIOU_Loss（Distance_IOU_Loss）</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/76/5d/aOZPneht_o.png"></p> 
<p>DIOU_Loss考虑了<strong>重叠面积</strong>和<strong>中心点距离</strong>，当目标框包裹预测框的时候，直接度量2个框的距离，因此DIOU_Loss收敛的更快。</p> 
<p>但就像前面好的目标框回归函数所说的，没有考虑到长宽比。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/a4/83/sTMtXyST_o.png"></p> 
<p>比如上面三种情况，目标框包裹预测框，本来DIOU_Loss可以起作用。但预测框的中心点的位置都是一样的，因此按照DIOU_Loss的计算公式，三者的值都是相同的。</p> 
<p>针对这个问题，又提出了CIOU_Loss，不对不说，科学总是在解决问题中，不断进步！！</p> 
<p><strong>4、CIOU_Loss</strong></p> 
<p>CIOU_Loss和DIOU_Loss前面的公式都是一样的，不过在此基础上还增加了一个影响因子，将预测框和目标框的长宽比都考虑了进去。</p> 
<p style="text-align:center;"><img alt="" height="102" src="https://images2.imgbox.com/c9/61/b91L5FK7_o.png" width="606"></p> 
<p> 其中v是衡量长宽比一致性的参数，我们也可以定义为：</p> 
<p style="text-align:center;"><img alt="" height="101" src="https://images2.imgbox.com/54/94/jM0kHzMV_o.png" width="363"></p> 
<p>这样CIOU_Loss就将目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比全都考虑进去了。</p> 
<p>再来综合的看下各个Loss函数的不同点：</p> 
<p><strong>IOU_Loss：</strong>主要考虑检测框和目标框重叠面积。</p> 
<p><strong>GIOU_Loss：</strong>在IOU的基础上，解决边界框不重合时的问题。</p> 
<p><strong>DIOU_Loss：</strong>在IOU和GIOU的基础上，考虑边界框中心点距离的信息。</p> 
<p><strong>CIOU_Loss：</strong>在DIOU的基础上，考虑边界框宽高比的尺度信息。</p> 
<p>YOLOv4中采用了<strong>CIOU_Loss</strong>的回归方式，使得预测框回归的<strong>速度和精度</strong>更高一些。</p> 
<h4>5.4.2 CmBN策略</h4> 
<p>BN就是仅仅利用当前迭代时刻信息进行norm，而CBN在计算当前时刻统计量时候会考虑前k个时刻统计量，从而实现扩大batch size操作。同时作者指出CBN操作不会引入比较大的内存开销，训练速度不会影响很多，但是训练时候会慢一些，比GN还慢。</p> 
<p>CmBN是CBN的改进版本，其把大batch内部的4个mini batch当做一个整体，对外隔离。CBN在第t时刻，也会考虑前3个时刻的统计量进行汇合，而CmBN操作不会，不再滑动cross,其仅仅在mini batch内部进行汇合操作，保持BN一个batch更新一次可训练参数。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/b5/ec/zsXj77XN_o.png"></p> 
<p>BN：无论每个batch被分割为多少个mini batch，其算法就是在每个mini batch前向传播后统计当前的BN数据（即每个神经元的期望和方差）并进行Nomalization，BN数据与其他mini batch的数据无关。CBN：每次iteration中的BN数据是其之前n次数据和当前数据的和（对非当前batch统计的数据进行了补偿再参与计算），用该累加值对当前的batch进行Nomalization。好处在于每个batch可以设置较小的size。CmBN：只在每个Batch内部使用CBN的方法，个人理解如果每个Batch被分割为一个mini batch，则其效果与BN一致；若分割为多个mini batch，则与CBN类似，只是把mini batch当作batch进行计算，其区别在于权重更新时间点不同，同一个batch内权重参数一样，因此计算不需要进行补偿。</p> 
<h4>5.4.3 自对抗训练(SAT)</h4> 
<p><code>SAT</code>为一种新型数据增强方式。在第一阶段，神经网络改变原始图像而不是网络权值。通过这种方式，神经网络对其自身进行一种对抗式的攻击，改变原始图像，制造图像上没有目标的假象。在第二阶段，训练神经网络对修改后的图像进行正常的目标检测。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/b0/4f/x2f0a5v8_o.png"></p> 
<p>Self-Adversarial Training是在一定程度上抵抗对抗攻击的数据增强技术。CNN计算出Loss, 然后通过反向传播改变图片信息，形成图片上没有目标的假象，然后对修改后的图像进行正常的目标检测。需要注意的是在SAT的反向传播的过程中，是不需要改变网络权值的。 使用对抗生成可以改善学习的决策边界中的薄弱环节，提高模型的鲁棒性。因此这种数据增强方式被越来越多的对象检测框架运用。</p> 
<h4>5.4.4 消除网格敏感度</h4> 
<p>对于<img alt="\large b_{x}=c_{x}" class="mathcode" src="https://images2.imgbox.com/e2/c6/2u67Qarp_o.gif">和 <img alt="\large b_{x}=c_{x}+1" class="mathcode" src="https://images2.imgbox.com/8f/6e/MMRSygAN_o.gif">的情况，我们需要<img alt="\large t_{x}" class="mathcode" src="https://images2.imgbox.com/ec/ac/oDo9JSKQ_o.gif">分别具有很大的负值和正值。但我们可以将<img alt="\large \sigma" class="mathcode" src="https://images2.imgbox.com/ae/cc/cZbYIt9E_o.gif">与一个比例因子(&gt;1.0)相乘，从而更轻松地实现这一目标</p> 
<p><img alt="" src="https://images2.imgbox.com/40/59/7GryJfcN_o.png"></p> 
<h4>5.4.5 余弦模拟退火</h4> 
<p>余弦调度会根据一个余弦函数来调整学习率。首先，较大的学习率会以较慢的速度减小。然后在中途时，学习的减小速度会变快，最后学习率的减小速度又会变得很慢。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/1c/40/ff4NECU0_o.png"></p> 
<p>这张图展示了学习率衰减的方式（下图中还应用了学习率预热）及其对<code>mAP</code>的影响。可能看起来并不明显，这种新的调度方法的进展更为稳定，而不是在停滞一段时间后又取得进展。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/7c/4c/tTEe4w2T_o.png"></p> 
<p>余弦模拟退火论文： https://arxiv.org/pdf/1608.03983.pdf </p> 
<h4>5.5 检测头推理策略</h4> 
<h4>5.5.1 SAM模块</h4> 
<p>注意力机制在<code>DL</code>设计中被广泛采用。在<code>SAM</code>中，最大值池化和平均池化分别用于输入<code>feature map</code>，创建两组<code>feature map</code>。结果被输入到一个卷积层，接着是一个<code>Sigmoid</code>函数来创建空间注意力。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/20/10/TVRIGnRZ_o.png"></p> 
<p> 将空间注意掩模应用于输入特征，输出精细的特征图。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/cf/56/X6pDJJ2o_o.png"></p> 
<p> 在<code>YOLOv4</code>中，使用修改后的<code>SAM</code>而不应用最大值池化和平均池化。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/3d/d0/ImVMX2W1_o.png"></p> 
<p>在<code>YOLOv4</code>中，<code>FPN</code>概念逐渐被实现/替换为经过修改的<code>SPP</code>、<code>PAN</code>和<code>PAN</code>。</p> 
<h4>5.5.2 DIoU-NMS </h4> 
<p><code>NMS</code>过滤掉预测相同对象的其他边界框，并保留具有最高可信度的边界框。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/bd/c9/3diQh7Uc_o.png"></p> 
<p> <code>DIoU</code>(前面讨论过的) 被用作非最大值抑制(<code>NMS</code>)的一个因素。该方法在抑制冗余框的同时，采用<code>IoU</code>和两个边界盒中心点之间的距离。这使得它在有遮挡的情况下更加健壮。</p> 
<h3>六、YOLOv5</h3> 
<p>YOLOv4出现之后不久，YOLOv5横空出世。YOLOv5在YOLOv4算法的基础上做了进一步的改进，检测性能得到进一步的提升。虽然YOLOv5算法并没有与YOLOv4算法进行性能比较与分析，但是YOLOv5在COCO数据集上面的测试效果还是挺不错的。大家对YOLOv5算法的创新性半信半疑，有的人对其持肯定态度，有的人对其持否定态度。在我看来，YOLOv5检测算法中还是存在很多可以学习的地方，虽然这些改进思路看来比较简单或者创新点不足，但是它们确定可以提升检测算法的性能。其实工业界往往更喜欢使用这些方法，而不是利用一个超级复杂的算法来获得较高的检测精度。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/13/93/7Dh7W2KB_o.png"></p> 
<h4>6.1 YOLOv5算法简介</h4> 
<p>YOLOv5是一种单阶段目标检测算法，该算法在YOLOv4的基础上添加了一些新的改进思路，使其速度与精度都得到了极大的性能提升。主要的改进思路如下所示：</p> 
<ul><li>输入端：在模型训练阶段，提出了一些改进思路，主要包括Mosaic数据增强、自适应锚框计算、自适应图片缩放；</li><li>基准网络：融合其它检测算法中的一些新思路，主要包括：Focus结构与CSP结构；</li><li>Neck网络：目标检测网络在BackBone与最后的Head输出层之间往往会插入一些层，Yolov5中添加了FPN+PAN结构；</li><li>Head输出层：输出层的锚框机制与YOLOv4相同，主要改进的是训练时的损失函数GIOU_Loss，以及预测框筛选的DIOU_nms。</li></ul> 
<h4>6.2 YOLOv5算法详解</h4> 
<h4>6.2.1 YOLOv5网络架构</h4> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/f0/34/vmEm9Jg4_o.jpg"></p> 
<p>上图展示了YOLOv5目标检测算法的整体框图。对于一个目标检测算法而言，我们通常可以将其划分为4个通用的模块，具体包括：输入端、基准网络、Neck网络与Head输出端，对应于上图中的4个红色模块。YOLOv5算法具有4个版本，具体包括：<strong>YOLOv5s、YOLOv5m、YOLOv5l、YOLOv5x</strong>四种，本文重点讲解<strong>YOLOv5s</strong>，其它的版本都在该版本的基础上对网络进行加深与加宽。</p> 
<ul><li>输入端-输入端表示输入的图片。该网络的输入图像大小为608*608，该阶段通常包含一个图像预处理阶段，即将输入图像缩放到网络的输入大小，并进行归一化等操作。在网络训练阶段，YOLOv5使用Mosaic数据增强操作提升模型的训练速度和网络的精度；并提出了一种自适应锚框计算与自适应图片缩放方法。</li><li>基准网络-基准网络通常是一些性能优异的分类器种的网络，该模块用来提取一些通用的特征表示。YOLOv5中不仅使用了CSPDarknet53结构，而且使用了Focus结构作为基准网络。</li><li>Neck网络-Neck网络通常位于基准网络和头网络的中间位置，利用它可以进一步提升特征的多样性及鲁棒性。虽然YOLOv5同样用到了SPP模块、FPN+PAN模块，但是实现的细节有些不同。</li><li>Head输出端-Head用来完成目标检测结果的输出。针对不同的检测算法，输出端的分支个数不尽相同，通常包含一个分类分支和一个回归分支。YOLOv4利用GIOU_Loss来代替Smooth L1 Loss函数，从而进一步提升算法的检测精度。</li></ul> 
<h4>6.2.2 YOLOv5基础组件</h4> 
<ul><li>CBL-CBL模块由Conv+BN+Leaky_relu激活函数组成，如上图中的模块1所示。</li><li>Res unit-借鉴ResNet网络中的残差结构，用来构建深层网络，CBM是残差模块中的子模块，如上图中的模块2所示。</li><li>CSP1_X-借鉴CSPNet网络结构，该模块由CBL模块、Res unint模块以及卷积层、Concate组成而成，如上图中的模块3所示。</li><li>CSP2_X-借鉴CSPNet网络结构，该模块由卷积层和X个Res unint模块Concate组成而成，如上图中的模块4所示。</li><li>Focus-如上图中的模块5所示，Focus结构首先将多个slice结果Concat起来，然后将其送入CBL模块中。</li><li>SPP-采用1×1、5×5、9×9和13×13的最大池化方式，进行多尺度特征融合，如上图中的模块6所示。</li></ul> 
<h4>6.2.3  输入端细节详解</h4> 
<ul><li><strong>Mosaic数据增强</strong>-YOLOv5中在训练模型阶段仍然使用了Mosaic数据增强方法，该算法是在CutMix数据增强方法的基础上改进而来的。CutMix仅仅利用了两张图片进行拼接，而Mosaic数据增强方法则采用了4张图片，并且按照随机缩放、随机裁剪和随机排布的方式进行拼接而成，具体的效果如下图所示。这种增强方法可以将几张图片组合成一张，这样不仅可以丰富数据集的同时极大的提升网络的训练速度，而且可以降低模型的内存需求。</li></ul> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/5d/f8/q8W2tSKY_o.jpg"></p> 
<ul><li><strong>自适应锚框计算</strong>-在YOLOv5系列算法中，针对不同的数据集，都需要设定特定长宽的锚点框。在网络训练阶段，模型在初始锚点框的基础上输出对应的预测框，计算其与GT框之间的差距，并执行反向更新操作，从而更新整个网络的参数，因此设定初始锚点框也是比较关键的一环。在YOLOv3和YOLOv4检测算法中，训练不同的数据集时，都是通过单独的程序运行来获得初始锚点框。YOLOv5中将此功能嵌入到代码中，每次训练时，根据数据集的名称自适应的计算出最佳的锚点框，用户可以根据自己的需求将功能关闭或者打开，具体的指令为parser.add_argument(’–noautoanchor’, action=‘store_ true’, help=‘disable autoanchor check’)，如果需要打开，只需要在训练代码时增加–noautoanch or选项即可。</li><li><strong>自适应图片缩放</strong>-针对不同的目标检测算法而言，我们通常需要执行图片缩放操作，即将原始的输入图片缩放到一个固定的尺寸，再将其送入检测网络中。YOLO系列算法中常用的尺寸包括416*416，608 *608等尺寸。原始的缩放方法存在着一些问题，由于在实际的使用中的很多图片的长宽比不同，因此缩放填充之后，两端的黑边大小都不相同，然而如果填充的过多，则会存在大量的信息冗余，从而影响整个算法的推理速度。为了进一步提升YOLOv5算法的推理速度，该算法提出一种方法能够自适应的添加最少的黑边到缩放之后的图片中。</li></ul> 
<h4>6.2.4  基准网络细节详解</h4> 
<ul><li><strong>Focus结构</strong>-该结构的主要思想是通过slice操作来对输入图片进行裁剪。如下图所示，原始输入图片大小为608*608*3，经过Slice与Concat操作之后输出一个304*304*12的特征映射；接着经过一个通道个数为32的Conv层（该通道个数仅仅针对的是YOLOv5s结构，其它结构会有相应的变化），输出一个304*304*32大小的特征映射。</li></ul> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/ed/a5/69gDE3pe_o.png"></p> 
<ul><li><strong>CSP结构</strong>-YOLOv4网络结构中，借鉴了CSPNet的设计思路，仅仅在主干网络中设计了CSP结构。而YOLOv5中设计了两种CSP结构，以YOLOv5s网络为例，CSP1_X结构应用于Backbone主干网络中，另一种CSP2_X结构则应用于Neck网络中。CSP1_X与CSP2_X模块的实现细节如3.1所示。</li></ul> 
<h4>6.2.5 Neck网络细节详解</h4> 
<ul><li>FPN+PAN-YOLOv5的Neck网络仍然使用了FPN+PAN结构，但是在它的基础上做了一些改进操作，YOLOv4的Neck结构中，采用的都是普通的卷积操作。而YOLOv5的Neck网络中，采用借鉴CSPnet设计的CSP2结构，从而加强网络特征融合能力。下图展示了YOLOv4与YOLOv5的Neck网络的具体细节，通过比较我们可以发现：（1）灰色区域表示第1个不同点，YOLOv5不仅利用CSP2_\1结构代替部分CBL模块，而且去掉了下方的CBL模块；（2）绿色区域表示第2个不同点，YOLOv5不仅将Concat操作之后的CBL模块更换为CSP2_1模块，而且更换了另外一个CBL模块的位置；（3）蓝色区域表示第3个不同点，YOLOv5中将原始的CBL模块更换为CSP2_1模块。</li></ul> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/85/28/Fa2nMbd9_o.jpg"></p> 
<h2>七、YOLOv6</h2> 
<p>YOLOv6 是美团视觉智能部研发的一款目标检测框架，致力于工业应用。本框架同时专注于检测的精度和推理效率，在工业界常用的尺寸模型中：YOLOv6-nano 在 COCO 上精度可达 <strong>35.0% AP</strong>，在 T4 上推理速度可达 <strong>1242 FPS</strong>；YOLOv6-s 在 COCO 上精度可达 <strong>43.1% AP</strong>，在 T4 上推理速度可达 <strong>520 FPS</strong>。在部署方面，YOLOv6 支持 GPU（TensorRT）、CPU（OPENVINO）、ARM（MNN、TNN、NCNN）等不同平台的部署，极大地简化工程部署时的适配工作。</p> 
<p><strong>精度与速度远超 YOLOv5 和 YOLOX 的新框架</strong></p> 
<p>目标检测作为计算机视觉领域的一项基础性技术，在工业界得到了广泛的应用，其中 YOLO 系列算法因其较好的综合性能，逐渐成为大多数工业应用时的首选框架。至今，业界已衍生出许多 YOLO 检测框架，其中以 YOLOv5、YOLOX和 PP-YOLOE最具代表性，但在实际使用中，我们发现上述框架在速度和精度方面仍有很大的提升的空间。基于此，我们通过研究并借鉴了业界已有的先进技术，开发了一套新的目标检测框架——YOLOv6。该框架支持模型训练、推理及多平台部署等全链条的工业应用需求，并在网络结构、训练策略等算法层面进行了多项改进和优化，在 COCO 数据集上，YOLOv6 在精度和速度方面均超越其他同体量算法，相关结果如下图所示：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/43/e1/QKtx1X4i_o.png"></p> 
<p>YOLOv6 主要在 Backbone、Neck、Head 以及训练策略等方面进行了诸多的改进：</p> 
<ul><li> <p>统一设计了更高效的 Backbone 和 Neck ：受到硬件感知神经网络设计思想的启发，基于 RepVGG style设计了可重参数化、更高效的骨干网络 EfficientRep Backbone 和 Rep-PAN Neck。</p> </li><li> <p>优化设计了更简洁有效的 Efficient Decoupled Head，在维持精度的同时，进一步降低了一般解耦头带来的额外延时开销。</p> </li><li> <p>在训练策略上，我们采用Anchor-free 无锚范式，同时辅以 SimOTA标签分配策略以及 SIoU边界框回归损失来进一步提高检测精度。</p> </li></ul> 
<h3>7.1 Hardware-friendly 的骨干网络设计</h3> 
<p>YOLOv5/YOLOX 使用的 Backbone 和 Neck 都基于 CSPNet[5] 搭建，采用了多分支的方式和残差结构。对于 GPU 等硬件来说，这种结构会一定程度上增加延时，同时减小内存带宽利用率。下图为计算机体系结构领域中的 Roofline Model介绍图，显示了硬件中计算能力和内存带宽之间的关联关系。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/86/50/SpdauvO9_o.png"></p> 
<p>于是，我们基于硬件感知神经网络设计的思想，对 Backbone 和 Neck 进行了重新设计和优化。该思想基于硬件的特性、推理框架/编译框架的特点，以硬件和编译友好的结构作为设计原则，在网络构建时，综合考虑硬件计算能力、内存带宽、编译优化特性、网络表征能力等，进而获得又快又好的网络结构。对上述重新设计的两个检测部件，我们在 YOLOv6 中分别称为 EfficientRep Backbone 和 Rep-PAN Neck，其主要贡献点在于：</p> 
<ul><li>引入了 RepVGG[4] style 结构。</li><li>基于硬件感知思想重新设计了 Backbone 和 Neck。</li></ul> 
<p>RepVGG Style 结构是一种在训练时具有多分支拓扑，而在实际部署时可以等效融合为单个 3x3 卷积的一种可重参数化的结构（融合过程如下图所示）。通过融合成的 3x3 卷积结构，可以有效利用计算密集型硬件计算能力（比如 GPU），同时也可获得 GPU/CPU 上已经高度优化的 NVIDIA cuDNN 和 Intel MKL 编译框架的帮助。</p> 
<p>实验表明，通过上述策略，YOLOv6 减少了在硬件上的延时，并显著提升了算法的精度，让检测网络更快更强。以 nano 尺寸模型为例，对比 YOLOv5-nano 采用的网络结构，本方法在速度上提升了21%，同时精度提升 3.6% AP。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/28/0c/WPJ2FhSU_o.png"> 
  <figcaption>
    Rep算子的融合过程 
  </figcaption> 
 </figure> 
</div> 
<p><strong>EfficientRep Backbone</strong>：在 Backbone 设计方面，我们基于以上 Rep 算子设计了一个高效的Backbone。相比于 YOLOv5 采用的 CSP-Backbone，该 Backbone 能够高效利用硬件（如 GPU）算力的同时，还具有较强的表征能力。</p> 
<p>下图为 EfficientRep Backbone 具体设计结构图，我们将 Backbone 中 stride=2 的普通 Conv 层替换成了 stride=2 的 RepConv层。同时，将原始的 CSP-Block 都重新设计为 RepBlock，其中 RepBlock 的第一个 RepConv 会做 channel 维度的变换和对齐。另外，我们还将原始的 SPPF 优化设计为更加高效的 SimSPPF。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/22/3e/lmfVeBnZ_o.png"> 
  <figcaption>
    EfficientRep Backbone 结构图 
  </figcaption> 
 </figure> 
</div> 
<p><strong>Rep-PAN</strong>：在 Neck 设计方面，为了让其在硬件上推理更加高效，以达到更好的精度与速度的平衡，我们基于硬件感知神经网络设计思想，为 YOLOv6 设计了一个更有效的特征融合网络结构。</p> 
<p>Rep-PAN 基于 PAN拓扑方式，用 RepBlock 替换了 YOLOv5 中使用的 CSP-Block，同时对整体 Neck 中的算子进行了调整，目的是在硬件上达到高效推理的同时，保持较好的多尺度特征融合能力（Rep-PAN 结构图如下图所示）。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/64/08/o9Kd5c4t_o.jpg"> 
  <figcaption>
    Rep-PAN 结构图 
  </figcaption> 
 </figure> 
</div> 
<h3>7.2 更简洁高效的 Decoupled Head</h3> 
<p>在 YOLOv6 中，我们采用了解耦检测头（Decoupled Head）结构，并对其进行了精简设计。原始 YOLOv5 的检测头是通过分类和回归分支融合共享的方式来实现的，而 YOLOX 的检测头则是将分类和回归分支进行解耦，同时新增了两个额外的 3x3 的卷积层，虽然提升了检测精度，但一定程度上增加了网络延时。</p> 
<p>因此，我们对解耦头进行了精简设计，同时综合考虑到相关算子表征能力和硬件上计算开销这两者的平衡，采用 Hybrid Channels 策略重新设计了一个更高效的解耦头结构，在维持精度的同时降低了延时，缓解了解耦头中 3x3 卷积带来的额外延时开销。通过在 nano 尺寸模型上进行消融实验，对比相同通道数的解耦头结构，精度提升 0.2% AP 的同时，速度提升6.8%。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/19/ce/hreOoVqt_o.jpg"> 
  <figcaption>
    Efficient Decoupled Head 结构图 
  </figcaption> 
 </figure> 
</div> 
<h3>7.3 更有效的训练策略</h3> 
<p>为了进一步提升检测精度，我们吸收借鉴了学术界和业界其他检测框架的先进研究进展：Anchor-free 无锚范式 、SimOTA 标签分配策略以及 SIoU 边界框回归损失。</p> 
<h4><strong>7.3.1 Anchor-free 无锚范式</strong></h4> 
<p>YOLOv6 采用了更简洁的 Anchor-free 检测方法。由于 Anchor-based检测器需要在训练之前进行聚类分析以确定最佳 Anchor 集合，这会一定程度提高检测器的复杂度；同时，在一些边缘端的应用中，需要在硬件之间搬运大量检测结果的步骤，也会带来额外的延时。而 Anchor-free 无锚范式因其泛化能力强，解码逻辑更简单，在近几年中应用比较广泛。经过对 Anchor-free 的实验调研，我们发现，相较于Anchor-based 检测器的复杂度而带来的额外延时，Anchor-free 检测器在速度上有51%的提升。</p> 
<h4><strong>7.3.2 SimOTA 标签分配策略</strong></h4> 
<p>为了获得更多高质量的正样本，YOLOv6 引入了 SimOTA [4]算法动态分配正样本，进一步提高检测精度。YOLOv5 的标签分配策略是基于 Shape 匹配，并通过跨网格匹配策略增加正样本数量，从而使得网络快速收敛，但是该方法属于静态分配方法，并不会随着网络训练的过程而调整。</p> 
<p>近年来，也出现不少基于动态标签分配的方法，此类方法会根据训练过程中的网络输出来分配正样本，从而可以产生更多高质量的正样本，继而又促进网络的正向优化。例如，OTA通过将样本匹配建模成最佳传输问题，求得全局信息下的最佳样本匹配策略以提升精度，但 OTA 由于使用了Sinkhorn-Knopp 算法导致训练时间加长，而 SimOTA[4]算法使用 Top-K 近似策略来得到样本最佳匹配，大大加快了训练速度。故 YOLOv6 采用了SimOTA 动态分配策略，并结合无锚范式，在 nano 尺寸模型上平均检测精度提升 1.3% AP。</p> 
<h4><strong>7.3.3 SIoU 边界框回归损失</strong></h4> 
<p>为了进一步提升回归精度，YOLOv6 采用了 SIoU边界框回归损失函数来监督网络的学习。目标检测网络的训练一般需要至少定义两个损失函数：分类损失和边界框回归损失，而损失函数的定义往往对检测精度以及训练速度产生较大的影响。</p> 
<p>近年来，常用的边界框回归损失包括IoU、GIoU、CIoU、DIoU loss等等，这些损失函数通过考虑预测框与目标框之前的重叠程度、中心点距离、纵横比等因素来衡量两者之间的差距，从而指导网络最小化损失以提升回归精度，但是这些方法都没有考虑到预测框与目标框之间方向的匹配性。SIoU 损失函数通过引入了所需回归之间的向量角度，重新定义了距离损失，有效降低了回归的自由度，加快网络收敛，进一步提升了回归精度。通过在 YOLOv6s 上采用 SIoU loss 进行实验，对比 CIoU loss，平均检测精度提升 0.3% AP。</p> 
<h2>八、YOLOv7</h2> 
<p>我们先整体来看下 YOLOV7，首先对输入的图片 resize 为 640x640 大小，输入到 backbone 网络中，然后经 head 层网络输出三层不同 size 大小的 **feature map**，经过 Rep 和 conv输出预测结果，这里以 coco 为例子，输出为 80 个类别，然后每个输出(x ,y, w, h, o) 即坐标位置和前后背景，3 是指的 anchor 数量，因此每一层的输出为 (80+5)x3 = 255再乘上 feature map 的大小就是最终的输出了。</p> 
<p class="img-center"><img alt="" height="818" src="https://images2.imgbox.com/e9/01/p0uy91QZ_o.png" width="1197"></p> 
<h3>8.1 backbone</h3> 
<p>YOLOV7 的 backbone 如下图所示</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/f5/fd/xaXxFUW8_o.png"></p> 
<p>总共有 50 层, 我在上图用黑色数字把关键层数标示出来了。<br> 首先是经过 <strong>4</strong> 层卷积层，如下图，<strong>CBS</strong> 主要是 <strong>Conv + BN + SiLU</strong> 构成，我在图中用不同的颜色表示不同的 size 和 stride, 如 (3, 2) 表示卷积核大小为 3 ，步长为 2。 在 config 中的配置如图。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/e2/fe/cv7LHQXD_o.png"></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/d7/5f/iDSg38Ps_o.png"></p> 
<p>经过 4个 CBS 后，特征图变为 160 * 160 * 128 大小。随后会经过论文中提出的 ELAN 模块，ELAN 由多个 CBS 构成，其输入输出特征大小保持不变，通道数在开始的两个 CBS 会有变化， 后面的几个输入通道都是和输出通道保持一致的，经过最后一个 CBS 输出为需要的通道。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/85/f5/cKwfNVxA_o.png"></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/08/9b/OBfdH1Ub_o.png"></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/19/cf/Yvw7BQH1_o.png"></p> 
<p><strong>MP</strong> 层 主要是分为 Maxpool 和 CBS , 其中 MP1 和 MP2 主要是通道数的比变化。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/69/8a/FDwKQaua_o.png"></p> 
<p>backbone的基本组件就介绍完了，我们整体来看下 backbone，经过 4 个 CBS 后，接入例如一个 ELAN ，然后后面就是三个 MP + ELAN 的输出，对应的就是 C3/C4/C5 的输出，大小分别为 80 * 80 * 512 ， 40 * 40 * 1024， 20 * 20 * 1024。 每一个 MP 由 5 层， ELAN 有 8 层， 所以整个 backbone 的层数为 4 + 8 + 13 * 3 = 51 层， 从 0 开始的话，最后一层就是第 50 层。</p> 
<h3>8.2 head</h3> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/96/1e/okCZrWY7_o.png"></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/2c/f0/N7E97R0f_o.png"></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/74/f0/M6d9yep1_o.png"></p> 
<p>YOLOV7 head 其实就是一个 **pafpn** 的结构，和之前的YOLOV4，YOLOV5 一样。首先，对于 backbone 最后输出的 32 倍降采样特征图 C5，然后经过 SPPCSP，通道数从1024变为512。先按照 top down 和 C4、C3融合，得到 P3、P4 和 P5；再按 bottom-up 去和 P4、P5 做融合。这里基本和 YOLOV5 是一样的，区别在于将 YOLOV5 中的 CSP 模块换成了 ELAN-H 模块， 同时下采样变为了 MP2 层。</p> 
<p>ELAN-H 模块是我自己命名的，它和 backbone 中的 ELAN 稍微有点区别就是 cat 的数量不同。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/38/ea/Y3k3SXMs_o.png"></p> 
<p>对于 **pafpn** 输出的 P3、P4 和 P5 ， 经过 RepConv 调整通道数，最后使用 1x1 卷积去预测 objectness、class 和 bbox 三部分。</p> 
<p>RepConv 在训练和推理是有一定的区别。训练时有三个分支的相加输出，部署时会将分支的参数重参数化到主分支上。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/49/63/arcaly5V_o.png"></p> 
<h3>8.3 loss function</h3> 
<p>主要分带和不带辅助训练头两种，对应的训练脚本是train.py 和 train_aux.py。</p> 
<p>    <strong>不带辅助训练头（分损失函数和匹配策略两部分讨论）。</strong><br>     <strong>损失函数</strong></p> 
<p>    整体和YOLOV5 保持一致，分为坐标损失、目标置信度损失（GT就是训练阶段的普通iou）和分类损失三部分。其中目标置信度损失和分类损失采用BCEWithLogitsLoss（带log的二值交叉熵损失），坐标损失采用CIoU损失。详细参见utils/loss.py 里面的 ComputeLossOTA 函数 配合 配置文件里的各部分的权重设置。</p> 
<p>  <strong>  匹配策略</strong></p> 
<p>   主要是参考了YOLOV5 和YOLOV6使用的当下比较火的simOTA.</p> 
<p> S1.训练前，会基于训练集中gt框，通过k-means聚类算法，先验获得9个从小到大排列的anchor框。(可选)</p> 
<p>S2.将每个gt与9个anchor匹配：Yolov5为分别计算它与9种anchor的宽与宽的比值（较大的宽除以较小的宽，比值大于1，下面的高同样操作）、高与高的比值，在宽比值、高比值这2个比值中，取最大的一个比值，若这个比值小于设定的比值阈值，这个anchor的预测框就被称为正样本。一个gt可能与几个anchor均能匹配上（此时最大9个）。所以一个gt可能在不同的网络层上做预测训练，大大增加了正样本的数量，当然也会出现gt与所有anchor都匹配不上的情况，这样gt就会被当成背景，不参与训练，说明anchor框尺寸设计的不好。</p> 
<p>S3.扩充正样本。根据gt框的中心位置，将最近的2个邻域网格也作为预测网格，也即一个groundtruth框可以由3个网格来预测；可以发现粗略估计正样本数相比前yolo系列，增加了三倍（此时最大27个匹配）。图下图浅黄色区域，其中实线是YOLO的真实网格，虚线是将一个网格四等分，如这个例子中，GT的中心在右下虚线网格，则扩充右和下真实网格也作为正样本。</p> 
<p> S4.获取与当前gt有top10最大iou的prediction结果。将这top10 （5-15之间均可，并不敏感）iou进行sum，就为当前gt的k。k最小取1。</p> 
<p> S5.根据损失函数计算每个GT和候选anchor损失（前期会加大分类损失权重，后面减低分类损失权重，如1:5-&gt;1:3），并保留损失最小的前K个。</p> 
<p> S6.去掉同一个anchor被分配到多个GT的情况。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/e2/e7/s9znrFSp_o.png"></p> 
<h4>带辅助训练头（分损失函数和匹配策略两部分讨论）。</h4> 
<p>    论文中，将负责最终输出的Head为lead Head，将用于辅助训练的Head称为auxiliary Head。本博客不重点讨论，原因是论文中后面的结构实验实现提升比较有限（0.3个点），具体可以看原文。</p> 
<p class="img-center"><img alt="" height="370" src="https://images2.imgbox.com/ec/93/ICgss0Hm_o.png" width="1200"></p> 
<p>一些细节：其loss函数和不带辅助头相同，加权系数不能过大（aux head loss 和lead head loss 按照0.25:1的比例），否则会导致lead head出来的结果精度变低。匹配策略和上面的不带辅助头（只有lead head）只有很少不同，其中辅助头：</p> 
<p>    *lead head中每个网格与gt如果匹配上，附加周边两个网格，而aux head附加4个网格（如上面导数第二幅图，匹配到浅黄+橘黄共5个网格）。</p> 
<p>    *lead head中将top10个样本iou求和取整，而aux head中取top20。</p> 
<p>    aux head更关注于recall，而lead head从aux head中精准筛选出样本。</p> 
<p>    需要注意依照yolov5中的中心点回归方式，仅能将图中红色特征grid，预测在图中红色+蓝色区域（实线组成的网格代表着特征图grid，虚线代表着一个grid分成了4个象限），是根本无法将中心点预测到gt处(蓝色点)！而该红色特征grid在训练时是会作为正样本的。在aux head中，模型也并没有针对这种情况对回归方式作出更改。所以其实在aux head中，即使被分配为正样本的区域，经过不断的学习，可能仍然无法完全拟合至效果特别好。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/99/19/LqlhB69A_o.png"></p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p>参考文献：</p> 
<p><a href="https://blog.csdn.net/leviopku/article/details/82660381" title="yolo系列之yolo v3【深度解析】_木盏的博客-CSDN博客_yolov3">yolo系列之yolo v3【深度解析】_木盏的博客-CSDN博客_yolov3</a></p> 
<p><a href="https://blog.csdn.net/x454045816/article/details/109759989" title="目标检测算法YOLOv4详解_智能算法的博客-CSDN博客">目标检测算法YOLOv4详解_智能算法的博客-CSDN博客</a></p> 
<p><a href="https://blog.csdn.net/x454045816/article/details/109759989" title="目标检测算法YOLOv4详解_智能算法的博客-CSDN博客">目标检测算法YOLOv4详解_智能算法的博客-CSDN博客</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/172121380" rel="nofollow" title="深入浅出Yolo系列之Yolov5核心基础知识完整讲解 - 知乎">深入浅出Yolo系列之Yolov5核心基础知识完整讲解 - 知乎</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/143747206?from=timeline" rel="nofollow" title="深入浅出Yolo系列之Yolov3&amp;Yolov4&amp;Yolov5&amp;Yolox核心基础知识完整讲解 - 知乎">深入浅出Yolo系列之Yolov3&amp;Yolov4&amp;Yolov5&amp;Yolox核心基础知识完整讲解 - 知乎</a></p> 
<p><a href="https://blog.csdn.net/App_12062011/article/details/77554288" title="目标检测（九）--YOLO v1,v2,v3_Eason.wxd的博客-CSDN博客_[6].目标检测(九)–yolo v1,v2,v3">目标检测（九）--YOLO v1,v2,v3_Eason.wxd的博客-CSDN博客_[6].目标检测(九)–yolo v1,v2,v3</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/161083602" rel="nofollow" title="一文读懂YOLO V5 与 YOLO V4 - 知乎">一文读懂YOLO V5 与 YOLO V4 - 知乎</a></p> 
<p><a href="https://mp.weixin.qq.com/s/RrQCP4pTSwpTmSgvly9evg" rel="nofollow" title="YOLOv6：又快又准的目标检测框架开源啦">YOLOv6：又快又准的目标检测框架开源啦</a></p> 
<p><a href="https://blog.csdn.net/u012863603/article/details/126118799" title="目标检测算法——YOLOV7——详解_TigerZ*的博客-CSDN博客">目标检测算法——YOLOV7——详解_TigerZ*的博客-CSDN博客</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/543743278" rel="nofollow" title="深入浅出 Yolo 系列之 Yolov7 基础网络结构详解 - 知乎">深入浅出 Yolo 系列之 Yolov7 基础网络结构详解 - 知乎</a></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/093f117878e9995271a548ec7901f4eb/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">git之工作区暂存区和仓库区</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2840290f96349ad38447fed3b39d8362/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【ESP32&#43;freeRTOS学习笔记-(九)事件组】</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>