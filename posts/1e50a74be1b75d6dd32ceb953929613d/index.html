<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>今日arXiv精选 | 9篇ICCV 2021最新论文 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="今日arXiv精选 | 9篇ICCV 2021最新论文" />
<meta property="og:description" content="关于 #今日arXiv精选 这是「AI 学术前沿」旗下的一档栏目，编辑将每日从arXiv中精选高质量论文，推送给读者。
The Power of Points for Modeling Humans in Clothing
Comment: In ICCV 2021. Project page: https://qianlim.github.io/POP
Link: http://arxiv.org/abs/2109.01137
Abstract
Currently it requires an artist to create 3D human avatars with realisticclothing that can move naturally. Despite progress on 3D scanning and modelingof human bodies, there is still no technology that can easily turn a staticscan into an animatable avatar. Automating the creation of such avatars wouldenable many applications in games, social networking, animation, and AR/VR toname a few." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/1e50a74be1b75d6dd32ceb953929613d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-09-03T19:44:01+08:00" />
<meta property="article:modified_time" content="2021-09-03T19:44:01+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">今日arXiv精选 | 9篇ICCV 2021最新论文</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p><strong><img src="https://images2.imgbox.com/04/a2/ZqaTmzXt_o.png"></strong></p> 
 <p><strong> 关于 #今日arXiv精选 </strong><br></p> 
 <p>这是「AI 学术前沿」旗下的一档栏目，编辑将每日从arXiv中精选高质量论文，推送给读者。</p> 
 <p style="text-align: left"><strong>The Power of Points for Modeling Humans in Clothing</strong></p> 
 <p style="text-align: left"><strong>Comment</strong>: In ICCV 2021. Project page: https://qianlim.github.io/POP</p> 
 <p style="text-align: left"><strong>Link</strong>: http://arxiv.org/abs/2109.01137</p> 
 <p style="text-align: left"><strong>Abstract</strong></p> 
 <p style="text-align: left">Currently it requires an artist to create 3D human avatars with realisticclothing that can move naturally. Despite progress on 3D scanning and modelingof human bodies, there is still no technology that can easily turn a staticscan into an animatable avatar. Automating the creation of such avatars wouldenable many applications in games, social networking, animation, and AR/VR toname a few. The key problem is one of representation. Standard 3D meshes arewidely used in modeling the minimally-clothed body but do not readily capturethe complex topology of clothing. Recent interest has shifted to implicitsurface models for this task but they are computationally heavy and lackcompatibility with existing 3D tools. What is needed is a 3D representationthat can capture varied topology at high resolution and that can be learnedfrom data. We argue that this representation has been with us all along -- thepoint cloud. Point clouds have properties of both implicit and explicitrepresentations that we exploit to model 3D garment geometry on a human body.We train a neural network with a novel local clothing geometric feature torepresent the shape of different outfits. The network is trained from 3D pointclouds of many types of clothing, on many bodies, in many poses, and learns tomodel pose-dependent clothing deformations. The geometry feature can beoptimized to fit a previously unseen scan of a person in clothing, enabling thescan to be reposed realistically. Our model demonstrates superior quantitativeand qualitative results in both multi-outfit modeling and unseen outfitanimation. The code is available for research purposes.</p> 
 <p style="text-align: left"><strong>NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo</strong></p> 
 <p style="text-align: left"><strong>Comment</strong>: To appear in ICCV 2021 (Oral). Project page:  https://weiyithu.github.io/NerfingMVS/</p> 
 <p style="text-align: left"><strong>Link</strong>: http://arxiv.org/abs/2109.01129</p> 
 <p style="text-align: left"><strong>Abstract</strong></p> 
 <p style="text-align: left">In this work, we present a new multi-view depth estimation method thatutilizes both conventional SfM reconstruction and learning-based priors overthe recently proposed neural radiance fields (NeRF). Unlike existing neuralnetwork based optimization method that relies on estimated correspondences, ourmethod directly optimizes over implicit volumes, eliminating the challengingstep of matching pixels in indoor scenes. The key to our approach is to utilizethe learning-based priors to guide the optimization process of NeRF. Our systemfirstly adapts a monocular depth network over the target scene by finetuning onits sparse SfM reconstruction. Then, we show that the shape-radiance ambiguityof NeRF still exists in indoor environments and propose to address the issue byemploying the adapted depth priors to monitor the sampling process of volumerendering. Finally, a per-pixel confidence map acquired by error computation onthe rendered image can be used to further improve the depth quality.Experiments show that our proposed framework significantly outperformsstate-of-the-art methods on indoor scenes, with surprising findings presentedon the effectiveness of correspondence-based optimization and NeRF-basedoptimization over the adapted depth priors. In addition, we show that theguided optimization scheme does not sacrifice the original synthesis capabilityof neural radiance fields, improving the rendering quality on both seen andnovel views. Code is available at https://github.com/weiyithu/NerfingMVS.</p> 
 <p style="text-align: left"><strong>The Functional Correspondence Problem</strong></p> 
 <p style="text-align: left"><strong>Comment</strong>: Accepted to ICCV 2021</p> 
 <p style="text-align: left"><strong>Link</strong>: http://arxiv.org/abs/2109.01097</p> 
 <p style="text-align: left"><strong>Abstract</strong></p> 
 <p style="text-align: left">The ability to find correspondences in visual data is the essence of mostcomputer vision tasks. But what are the right correspondences? The task ofvisual correspondence is well defined for two different images of same objectinstance. In case of two images of objects belonging to same category, visualcorrespondence is reasonably well-defined in most cases. But what aboutcorrespondence between two objects of completely different category -- e.g., ashoe and a bottle? Does there exist any correspondence? Inspired by humans'ability to: (a) generalize beyond semantic categories and; (b) infer functionalaffordances, we introduce the problem of functional correspondences in thispaper. Given images of two objects, we ask a simple question: what is the setof correspondences between these two images for a given task? For example, whatare the correspondences between a bottle and shoe for the task of pounding orthe task of pouring. We introduce a new dataset: FunKPoint that has groundtruth correspondences for 10 tasks and 20 object categories. We also introducea modular task-driven representation for attacking this problem and demonstratethat our learned representation is effective for this task. But mostimportantly, because our supervision signal is not bound by semantics, we showthat our learned representation can generalize better on few-shotclassification problem. We hope this paper will inspire our community to thinkbeyond semantics and focus more on cross-category generalization and learningrepresentations for robotics tasks.</p> 
 <p style="text-align: left"><strong>SLIDE: Single Image 3D Photography with Soft Layering and Depth-aware Inpainting</strong></p> 
 <p style="text-align: left"><strong>Comment</strong>: ICCV 2021 (Oral); Project page: https://varunjampani.github.io/slide  ; Video: https://www.youtube.com/watch?v=RQio7q-ueY8</p> 
 <p style="text-align: left"><strong>Link</strong>: http://arxiv.org/abs/2109.01068</p> 
 <p style="text-align: left"><strong>Abstract</strong></p> 
 <p style="text-align: left">Single image 3D photography enables viewers to view a still image from novelviewpoints. Recent approaches combine monocular depth networks with inpaintingnetworks to achieve compelling results. A drawback of these techniques is theuse of hard depth layering, making them unable to model intricate appearancedetails such as thin hair-like structures. We present SLIDE, a modular andunified system for single image 3D photography that uses a simple yet effectivesoft layering strategy to better preserve appearance details in novel views. Inaddition, we propose a novel depth-aware training strategy for our inpaintingmodule, better suited for the 3D photography task. The resulting SLIDE approachis modular, enabling the use of other components such as segmentation andmatting for improved layering. At the same time, SLIDE uses an efficientlayered depth formulation that only requires a single forward pass through thecomponent networks to produce high quality 3D photos. Extensive experimentalanalysis on three view-synthesis datasets, in combination with user studies onin-the-wild image collections, demonstrate superior performance of ourtechnique in comparison to existing strong baselines while being conceptuallymuch simpler. Project page: https://varunjampani.github.io/slide</p> 
 <p style="text-align: left"><strong>4D-Net for Learned Multi-Modal Alignment</strong></p> 
 <p style="text-align: left"><strong>Comment</strong>: ICCV 2021</p> 
 <p style="text-align: left"><strong>Link</strong>: http://arxiv.org/abs/2109.01066</p> 
 <p style="text-align: left"><strong>Abstract</strong></p> 
 <p style="text-align: left">We present 4D-Net, a 3D object detection approach, which utilizes 3D PointCloud and RGB sensing information, both in time. We are able to incorporate the4D information by performing a novel dynamic connection learning across variousfeature representations and levels of abstraction, as well as by observinggeometric constraints. Our approach outperforms the state-of-the-art and strongbaselines on the Waymo Open Dataset. 4D-Net is better able to use motion cuesand dense image information to detect distant objects more successfully.</p> 
 <p style="text-align: left"><strong>Adversarial Robustness for Unsupervised Domain Adaptation</strong></p> 
 <p style="text-align: left"><strong>Comment</strong>: Accepted by ICCV 2021</p> 
 <p style="text-align: left"><strong>Link</strong>: http://arxiv.org/abs/2109.00946</p> 
 <p style="text-align: left"><strong>Abstract</strong></p> 
 <p style="text-align: left">Extensive Unsupervised Domain Adaptation (UDA) studies have shown greatsuccess in practice by learning transferable representations across a labeledsource domain and an unlabeled target domain with deep models. However,previous works focus on improving the generalization ability of UDA models onclean examples without considering the adversarial robustness, which is crucialin real-world applications. Conventional adversarial training methods are notsuitable for the adversarial robustness on the unlabeled target domain of UDAsince they train models with adversarial examples generated by the supervisedloss function. In this work, we leverage intermediate representations learnedby multiple robust ImageNet models to improve the robustness of UDA models. Ourmethod works by aligning the features of the UDA model with the robust featureslearned by ImageNet pre-trained models along with domain adaptation training.It utilizes both labeled and unlabeled domains and instills robustness withoutany adversarial intervention or label requirement during domain adaptationtraining. Experimental results show that our method significantly improvesadversarial robustness compared to the baseline while keeping clean accuracy onvarious UDA benchmarks.</p> 
 <p style="text-align: left"><strong>Generative Models for Multi-Illumination Color Constancy</strong></p> 
 <p style="text-align: left"><strong>Comment</strong>: Accepted in International Conference on Computer Vision Workshop  (ICCVW) 2021</p> 
 <p style="text-align: left"><strong>Link</strong>: http://arxiv.org/abs/2109.00863</p> 
 <p style="text-align: left"><strong>Abstract</strong></p> 
 <p style="text-align: left">In this paper, the aim is multi-illumination color constancy. However, mostof the existing color constancy methods are designed for single light sources.Furthermore, datasets for learning multiple illumination color constancy arelargely missing. We propose a seed (physics driven) based multi-illuminationcolor constancy method. GANs are exploited to model the illumination estimationproblem as an image-to-image domain translation problem. Additionally, a novelmulti-illumination data augmentation method is proposed. Experiments on singleand multi-illumination datasets show that our methods outperform sota methods.</p> 
 <p style="text-align: left"><strong>SlowFast Rolling-Unrolling LSTMs for Action Anticipation in Egocentric Videos</strong></p> 
 <p style="text-align: left"><strong>Comment</strong>: Accepted to EPIC@ICCV 2021</p> 
 <p style="text-align: left"><strong>Link</strong>: http://arxiv.org/abs/2109.00829</p> 
 <p style="text-align: left"><strong>Abstract</strong></p> 
 <p style="text-align: left">Action anticipation in egocentric videos is a difficult task due to theinherently multi-modal nature of human actions. Additionally, some actionshappen faster or slower than others depending on the actor or surroundingcontext which could vary each time and lead to different predictions. Based onthis idea, we build upon RULSTM architecture, which is specifically designedfor anticipating human actions, and propose a novel attention-based techniqueto evaluate, simultaneously, slow and fast features extracted from threedifferent modalities, namely RGB, optical flow, and extracted objects. Twobranches process information at different time scales, i.e., frame-rates, andseveral fusion schemes are considered to improve prediction accuracy. Weperform extensive experiments on EpicKitchens-55 and EGTEA Gaze+ datasets, anddemonstrate that our technique systematically improves the results of RULSTMarchitecture for Top-5 accuracy metric at different anticipation times.</p> 
 <p style="text-align: left"><strong>Self-Calibrating Neural Radiance Fields</strong></p> 
 <p style="text-align: left"><strong>Comment</strong>: Accepted in ICCV21, Project Page:  https://postech-cvlab.github.io/SCNeRF/</p> 
 <p style="text-align: left"><strong>Link</strong>: http://arxiv.org/abs/2108.13826</p> 
 <p style="text-align: left"><strong>Abstract</strong></p> 
 <p style="text-align: left">In this work, we propose a camera self-calibration algorithm for genericcameras with arbitrary non-linear distortions. We jointly learn the geometry ofthe scene and the accurate camera parameters without any calibration objects.Our camera model consists of a pinhole model, a fourth order radial distortion,and a generic noise model that can learn arbitrary non-linear cameradistortions. While traditional self-calibration algorithms mostly rely ongeometric constraints, we additionally incorporate photometric consistency.This requires learning the geometry of the scene, and we use Neural RadianceFields (NeRF). We also propose a new geometric loss function, viz., projectedray distance loss, to incorporate geometric consistency for complex non-linearcamera models. We validate our approach on standard real image datasets anddemonstrate that our model can learn the camera intrinsics and extrinsics(pose) from scratch without COLMAP initialization. Also, we show that learningaccurate camera models in a differentiable manner allows us to improve PSNRover baselines. Our module is an easy-to-use plugin that can be applied to NeRFvariants to improve performance. The code and data are currently available athttps://github.com/POSTECH-CVLab/SCNeRF.</p> 
 <p>·</p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ede3fe3aee8e84da0e337183217a6a45/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Pytorch加载保存好的模型发现与实际保存模型的参数不一致</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/509425528fabcadd37a2b3c0c88ca922/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">解决idea没有显示maven的问题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>