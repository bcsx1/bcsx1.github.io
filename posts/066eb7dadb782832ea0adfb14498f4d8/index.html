<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>YOLO系列算法 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="YOLO系列算法" />
<meta property="og:description" content="目录 YOLO系列算法yolo算法Yolo算法思想Yolo的网络结构网络输入网络输出7X7网格30维向量 Yolo模型的训练训练样本的构建损失函数模型训练 模型预测yolo总结 yoloV2预测更准确（better）batch normalization使用高分辨率图像微调分类模型采用Anchor Boxes聚类提取anchor尺度边框位置的预测细粒度特征融合多尺度训练 速度更快（Faster）识别对象更多 yoloV3算法简介多尺度检测网络模型结构先验框logistic回归yoloV3模型的输入与输出 yoloV4总结 YOLO系列算法 学习目标
知道yolo网络架构，理解其输入输出知道yolo模型的训练样本构建的方法理解yolo模型的损失函数知道yoloV2模型的改进方法知道yoloV3的多尺度检测方法知道yoloV3模型的网络结构及网络输出了解yoloV3模型先验框设计的方法知道yoloV3模型为什么适用于多标签的目标分类了解yoloV4模型 YOLO系列算法是一类典型的one-stage目标检测算法，其利用anchor box将分类与目标定位的回归问题结合起来，从而做到了高效、灵活和泛化性能好，所以在工业界也十分受欢迎，接下来我们介绍YOLO 系列算法。
yolo算法 Yolo算法采用一个单独的CNN模型实现end-to-end的目标检测，核心思想就是利用整张图作为网络的输入，直接在输出层回归 bounding box（边界框） 的位置及其所属的类别，整个系统如下图所示：
首先将输入图片resize到448x448，然后送入CNN网络，最后处理网络预测结果得到检测的目标。相比R-CNN算法，其是一个统一的框架，其速度更快。
Yolo算法思想 在介绍Yolo算法之前，我们回忆下RCNN模型，RCNN模型提出了候选区(Region Proposals)的方法，先从图片中搜索出一些可能存在对象的候选区（Selective Search），大概2000个左右，然后对每个候选区进行对象识别，但处理速度较慢。
Yolo意思是You Only Look Once，它并没有真正的去掉候选区域，而是创造性的将候选区和目标分类合二为一，看一眼图片就能知道有哪些对象以及它们的位置。
Yolo模型采用预定义预测区域的方法来完成目标检测，具体而言是将原始图像划分为 7x7=49 个网格（grid），每个网格允许预测出2个边框（bounding box，包含某个对象的矩形框），总共 49x2=98 个bounding box。我们将其理解为98个预测区，很粗略的覆盖了图片的整个区域，就在这98个预测区中进行目标检测。
只要得到这98个区域的目标分类和回归结果，再进行NMS就可以得到最终的目标检测结果。那具体要怎样实现呢？
Yolo的网络结构 YOLO的结构非常简单，就是单纯的卷积、池化最后加了两层全连接，从网络结构上看，与前面介绍的CNN分类网络没有本质的区别，最大的差异是输出层用线性函数做激活函数，因为需要预测bounding box的位置（数值型），而不仅仅是对象的概率。所以粗略来说，YOLO的整个结构就是输入图片经过神经网络的变换得到一个输出的张量，如下图所示：
网络结构比较简单，重点是我们要理解网络输入与输出之间的关系。
网络输入 网络的输入是原始图像，唯一的要求是缩放到448x448的大小。主要是因为Yolo的网络中，卷积层最后接了两个全连接层，全连接层是要求固定大小的向量作为输入，所以Yolo的输入图像的大小固定为448x448。
网络输出 网络的输出就是一个7x7x30 的张量（tensor）。那这个输出结果我们要怎么理解那？
7X7网格 根据YOLO的设计，输入图像被划分为 7x7 的网格（grid），输出张量中的 7x7 就对应着输入图像的 7x7 网格。或者我们把 7x7x30 的张量看作 7x7=49个30维的向量，也就是输入图像中的每个网格对应输出一个30维的向量。如下图所示，比如输入图像左上角的网格对应到输出张量中左上角的向量。
30维向量 30维的向量包含：2个bbox的位置和置信度以及该网格属于20个类别的概率
2个bounding box的位置 每个bounding box需要4个数值来表示其位置，(Center_x,Center_y,width,height)，即(bounding box的中心点的x坐标，y坐标，bounding box的宽度，高度)，2个bounding box共需要8个数值来表示其位置。2个bounding box的置信度 bounding box的置信度 = 该bounding box内存在对象的概率 * 该bounding box与该对象实际bounding box的IOU，用公式表示就是： Pr(Object)是bounding box内存在对象的概率" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/066eb7dadb782832ea0adfb14498f4d8/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-03-03T11:09:57+08:00" />
<meta property="article:modified_time" content="2022-03-03T11:09:57+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">YOLO系列算法</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#YOLO_2" rel="nofollow">YOLO系列算法</a></li><li><ul><li><a href="#yolo_21" rel="nofollow">yolo算法</a></li><li><ul><li><a href="#Yolo_30" rel="nofollow">Yolo算法思想</a></li><li><a href="#Yolo_46" rel="nofollow">Yolo的网络结构</a></li><li><ul><li><a href="#_55" rel="nofollow">网络输入</a></li><li><a href="#_59" rel="nofollow">网络输出</a></li><li><ul><li><a href="#7X7_63" rel="nofollow">7X7网格</a></li><li><a href="#30_70" rel="nofollow">30维向量</a></li></ul> 
    </li></ul> 
    </li><li><a href="#Yolo_89" rel="nofollow">Yolo模型的训练</a></li><li><ul><li><a href="#_93" rel="nofollow">训练样本的构建</a></li><li><a href="#_122" rel="nofollow">损失函数</a></li><li><a href="#_139" rel="nofollow">模型训练</a></li></ul> 
    </li><li><a href="#_145" rel="nofollow">模型预测</a></li><li><a href="#yolo_149" rel="nofollow">yolo总结</a></li></ul> 
   </li><li><a href="#yoloV2_161" rel="nofollow">yoloV2</a></li><li><ul><li><a href="#better_165" rel="nofollow">预测更准确（better）</a></li><li><ul><li><a href="#batch_normalization_170" rel="nofollow">batch normalization</a></li><li><a href="#_174" rel="nofollow">使用高分辨率图像微调分类模型</a></li><li><a href="#Anchor_Boxes_188" rel="nofollow">采用Anchor Boxes</a></li><li><a href="#anchor_192" rel="nofollow">聚类提取anchor尺度</a></li><li><a href="#_201" rel="nofollow">边框位置的预测</a></li><li><a href="#_239" rel="nofollow">细粒度特征融合</a></li><li><a href="#_253" rel="nofollow">多尺度训练</a></li></ul> 
    </li><li><a href="#Faster_260" rel="nofollow">速度更快（Faster）</a></li><li><a href="#_272" rel="nofollow">识别对象更多</a></li></ul> 
   </li><li><a href="#yoloV3_276" rel="nofollow">yoloV3</a></li><li><ul><li><a href="#_280" rel="nofollow">算法简介</a></li><li><a href="#_292" rel="nofollow">多尺度检测</a></li><li><a href="#_309" rel="nofollow">网络模型结构</a></li><li><a href="#_337" rel="nofollow">先验框</a></li><li><a href="#logistic_351" rel="nofollow">logistic回归</a></li><li><a href="#yoloV3_358" rel="nofollow">yoloV3模型的输入与输出</a></li></ul> 
   </li><li><a href="#yoloV4_371" rel="nofollow">yoloV4</a></li><li><a href="#_399" rel="nofollow">总结</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="YOLO_2"></a>YOLO系列算法</h2> 
<p><strong>学习目标</strong></p> 
<ul><li>知道yolo网络架构，理解其输入输出</li><li>知道yolo模型的训练样本构建的方法</li><li>理解yolo模型的损失函数</li><li>知道yoloV2模型的改进方法</li><li>知道yoloV3的多尺度检测方法</li><li>知道yoloV3模型的网络结构及网络输出</li><li>了解yoloV3模型先验框设计的方法</li><li>知道yoloV3模型为什么适用于多标签的目标分类</li><li>了解yoloV4模型</li></ul> 
<p><img src="https://images2.imgbox.com/a6/fa/63UD0i1H_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-u1IVdIYj-1646276606890)(笔记图片/image-20200915142921616.png)]"></p> 
<p>YOLO系列算法是一类典型的one-stage目标检测算法，其利用anchor box将分类与目标定位的回归问题结合起来，从而做到了高效、灵活和泛化性能好，所以在工业界也十分受欢迎，接下来我们介绍YOLO 系列算法。</p> 
<h3><a id="yolo_21"></a>yolo算法</h3> 
<p>Yolo算法采用一个单独的CNN模型实现end-to-end的目标检测，核心思想就是利用整张图作为网络的输入，直接在输出层回归 bounding box（边界框） 的位置及其所属的类别，整个系统如下图所示：</p> 
<p><img src="https://images2.imgbox.com/31/29/8DKXfun3_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-W7Rgq5YO-1646276606891)(笔记图片/image-20200915144129736.png)]"></p> 
<p>首先将输入图片resize到448x448，然后送入CNN网络，最后处理网络预测结果得到检测的目标。相比R-CNN算法，其是一个统一的框架，其速度更快。</p> 
<h4><a id="Yolo_30"></a>Yolo算法思想</h4> 
<p>在介绍Yolo算法之前，我们回忆下RCNN模型，RCNN模型提出了候选区(Region Proposals)的方法，先从图片中搜索出一些可能存在对象的候选区（Selective Search），大概2000个左右，然后对每个候选区进行对象识别，但处理速度较慢。</p> 
<p><img src="https://images2.imgbox.com/19/4e/nGdUAfVD_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-uIVmkMmT-1646276606892)(笔记图片/image-20200915150333995.png)]"></p> 
<p>Yolo意思是You Only Look Once，它并没有真正的去掉候选区域，而是创造性的将候选区和目标分类合二为一，看一眼图片就能知道有哪些对象以及它们的位置。</p> 
<p>Yolo模型采用预定义预测区域的方法来完成目标检测，具体而言是将原始图像划分为 7x7=49 个网格（grid），每个网格允许预测出2个边框（bounding box，包含某个对象的矩形框），总共 49x2=98 个bounding box。我们将其理解为98个预测区，很粗略的覆盖了图片的整个区域，就在这98个预测区中进行目标检测。</p> 
<p><img src="https://images2.imgbox.com/90/63/rSkXCerF_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-pKAVNy6q-1646276606892)(笔记图片/image-20200915150718666.png)]"></p> 
<p>只要得到这98个区域的目标分类和回归结果，再进行NMS就可以得到最终的目标检测结果。那具体要怎样实现呢？</p> 
<h4><a id="Yolo_46"></a>Yolo的网络结构</h4> 
<p>YOLO的结构非常简单，就是单纯的卷积、池化最后加了两层全连接，从网络结构上看，与前面介绍的CNN分类网络没有本质的区别，最大的差异是输出层用线性函数做激活函数，因为需要预测bounding box的位置（数值型），而不仅仅是对象的概率。所以粗略来说，YOLO的整个结构就是输入图片经过神经网络的变换得到一个输出的张量，如下图所示：</p> 
<p><img src="https://images2.imgbox.com/44/35/oCx0v6TM_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-5IrhY9yq-1646276606893)(笔记图片/image-20200915151948836.png)]"></p> 
<p>网络结构比较简单，重点是我们要理解网络输入与输出之间的关系。</p> 
<h5><a id="_55"></a>网络输入</h5> 
<p>网络的输入是原始图像，唯一的要求是缩放到448x448的大小。主要是因为Yolo的网络中，卷积层最后接了两个全连接层，全连接层是要求固定大小的向量作为输入，所以Yolo的输入图像的大小固定为448x448。</p> 
<h5><a id="_59"></a>网络输出</h5> 
<p>网络的输出就是一个7x7x30 的张量（tensor）。那这个输出结果我们要怎么理解那？</p> 
<h6><a id="7X7_63"></a>7X7网格</h6> 
<p>根据YOLO的设计，输入图像被划分为 7x7 的网格（grid），输出张量中的 7x7 就对应着输入图像的 7x7 网格。或者我们把 7x7x30 的张量看作 7x7=49个30维的向量，也就是输入图像中的每个网格对应输出一个30维的向量。如下图所示，比如输入图像左上角的网格对应到输出张量中左上角的向量。</p> 
<p><img src="https://images2.imgbox.com/1d/68/smA47EGO_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-Opjy0IEm-1646276606893)(笔记图片/image-20200915152825629.png)]"></p> 
<h6><a id="30_70"></a>30维向量</h6> 
<p>30维的向量包含：2个bbox的位置和置信度以及该网格属于20个类别的概率</p> 
<p><img src="https://images2.imgbox.com/21/f0/pn48TtOS_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-dvHTUC0q-1646276606894)(笔记图片/image-20200915153123684.png)]"></p> 
<ul><li><strong>2个bounding box的位置</strong> 每个bounding box需要4个数值来表示其位置，(Center_x,Center_y,width,height)，即(bounding box的中心点的x坐标，y坐标，bounding box的宽度，高度)，2个bounding box共需要8个数值来表示其位置。</li><li><strong>2个bounding box的置信度</strong> bounding box的置信度 = 该bounding box内存在对象的概率 * 该bounding box与该对象实际bounding box的IOU，用公式表示就是：</li></ul> 
<p><img src="https://images2.imgbox.com/f6/51/G10ndUZ9_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-yCA8yb7G-1646276606894)(笔记图片/image-20200915153543735.png)]"></p> 
<p>Pr(Object)是bounding box内存在对象的概率</p> 
<ul><li><strong>20个对象分类的概率</strong></li></ul> 
<p>Yolo支持识别20种不同的对象（人、鸟、猫、汽车、椅子等），所以这里有20个值表示该网格位置存在任一种对象的概率.</p> 
<h4><a id="Yolo_89"></a>Yolo模型的训练</h4> 
<p>在进行模型训练时，我们需要构造训练样本和设计损失函数，才能利用梯度下降对网络进行训练。</p> 
<h5><a id="_93"></a>训练样本的构建</h5> 
<p>将一幅图片输入到yolo模型中，对应的输出是一个7x7x30张量，构建标签label时对于原图像中的每一个网格grid都需要构建一个30维的向量。对照下图我们来构建目标向量：</p> 
<p><img src="https://images2.imgbox.com/79/ac/bJhpdhWT_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-dRlyHzQJ-1646276606895)(笔记图片/image-20200915155204485.png)]"></p> 
<ul><li><strong>20个对象分类的概率</strong></li></ul> 
<p>对于输入图像中的每个对象，先找到其中心点。比如上图中自行车，其中心点在黄色圆点位置，中心点落在黄色网格内，所以这个黄色网格对应的30维向量中，自行车的概率是1，其它对象的概率是0。所有其它48个网格的30维向量中，该自行车的概率都是0。这就是所谓的"中心点所在的网格对预测该对象负责"。狗和汽车的分类概率也是同样的方法填写</p> 
<ul><li><strong>2个bounding box的位置</strong></li></ul> 
<p>训练样本的bbox位置应该填写对象真实的位置bbox，但一个对象对应了2个bounding box，该填哪一个呢？需要根据网络输出的bbox与对象实际bbox的IOU来选择，所以要在训练过程中动态决定到底填哪一个bbox。</p> 
<ul><li><strong>2个bounding box的置信度</strong></li></ul> 
<p>预测置信度的公式为：</p> 
<p><img src="https://images2.imgbox.com/a2/aa/ksnzQQki_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-2TgxHUzV-1646276606896)(笔记图片/image-20200915155812745.png)]"></p> 
<p><em>IOUpredtruth</em>利用网络输出的2个bounding box与对象真实bounding box计算出来。然后看这2个bounding box的IOU，哪个比较大，就由哪个bounding box来负责预测该对象是否存在，即该bounding box的置信度目标值为1，同时对象真实bounding box的位置也就填入该bounding box。另一个不负责预测的bounding box的置信度目标值为0。</p> 
<p>上图中自行车所在的grid对应的结果如下图所示：</p> 
<p><img src="https://images2.imgbox.com/05/7d/vNtrW7CM_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-t5Bd4u8i-1646276606897)(笔记图片/image-20200915160053996.png)]"></p> 
<h5><a id="_122"></a>损失函数</h5> 
<p>损失就是网络实际输出值与样本标签值之间的偏差：</p> 
<p><img src="https://images2.imgbox.com/56/ab/urpXKZ5q_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-Ft5wrUDh-1646276606897)(笔记图片/image-20200915160218266.png)]"></p> 
<p>yolo给出的损失函数：</p> 
<p><img src="https://images2.imgbox.com/71/78/enXEdQvK_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-1xT6CEhL-1646276606898)(笔记图片/image-20200915160632201.png)]"></p> 
<p>注：其中<em>1iobj</em>表示目标是否出现在网格单元i中，<em>1ijobj</em>表示单元格i中的第j个边界框预测器负责该预测，YOLO设置 <em>λcoord=5</em> 来调高位置误差的权重， <em>λnoobj=0.5</em> 即调低不存在对象的bounding box的置信度误差的权重。</p> 
<p><img src="https://images2.imgbox.com/53/24/WSyrz3BU_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-pKMlwwEX-1646276606899)(笔记图片/image-20200915160850102.png)]"></p> 
<h5><a id="_139"></a>模型训练</h5> 
<p>Yolo先使用ImageNet数据集对前20层卷积网络进行预训练，然后使用完整的网络，在PASCAL VOC数据集上进行对象识别和定位的训练。</p> 
<p>Yolo的最后一层采用线性激活函数，其它层都是Leaky ReLU。训练中采用了drop out和数据增强（data augmentation）来防止过拟合.</p> 
<h4><a id="_145"></a>模型预测</h4> 
<p>将图片resize成448x448的大小，送入到yolo网络中，输出一个 7x7x30 的张量（tensor）来表示图片中所有网格包含的对象（概率）以及该对象可能的2个位置（bounding box）和可信程度（置信度）。在采用NMS（Non-maximal suppression，非极大值抑制）算法选出最有可能是目标的结果。</p> 
<h4><a id="yolo_149"></a>yolo总结</h4> 
<p><strong>优点</strong></p> 
<ul><li>速度非常快，处理速度可以达到45fps，其快速版本（网络较小）甚至可以达到155fps。</li><li>训练和预测可以端到端的进行，非常简便。</li></ul> 
<p><strong>缺点</strong></p> 
<ul><li>准确率会打折扣</li><li>对于小目标和靠的很近的目标检测效果并不好</li></ul> 
<h3><a id="yoloV2_161"></a>yoloV2</h3> 
<p>YOLOv2相对v1版本，在继续保持处理速度的基础上，从预测更准确（Better），速度更快（Faster），识别对象更多（Stronger）这三个方面进行了改进。其中识别更多对象也就是扩展到能够检测9000种不同对象，称之为YOLO9000。 下面我们看下yoloV2的都做了哪些改进？</p> 
<h4><a id="better_165"></a>预测更准确（better）</h4> 
<p><img src="https://images2.imgbox.com/c6/7d/kJ5Hpsc2_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-YxQQFlBl-1646276606899)(笔记图片/image-20200915164503590.png)]"></p> 
<h5><a id="batch_normalization_170"></a>batch normalization</h5> 
<p>批标准化有助于解决反向传播过程中的梯度消失和梯度爆炸问题，降低对一些超参数的敏感性，并且每个batch分别进行归一化的时候，起到了一定的正则化效果，从而能够获得更好的收敛速度和收敛效果。在yoloV2中卷积后全部加入Batch Normalization，网络会提升2%的mAP。</p> 
<h5><a id="_174"></a>使用高分辨率图像微调分类模型</h5> 
<p>YOLO v1使用ImageNet的图像分类样本采用 224x224 作为输入，来训练CNN卷积层。然后在训练对象检测时，检测用的图像样本采用更高分辨率的 448x448 的图像作为输入。但这样切换对模型性能有一定影响。</p> 
<p><img src="https://images2.imgbox.com/5a/a8/r48XwJey_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-fW6qh1ic-1646276606900)(笔记图片/image-20200915165005595.png)]"></p> 
<p>YOLOV2在采用 224x224 图像进行分类模型预训练后，再采用 448x448 的高分辨率样本对分类模型进行微调（10个epoch），使网络特征逐渐适应 448x448 的分辨率。然后再使用 448x448 的检测样本进行训练，缓解了分辨率突然切换造成的影响。</p> 
<p><img src="https://images2.imgbox.com/43/89/l0IW5aBI_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-UWAHMKNX-1646276606901)(笔记图片/image-20200915165015860.png)]"></p> 
<p>使用该技巧后网络的mAP提升了约4%。</p> 
<h5><a id="Anchor_Boxes_188"></a>采用Anchor Boxes</h5> 
<p>YOLO1并没有采用先验框，并且每个grid只预测两个bounding box，整个图像98个。YOLO2如果每个grid采用5个先验框，总共有13x13x5=845个先验框。通过引入anchor boxes，使得预测的box数量更多（13x13xn）。</p> 
<h5><a id="anchor_192"></a>聚类提取anchor尺度</h5> 
<p>Faster-rcnn选择的anchor比例都是手动指定的，但是不一定完全适合数据集。YOLO2尝试统计出更符合样本中对象尺寸的先验框，这样就可以减少网络微调先验框到实际位置的难度。YOLO2的做法是对训练集中标注的边框进行聚类分析，以寻找尽可能匹配样本的边框尺寸。</p> 
<p><img src="https://images2.imgbox.com/0a/10/CEp6cbAR_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-D6pwflqE-1646276606901)(笔记图片/image-20200915165616802.png)]"></p> 
<p>YoloV2选择了聚类的五种尺寸最为anchor box。</p> 
<h5><a id="_201"></a>边框位置的预测</h5> 
<p>Yolov2中将边框的结果约束在特定的网格中：</p> 
<p><img src="https://images2.imgbox.com/2f/fa/kYRWnRqL_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-GNolV45x-1646276606902)(笔记图片/image-20200915171150105.png)]"></p> 
<p>其中，</p> 
<p><em>bx,by,bw,bh</em>是预测边框的中心和宽高。Pr(object)∗IOU(b,object)<em>是预测边框的置信度，YOLO1是直接预测置信度的值，这里对预测参数</em>to*进行σ变换后作为置信度的值。 <em>cx,cy</em>是当前网格左上角到图像左上角的距离，要先将网格大小归一化，即令一个网格的宽=1，高=1。 <em>pw,ph</em>是先验框的宽和高。 σ是sigmoid函数。 <em>tx,ty,tw,th,to</em>是要学习的参数，分别用于预测边框的中心和宽高，以及置信度。</p> 
<p>如下图所示：</p> 
<p><img src="https://images2.imgbox.com/02/16/OTy6MJwV_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-YZ1n8CQA-1646276606903)(笔记图片/image-20200915171632888.png)]"></p> 
<p>由于σ函数将 <em>tx,ty</em>约束在(0,1)范围内，预测边框的蓝色中心点被约束在蓝色背景的网格内。约束边框位置使得模型更容易学习，且预测更为稳定。</p> 
<p>假设网络预测值为：</p> 
<p><img src="https://images2.imgbox.com/cf/24/kLRQSMCS_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-0U86C3iJ-1646276606903)(笔记图片/image-20200915171823875.png)]"></p> 
<p>anchor框为：</p> 
<p><img src="https://images2.imgbox.com/22/9c/CLKvxTC1_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-4lPDA1iV-1646276606904)(笔记图片/image-20200915171844683.png)]"></p> 
<p>则目标在特征图中的位置：</p> 
<p><img src="https://images2.imgbox.com/60/43/6b8nlTL5_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-zwPv90Az-1646276606904)(笔记图片/image-20200915171906489.png)]"></p> 
<p>在原图像中的位置：</p> 
<p><img src="https://images2.imgbox.com/22/bd/CFguhnRM_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-EhnvQ4HH-1646276606905)(笔记图片/image-20200915171925122.png)]"></p> 
<h5><a id="_239"></a>细粒度特征融合</h5> 
<p>图像中对象会有大有小，输入图像经过多层网络提取特征，最后输出的特征图中，较小的对象可能特征已经不明显甚至被忽略掉了。为了更好的检测出一些比较小的对象，最后输出的特征图需要保留一些更细节的信息。</p> 
<p>YOLO2引入一种称为passthrough层的方法在特征图中保留一些细节信息。具体来说，就是在最后一个pooling之前，特征图的大小是26x26x512，将其1拆4，直接传递（passthrough）到pooling后（并且又经过一组卷积）的特征图，两者叠加到一起作为输出的特征图。</p> 
<p><img src="https://images2.imgbox.com/c0/b7/GCAp9r1x_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-KZRm3jxs-1646276606906)(笔记图片/image-20200915172541517.png)]"></p> 
<p>具体的拆分方法如下所示：</p> 
<p><img src="https://images2.imgbox.com/d9/42/UGppzA36_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-fkXadG47-1646276606906)(笔记图片/image-20200915172504922.png)]"></p> 
<h5><a id="_253"></a>多尺度训练</h5> 
<p>YOLO2中没有全连接层，可以输入任何尺寸的图像。因为整个网络下采样倍数是32，采用了{320,352,…,608}等10种输入图像的尺寸，这些尺寸的输入图像对应输出的特征图宽和高是{10,11,…19}。训练时每10个batch就随机更换一种尺寸，使网络能够适应各种大小的对象检测。</p> 
<p><img src="https://images2.imgbox.com/ca/83/rBEoocU8_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-JQrIsMnB-1646276606907)(笔记图片/image-20200915172724192.png)]"></p> 
<h4><a id="Faster_260"></a>速度更快（Faster）</h4> 
<p>yoloV2提出了Darknet-19（有19个卷积层和5个MaxPooling层）网络结构作为特征提取网络。DarkNet-19比VGG-16小一些，精度不弱于VGG-16，但浮点运算量减少到约⅕，以保证更快的运算速度。</p> 
<p><img src="https://images2.imgbox.com/af/c0/k7tFpSFW_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-ZYc3R4N9-1646276606908)(笔记图片/image-20200915173047956.png)]"></p> 
<p>yoloV2的网络中只有卷积+pooling，从416x416x3 变换到 13x13x5x25。增加了batch normalization，增加了一个passthrough层，去掉了全连接层，以及采用了5个先验框,网络的输出如下图所示：</p> 
<p><img src="https://images2.imgbox.com/fa/de/HnS2bY46_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-CPmETzdf-1646276606909)(笔记图片/image-20200915173440874.png)]"></p> 
<h4><a id="_272"></a>识别对象更多</h4> 
<p>VOC数据集可以检测20种对象，但实际上对象的种类非常多，只是缺少相应的用于对象检测的训练样本。YOLO2尝试利用ImageNet非常大量的分类样本，联合COCO的对象检测数据集一起训练，使得YOLO2即使没有学过很多对象的检测样本，也能检测出这些对象。</p> 
<h3><a id="yoloV3_276"></a>yoloV3</h3> 
<p>yoloV3以V1，V2为基础进行的改进，主要有：利用多尺度特征进行目标检测；先验框更丰富；调整了网络结构；对象分类使用logistic代替了softmax,更适用于多标签分类任务。</p> 
<h4><a id="_280"></a>算法简介</h4> 
<p>YOLOv3是YOLO (You Only Look Once)系列目标检测算法中的第三版，相比之前的算法，尤其是针对小目标，精度有显著提升。</p> 
<p><img src="https://images2.imgbox.com/ab/7a/zXFP9Ww4_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-8tpBI0LU-1646276606909)(笔记图片/image-20200502103836394.png)]"></p> 
<p>yoloV3的流程如下图所示，对于每一幅输入图像，YOLOv3会预测三个不同尺度的输出，目的是检测出不同大小的目标。</p> 
<p><img src="https://images2.imgbox.com/71/e7/yqrKl9el_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-UuCrrIXI-1646276606909)(笔记图片/image-20200502104048380.png)]"></p> 
<h4><a id="_292"></a>多尺度检测</h4> 
<p>通常一幅图像包含各种不同的物体，并且有大有小。比较理想的是一次就可以将所有大小的物体同时检测出来。因此，网络必须具备能够“看到”不同大小的物体的能力。因为网络越深，特征图就会越小，所以网络越深小的物体也就越难检测出来。</p> 
<p>在实际的feature map中，随着网络深度的加深，浅层的feature map中主要包含低级的信息（物体边缘，颜色，初级位置信息等），深层的feature map中包含高等信息（例如物体的语义信息：狗，猫，汽车等等）。因此在不同级别的feature map对应不同的scale，所以我们可以在不同级别的特征图中进行目标检测。如下图展示了多种scale变换的经典方法。</p> 
<p><img src="https://images2.imgbox.com/71/e4/tT4EVNme_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-0ma9rzl7-1646276606910)(笔记图片/image-20200502104855459.png)]"></p> 
<p>(a) 这种方法首先建立图像金字塔，不同尺度的金字塔图像被输入到对应的网络当中，用于不同scale物体的检测。但这样做的结果就是每个级别的金字塔都需要进行一次处理，速度很慢。</p> 
<p>(b) 检测只在最后一层feature map阶段进行，这个结构无法检测不同大小的物体</p> 
<p>© 对不同深度的feature map分别进行目标检测。SSD中采用的便是这样的结构。这样小的物体会在浅层的feature map中被检测出来，而大的物体会在深层的feature map被检测出来，从而达到对应不同scale的物体的目的，缺点是每一个feature map获得的信息仅来源于之前的层，之后的层的特征信息无法获取并加以利用。</p> 
<p>(d) 与©很接近，但不同的是，当前层的feature map会对未来层的feature map进行上采样，并加以利用。因为有了这样一个结构，当前的feature map就可以获得“未来”层的信息，这样的话低阶特征与高阶特征就有机融合起来了，提升检测精度。在YOLOv3中，就是采用这种方式来实现目标多尺度的变换的。</p> 
<h4><a id="_309"></a>网络模型结构</h4> 
<p>在基本的图像特征提取方面，YOLO3采用了Darknet-53的网络结构（含有53个卷积层），它借鉴了残差网络ResNet的做法，在层之间设置了shortcut，来解决深层网络梯度的问题，shortcut如下图所示：包含两个卷积层和一个shortcut connections。</p> 
<p><img src="https://images2.imgbox.com/6e/2a/uUZTC7ko_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-jutdPg1x-1646276606910)(笔记图片/image-20200502110956252.png)]"></p> 
<p>yoloV3的模型结构如下所示：整个v3结构里面，没有池化层和全连接层，网络的下采样是通过设置卷积的stride为2来达到的，每当通过这个卷积层之后图像的尺寸就会减小到一半。</p> 
<p><img src="https://images2.imgbox.com/82/84/toWy53bZ_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-ild5r13B-1646276606910)(笔记图片/image-20210106152945470.png)]"></p> 
<p>下面我们看下网络结构：</p> 
<ul><li>基本组件：蓝色方框内部分</li></ul> 
<p>1、CBL：Yolov3网络结构中的最小组件，由Conv+Bn+Leaky_relu激活函数三者组成。 2、Res unit：借鉴Resnet网络中的残差结构，让网络可以构建的更深。 3、ResX：由一个CBL和X个残差组件构成，是Yolov3中的大组件。每个Res模块前面的CBL都起到下采样的作用，因此经过5次Res模块后，得到的特征图是416-&gt;208-&gt;104-&gt;52-&gt;26-&gt;13大小。</p> 
<ul><li>其他基础操作：</li></ul> 
<p>1、Concat：张量拼接，会扩充两个张量的维度，例如26×26×256和26×26×512两个张量拼接，结果是26×26×768。</p> 
<p>2、Add：张量相加，张量直接相加，不会扩充维度，例如104×104×128和104×104×128相加，结果还是104×104×128。</p> 
<ul><li>Backbone中卷积层的数量：</li></ul> 
<p>每个ResX中包含1+2×X个卷积层，因此整个主干网络Backbone中一共包含1+（1+2×1）+（1+2×2）+（1+2×8）+（1+2×8）+（1+2×4）=52，再加上一个FC全连接层，即可以组成一个Darknet53分类网络。不过在目标检测Yolov3中，去掉FC层，仍然把Yolov3的主干网络叫做Darknet53结构。</p> 
<h4><a id="_337"></a>先验框</h4> 
<p>yoloV3采用K-means聚类得到先验框的尺寸，为每种尺度设定3种先验框，总共聚类出9种尺寸的先验框。</p> 
<p><img src="https://images2.imgbox.com/a3/a7/tG7QeGwk_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-HDp6JtPa-1646276606911)(笔记图片/image-20200502103458654.png)]"></p> 
<p>在COCO数据集这9个先验框是：(10x13)，(16x30)，(33x23)，(30x61)，(62x45)，(59x119)，(116x90)，(156x198)，(373x326)。在最小的(13x13)特征图上（有最大的感受野）应用较大的先验框(116x90)，(156x198)，(373x326)，适合检测较大的对象。中等的(26x26)特征图上（中等感受野）应用中等的先验框(30x61)，(62x45)，(59x119)，适合检测中等大小的对象。较大的(52x52)特征图上（较小的感受野）应用,其中较小的先验框(10x13)，(16x30)，(33x23)，适合检测较小的对象。</p> 
<p>直观上感受9种先验框的尺寸，下图中蓝色框为聚类得到的先验框。黄色框式ground truth，红框是对象中心点所在的网格。</p> 
<p><img src="https://images2.imgbox.com/19/7f/BOiH5PUd_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-GdVWCyx7-1646276606911)(笔记图片/image-20200502103442569.png)]"></p> 
<h4><a id="logistic_351"></a>logistic回归</h4> 
<p>预测对象类别时不使用softmax，而是被替换为一个1x1的卷积层+logistic激活函数的结构。使用softmax层的时候其实已经假设每个输出仅对应某一个单个的class，但是在某些class存在重叠情况（例如woman和person）的数据集中，使用softmax就不能使网络对数据进行很好的预测。</p> 
<p><img src="https://images2.imgbox.com/3f/e0/WRwDOSs9_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-c20hqyzy-1646276606911)(笔记图片/image-20200502112750112.png)]"></p> 
<h4><a id="yoloV3_358"></a>yoloV3模型的输入与输出</h4> 
<p>YoloV3的输入输出形式如下图所示：</p> 
<p><img src="https://images2.imgbox.com/e6/77/8DSD229p_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-Bf3aFnQa-1646276606912)(笔记图片/image-20200502111721372.png)]"></p> 
<p>输入416×416×3的图像，通过darknet网络得到三种不同尺度的预测结果，每个尺度都对应N个通道，包含着预测的信息；</p> 
<p>每个网格每个尺寸的anchors的预测结果。</p> 
<p>YOLOv3共有13×13×3 + 26×26×3 + 52×52×3个预测 。每个预测对应85维，分别是4（坐标值）、1（置信度分数）、80（coco类别概率）。</p> 
<h3><a id="yoloV4_371"></a>yoloV4</h3> 
<p>YOLO之父在2020年初宣布退出CV界，YOLOv4 的作者并不是YOLO系列 的原作者。YOLO V4是YOLO系列一个重大的更新，其在COCO数据集上的平均精度(AP)和帧率精度(FPS)分别提高了10% 和12%，并得到了Joseph Redmon的官方认可，被认为是当前最强的实时对象检测模型之一。</p> 
<p>yoloV4总结了大部分检测技巧，然后经过筛选，排列组合，挨个实验（ablation study）哪些方法有效，总体来说，Yolov4并没有创造新的改进，而是使用了大量的目标检测的技巧。在这里我们主要给大家看下它的网络架构：</p> 
<p><img src="https://images2.imgbox.com/4e/8b/ECFBg1kO_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-kN8ElNBY-1646276606912)(笔记图片/image-20200915180225300.png)]"></p> 
<p>Yolov4的结构图和Yolov3是相似的，不过使用各种新的算法思想对各个子结构都进行了改进。 先整理下Yolov4的结构组件</p> 
<ul><li>基本组件：</li><li>CBM：Yolov4网络结构中的最小组件，由Conv+Bn+Mish激活函数三者组成。</li><li>CBL：由Conv+Bn+Leaky_relu激活函数三者组成。</li><li>Res unit：借鉴Resnet网络中的残差结构，让网络可以构建的更深。</li><li>CSPX：由三个卷积层和X个Res unint模块Concate组成。</li><li>SPP：采用1×1，5×5，9×9，13×13的最大池化的方式，进行多尺度融合。</li><li>其他基础操作：</li><li>Concat：张量拼接，维度会扩充，和Yolov3中的解释一样，对应于cfg文件中的route操作。</li><li>Add：张量相加，不会扩充维度，对应于cfg文件中的shortcut操作。</li><li>Backbone中卷积层的数量： 每个CSPX中包含3+2×X个卷积层，因此整个主干网络Backbone中一共包含2+（3+2×1）+2+（3+2×2）+2+（3+2×8）+2+（3+2×8）+2+（3+2×4）+1=72。</li></ul> 
<p><strong>注意：</strong></p> 
<p>网络的输入大小不是固定的，在yoloV3中输入默认是416×416，在yoloV4中默认是608×608，在实际项目中也可以根据需要修改，比如320×320，一般是32的倍数。 输入图像的大小和最后的三个特征图的大小也是对应的，比如416×416的输入，最后的三个特征图大小是13×13，26×26，52×52， 如果是608×608，最后的三个特征图大小则是19×19，38×38，76×76。</p> 
<h3><a id="_399"></a>总结</h3> 
<ul><li>知道yolo网络架构，理解其输入输出</li></ul> 
<p>YOLO的整个结构就是输入图片经过神经网络的变换得到一个输出的张量</p> 
<ul><li>知道yolo模型的训练样本构建的方法</li></ul> 
<p>对于原图像中的每一个网格grid都需要构建一个30维的向量：分类，置信度，回归的目标值</p> 
<ul><li>理解yolo模型的损失函数</li></ul> 
<p>损失函数分为3部分：分类损失，回归损失，置信度损失</p> 
<ul><li>知道yoloV2模型的改进方法</li></ul> 
<p>使用了BN层，高分辨率训练，采用Anchorbox，聚类得到anchorbox的尺寸，改进边界框预测的方法，特征融合，多尺度训练，网络模型使用darknet19，利用imagenet数据集识别更多的目标</p> 
<ul><li>yoloV3的多尺度检测方法</li></ul> 
<p>在YOLOv3中采用FPN结构来提高对应多尺度目标检测的精度，当前的feature map利用“未来”层的信息，将低阶特征与高阶特征进行融合，提升检测精度。</p> 
<ul><li>yoloV3模型的网络结构</li></ul> 
<p>以darknet-53为基础，借鉴resnet的思想，在网络中加入了残差模块，利于解决深层次网络的梯度问题</p> 
<p>整个v3结构里面，没有池化层和全连接层，只有卷积层</p> 
<p>网络的下采样是通过设置卷积的stride为2来达到的</p> 
<ul><li>yoloV3模型先验框设计的方法</li></ul> 
<p>采用K-means聚类得到先验框的尺寸，为每种尺度设定3种先验框，总共聚类出9种尺寸的先验框。</p> 
<ul><li>yoloV3模型为什么适用于多标签的目标分类</li></ul> 
<p>预测对象类别时不使用softmax，而是使用logistic的输出进行预测</p> 
<ul><li>yoloV3模型的输入输出</li></ul> 
<p>对于416×416×3的输入图像，在每个尺度的特征图的每个网格设置3个先验框，总共有 13×13×3 + 26×26×3 + 52×52×3 = 10647 个预测。每一个预测是一个(4+1+80)=85维向量，这个85维向量包含边框坐标（4个数值），边框置信度（1个数值），对象类别的概率（对于COCO数据集，有80种对象）。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2b01c7d2caea9190fe4f32817612cb19/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">C&#43;&#43;11标准库(第二版)笔记整理</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/bd8bf13a77a1573bbab7dc1289e84e3f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">python网络编程基础，进程，线程介绍。有代码</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>