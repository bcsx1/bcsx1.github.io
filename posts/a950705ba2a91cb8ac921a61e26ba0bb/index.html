<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>长短期记忆（LSTM）详解 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="长短期记忆（LSTM）详解" />
<meta property="og:description" content="入门小菜鸟，希望像做笔记记录自己学的东西，也希望能帮助到同样入门的人，更希望大佬们帮忙纠错啦~侵权立删。
✨完整代码在我的github上，有需要的朋友可以康康✨
​​​​​​https://github.com/tt-s-t/Deep-Learning.git
目录
一、背景
二、原理
1、前向传播
（1）输入门、遗忘门和输出门
（2）候选记忆细胞
（3）记忆细胞
（4）隐藏状态
（5）输出
2、反向传播
（1）输出层参数
（2）过渡
（3）候选记忆细胞的参数
（4）输出门的参数
（5）遗忘门的参数
（6）输入门的参数
（7）上一隐藏状态and记忆细胞
三、总结
四、LSTM的优缺点
1、优点
2、缺点
五、LSTM代码实现
1、numpy实现LSTM类
（1）前期准备
（2）初始化参数
（3）前向传播
（4）反向传播
（5）预测
2、调用我们实现的LSTM进行训练与预测
3、结果
一、背景 当时间步数(T)较大或时间步(t)较小的时候，RNN的梯度较容易出现衰减或爆炸。虽然裁剪梯度可以应对梯度爆炸，但是无法解决梯度衰减的问题。这个原因使得RNN在实际中难以捕捉时间序列中时间步(t)距离较大的依赖关系。因此LSTM应运而生。
RNN详解可以看看：RNN循环神经网络_tt丫的博客-CSDN博客_rnn应用领域
二、原理 1、前向传播 输入：当前时间步的输入与上一时间步隐藏状态
（1）输入门、遗忘门和输出门 输入门：
遗忘门：
输出门：
他们都在后面起到一个比例调节的作用。
其中，，
为激活函数（sigmoid函数），故取值范围为：[0,1]
n为样本数，d为输入的特征数，h为隐藏大小。
（2）候选记忆细胞 （3）记忆细胞 当前时间步记忆细胞的计算组合了上一时间步记忆细胞和当前时间步候选记忆细胞的信息。
遗忘门控制上一时间步的记忆细胞中的信息是否传递到当前时间步的记忆细胞，而输出门则控制当前时间步的输入通过候选记忆细胞的如何流入当前时间步的记忆细胞。
如果遗忘门一直近似为1且输入门一直近似为0，则说明：过去的记忆细胞将一直通过时间保存并传递到当前时间步，而当前输入则被屏蔽掉。
这个设计可以应对循环神经网络中的梯度衰减问题（可以有选择地对前面的信息进行保留，不至于直接出现指数项），并更好地捕捉时间序列中时间步距离较大的依赖关系（存在中）。 （4）隐藏状态 我们通过输出门来控制从记忆细胞到隐藏状态的信息流动。
当输出门近似为1时，记忆细胞信息将传递到隐藏状态供输出层使用；近似为0时，记忆细胞信息只自己保留。
（5）输出 2、反向传播 已知（注：*是矩阵乘法，•是矩阵上对应元素相乘）
（1）输出层参数 Note：这里的指的是上一次（即t&#43;1时间步）计算得到的
；；
（2）过渡 对于链式法则涉及到记忆细胞的，我们设为
Note：同样的，这里的指的是上一次（即t&#43;1时间步）计算得到的
对于链式法则涉及到候选记忆细胞的，我们设为
对于链式法则涉及到输出门的，我们设为
对于链式法则涉及到遗忘门的，我们设为
对于链式法则涉及到输入门的，我们设为" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/a950705ba2a91cb8ac921a61e26ba0bb/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-01-23T10:10:19+08:00" />
<meta property="article:modified_time" content="2023-01-23T10:10:19+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">长短期记忆（LSTM）详解</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><span style="color:#a5a5a5;">入门小菜鸟，希望像做笔记记录自己学的东西，也希望能帮助到同样入门的人，更希望大佬们帮忙纠错啦~侵权立删。</span></p> 
<blockquote> 
 <p>✨完整代码在我的github上，有需要的朋友可以康康✨</p> 
 <p><a href="https://github.com/tt-s-t/Deep-Learning.git" title="​​​​​​https://github.com/tt-s-t/Deep-Learning.git">​​​​​​https://github.com/tt-s-t/Deep-Learning.git</a></p> 
</blockquote> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81%E8%83%8C%E6%99%AF-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81%E8%83%8C%E6%99%AF" rel="nofollow">一、背景</a></p> 
<p id="%E4%BA%8C%E3%80%81%E5%8E%9F%E7%90%86-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81%E5%8E%9F%E7%90%86" rel="nofollow">二、原理</a></p> 
<p id="1%E3%80%81%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD-toc" style="margin-left:40px;"><a href="#1%E3%80%81%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD" rel="nofollow">1、前向传播</a></p> 
<p id="%EF%BC%881%EF%BC%89%E8%BE%93%E5%85%A5%E9%97%A8%E3%80%81%E9%81%97%E5%BF%98%E9%97%A8%E5%92%8C%E8%BE%93%E5%87%BA%E9%97%A8-toc" style="margin-left:80px;"><a href="#%EF%BC%881%EF%BC%89%E8%BE%93%E5%85%A5%E9%97%A8%E3%80%81%E9%81%97%E5%BF%98%E9%97%A8%E5%92%8C%E8%BE%93%E5%87%BA%E9%97%A8" rel="nofollow">（1）输入门、遗忘门和输出门</a></p> 
<p id="%C2%A0%EF%BC%882%EF%BC%89%E5%80%99%E9%80%89%E8%AE%B0%E5%BF%86%E7%BB%86%E8%83%9E-toc" style="margin-left:80px;"><a href="#%C2%A0%EF%BC%882%EF%BC%89%E5%80%99%E9%80%89%E8%AE%B0%E5%BF%86%E7%BB%86%E8%83%9E" rel="nofollow"> （2）候选记忆细胞</a></p> 
<p id="%C2%A0%EF%BC%883%EF%BC%89%E8%AE%B0%E5%BF%86%E7%BB%86%E8%83%9E-toc" style="margin-left:80px;"><a href="#%C2%A0%EF%BC%883%EF%BC%89%E8%AE%B0%E5%BF%86%E7%BB%86%E8%83%9E" rel="nofollow"> （3）记忆细胞</a></p> 
<p id="%C2%A0%EF%BC%884%EF%BC%89%E9%9A%90%E8%97%8F%E7%8A%B6%E6%80%81-toc" style="margin-left:80px;"><a href="#%C2%A0%EF%BC%884%EF%BC%89%E9%9A%90%E8%97%8F%E7%8A%B6%E6%80%81" rel="nofollow"> （4）隐藏状态</a></p> 
<p id="%C2%A0%EF%BC%885%EF%BC%89%E8%BE%93%E5%87%BA-toc" style="margin-left:80px;"><a href="#%C2%A0%EF%BC%885%EF%BC%89%E8%BE%93%E5%87%BA" rel="nofollow"> （5）输出</a></p> 
<p id="2%E3%80%81%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-toc" style="margin-left:40px;"><a href="#2%E3%80%81%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD" rel="nofollow">2、反向传播</a></p> 
<p id="%EF%BC%881%EF%BC%89%E8%BE%93%E5%87%BA%E5%B1%82%E5%8F%82%E6%95%B0-toc" style="margin-left:80px;"><a href="#%EF%BC%881%EF%BC%89%E8%BE%93%E5%87%BA%E5%B1%82%E5%8F%82%E6%95%B0" rel="nofollow">（1）输出层参数</a></p> 
<p id="%EF%BC%882%EF%BC%89%E8%BF%87%E6%B8%A1-toc" style="margin-left:80px;"><a href="#%EF%BC%882%EF%BC%89%E8%BF%87%E6%B8%A1" rel="nofollow">（2）过渡</a></p> 
<p id="%EF%BC%883%EF%BC%89%E5%80%99%E9%80%89%E8%AE%B0%E5%BF%86%E7%BB%86%E8%83%9E%E7%9A%84%E5%8F%82%E6%95%B0-toc" style="margin-left:80px;"><a href="#%EF%BC%883%EF%BC%89%E5%80%99%E9%80%89%E8%AE%B0%E5%BF%86%E7%BB%86%E8%83%9E%E7%9A%84%E5%8F%82%E6%95%B0" rel="nofollow">（3）候选记忆细胞的参数</a></p> 
<p id="%EF%BC%884%EF%BC%89%E8%BE%93%E5%87%BA%E9%97%A8%E7%9A%84%E5%8F%82%E6%95%B0-toc" style="margin-left:80px;"><a href="#%EF%BC%884%EF%BC%89%E8%BE%93%E5%87%BA%E9%97%A8%E7%9A%84%E5%8F%82%E6%95%B0" rel="nofollow">（4）输出门的参数</a></p> 
<p id="%EF%BC%885%EF%BC%89%E9%81%97%E5%BF%98%E9%97%A8%E7%9A%84%E5%8F%82%E6%95%B0-toc" style="margin-left:80px;"><a href="#%EF%BC%885%EF%BC%89%E9%81%97%E5%BF%98%E9%97%A8%E7%9A%84%E5%8F%82%E6%95%B0" rel="nofollow">（5）遗忘门的参数</a></p> 
<p id="%EF%BC%886%EF%BC%89%E8%BE%93%E5%85%A5%E9%97%A8%E7%9A%84%E5%8F%82%E6%95%B0-toc" style="margin-left:80px;"><a href="#%EF%BC%886%EF%BC%89%E8%BE%93%E5%85%A5%E9%97%A8%E7%9A%84%E5%8F%82%E6%95%B0" rel="nofollow">（6）输入门的参数</a></p> 
<p id="%EF%BC%887%EF%BC%89%E4%B8%8A%E4%B8%80%E9%9A%90%E8%97%8F%E7%8A%B6%E6%80%81and%E8%AE%B0%E5%BF%86%E7%BB%86%E8%83%9E-toc" style="margin-left:80px;"><a href="#%EF%BC%887%EF%BC%89%E4%B8%8A%E4%B8%80%E9%9A%90%E8%97%8F%E7%8A%B6%E6%80%81and%E8%AE%B0%E5%BF%86%E7%BB%86%E8%83%9E" rel="nofollow">（7）上一隐藏状态and记忆细胞</a></p> 
<p id="%E4%B8%89%E3%80%81%E6%80%BB%E7%BB%93-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81%E6%80%BB%E7%BB%93" rel="nofollow">三、总结</a></p> 
<p id="%E5%9B%9B%E3%80%81LSTM%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9-toc" style="margin-left:0px;"><a href="#%E5%9B%9B%E3%80%81LSTM%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9" rel="nofollow">四、LSTM的优缺点</a></p> 
<p id="1%E3%80%81%E4%BC%98%E7%82%B9-toc" style="margin-left:40px;"><a href="#1%E3%80%81%E4%BC%98%E7%82%B9" rel="nofollow">1、优点</a></p> 
<p id="2%E3%80%81%E7%BC%BA%E7%82%B9-toc" style="margin-left:40px;"><a href="#2%E3%80%81%E7%BC%BA%E7%82%B9" rel="nofollow">2、缺点</a></p> 
<p id="%E4%BA%94%E3%80%81LSTM%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-toc" style="margin-left:0px;"><a href="#%E4%BA%94%E3%80%81LSTM%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0" rel="nofollow">五、LSTM代码实现</a></p> 
<p id="1%E3%80%81numpy%E5%AE%9E%E7%8E%B0LSTM%E7%B1%BB-toc" style="margin-left:40px;"><a href="#1%E3%80%81numpy%E5%AE%9E%E7%8E%B0LSTM%E7%B1%BB" rel="nofollow">1、numpy实现LSTM类</a></p> 
<p id="%EF%BC%881%EF%BC%89%E5%89%8D%E6%9C%9F%E5%87%86%E5%A4%87-toc" style="margin-left:80px;"><a href="#%EF%BC%881%EF%BC%89%E5%89%8D%E6%9C%9F%E5%87%86%E5%A4%87" rel="nofollow">（1）前期准备</a></p> 
<p id="%EF%BC%882%EF%BC%89%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%82%E6%95%B0-toc" style="margin-left:80px;"><a href="#%EF%BC%882%EF%BC%89%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%82%E6%95%B0" rel="nofollow">（2）初始化参数</a></p> 
<p id="%EF%BC%883%EF%BC%89%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD-toc" style="margin-left:80px;"><a href="#%EF%BC%883%EF%BC%89%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD" rel="nofollow">（3）前向传播</a></p> 
<p id="%EF%BC%884%EF%BC%89%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-toc" style="margin-left:80px;"><a href="#%EF%BC%884%EF%BC%89%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD" rel="nofollow">（4）反向传播</a></p> 
<p id="%EF%BC%885%EF%BC%89%E9%A2%84%E6%B5%8B-toc" style="margin-left:80px;"><a href="#%EF%BC%885%EF%BC%89%E9%A2%84%E6%B5%8B" rel="nofollow">（5）预测</a></p> 
<p id="2%E3%80%81%E8%B0%83%E7%94%A8%E6%88%91%E4%BB%AC%E5%AE%9E%E7%8E%B0%E7%9A%84LSTM%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%A2%84%E6%B5%8B-toc" style="margin-left:40px;"><a href="#2%E3%80%81%E8%B0%83%E7%94%A8%E6%88%91%E4%BB%AC%E5%AE%9E%E7%8E%B0%E7%9A%84LSTM%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%A2%84%E6%B5%8B" rel="nofollow">2、调用我们实现的LSTM进行训练与预测</a></p> 
<p id="3%E3%80%81%E7%BB%93%E6%9E%9C-toc" style="margin-left:40px;"><a href="#3%E3%80%81%E7%BB%93%E6%9E%9C" rel="nofollow">3、结果</a></p> 
<h2 id="%E4%B8%80%E3%80%81%E8%83%8C%E6%99%AF">一、背景</h2> 
<p>       当时间步数(T)较大或时间步(t)较小的时候，RNN的梯度较容易出现衰减或爆炸。虽然裁剪梯度可以应对梯度爆炸，但是无法解决梯度衰减的问题。这个原因使得RNN在实际中难以捕捉时间序列中时间步(t)距离较大的依赖关系。因此LSTM应运而生。</p> 
<p>      RNN详解可以看看：<a href="https://blog.csdn.net/weixin_55073640/article/details/122931040" title="RNN循环神经网络_tt丫的博客-CSDN博客_rnn应用领域">RNN循环神经网络_tt丫的博客-CSDN博客_rnn应用领域</a></p> 
<hr> 
<h2 id="%E4%BA%8C%E3%80%81%E5%8E%9F%E7%90%86">二、原理</h2> 
<h3 id="1%E3%80%81%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD">1、前向传播</h3> 
<p>输入：当前时间步的输入<img alt="X_{t}" class="mathcode" src="https://images2.imgbox.com/38/38/I1JJF3ae_o.png">与上一时间步隐藏状态<img alt="H_{t-1}" class="mathcode" src="https://images2.imgbox.com/0d/b9/HRVT1BlK_o.png"></p> 
<h4 id="%EF%BC%881%EF%BC%89%E8%BE%93%E5%85%A5%E9%97%A8%E3%80%81%E9%81%97%E5%BF%98%E9%97%A8%E5%92%8C%E8%BE%93%E5%87%BA%E9%97%A8">（1）输入门、遗忘门和输出门</h4> 
<p>输入门：<img alt="\mathrm{I}_{\mathrm{t}}=\sigma\left(\mathrm{X}_{\mathrm{t}} \mathrm{W}_{\mathrm{xi}}+\mathrm{H}_{\mathrm{t}-1} \mathrm{~W}_{\mathrm{hi}}+\mathrm{b}_{\mathrm{i}}\right)" class="mathcode" src="https://images2.imgbox.com/35/02/uZQpl9P3_o.png"></p> 
<p>遗忘门：<img alt="\mathrm{F}_{\mathrm{t}}=\sigma\left(\mathrm{X}_{\mathrm{t}} \mathrm{W}_{\mathrm{xf}}+\mathrm{H}_{\mathrm{t}-1} \mathrm{~W}_{\mathrm{hf}}+\mathrm{b}_{\mathrm{f}}\right)" class="mathcode" src="https://images2.imgbox.com/80/eb/05TkJ5Lq_o.png"></p> 
<p>输出门：<img alt="\mathrm{O}_{\mathrm{t}}=\sigma\left(\mathrm{X}_{\mathrm{t}} \mathrm{W}_{\mathrm{xo}}+\mathrm{H}_{\mathrm{t}-1} \mathrm{~W}_{\mathrm{ho}}+\mathrm{b}_{\mathrm{o}}\right)" class="mathcode" src="https://images2.imgbox.com/dd/87/1jVFABeU_o.png"></p> 
<p>他们都在后面起到一个比例调节的作用。</p> 
<blockquote> 
 <p>其中，<img alt="\mathrm{X}_{\mathrm{t}} \in \mathbb{R}^{\mathrm{n} * \mathrm{d}},H_{t-1} \in \mathbb{R}^{n*h}, R_{t},Z_{t} \in \mathbb{R}^{\mathrm{n} * h}" src="https://images2.imgbox.com/4d/21/d95yVPx9_o.png">，<img alt="I_{t}, F_{t}, O_{t} \in R^{n*h}" class="mathcode" src="https://images2.imgbox.com/37/d0/vxwUCK04_o.png"></p> 
 <p><img alt="\sigma" src="https://images2.imgbox.com/b8/cb/YoKJ8xt3_o.png">为激活函数（sigmoid函数），故取值范围为：[0,1]</p> 
 <p>n为样本数，d为输入的特征数，h为隐藏大小。</p> 
</blockquote> 
<p><img alt="" height="299" src="https://images2.imgbox.com/09/63/klHYi6Mo_o.png" width="468"></p> 
<h4 id="%C2%A0%EF%BC%882%EF%BC%89%E5%80%99%E9%80%89%E8%AE%B0%E5%BF%86%E7%BB%86%E8%83%9E"> （2）候选记忆细胞</h4> 
<p>   <img alt="\widetilde{C_{t}} = tanh(X_{t}W_{xc}+H_{t-1}W_{hc}+b_{c})" class="mathcode" src="https://images2.imgbox.com/c6/88/YxcbAYBu_o.png"></p> 
<p><img alt="" height="306" src="https://images2.imgbox.com/4e/cd/rc7rFFhV_o.png" width="472"></p> 
<h4 id="%C2%A0%EF%BC%883%EF%BC%89%E8%AE%B0%E5%BF%86%E7%BB%86%E8%83%9E"> （3）记忆细胞</h4> 
<p>        当前时间步记忆细胞的计算组合了上一时间步记忆细胞和当前时间步候选记忆细胞的信息。</p> 
<p>       <img alt="C_{t}=F_{t} \odot C_{t-1}+I_{t} \odot \tilde{C}_{t}" class="mathcode" src="https://images2.imgbox.com/0e/44/Sf5EpLQF_o.png"></p> 
<p>       遗忘门<img alt="F_{t}" class="mathcode" src="https://images2.imgbox.com/28/0a/4WHQty6N_o.png">控制上一时间步的记忆细胞<img alt="C_{t-1}" class="mathcode" src="https://images2.imgbox.com/f4/ea/Sq68rbUJ_o.png">中的信息是否传递到当前时间步的记忆细胞，而输出门<img alt="I_{t}" class="mathcode" src="https://images2.imgbox.com/87/67/55oeHQ6c_o.png">则控制当前时间步的输入<img alt="X_{t}" class="mathcode" src="https://images2.imgbox.com/e8/96/UsgXvuMZ_o.png">通过候选记忆细胞的<img alt="\tilde{C}_{t}" class="mathcode" src="https://images2.imgbox.com/0f/89/q3tu9mvZ_o.png">如何流入当前时间步的记忆细胞。</p> 
<p>       如果遗忘门一直近似为1且输入门一直近似为0，则说明：过去的记忆细胞将一直通过时间保存并传递到当前时间步，而当前输入<img alt="X_{t}" class="mathcode" src="https://images2.imgbox.com/0f/6f/hDnQHXE3_o.png">则被屏蔽掉。</p> 
<p>       这个设计可以应对循环神经网络中的梯度衰减问题（可以有选择地对前面的信息进行保留，不至于直接出现指数项），并更好地捕捉时间序列中时间步距离较大的依赖关系（存在<img alt="C_{t-1}" class="mathcode" src="https://images2.imgbox.com/81/7c/dGzX8e0z_o.png">中）。 </p> 
<p><img alt="" height="270" src="https://images2.imgbox.com/0d/05/n3uGh7Nh_o.png" width="536"></p> 
<h4 id="%C2%A0%EF%BC%884%EF%BC%89%E9%9A%90%E8%97%8F%E7%8A%B6%E6%80%81"> （4）隐藏状态</h4> 
<p>       我们通过输出门<img alt="O_{t}" class="mathcode" src="https://images2.imgbox.com/be/52/KopO8e8O_o.png">来控制从记忆细胞到隐藏状态<img alt="H_{t}" class="mathcode" src="https://images2.imgbox.com/48/37/To4g9Tgg_o.png">的信息流动。</p> 
<p>      <img alt="H_{t}=O_{t}\odot tanh(C_{t})" class="mathcode" src="https://images2.imgbox.com/e3/7b/mEJxfYDU_o.png"></p> 
<p>       当输出门<img alt="O_{t}" class="mathcode" src="https://images2.imgbox.com/f9/7d/2WBOKOaC_o.png">近似为1时，记忆细胞信息将传递到隐藏状态供输出层使用；近似为0时，记忆细胞信息只自己保留。</p> 
<p><img alt="" height="273" src="https://images2.imgbox.com/5b/2c/AeQHxvnd_o.png" width="549"></p> 
<h4 id="%C2%A0%EF%BC%885%EF%BC%89%E8%BE%93%E5%87%BA"> （5）输出</h4> 
<p><img alt="Y_{t}=softmax(H_{t}W_{hd}+B_{d})" class="mathcode" src="https://images2.imgbox.com/42/50/IQts2KVf_o.png"></p> 
<h3 id="2%E3%80%81%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD">2、反向传播</h3> 
<p>已知<img alt="dY_{t}" class="mathcode" src="https://images2.imgbox.com/a6/45/HMoULSLS_o.png">（注：*是矩阵乘法，•是矩阵上对应元素相乘）</p> 
<h4 id="%EF%BC%881%EF%BC%89%E8%BE%93%E5%87%BA%E5%B1%82%E5%8F%82%E6%95%B0">（1）输出层参数</h4> 
<p>Note：这里的<img alt="dH_{t-1}" class="mathcode" src="https://images2.imgbox.com/1a/17/rtorCq3d_o.png">指的是上一次（即t+1时间步）计算得到的<img alt="dH_{t-1}" class="mathcode" src="https://images2.imgbox.com/0f/c6/KARNgheM_o.png"></p> 
<p><img alt="dW_{hd}=\sum_{t}H_{t}^{T}*dY_{t}" class="mathcode" src="https://images2.imgbox.com/6d/15/DUh1JYQD_o.png">；<img alt="dB_{d}=\sum_{t}dY_{t}" class="mathcode" src="https://images2.imgbox.com/60/28/sMlZYgu4_o.png">；<img alt="dH_{t}=\left\{\begin{matrix} dY_{t}*W_{hd}^{T}+dH_{t-1} &amp;,0&lt;t&lt;T \\ dY_{t}*W_{hd}^{T}&amp; ,t=T \end{matrix}\right." class="mathcode" src="https://images2.imgbox.com/6e/fd/HROpQ9g1_o.png"></p> 
<h4 id="%EF%BC%882%EF%BC%89%E8%BF%87%E6%B8%A1">（2）过渡</h4> 
<p>对于链式法则涉及到记忆细胞的，我们设为</p> 
<p><img alt="dC_{t}=\left\{\begin{matrix} dH_{t} \cdot O_{t} \cdot (1-(tanh(C_{t}))^{2})+dC_{t-1}&amp;,0&lt;t&lt;T \\ dH_{t} \cdot O_{t} \cdot (1-(tanh(C_{t}))^{2})&amp; ,t=T \end{matrix}\right." class="mathcode" src="https://images2.imgbox.com/a3/bf/HCm00gvo_o.png"></p> 
<blockquote> 
 <p>Note：同样的，这里的<img alt="dC_{t-1}" class="mathcode" src="https://images2.imgbox.com/a9/6a/OS9gWogl_o.png">指的是上一次（即t+1时间步）计算得到的<img alt="dC_{t-1}" class="mathcode" src="https://images2.imgbox.com/21/08/lCbPloOU_o.png"></p> 
</blockquote> 
<p>对于链式法则涉及到候选记忆细胞的，我们设为<img alt="d\widetilde{C_{t}'}=dC_{t} \cdot I_{t} \cdot (1-\widetilde{C_{t}}^{2})" class="mathcode" src="https://images2.imgbox.com/20/81/TIAvmF1B_o.png"></p> 
<p>对于链式法则涉及到输出门的，我们设为<img alt="dO_{t}'=dH_{t} \cdot tanh(C_{t}) \cdot O_{t} \cdot (1-O_{t})" class="mathcode" src="https://images2.imgbox.com/ee/95/xJ9VcIzU_o.png"></p> 
<p>对于链式法则涉及到遗忘门的，我们设为<img alt="dF_{t}'=dC_{t} \cdot C_{t-1} \cdot F_{t} \cdot (1-F_{t})" class="mathcode" src="https://images2.imgbox.com/33/49/NtJls2Ja_o.png"></p> 
<p>对于链式法则涉及到输入门的，我们设为<img alt="dI_{t}'=dC_{t} \cdot \widetilde{C_{t}} \cdot I_{t} \cdot (1-I_{t})" class="mathcode" src="https://images2.imgbox.com/f7/e9/St0w6Mtz_o.png"></p> 
<h4 id="%EF%BC%883%EF%BC%89%E5%80%99%E9%80%89%E8%AE%B0%E5%BF%86%E7%BB%86%E8%83%9E%E7%9A%84%E5%8F%82%E6%95%B0">（3）候选记忆细胞的参数</h4> 
<p><img alt="dW_{xc}=\sum_{t}X_{t}^{T}*d\widetilde{C_{t}}'" class="mathcode" src="https://images2.imgbox.com/f1/ad/ZcxhMhDk_o.png">；<img alt="dW_{hc}=\sum_{t}H_{t-1}^{T}*d\widetilde{C_{t}}'" class="mathcode" src="https://images2.imgbox.com/00/5d/EhSLVh3R_o.png">；<img alt="db_{c}=\sum_{t}d\widetilde{C_{t}}'" class="mathcode" src="https://images2.imgbox.com/c2/d8/9KeVy1Ky_o.png"></p> 
<h4 id="%EF%BC%884%EF%BC%89%E8%BE%93%E5%87%BA%E9%97%A8%E7%9A%84%E5%8F%82%E6%95%B0">（4）输出门的参数</h4> 
<p><img alt="dW_{xo}=\sum_{t}X_{t}^{T}*dO_{t}'" class="mathcode" src="https://images2.imgbox.com/53/d0/BOvJcyUa_o.png">；<img alt="dW_{ho}=\sum_{t}H_{t-1}^{T}*dO_{t}'" class="mathcode" src="https://images2.imgbox.com/df/01/DRKKCBet_o.png">；<img alt="db_{o}=\sum_{t}dO_{t}'" class="mathcode" src="https://images2.imgbox.com/9e/a6/6OltKacK_o.png"></p> 
<h4 id="%EF%BC%885%EF%BC%89%E9%81%97%E5%BF%98%E9%97%A8%E7%9A%84%E5%8F%82%E6%95%B0">（5）遗忘门的参数</h4> 
<p><img alt="dW_{xf}=\sum_{t}X_{t}^{T}*dF_{t}'" class="mathcode" src="https://images2.imgbox.com/a5/e2/XRVy6aSD_o.png">；<img alt="dW_{hf}=\sum_{t}H_{t-1}^{T}*dF_{t}'" class="mathcode" src="https://images2.imgbox.com/74/d2/Yhv9awWh_o.png">；<img alt="db_{f}=\sum_{t}dF_{t}'" class="mathcode" src="https://images2.imgbox.com/09/51/3rpTxNg2_o.png"></p> 
<h4 id="%EF%BC%886%EF%BC%89%E8%BE%93%E5%85%A5%E9%97%A8%E7%9A%84%E5%8F%82%E6%95%B0">（6）输入门的参数</h4> 
<p><img alt="dW_{xi}=\sum_{t}X_{t}^{T}*dI_{t}'" class="mathcode" src="https://images2.imgbox.com/42/16/QaVb3En9_o.png">；<img alt="dW_{hi}=\sum_{t}H_{t-1}^{T}*dI_{t}'" class="mathcode" src="https://images2.imgbox.com/5a/18/swEK1Ji2_o.png">；<img alt="db_{i}=\sum_{t}dI_{t}'" class="mathcode" src="https://images2.imgbox.com/6c/7f/rhl1biGQ_o.png"></p> 
<h4 id="%EF%BC%887%EF%BC%89%E4%B8%8A%E4%B8%80%E9%9A%90%E8%97%8F%E7%8A%B6%E6%80%81and%E8%AE%B0%E5%BF%86%E7%BB%86%E8%83%9E">（7）上一隐藏状态and记忆细胞</h4> 
<p><img alt="dH_{t-1}=d\widetilde{C_{t}}'*W_{hc}+dI_{t}'*W_{hi}+dF_{t}'*W_{hf}+dO_{t}'*W_{ho}" class="mathcode" src="https://images2.imgbox.com/ea/6c/15fBIvnw_o.png"></p> 
<p><img alt="dC_{t-1}=dC_{t} \cdot F_{t}" class="mathcode" src="https://images2.imgbox.com/be/14/uzSKlCFc_o.png"></p> 
<hr> 
<h2 id="%E4%B8%89%E3%80%81%E6%80%BB%E7%BB%93">三、总结</h2> 
<p>       LSTM 的核心概念在于细胞状态以及“门”结构。细胞状态相当于信息传输的路径，让信息能在序列连中传递下去——即网络的“记忆”。理论上讲，细胞状态能够将序列处理过程中的相关信息一直传递下去。</p> 
<p>      因此，即使是较早时间步的信息也能携带到较后时间步的细胞中来，这克服了短时记忆的影响（RNN可能会因为指数项的累积，变得越来越小或大到“面目全非”，LSTM将累积下来的影响由指数运算转化为了加法运算与参数学习控制去留）。信息的添加和移除我们通过“门”结构来实现，“门”结构在训练过程中会去学习该保存或遗忘哪些信息。</p> 
<blockquote> 
 <p>每个门起到的作用顾名思义：</p> 
 <p>     遗忘门：决定什么信息需要从记忆细胞中删除——0：将<img alt="C_{t-1}" class="mathcode" src="https://images2.imgbox.com/c2/ca/tchjxtDB_o.png">（过去的记忆）的值删除，1：保留<img alt="C_{t-1}" class="mathcode" src="https://images2.imgbox.com/2e/90/ywPO6LfH_o.png">的值；</p> 
 <p>     输入门：决定输入的哪些新信息（输入信息通过候选记忆细胞传入）需要增加至记忆细胞中；</p> 
 <p>     输出门：决定从记忆细胞中选出哪些信息进行输出。</p> 
</blockquote> 
<hr> 
<h2 id="%E5%9B%9B%E3%80%81LSTM%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9">四、LSTM的优缺点</h2> 
<h3 id="1%E3%80%81%E4%BC%98%E7%82%B9">1、优点</h3> 
<p>       继承了大部分RNN模型的特性，同时解决了梯度反传过程由于逐步缩减而产生的梯度衰减问题，使用门控与记忆细胞来学习如何取舍过去的信息，如何提取当前的输入信息。</p> 
<blockquote> 
 <p>       具体改进点：RNN这部分的取舍过去信息和提取当前输入信息都是由参数学习得到的（权重参数），结构过于简单，原理也上也有所欠缺（一般我们判断过去信息对现在重不重要也是需要根据过去信息以及当前状态同时决定的，而非直接由一个U和W权重矩阵决定）</p> 
</blockquote> 
<h3 id="2%E3%80%81%E7%BC%BA%E7%82%B9">2、缺点</h3> 
<p>（1）并行处理上存在劣势，难以落地；</p> 
<p>（2）RNN的梯度问题在LSTM及其变种里面得到了一定程度的解决，但还是不够。它可以处理100个量级的序列，而对于1000个量级，或者更长的序列则依然会很棘手；</p> 
<p>（3）模型结构复杂，计算费时。每一个LSTM的cell里面都意味着有4个全连接层(MLP)，如果LSTM的时间跨度很大，并且网络又很深，这个计算量会很大，很耗时。</p> 
<hr> 
<h2 id="%E4%BA%94%E3%80%81LSTM%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">五、LSTM代码实现</h2> 
<p>       这里只展示我用numpy搭建的LSTM网络，并且实现对“abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz”序列数据的预测。详细地可以在我的github的LSTM文件夹上看，包括用pytorch实现的LSTM实现文本生成，以及这个numpy搭建的LSTM实现对序列数据预测的完整版本。</p> 
<p><a class="link-info" href="http://xn--https-kt3baaaaa//github.com/tt-s-t/Deep-Learning.git" title="http://​​​​​​https://github.com/tt-s-t/Deep-Learning.git">http://​​​​​​https://github.com/tt-s-t/Deep-Learning.git</a></p> 
<p>首先我们定义一个LSTM类。</p> 
<h3 id="1%E3%80%81numpy%E5%AE%9E%E7%8E%B0LSTM%E7%B1%BB">1、numpy实现LSTM类</h3> 
<h4 id="%EF%BC%881%EF%BC%89%E5%89%8D%E6%9C%9F%E5%87%86%E5%A4%87">（1）前期准备</h4> 
<pre><code class="language-python">import numpy as np

def sigmoid(x):
    x_ravel = x.ravel()  # 将numpy数组展平
    length = len(x_ravel)
    y = []
    for index in range(length):
        if x_ravel[index] &gt;= 0:
            y.append(1.0 / (1 + np.exp(-x_ravel[index])))
        else:
            y.append(np.exp(x_ravel[index]) / (np.exp(x_ravel[index]) + 1))
    return np.array(y).reshape(x.shape)

def tanh(x):
    result = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))
    return result</code></pre> 
<h4 id="%EF%BC%882%EF%BC%89%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%82%E6%95%B0">（2）初始化参数</h4> 
<pre><code class="language-python">class LSTM(object):
    def __init__(self, input_size, hidden_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        #输入门
        self.Wxi = np.random.randn(input_size, hidden_size)
        self.Whi = np.random.randn(hidden_size, hidden_size)
        self.B_i  = np.zeros((1, hidden_size))
        #遗忘门
        self.Wxf = np.random.randn(input_size, hidden_size)
        self.Whf = np.random.randn(hidden_size, hidden_size)
        self.B_f = np.zeros((1, hidden_size))
        #输出门
        self.Wxo = np.random.randn(input_size, hidden_size)
        self.Who = np.random.randn(hidden_size, hidden_size)
        self.B_o = np.zeros((1, hidden_size))
        #候选记忆细胞
        self.Wxc = np.random.randn(input_size, hidden_size)
        self.Whc = np.random.randn(hidden_size, hidden_size)
        self.B_c = np.zeros((1, hidden_size))
        #输出
        self.W_hd = np.random.randn(hidden_size, input_size)
        self.B_d = np.zeros((1, input_size))</code></pre> 
<h4 id="%EF%BC%883%EF%BC%89%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD">（3）前向传播</h4> 
<pre><code class="language-python">    def forward(self,X,Ht_1,Ct_1): #前向传播
        #存储
        self.it_stack = {} #输入门存储
        self.ft_stack = {} #遗忘门存储
        self.ot_stack = {} #输出门存储
        self.cc_stack = {} #候选记忆细胞存储
        self.c_stack = {} #记忆细胞存储
        self.X_stack = {} #X存储
        self.Ht_stack = {} #隐藏状态存储
        self.Y_stack = {} #输出存储

        self.Ht_stack[-1] = Ht_1
        self.c_stack[-1] = Ct_1
        self.T = X.shape[0]

        for t in range(self.T):
            self.X_stack[t] = X[t].reshape(-1,1).T
            #输入门
            net_i = np.matmul(self.X_stack[t], self.Wxi) + np.matmul(self.Ht_stack[t-1], self.Whi) + self.B_i
            it = sigmoid(net_i)
            self.it_stack[t] = it
            #遗忘门
            net_f = np.matmul(self.X_stack[t], self.Wxf) + np.matmul(self.Ht_stack[t-1], self.Whf) + self.B_f
            ft = sigmoid(net_f)
            self.ft_stack[t] = ft
            #输出门
            net_o = np.matmul(self.X_stack[t], self.Wxo) + np.matmul(self.Ht_stack[t-1], self.Who) + self.B_o
            ot = sigmoid(net_o)
            self.ot_stack[t] = ot
            #候选记忆细胞
            net_cc = np.matmul(self.X_stack[t], self.Wxc) + np.matmul(self.Ht_stack[t-1], self.Whc) + self.B_c
            cct = tanh(net_cc)
            self.cc_stack[t] = cct
            #记忆细胞
            Ct = ft*self.c_stack[t-1]+it*cct
            self.c_stack[t] = Ct
            #隐藏状态
            Ht = ot*tanh(Ct)
            self.Ht_stack[t] = Ht
            #输出
            y = np.matmul(Ht, self.W_hd) + self.B_d
            Yt = np.exp(y) / np.sum(np.exp(y)) #softmax
            self.Y_stack[t] = Yt</code></pre> 
<h4 id="%EF%BC%884%EF%BC%89%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD">（4）反向传播</h4> 
<pre><code class="language-python">    def backward(self,target,lr):
        #初始化
        dH_1, dnet_ct_1 = np.zeros([1,self.hidden_size]), np.zeros([1,self.hidden_size])

        dWxi, dWhi, dBi = np.zeros_like(self.Wxi), np.zeros_like(self.Whi), np.zeros_like(self.B_i)
        dWxf, dWhf, dBf = np.zeros_like(self.Wxf), np.zeros_like(self.Whf), np.zeros_like(self.B_f)
        dWxo, dWho, dBo = np.zeros_like(self.Wxo), np.zeros_like(self.Who), np.zeros_like(self.B_o)
        dWxc, dWhc, dBc = np.zeros_like(self.Wxc), np.zeros_like(self.Whc), np.zeros_like(self.B_c)
        dWhd,dBd = np.zeros_like(self.W_hd),np.zeros_like(self.B_d)

        self.loss = 0

        for t in reversed(range(self.T)): #反过来开始，越往前面分支越多       
            dY = self.Y_stack[t] - target[t].reshape(-1,1).T
            self.loss += -np.sum(np.log(self.Y_stack[t]) * target[t].reshape(-1,1).T)
            #对输出的参数
            dWhd += np.matmul(self.Ht_stack[t].T,dY)
            dBd += dY

            dH = np.matmul(dY, self.W_hd.T) + dH_1 #dH更新

            #对有关输入门，遗忘门，输出门，候选记忆细胞中参数的求导的共同点
            temp = tanh(self.c_stack[t])
            dnet_ct = dH * self.ot_stack[t] * (1-temp*temp) + dnet_ct_1 #记忆细胞
            dnet_cct = dnet_ct * self.it_stack[t] * (1 - self.cc_stack[t]*self.cc_stack[t]) #候选记忆细胞
            dnet_o = dH * temp * self.ot_stack[t] * (1 - self.ot_stack[t]) #输出门
            dnet_f = dnet_ct * self.c_stack[t-1] * self.ft_stack[t] * (1 - self.ft_stack[t]) #遗忘门
            dnet_i = dnet_ct * self.cc_stack[t] * self.it_stack[t] * (1 - self.it_stack[t]) #输入门

            #候选记忆细胞中参数
            dWxc += np.matmul(self.X_stack[t].T, dnet_cct)
            dWhc += np.matmul(self.Ht_stack[t-1].T, dnet_cct)
            dBc += dnet_cct

            #输出门
            dWxo += np.matmul(self.X_stack[t].T, dnet_o)
            dWho += np.matmul(self.Ht_stack[t-1].T, dnet_o)
            dBo += dnet_o

            #遗忘门
            dWxf += np.matmul(self.X_stack[t].T, dnet_f)
            dWhf += np.matmul(self.Ht_stack[t-1].T, dnet_f)
            dBf += dnet_f

            #输入门
            dWxi += np.matmul(self.X_stack[t].T, dnet_i)
            dWhi += np.matmul(self.Ht_stack[t-1].T, dnet_i)
            dBi += dnet_i

            #Ht-1和Ct-1
            dH_1 = np.matmul(dnet_cct, self.Whc) + np.matmul(dnet_i, self.Whi) + np.matmul(dnet_f, self.Whf) + np.matmul(dnet_o, self.Who)
            dnet_ct_1 = dnet_ct * self.ft_stack[t]

        #候选记忆细胞
        self.Wxc += -lr * dWxc
        self.Whc += -lr * dWhc
        self.B_c += -lr * dBc
        #输出门
        self.Wxo += -lr * dWxo
        self.Who += -lr * dWho
        self.B_o += -lr * dBo
        #遗忘门
        self.Wxf += -lr * dWxf
        self.Whf += -lr * dWhf
        self.B_f += -lr * dBf
        #输入门
        self.Wxi += -lr * dWxi
        self.Whi += -lr * dWhi
        self.B_i += -lr * dBi

        return self.loss</code></pre> 
<h4 id="%EF%BC%885%EF%BC%89%E9%A2%84%E6%B5%8B">（5）预测</h4> 
<pre><code class="language-python">    def pre(self,input_onehot,h_prev,c_prev,next_len,vocab): #input_onehot为输入的一个词的onehot编码，next_len为需要生成的单词长度，vocab是"索引-词"的词典
        xs, hs, cs = {}, {}, {} #字典形式存储
        hs[-1] = np.copy(h_prev) #隐藏状态赋予
        cs[-1] = np.copy(c_prev)
        xs[0] = input_onehot
        pre_vocab = []
        for t in range(next_len):
            #输入门
            net_i = np.matmul(xs[t], self.Wxi) + np.matmul(hs[t-1], self.Whi) + self.B_i
            it = sigmoid(net_i)
            #遗忘门
            net_f = np.matmul(xs[t], self.Wxf) + np.matmul(hs[t-1], self.Whf) + self.B_f
            ft = sigmoid(net_f)
            #输出门
            net_o = np.matmul(xs[t], self.Wxo) + np.matmul(hs[t-1], self.Who) + self.B_o
            ot = sigmoid(net_o)
            #候选记忆细胞
            net_cc = np.matmul(xs[t], self.Wxc) + np.matmul(hs[t-1], self.Whc) + self.B_c
            cct = tanh(net_cc)
            #记忆细胞
            Ct = ft*cs[t-1]+it*cct
            cs[t] = Ct
            #隐藏状态
            Ht = ot*tanh(Ct)
            hs[t] = Ht
            #输出
            Ot = np.matmul(Ht, self.W_hd) + self.B_d
            Yt = np.exp(Ot) / np.sum(np.exp(Ot)) #softmax
            pre_vocab.append(vocab[np.argmax(Yt)])

            xs[t+1] = np.zeros((1, self.input_size)) # init
            xs[t+1][0,np.argmax(Yt)] = 1
        return pre_vocab</code></pre> 
<h3 id="2%E3%80%81%E8%B0%83%E7%94%A8%E6%88%91%E4%BB%AC%E5%AE%9E%E7%8E%B0%E7%9A%84LSTM%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%A2%84%E6%B5%8B">2、调用我们实现的LSTM进行训练与预测</h3> 
<pre><code class="language-python">from lstm_model import LSTM
import numpy as np
import math

class Dataset(object):
    def __init__(self,txt_data, sequence_length):
        self.txt_len = len(txt_data) #文本长度
        vocab = list(set(txt_data)) #所有字符合集
        self.n_vocab = len(vocab) #字典长度
        self.sequence_length = sequence_length
        self.vocab_to_index = dict((c, i) for i, c in enumerate(vocab)) #词-索引字典
        self.index_to_vocab = dict((i, c) for i, c in enumerate(vocab)) #索引-词字典
        self.txt_index = [self.vocab_to_index[i] for i in txt_data] #输入文本的索引表示

    def one_hot(self,input):
        onehot_encoded = []
        for i in input:
            letter = [0 for _ in range(self.n_vocab)] 
            letter[i] = 1
            onehot_encoded.append(letter)
        onehot_encoded = np.array(onehot_encoded)
        return onehot_encoded
    
    def __getitem__(self, index):
        return (
            self.txt_index[index:index+self.sequence_length],
            self.txt_index[index+1:index+self.sequence_length+1]
        )

#输入的有规律的序列数据
#txt_data = "abc abc abc abc abc abc abc abc abc abc abc abc abc abc abc abc abc abc abc abc abc abc abc abc abc abc abc"
txt_data = "abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz "

#config
max_epoch = 5000
sequence_length = 28
dataset = Dataset(txt_data,sequence_length)
batch_num = math.ceil(dataset.txt_len /sequence_length) #向上取整
hidden_size = 32
lr = 1e-3

model = LSTM(dataset.n_vocab,hidden_size)

#训练
for epoch in range(max_epoch):
    h_prev = np.zeros((1, hidden_size))
    c_prev = np.zeros((1, hidden_size))
    loss = 0
    for b in range(batch_num):
        (x,y) = dataset[b]
        input = dataset.one_hot(x)
        target = dataset.one_hot(y)
        ps = model.forward(input,h_prev,c_prev) #注意：每个batch的h都是从0初始化开始，batch与batch间的隐藏状态没有关系
        loss += model.backward(target,lr)
    print("epoch: ",epoch)
    print("loss: ",loss/batch_num)

#预测
input_txt = 'a'
input_onehot = dataset.one_hot([dataset.vocab_to_index[input_txt]])
next_len = 50 #预测后几个word
h_prev = np.zeros((1, hidden_size))
c_prev = np.zeros((1, hidden_size))
pre_vocab = ['a']
pre_vocab1 = model.pre(input_onehot,h_prev,c_prev,next_len,dataset.index_to_vocab)
pre_vocab = pre_vocab + pre_vocab1
print(''.join(pre_vocab))</code></pre> 
<h3 id="3%E3%80%81%E7%BB%93%E6%9E%9C">3、结果</h3> 
<p>以a开头预测后续的50个字符。</p> 
<h3 id="%E2%80%8B%E7%BC%96%E8%BE%91"><img alt="" height="266" src="https://images2.imgbox.com/9f/2e/PWMzdKS8_o.png" width="589"></h3> 
<hr> 
<p>欢迎大家在评论区批评指正，谢谢~</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/55f90ae53582166c108cad3bd3425e05/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">修改linux/debain的mtl值</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ac75e46a72f56c49c05c58676d6e914b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">通讯录管理系统（C语言）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>