<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>论文阅读记录 51-100篇 20200316-20210817 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="论文阅读记录 51-100篇 20200316-20210817" />
<meta property="og:description" content="2021-8-17
A high fidelity synthetic face framework for computer vision
CG人脸生成
略读。用的类似3DMM的方法，分成了identity和expression。这优化好像是数据集的基向量和系数同时进行。采样时用GMM. expression采用了51维表示和15个姿态参数，他的标注方法我没看懂。。。texture没细看，大概用了blender，用vae在albedo texture data上表示。Hair用3D hair strands对单根头发表示。用Blender’s particle hair system，头发区域会deformed根据head shape change。头发颜色用三个标量表示，proportion of melanin（黑色素）, pheomelanin（褐色素）and proportion of gray hairs（白发比例）。Hair的表示采用了volumetric flow direction and occupancy based parametrization. 每一个hair包括两个UV maps 表示length和density，和一个volume map表示flow direction. 可以用PCA进行降维. 此外还有illumination部分，用了HDR（environment map？），并用PCA进行降维。
项目需要，3D头发表示
2021-8-16
NeRF&#43;&#43;: Analyzing and Improving Neural Radiance Fields
arXiv2021
NeRF理论上可能得到病态解，之所以没得到，如果在一个shape-radiance ambiguity中，不同角度很高频，是因为Nerf的网络结果对于方向比较低频，因为方向在MLP的靠后层送入，且只用了4阶傅里叶。第二点，此外远方场景有远距离生成不好，生成好的距离偏近的tradeoff，所以采用两个NeRF，内层和外层分别渲染. 内层为世界原点周围的半径为1的球。 对于半径r大于1的外层渲染，可以写成四元组 ( x ′ , y ′ , z ′ , 1 / r ) (x&#39;, y&#39;, z&#39;, 1/r) (x′,y′,z′,1/r)，其中 ( x ′ , y ′ , z ′ ) (x&#39;, y&#39;, z&#39;) (x′,y′,z′)是unit vector." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/601a36a33d573d247a09695eb95f44a6/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-03-16T18:26:07+08:00" />
<meta property="article:modified_time" content="2020-03-16T18:26:07+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">论文阅读记录 51-100篇 20200316-20210817</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <ol><li> <p>2021-8-17<br> A high fidelity synthetic face framework for computer vision<br> CG人脸生成<br> 略读。用的类似3DMM的方法，分成了identity和expression。这优化好像是数据集的基向量和系数同时进行。采样时用GMM. expression采用了51维表示和15个姿态参数，他的标注方法我没看懂。。。texture没细看，大概用了blender，用vae在albedo texture data上表示。Hair用3D hair strands对单根头发表示。用Blender’s particle hair system，头发区域会deformed根据head shape change。头发颜色用三个标量表示，proportion of melanin（黑色素）, pheomelanin（褐色素）and proportion of gray hairs（白发比例）。Hair的表示采用了volumetric flow direction and occupancy based parametrization. 每一个hair包括两个UV maps 表示length和density，和一个volume map表示flow direction. 可以用PCA进行降维. 此外还有illumination部分，用了HDR（environment map？），并用PCA进行降维。<br> 项目需要，3D头发表示<br> <img src="https://images2.imgbox.com/37/94/NGUCiYrp_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/29/44/ZQfCVdMI_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/8e/ec/ikvFsJdf_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a5/36/H71bEstp_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/60/4c/Vf5JQOe5_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/00/0a/KkolpHKH_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ce/01/uVltvnue_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2021-8-16<br> NeRF++: Analyzing and Improving Neural Radiance Fields<br> arXiv2021<br> NeRF理论上可能得到病态解，之所以没得到，如果在一个shape-radiance ambiguity中，不同角度很高频，是因为Nerf的网络结果对于方向比较低频，因为方向在MLP的靠后层送入，且只用了4阶傅里叶。第二点，此外远方场景有远距离生成不好，生成好的距离偏近的tradeoff，所以采用两个NeRF，内层和外层分别渲染. 内层为世界原点周围的半径为1的球。 对于半径r大于1的外层渲染，可以写成四元组<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           ( 
          
          
          
            x 
           
          
            ′ 
           
          
         
           , 
          
          
          
            y 
           
          
            ′ 
           
          
         
           , 
          
          
          
            z 
           
          
            ′ 
           
          
         
           , 
          
         
           1 
          
         
           / 
          
         
           r 
          
         
           ) 
          
         
        
          (x', y', z', 1/r) 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.00189em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.751892em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.751892em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.751892em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord">1</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right: 0.02778em;">r</span><span class="mclose">)</span></span></span></span></span>，其中<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           ( 
          
          
          
            x 
           
          
            ′ 
           
          
         
           , 
          
          
          
            y 
           
          
            ′ 
           
          
         
           , 
          
          
          
            z 
           
          
            ′ 
           
          
         
           ) 
          
         
        
          (x', y', z') 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.00189em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.751892em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.751892em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.751892em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>是unit vector.<br> 项目需要，经典文章<br> <img src="https://images2.imgbox.com/1c/91/kTIZiJdS_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/79/94/vkpGjhbs_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ce/0c/JsfThSCM_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/27/df/5tDVegAc_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/5a/88/CSJXDWIu_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/50/8d/GfyU1IRb_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/56/13/cKgzadJK_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/7a/d2/X5rsJbN5_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/95/b8/cfJts7Un_o.png" alt="在这里插入图片描述"></p> </li><li> <p>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis<br> ECCV2020 多视角生成，3D渲染<br> 这个Related Work可能有用: Niemeyer et al.和Sitzmann et al. 用不需要3Dshape标注来生成2D图像. Volumetric方法相比mesh-based method能表示更好，更适合用梯度方法. 输入是5维，3维坐标和2维角度，输出是颜色和密度. 约束包括不同视角相同位置的密度一致. 该函数把位置<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           x 
          
         
        
          \bm x 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.44444em; vertical-align: 0em;"></span><span class="mord"><span class="mord boldsymbol">x</span></span></span></span></span></span>用8层FC映射到密度<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           σ 
          
         
        
          \sigma 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">σ</span></span></span></span></span>，和256维向量，该向量和相机方向concate，得到RGB颜色. 渲染方程积分用分段采样近似。 输入直接送入网咯效果不好，从而用高频编码。此外用了Hierarchical volume sampling进行加速，用两个网络，coarse和fine. 先采样<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            N 
           
          
            c 
           
          
         
        
          N_c 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: -0.10903em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>个算coarse网络，再用coarse网络的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            N 
           
          
            c 
           
          
         
        
          N_c 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: -0.10903em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>个位置的“颜色影响权重”形成分段constant分布进行采样. 一个sense优化一个网络. Loss就是重构，数据集似乎是一个sense的多个方向. 最大的收获还是点的建模方式<br> 项目启发，经典文章<br> <img src="https://images2.imgbox.com/8d/da/gBetQVMn_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/6f/d0/tFWcc1Mk_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/31/37/rJ2De2In_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/15/19/8SLmw4hs_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/bc/07/HoEY0CnR_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/7f/69/XCvD6gLn_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a3/a4/b8K7fibm_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/fa/16/JyzYzmXQ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/bb/64/I3e6sf1v_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/f7/1c/64NwDVS1_o.png" alt="在这里插入图片描述"></p> </li><li> <p>Improved StyleGAN Embedding: Where are the Good Latents?<br> arXiv2020 12 StyleGAN隐空间位置<br> 略读。找个了StyleGAN的隐空间，适合做重构和编辑。在W Space中，先过LeakyReLU_5，再利用PCA进行whiten，得到P_N Space，对18个维度各自取值，形成P_N^+ 空间. 此时该空间很像标准高斯。Loss除了重构，还有P_N^+空间的L2。实验集中在重构、编辑、conditional embedding quality三部分<br> 项目需要<br> <img src="https://images2.imgbox.com/0b/60/PbEASo5Z_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ee/0d/GwlE1Wgo_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/97/55/6JXwOZJ8_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/08/57/SOBO36LY_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/49/46/kQs6izle_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/1b/4e/YkBr6H8s_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/15/43/NRwtaUlJ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/02/3f/tcywbKev_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/2b/c6/oF2Iyzln_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/5a/f5/MLViJiJY_o.png" alt="在这里插入图片描述"></p> </li><li> <p>SEAN: Image Synthesis with Semantic Region-Adaptive Normalization<br> CVPR2020, Face Parsing的编辑<br> 略读。和MaskGAN同任务. 类似SPADE，但是可以做编辑。loss包括conditional GAN、判别器的feature matching、perceptual loss. 具体见图。512维style<br> <img src="https://images2.imgbox.com/ef/fd/K7AavpBn_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/54/08/JUClAnXG_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/fb/68/u3Ookerj_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/63/ce/mWRVdgie_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2021-7-31<br> CONFIG: Controllable Neural Face Image Generation<br> ECCV2020. Face Generation<br> 略读。最大亮点是借用了个人脸生成器，也是他们微软的文章。并且认为这是两个Domain，但是能提出共性来。另外用VGGFace算perceptual loss作为fine-tune也是一个亮点吧。训练两个阶段，看图。此外，还有单张图的Fine-tune.<br> 项目需要<br> <img src="https://images2.imgbox.com/85/cc/5XXmHRp9_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/dd/3c/SnkhDRTo_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/6b/d7/DnZGb6WK_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/9f/10/yZ93Bm8B_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/2a/42/Jm4xJ0cX_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/02/0b/vuNpezpG_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/5d/59/1qhiTBCY_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/2f/c2/yfM1SgRT_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/17/3e/FLxatpPn_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/93/42/LzQPJnyH_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2021-7-29<br> Two-phase Hair Image Synthesis by Self-Enhancing Generative Model<br> Journal of Computer Graphics Forum 2019. Hair generation<br> Pix2pix+头发精修. 除了用gabor filter提orientation map，还提出了texture map，直接用gabor filter响应最大的值。Gabor filter公式写得好。Loss包括pixel-level reconstruction（数据集成对有监督）、adv、gram matrix（这个公式写得好，要除以CHW）、feature matching生成图算一次orientation map和texture map和输入保持一致。整个网络输入是2channle的，一个是前景segmentation，一个是笔画走向. 这篇文章还能做超分，没细看<br> Hair项目需要<br> <img src="https://images2.imgbox.com/1a/5c/KdhtaEdV_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/26/d7/tYM6SWuv_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/51/38/3w0JJSum_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b5/03/Z8dS6oeV_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ed/06/HbJad22w_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/01/cd/b0wYfTw0_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/51/07/DMbr549t_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2021-7-22<br> Towards High Fidelity Face Relighting with Realistic Shadows<br> CVPR2021<br> 略读，能产生shadow的relighting，几乎没看懂。。没用在Lab空间做，而是在YUV空间。数据集为DPR的造数据集和Extended Yale Face Database B，网络预测了ratio image<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            R 
           
          
            t 
           
          
         
        
          R_t 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.280556em;"><span class="" style="top: -2.55em; margin-left: -0.00773em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>和光照<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            l 
           
          
            p 
           
          
         
        
          l_p 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.980548em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，其中shadow border weights更加强调了阴影区域. 此外还算了环境光。shadow mask则是根据3D shape和SH光照推算<br> 顶会光照文<br> <img src="https://images2.imgbox.com/11/14/3wBZOqGN_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/dd/92/ogBB10Bs_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/e1/e6/byGH9MvS_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ad/f9/eAX2Pbs7_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b8/76/hA6gf0ZN_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2021-7-22<br> Neural Hair Rendering<br> ECCV2020 Hair render<br> 头发编辑。头发分为structure和appearance两部分。在real分支中，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            S 
           
          
            r 
           
          
         
        
          S_r 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: -0.05764em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>是通过一个方向filter提取的，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            E 
           
          
            a 
           
          
         
        
          E_a 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: -0.05764em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>提取appearance，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            G 
           
          
            r 
           
          
         
        
          G_r 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>生成真实头发重构。<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            S 
           
          
            f 
           
          
         
        
          S_f 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.969438em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.05764em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span>则利用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           h 
          
         
        
          h 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathdefault">h</span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            x 
           
          
            r 
           
          
         
        
          x_r 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.58056em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>进行渲染得到，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            E 
           
          
            f 
           
          
         
        
          E_f 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.969438em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.05764em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span>提取和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            E 
           
          
            r 
           
          
         
        
          E_r 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: -0.05764em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>相同内容，包括ID和hair 的shape和structure. <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            x 
           
          
            f 
           
          
         
        
          x_f 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.716668em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span>永远是黄头发，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            G 
           
          
            f 
           
          
         
        
          G_f 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.969438em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span>生成黄发人进行重构。训练分两阶段，先按照公式7训练，再fine-tune实现video的temporally-smooth（不同帧之间头发的丝滑过度），fine-tune时，50%概率利用motion flow来warp上一时刻的图像，另50%概率给0. <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           a 
          
         
        
          a 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">a</span></span></span></span></span>和wrap image一起concatenate之后，通过SPADE的方式送入<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            G 
           
          
            r 
           
          
         
        
          G_r 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span><br> 项目需要，Neural rendering<br> <img src="https://images2.imgbox.com/df/b9/72QRKseD_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/66/6b/8ygzuVzs_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/4a/a2/RnCBGPmf_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/e7/82/hoJioKIS_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/da/53/NORdLv6P_o.png" alt="在这里插入图片描述"><br> 上图<a href="https://mlchai.com/files/neural_hair_rendering_video.mp4" rel="nofollow">视频链接</a></p> </li><li> <p>2021-7-21<br> GIF: Generative Interpretable Faces<br> 3DV2020 CG人脸生成<br> 结合StyleGAN2和CG。把人脸3D的texture rendering和normal rendering放到了noise中直接相加（为什么要这样做？这样的话style vector就不能用texture和normal的信息了呀。另外代码中noise过了三次卷积，可能是为了对齐shape，并做一些后处理）人脸用了FLAME的子集RingNet建模，它比3DMM好像多了人脸转交pose的信息，FLAME中pose采用了<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           θ 
          
         
           ∈ 
          
          
          
            R 
           
          
            15 
           
          
         
        
          \bm \theta \in \mathbb R^{15} 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.73354em; vertical-align: -0.0391em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.03194em;">θ</span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span></span>，包括axis-angle rotations for global rotation and rotations aroundjoints for neck, jaw, and eyeballs). 此外还有一个Texture consistency，好像是在不改变style embedding、appearance和lighting的情况下，再生成一张脸，约束脸部的texture map保持一样（这样两张脸不是不能对的很齐嘛）<br> CG人脸生成<br> <img src="https://images2.imgbox.com/08/ad/Uh5J5xd5_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/92/5a/ynfFGVV9_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/30/0f/0sd5mpnC_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/59/27/txeKz3PK_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2021-7-21<br> Relighting Images in the Wild with a Self-Supervised Siamese Auto-Encoder<br> WACV2021 Relighting with self-supervised<br> 略读。Loss分三项 1. GAN 2. reconstruction 3. 图像翻转、旋转、inversion（自己提的一个操作）扩充后SH的约束。（这我感觉做不出来吧，约束太少了，怎么保证光照信息不会全0呢，以及怎么保证光照随SH单调。图像反转后域是不是都变了？）而且公式5、6怀疑正负号有错。<br> 自监督光照<br> <img src="https://images2.imgbox.com/21/88/d1yvkn6R_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/fd/09/S2vUxkGt_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/9c/24/4dBrYRhF_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/dd/20/DHRj8V9b_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/1f/c6/9WPhH88g_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/d4/0b/eVftBXeS_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/c6/70/aDRCJMZX_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b0/0c/7grRUjS6_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2021-7-21<br> Real-Time Hair Rendering using Sequential Adversarial Networks<br> ECCV2018 头发渲染<br> 利用3个独立的autoencoder，实现从CG hair model到真实头发的渲染. CG hair model中每条线都是一根头发。头发可以把颜色、光照、结构一步步解耦出来。每一个autoencoder采用了VAE+GAN. 数据为CelebAHQ自行分割。从gray图到orientation map采用了DoG filter.<br> 头发渲染，项目需要<br> <img src="https://images2.imgbox.com/d2/1e/GnKa8424_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/96/46/BxVAsAlm_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/19/77/zSWB83Qh_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/4d/dd/uFrkgTNs_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/7c/a2/iQ3l8H2U_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/81/99/Wij1nfJS_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2021-7-17<br> GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields<br> CVPR2021 Best Paper, Neural Rendering<br> 该方法利用无标注数据集，实现图像基于3D的生成。具体为，每一个3D方向点映射到特征空间<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           f 
          
         
        
          \bm f 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right: 0.11042em;">f</span></span></span></span></span></span>和volume density <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           σ 
          
         
        
          \sigma 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">σ</span></span></span></span></span>, 经过Composition Operator <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           C 
          
         
        
          \mathcal C 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathcal" style="margin-right: 0.05834em;">C</span></span></span></span></span>, <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            π 
           
           
           
             v 
            
           
             o 
            
           
             l 
            
           
          
         
        
          \pi_{vol} 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.58056em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03588em;">v</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight" style="margin-right: 0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>是一个物理渲染过程，这里只渲染到了低纬度，高维采用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            π 
           
          
            θ 
           
           
           
             n 
            
           
             e 
            
           
             u 
            
           
             r 
            
           
             a 
            
           
             l 
            
           
          
         
        
          \pi_{\theta}^{neural} 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.13222em; vertical-align: -0.283108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -2.41689em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.02778em;">θ</span></span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight" style="margin-right: 0.02778em;">r</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight" style="margin-right: 0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.283108em;"><span class=""></span></span></span></span></span></span></span></span></span></span>进行2D渲染。整体是GAN的结构。（一些没搞懂的问题，shape和style如何分离的，STR如何获得）<br> 项目需要，3D编辑好文<img src="https://images2.imgbox.com/df/cb/5MsI0pOO_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/6d/d9/JCpdbgRo_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/64/9f/XNfIaXBQ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/92/a3/d1z1LPW0_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/48/70/zayEjy7g_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/2a/68/TC9WwzZy_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a6/c1/4yMr1evN_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/5b/50/LSsQDCmf_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/92/a4/MeqNN7aS_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/46/f1/rVphW1rU_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/0e/66/jLyYU95E_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ec/f9/0v8iiBpC_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/2d/4e/WRGprlji_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/db/1c/NoDJogjn_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/32/79/bAV8eyM9_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/65/49/Mom5gtVc_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2021-6-29<br> Image-to-image Translation via Hierarchical Style Disentanglement<br> CVPR2021 oral, multi-att，multi-modal，instance-level人脸属性编辑<br> 该方法像是ModularGAN的加强版，编辑过程包括E,T,G. E负责编码得到整张图的Feature map，T负责转换feature map，来使图像attribute符合style，G负责把feature map转为图像。注意T是不同属性tags用不同的参数. 判别器则精细到了tags-attribute，即不同属性的不同取值用一个D，有点类似StarGAN v2. Loss包括各种重构，风格编码重构，对抗. 风格编码从高斯分布提取，经过M网络转换，也可以从图像中通过F网络提取，这一块也很想StarGAN v2. 一些细节：把属性分为了tags，每个tags有不同的取值。中判别器是对于每种属性（文中的tags）的每种取值（文中的attributes）各一个。为了缓解编辑时属性两侧取值的无关属性不平衡（例如黄头发女性多，改变发色可能影响性别），把无关属性取值也输入D当中，形成cGAN（感觉上这样能防止一部分变无关属性吧）。网络结构backbone是一堆resblock，有StarGANv2的感觉。<br> 项目需要，人脸编辑好文<br> <img src="https://images2.imgbox.com/79/c3/LLQ5SWGT_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ac/6a/0mRwBo4B_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/b7/08/LRgvwvPm_o.png" alt="在这里插入图片描述"><br> ![在这里插入图片描述![](https://img-blog.csdnimg.cn/20210629163708150.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyMDcxODQ5,size_16,color_FFFFFF,t_70)![在这里插入图片描述![](https://img-blog.csdnimg.cn/20210629163710816.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyMDcxODQ5,size_16,color_FFFFFF,t_70)</p> </li><li> <p>2021.6.27<br> SC-FEGAN: Face Editing Generative Adversarial Network with User’s Sketch and Color<br> ICCV2019 交互式编辑<br> 其实我觉得这个方法更像是inpainting，重点是造数据，模拟mask生成（随机画线），color map通过face parsing提取各自部分的中位数颜色得到，sketch也有方法提取，头发区域用GFC特殊提取；生成器用了gated conv. Loss包括WGAN-GP、像素重构、perceptual loss重构、style gram matrix重构，total variance（只用重构图像就能重构这么好呀）<br> 项目需要、有趣工作<br> <img src="https://images2.imgbox.com/49/57/Q9xV4PvG_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ef/cb/VtXHJAKn_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/d3/5a/Tilm7til_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b9/f2/irwRl0Kl_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/69/fd/RQ30trsn_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/0b/7f/jA8YOgfW_o.png" alt="在这里插入图片描述"><br> 这里VGG Loss居然这么影响效果</p> </li><li> <p>2021.6.25<br> MichiGAN: Multi-Input-Conditioned Hair Image Generation for Portrait Editing<br> SIGGRAPH2020 编辑头发，基于学习，Barbershop和LOHO的之前工作<br> 精读。头发分appearance（颜色）、structure（直、卷等）、shape三个部分。<br> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            I 
           
           
           
             o 
            
           
             u 
            
           
             t 
            
           
          
         
           = 
          
         
           G 
          
         
           ( 
          
         
           M 
          
         
           , 
          
         
           O 
          
         
           , 
          
          
          
            I 
           
           
           
             r 
            
           
             e 
            
           
             f 
            
           
          
         
           , 
          
         
           I 
          
         
           ) 
          
         
        
          I_{out}=\mathcal G(M, O, I_{ref}, I) 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.280556em;"><span class="" style="top: -2.55em; margin-left: -0.07847em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.03611em; vertical-align: -0.286108em;"></span><span class="mord mathcal" style="margin-right: 0.0593em;">G</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.10903em;">M</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.02778em;">O</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.07847em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.02778em;">r</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right: 0.10764em;">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.07847em;">I</span><span class="mclose">)</span></span></span></span></span>，其中<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           M 
          
         
        
          M 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.10903em;">M</span></span></span></span></span>是shape mask，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           O 
          
         
        
          O 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.02778em;">O</span></span></span></span></span>是Gabor filters（方向检测）的feature map，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            I 
           
           
           
             r 
            
           
             e 
            
           
             f 
            
           
          
         
        
          I_{ref} 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.969438em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.07847em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.02778em;">r</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right: 0.10764em;">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span>提供了appearance. background需要和头发区域巧妙融合在一起，所以在backbone靠后层不断用mask blend. 训练loss就是Lab空间重构、gabor kernel 的feature map重构、perceptual loss、对抗. （所以这里也没有真实成对数据啊，效果到底怎么样未知）. 此外该文章可以用于交互式编辑. 该文章细节处需要inpainting方法<br> 项目需要<br> <img src="https://images2.imgbox.com/62/62/bfUspfeU_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/75/18/6FrUIIDS_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/cc/08/7xTCTzaY_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/79/f0/D8xP7yfZ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/95/a3/78SWibC4_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/cf/d0/w3z2mtRW_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/76/2d/uSBUhEJR_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a1/c8/1krzyUzS_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ec/cf/6yE3XEWT_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2021.6.23<br> Barbershop: GAN-based Image Compositing using Segmentation Masks<br> arXiv2106 StyleGAN2 embedding操纵，类似LOLO，编辑头发，也能编辑其他区域<br> 精读，设计FS空间，其中F是StyleGAN2生成的中间一层<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           32 
          
         
           × 
          
         
           × 
          
         
           512 
          
         
        
          32\times\times512 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">3</span><span class="mord">2</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">×</span><span class="mord">5</span><span class="mord">1</span><span class="mord">2</span></span></span></span></span>，S是剩下的W噪声<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           10 
          
         
           × 
          
         
           512 
          
         
        
          10\times 512 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">1</span><span class="mord">0</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">5</span><span class="mord">1</span><span class="mord">2</span></span></span></span></span>. 方法是先重构，向最终Segmentation对齐，对于每张输入图和 最终Segmentation中从该图来的Semantic Mask重叠 的位置，F直接复制过来，否则从重构的W生成. 最终Blend图的F来自各个对齐图的F. S则可以来自另外的图像. S控制了更精细的appearance. 为了和Mask图像对齐，引入了人脸Segmentation网络，并优化. align F还用了Gram Matrix的loss（Gram matrix到底怎么用的，这里似乎是匹配内容，而LOHO似乎匹配风格），其他基本都是LPIPS Loss. User Study显示该方法远好于LOHO和MichiGAN. 主要该方法对于不一致的人脸鲁棒一些吧<br> 项目需要<br> <img src="https://images2.imgbox.com/bd/d8/KGkE9ZxG_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/13/ca/BoiXGNbU_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/fb/05/mH93hbui_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/df/61/UZe9AUr2_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a0/6e/xt9Pu26r_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/be/1e/86cIstJZ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/d9/d9/ZCyDtooz_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2021.6.21<br> LOHO: Latent Optimization of Hairstyles via Orthogonalization<br> CVPR2021 StyleGAN2 embedding操纵，类似Image2StyleGAN++，编辑头发<br> 精读，基于优化，得到StyleGAN2的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            W 
           
          
            + 
           
          
         
           , 
          
         
           N 
          
         
        
          \mathcal {W}^+,\mathcal N 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.965771em; vertical-align: -0.19444em;"></span><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right: 0.08222em;">W</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.771331em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathcal" style="margin-right: 0.14736em;">N</span></span></span></span></span>. 头发分成1）perceptual structure；2）appearance；3）style. Loss分为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            L 
           
          
            f 
           
          
         
        
          L_f 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.969438em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span>用perceptual loss保持person1的id；<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            L 
           
          
            r 
           
          
         
        
          L_r 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>用perceptual loss保持person2的structure；<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            L 
           
          
            a 
           
          
         
        
          L_a 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>用vgg feature average pooling保持person3的appearance；<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            L 
           
          
            s 
           
          
         
        
          L_s 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>用vgg feature的gram matrix保持style，以上loss都是有各自mask的. <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            L 
           
          
            n 
           
          
         
        
          L_n 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> noise map损失用于让<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           n 
          
         
           ∈ 
          
         
           N 
          
         
        
          n\in \mathcal N 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5782em; vertical-align: -0.0391em;"></span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathcal" style="margin-right: 0.14736em;">N</span></span></span></span></span>正则化（？）. 优化时分了两步，第一步先专心优化形状。传梯度时，投影的方法剔除了person2的style和appearance影响. 数据集来自FFHQ. 该方法需要头发较为对齐<br> 项目需要<br> <img src="https://images2.imgbox.com/dc/c5/X7Glrd98_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/80/9f/dUVjNUe6_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/c9/bb/vbdXhy7V_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/20/44/pyy4SVbu_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/71/5a/5xl7PI0O_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/23/ea/uMFXGabV_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b0/bb/IcydySTo_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ad/a8/J1AsPUVy_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/bd/9f/PndIMTXf_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/30/1f/S2dVCeXr_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2021.5.26<br> Im2Vec: Synthesizing Vector Graphics without Vector Supervision<br> CVPR2021 oral<br> 矢量图重构，训练不需要标注<br> 结构化生成 paper reading<br> <img src="https://images2.imgbox.com/54/76/BFjCmsum_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/20/ad/t8a5gZH1_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/e8/41/0x9vmN1W_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/fd/15/MQ9QDgTK_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/38/5d/KoFOzHLQ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/6e/21/30mWGJdo_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/6e/43/46j2gutl_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/6c/37/Uw8NZdTA_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ce/19/gxp3L2Fp_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020.12.15<br> House-GAN: Relational Generative Adversarial Networks for Graph-constrained House Layout Generation<br> ECCV2020 oral<br> 房型图生成，Conv-MPN输入，CONV-MPN是对GNN的一种结点用3维图像信息的扩展<br> 图+GAN paper reading<br> <img src="https://images2.imgbox.com/71/6c/tIZCUvcs_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a5/74/PnymQf25_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/cf/1e/txCd0b9z_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/e8/29/IoNbFLPJ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/48/13/bPxYLWUa_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ac/fb/QXcEhFbr_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a5/bb/iamY8fSn_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020.11.16<br> AOT: Appearance Optimal Transport Based Identity Swapping for Forgery Detection<br> NeurIPS2020 略读（只扫了一眼方法）<br> 对其他论文的换脸之后进行精修。基于最优传输Appearance Optimal Transport(AOT). 和Portrait lighting transfer using a mass transport approach类似. 换脸之后得到<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            X 
           
          
            r 
           
          
         
        
          X_r 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: -0.07847em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，用Perceptual Encoder提特征，得到<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            F 
           
           
           
             X 
            
           
             r 
            
           
          
            1 
           
          
         
        
          F_{X_r}^1 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.18954em; vertical-align: -0.375431em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -2.42467em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.164543em;"><span class="" style="top: -2.357em; margin-left: -0.07847em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span class=""></span></span></span></span></span></span></span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.375431em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            F 
           
           
           
             X 
            
           
             r 
            
           
          
            2 
           
          
         
        
          F_{X_r}^2 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.18954em; vertical-align: -0.375431em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -2.42467em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.164543em;"><span class="" style="top: -2.357em; margin-left: -0.07847em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span class=""></span></span></span></span></span></span></span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.375431em;"><span class=""></span></span></span></span></span></span></span></span></span></span>等。和真实图片的特征<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            F 
           
           
           
             x 
            
           
             t 
            
           
          
            1 
           
          
         
        
          F_{x_t}^1 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.16121em; vertical-align: -0.3471em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -2.453em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.296343em;"><span class="" style="top: -2.357em; margin-left: 0em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span class=""></span></span></span></span></span></span></span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.3471em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            F 
           
           
           
             x 
            
           
             t 
            
           
          
            2 
           
          
         
        
          F_{x_t}^2 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.16121em; vertical-align: -0.3471em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -2.453em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.296343em;"><span class="" style="top: -2.357em; margin-left: 0em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span class=""></span></span></span></span></span></span></span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.3471em;"><span class=""></span></span></span></span></span></span></span></span></span></span>等之间解最优传输（实际上是在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           v 
          
         
        
          v 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">v</span></span></span></span></span>上解，不是在像素上，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           v 
          
         
           = 
          
         
           ( 
          
         
           f 
          
         
           , 
          
         
           x 
          
         
           , 
          
         
           n 
          
         
           ) 
          
         
        
          v=(f,x,n) 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">v</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.10764em;">f</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault">n</span><span class="mclose">)</span></span></span></span></span>，除了特征<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           f 
          
         
        
          f 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.10764em;">f</span></span></span></span></span>外，还有像素级position、normal），不好直接解，借鉴WGAN的方式，用了两层和三层的全连接小网络。最后Face Decoder的loss包括如下截图. <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            L 
           
           
           
             M 
            
           
             S 
            
           
             D 
            
           
          
         
        
          \mathcal L_{MSD} 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathcal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.328331em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.10903em;">M</span><span class="mord mathdefault mtight" style="margin-right: 0.05764em;">S</span><span class="mord mathdefault mtight" style="margin-right: 0.02778em;">D</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>是一个亮点，编辑后，对真实和虚假图像进行随机Mask融合，然后用D去猜Mask<br> 导师推荐<br> <img src="https://images2.imgbox.com/7d/a9/IE5xgZ02_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ce/48/EOTHmMfC_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/f5/6d/TqPJNnRQ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/0c/c2/pgXFAGxH_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/c9/a8/BHSOJMNk_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/02/d2/i4XfJhZH_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/2b/8c/jzNNqN4X_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/f4/ce/rgTivhs4_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/46/53/eJXHScfe_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020.9.2<br> Image2StyleGAN++: How to Edit the Embedded Images?<br> CVPR2020 StyleGAN embedding操纵<br> 跳读。基于Image2StyleGAN做了一些改进。1：优化涉及到了noise。并且先优化w，再优化noise（优化noise不会提升PSNR），然后再优化w，挺好的思路。2）设计了很多Mask，这样可以做两张图像之间的融合，利用mask。3）设计了Style Loss，我觉得好像就还是单层的VGG Perceptual Loss. 4）从不同的spatial位置对卷积中间的特征结果进行融合。基于这些改进，做了非常非常多的application，有些没有细看。比较有借鉴意义的是stylegan做inpainting、基于mask的local style transfer和attribute transfer<br> StyleGAN猜w和n<br> <img src="https://images2.imgbox.com/0f/39/9e42VORn_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a4/61/Ud9MUAxH_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/8d/c5/qyM2ezGF_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b0/f3/8VKJgF25_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/47/3c/m3Yt3qQa_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/64/10/iN9ojIqW_o.png" alt="在这里插入图片描述"></p> </li><li> <p>Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?<br> ICCV2019 StyleGAN猜latent code<br> 读的还行吧，不算精读，不算扫读。方法很直接，对StyleGAN里的参数w进行<strong>优化</strong>，优化的初始值很重要，对于人脸这种比较一致的集合，初始化成w的平均值会比较好；对于猫狗这种差异大的，初始化成一个U(-1, 1). loss function两项：1）perceptual loss；2）pixel-level loss. 用这些w做了不少实验，包括interpolation、combination（style transfer）、把图像反转/平移之类的再进行优化。<br> StyleGAN猜w<br> <img src="https://images2.imgbox.com/37/07/p8RDiXSd_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/e8/33/fdtdXGyn_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/4f/b6/3oYMIiI0_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/75/d6/S70pXSTb_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ad/ef/fLyRcovP_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/59/89/hktX2qQw_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/62/f9/MkDTOl8R_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-9-1<br> StyleFlow: Aribute-conditioned Exploration of StyleGAN-Generated Images using Conditional Continuous Normalizing Flows<br> arXiv 202008 StyleGAN based 光照编辑<br> 文章很长，方法部分读的细，实验几乎只看了几眼图。StyleGAN中的w到Image这一段不动，但是要编辑，就需要一个合理的w。所以本文设计了一个神经网络Φ用于给定先验分布z和属性a，生成合理的w；换句话说就是学习了一个映射。这个过程建模成了continuous normalizing flows（比较复杂，是一个ODE）。w、a、Image先采样，形成数据集。然后训练Φ。目标函数：1）让<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            z 
           
          
            0 
           
          
         
        
          z_0 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.58056em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.04398em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>通过约束L2符合高斯；2）让w的概率最高。（这里很难讲，建议看原文）。编辑的时候，光照只改变7-11层的w，比较靠后，不用全变。<br> jiping老师推荐，和DisentangledGAN功能上很像<br> <img src="https://images2.imgbox.com/c4/f4/rwM80J9j_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/2a/ba/YvoqiYvd_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/4d/e6/C6r8v3c2_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/f1/7b/7A02k08T_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/1b/eb/2I2jYn6V_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/81/6d/3BWZQJln_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/2a/b1/JB9ZQMQM_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/0e/e4/7MOrUqOm_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-8-31<br> Analyzing and Improving the Image Quality of StyleGAN<br> CVPR2020 StyleGAN2<br> 精读，写的真tm好。对StyleGAN进行了修补。（1）为了防止生成图像中的局部“水滴”瑕疵，没有直接采用in，用另一种结构进行替换。比较复杂，建议看原文。总之这种新结构把in、adain的shift，都放到了网络权重上。（2）Lazy regularization，过16minibatches才算一次参数L1。（3）Path length regularization. 约束了外部风格w对生成图像的影响偏导，让path稳定。（4）换掉progressive growing的网络结构，实验表明，生成器用低分辨率到高分辨率skip 叠加的方式，判别器用residual net更好（residual本来就是为了做分类）（5）这样的生成结构在高分辨率的结果锐利、但不真实，所以倍增了channel数量。论文还做了images到latent space的映射，没细看；该映射也可以用来进行伪造判定。<br> StyleFlow前置、经典文章，不得不看<br> <img src="https://images2.imgbox.com/53/46/WWLRVt1C_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a0/31/92a6Jcbo_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/c5/9f/ljJAUUto_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/1a/a8/CCYQy1XJ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/84/41/ptEP6yW2_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/1a/6e/7Tepb1HZ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/3f/af/c3gmzBGa_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/06/8e/mawNvq90_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/1a/f5/5ojB9Krt_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/85/69/KfYd43lB_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-8-11<br> Illumination-invariant Face Recognition with Deep Relit Face Images<br> WACV19 Relighting<br> 扫读，只读了Relighting有关的方法部分。用Relighting可以做face recognition。relighting方法比较trivial，不过亮点是球谐光照的漫反射写的比较详细，而且是和neural face editing不一样的写法，而且写的更贴近原理，不过两种表达方式等价。用E2FAR的方法估计shape，设计了一个DLRN的预训好的resnet估计lighting，albedo似乎从剩下的部分算出来。另外这边文章用的是BFM-2017<br> Relighting相关<br> <img src="https://images2.imgbox.com/77/25/W9V2RHNT_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/2c/37/yPvy6Low_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/76/cf/OUIp8vqa_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ec/7f/82Qu193z_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-8-11<br> Disentangled and Controllable Face Image Generation via 3D Imitative-Contrastive Learning<br> CVPR20 物理人脸生成<br> 扫读，实验部分大致看了relighting相关。很好的文章啊！生成的时候把人脸分成3DMM那一堆，每个参数都用vae训了一下，让它能到人工设计分布。生成中用网络生成，训练分两大步，第一步imitative learning和rendering生成的计算face recognition id loss，68特征点loss，光照预测loss，和人脸部分平均颜色回归。这里光照预测用的应该是3DFaceDeepReconstruction。第二大步，为了精细解绑，设计了contrastive loss，对于只改变表情，计算rendering在表情改变前后的flow field，并warp到网络生成的图像上，和网络直接生成的结果做差，进行对比；另一方面，对于只改变光照，要求id不变，hair segmentation probability map不变，68特征点不变。用了Style GAN的网络。无成对数据训练！文章末尾还进行了一些latent space理论特性的验证<br> Relighting相关<br> <img src="https://images2.imgbox.com/2c/b2/q7z2MHtX_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/65/41/n8p96bQg_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/72/f4/bOtP36Ry_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/96/6f/oUk6yGH9_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/3d/4b/HdJ9n5eX_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/14/0e/xZgoBl3z_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-8-10<br> SfSNet: Learning Shape, Reflectance and Illuminance of Faces ‘in the wild’<br> CVPR2018 人脸3D重建和再生成<br> 扫读，没读实验。这论文可能时间有点久，现在看来可能有些trivial。另外论文写的奇怪，训练过程到底是什么样的。哪些loss对应生成数据，哪些loss对应真实数据？我猜应该是image-level的对应真实，其他的对应生成吧。朗博反射模型。残差的网络结构可能是个亮点<br> 人脸3D重建<br> <img src="https://images2.imgbox.com/2b/b1/6vQcky9x_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/7f/37/i2Agt2CU_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/3d/76/LGMq9icA_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/39/c8/n3KIhx3t_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-8-09<br> Towards High-fidelity Nonlinear 3D Face Morphable Model<br> CVPR19 人脸3D重建<br> 扫读，没读实验。仅仅读了方法部分，nonlinear 3DMM改进版。考虑到nonlinear 3DMM中正则项会丢失很多shape和albedo细节，把原先学的shape和albedo当作代理Proxy，让Encoder再生成一个真正的shape和albedo，训的时候用新的shape和albedo proxy在一起，优化新的shape；新的albedo同理。不优化新shape和新albedo在一起的那支，因为会让新shape获得在新albedo和albedo proxy之间的不好解。另外生成的网络换成了两路global+local，local有四个，每个去预测单独的部分<br> Nonlinear 3DMM后续作品<br> <img src="https://images2.imgbox.com/79/5a/NOmLjosU_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/c0/eb/SQuLbjAJ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/e5/45/KLULdQel_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/4b/1c/GZeRhCtS_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/e6/1d/Dl4UXbiB_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-8-09<br> On Learning 3D Face Morphable Model from In-the-wild Images<br> PAMI19 人脸3D重建<br> 扫读，没读实验。重读。3DMM的nonlinear版。比较有趣的是albedo用了UV坐标系，并且shape也用了UV坐标系，很神奇，我觉的y的UV坐标系不是固定的吗?不过都用UV图像可以用CNN网络训练。3阶球谐光照和漫反射。用一个外部的Mask分割只重建人脸区域，并且能免去头发等遮挡。Loss有图像重构、特征点重构（似乎在UV坐标系上进行）、VGG Perception Loss。正则化项很有启发，albedo要对称、albedo要局部一致（这个loss有点迷）、shape要平滑（total variance）。由于优化空间太大，所以先用其他方法生成shape、texture、projection matrix的ground truth重构，待初步收敛再真正训练。可以做relighting，这里relighting不知道为什么感觉背景也变了，不懂。。。<br> 经典人脸3D重建、Relighting相关<br> <img src="https://images2.imgbox.com/7e/5b/KmzHTjX4_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/3c/5c/7j0uP0DS_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/74/a5/Xiw0Tefh_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/cc/b8/tyl8YKEQ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a4/40/J4SdQUE7_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/74/cd/0foBxvJ6_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ef/2b/tpuTB3Xk_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/34/ae/OeZLehdv_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/22/77/7yLXHme1_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-7-30<br> Accurate 3D Face Reconstruction withWeakly-Supervised Learning: From Single Image to Image Set<br> CVPRW2019 人脸3D重建<br> 3DMM + HS光照，HS9个系数。一共学习239个系数。Loss很naive啊，和ground truth之间pixel重构，68特征点重构（不知道是不是能从3D模型直接得到3D特征点），FaceNet的输出作为feature的inception loss。还有两项正则化，分别是3DMM系数的L2，还有一个没看懂的。。flattening constrain to penalize the texture map variance。这Loss不会像None Linear 3DMM说的那样产生病态解嘛？对同人的多张图片的3DMM中id系数进行加权训练，学一个正的权重系数网络。<br> 人脸3D重建 Relighting被huawei推荐<br> <img src="https://images2.imgbox.com/e0/b0/yuSqOsvH_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/20/44/andpi7C7_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/cd/1a/vsOXzaym_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/3b/83/tn2ZQ1BL_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/2d/14/fVjMXgXK_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/97/fd/Aim7AGQB_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-7-29<br> Face Alignment in Full Pose Range: A 3D Total Solution<br> 3DDFA 3D人脸重建<br> 实验部分没读。级联生成人脸参数。旋转参数用了四元数，因为欧拉角会出现两组欧拉角映射到一组姿态的问题。Loss应该就是直接和Ground Truth反向传播时因为不同参数发挥的作用重要程度可能不同，所以弄很多种不同的优化策略，又是雅可比矩阵、又是泰勒展开。。。搞出来PAF和PNCC两种特征图。<br> Relighting DPR采用<br> <img src="https://images2.imgbox.com/e3/c1/Dvx7iKuC_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/e0/43/D0qdCtcA_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/46/4e/ZOttf6T5_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-7-22<br> Portrait lighting transfer using a mass transport approach<br> SIGGRAPH2017 Relighting DIP方法 样例级<br> 扫读。和颜色直方图方法类似。构建图像的复合向量：c = {2维像素位置、3维RGB、3维法向}叉乘，然后再和参考图像之间构成一一映射，使得<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           c 
          
         
        
          c 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">c</span></span></span></span></span>之间的距离和最小。这个和推土距离同理，只不过推土距离是一维的，这个8维。不同维度之间可以有不同的权重系数。这个映射可能会产生不好结果，尤其是姿态差异大时，所以会在每个像素用高斯模糊去多次采样，谓之正则化。另外，这种算法比较耗时，所以换成一种迭代的方式，我没有细看。如果只想改变亮度，则可以在Lab空间中的L分量上搞，不用RGB。当分辨率过高时，设计了一种两阶段的方式。实验没有细看。<br> Relighting CG这边的文章<br> <img src="https://images2.imgbox.com/3c/f8/ILwWESKP_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/3c/91/nYY0Poeu_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/eb/0b/0muvKwT8_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/2c/cf/LjGX8EPN_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b5/39/2HGIz9Z6_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/27/db/XJoVXNaM_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-7-20<br> Single Image Portrait Relighting<br> TOG2019 环境光Relighting<br> 实验部分没怎么读。采了一个很复杂的数据集，每个人7个机位，然后套上一堆不同的环境光。然后直接用神经网络训。Loss就是ground truth，恒等重构，还有光着重构。autoencoder结构，把环境光照解出来。造数据集的过程是亮点，可以参考。<br> 光照文章<br> <img src="https://images2.imgbox.com/d8/24/G8bmLoid_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/7d/9a/DHTIRdFx_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/d7/23/iq86QcQ3_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/fd/62/3vObCzM1_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/78/6d/ixuLJNFP_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-7-16<br> Learning Physics-guided Face Relighting under Directional Light<br> CVPR2020 relighting<br> 扫读。把人脸分解出反照、形状三部分，然后换光源方向和大小，重新渲染。用了一个residual阶段来预测非漫反射光，是个亮点。用visibility来标记可见与不可见区域。公式推的不错。这个图片好像只能是黑背景？训练方面用PMS先得到albedo、shading、residual，当作ground truth，然后训练，loss可以<strong>选用</strong>L1、L2、LPIPS等等。该方法可以用于复杂环境光，但是我没看懂到底怎么做的<br> 光照文章<br> <img src="https://images2.imgbox.com/a1/c5/4fQGvHtj_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/16/82/6ZcFQnAQ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/d7/2b/xHk7itRm_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a2/30/s8ZVNNFh_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/5b/d6/ck6X7VP5_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ec/7f/LQYRyhnG_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/01/76/TIOFOzQQ_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-7-15<br> Deep Single-Image Portrait Relighting<br> ICCV2019 Relighting<br> 扫读。先用3DDFA拿人脸3DMM形状参数，再用ARAP based method（ESGP2007）通过特征点三角剖分进行精修。再用SfsNet进行光照提取和渲染，造数据集，包括目标光照和图像ground truth。 神经网络训一波，光照重构、人脸生成图像重构、GAN对抗、perception loss. 分辨率512到1024. 数据集从Celeba-HQ构造. Multi-Pie测试、算定量指标（这个数据集光照成对）。<br> 光照文章<br> <img src="https://images2.imgbox.com/8b/88/ZJ5JO6jp_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/57/40/7MkfvUSk_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/96/86/PI35XSRR_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/5e/87/Gt09mDUZ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a9/d5/ZqVlW1Nc_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/7c/09/kBw2wAJj_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/50/fe/tPkC2ZRG_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-6-13<br> A Morphable Model For The Synthesis Of 3D Faces<br> SIGGRAPH1999 3DMM<br> 把人脸分解为shape和texture，用pca得到shape和texture的低维表达。其中低维表达的系数<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           α 
          
         
           , 
          
         
           β 
          
         
        
          \alpha, \beta 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.05278em;">β</span></span></span></span></span>服从高斯分布，是主要优化的参数。<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            S 
           
          
            ˉ 
           
          
         
        
          \bar S 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.82011em; vertical-align: 0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.82011em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathdefault" style="margin-right: 0.05764em;">S</span></span><span class="" style="top: -3.25233em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.16666em;">ˉ</span></span></span></span></span></span></span></span></span></span>, <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            C 
           
          
            ˉ 
           
          
         
        
          \bar C 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.82011em; vertical-align: 0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.82011em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathdefault" style="margin-right: 0.07153em;">C</span></span><span class="" style="top: -3.25233em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.16666em;">ˉ</span></span></span></span></span></span></span></span></span></span>是平均脸，主要优化对象。<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           ρ 
          
         
        
          \rho 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathdefault">ρ</span></span></span></span></span>是照相机参数，也是优化对象。“随机”梯度下降优化时似乎是对一小块区域进行优化。<br> 3D人脸经典文章<br> <img src="https://images2.imgbox.com/12/3b/4IIlCTim_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/d6/b0/0wjVG9Hh_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/23/1b/RylFzaoO_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/7e/3b/0JVRPsTn_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/76/67/orGSAGFE_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/53/67/i8nA8Vrq_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b8/d0/hMeTWznl_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/4d/82/W9aFVetn_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ef/d8/IrGd9Qzg_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/fe/71/PXCzYkgj_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-6-4<br> Face2Face: Real-time Face Capture and Reenactment of RGB Videos<br> CVPR16 视频表情操纵，经典文章<br> 关注人脸生成，表情操纵，嘴巴召回。multi-linear PCA model去进行人脸生成。<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            a 
           
           
           
             i 
            
           
             d 
            
           
          
         
        
          a_{id} 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.58056em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            a 
           
           
           
             a 
            
           
             l 
            
           
             b 
            
           
          
         
        
          a_{alb} 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.58056em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>是均值，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           α 
          
         
           , 
          
         
           β 
          
         
           , 
          
         
           δ 
          
         
        
          \alpha, \beta,\delta 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.05278em;">β</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.03785em;">δ</span></span></span></span></span>是标准差。各个E看起来是标准高斯分布。<br> 视频人脸经典文章<br> <img src="https://images2.imgbox.com/2d/48/ZTwrOLMk_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ff/00/neOVmzIZ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/da/14/4jlXGmVf_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/02/49/NKUlFm7a_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/09/1b/u7pAfw5u_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/fc/66/iRFd8wys_o.png" alt="在这里插入图片描述"><br> 上述内容说明了如何进行人脸生成。下一part说明如何进行表情操纵<br> <img src="https://images2.imgbox.com/b5/8a/Q7e7sT4Q_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/7a/26/W5gkxm5M_o.png" alt="在这里插入图片描述"><br> 下一part说明如何补嘴巴。嘴巴会根据一个距离度量，对target 每一帧进行改进kmeans的10类聚类，和其他每个样本距离都最小的作为cluster的representative。作为召回库。召回时选一个距离最近的cluster。<br> <img src="https://images2.imgbox.com/79/60/FePFnb72_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/0e/d5/bL5eLaL1_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/48/0b/wuARDyZ6_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/6f/2c/ZB5fbZkV_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/04/e0/7pz9WDiT_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-5-22<br> Image super-resolution using very deep residual channel attention networks<br> ECCV2018 超分<br> 对超分不了解，不过应该是一篇经典文章了。贡献：1、网络非常深，有10个Residual Group，每个Residual Group有20个Residual channel attention block，每个block内又有差不多4层卷积。。。2、residual in residual，嵌套残差。3、channel attention引入residual group，注意是channel不是spatial，每个channel出一个系数，然后乘上<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            X 
           
           
           
             g 
            
           
             , 
            
           
             b 
            
           
          
         
        
          X_{g,b} 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.969438em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.07847em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03588em;">g</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，再加到<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            F 
           
           
           
             g 
            
           
             , 
            
           
             b 
            
           
             − 
            
           
             1 
            
           
          
         
        
          F_{g,b-1} 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.969438em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03588em;">g</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight">b</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span>。4、本文一再强调图像中的高频和低频部分应该用不同的处理，低频直接过，高频靠生成，所以Residual in residual和channel attention应运而生。Loss就直接和ground truth算L1，不是GAN结构。实验很充分，BI/BD两种图像退化策略，PSNR/SSIM算相似度，resnet-50算top1和top5error，超分尺度2 3 4 8，看起来对纹理类细节很好. 更多数据集之类的细节看论文。<br> 课程需要<br> <img src="https://images2.imgbox.com/bb/ce/5EaJGPxq_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/fa/4f/isynzkPS_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/55/90/4leUnz8w_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/14/b6/LsbK6sRx_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/e0/d0/P1xI5FZg_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/66/83/cPl4XmjS_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-5-7<br> Neural Face Editing with Intrinsic Image Disentangling<br> CVPR2017 人脸物理分解 编辑<br> 人脸的物理分解。数据集无监督。先用3DMM预训练，Loss包括<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            N 
           
          
            e 
           
          
         
        
          N_e 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: -0.10903em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>的重构，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            Z 
           
          
            L 
           
          
         
        
          Z_L 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07153em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.328331em;"><span class="" style="top: -2.55em; margin-left: -0.07153em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>光照的重构，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           ∇ 
          
          
          
            A 
           
          
            e 
           
          
         
        
          \nabla A_e 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord">∇</span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>的范数，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           ∇ 
          
          
          
            S 
           
          
            e 
           
          
         
        
          \nabla S_e 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord">∇</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: -0.05764em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>的范数。<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           U 
          
         
           V 
          
         
        
          UV 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.10903em;">U</span><span class="mord mathdefault" style="margin-right: 0.22222em;">V</span></span></span></span></span>的重构，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            N 
           
          
            i 
           
          
         
        
          N_i 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.10903em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>的重构。这里UV的implicit coordinate system没太看懂。预训练完成后，Loss包括adv和rec（auto-encoder）。人脸编辑靠DFI类似方法。<br> 3D人脸<br> <img src="https://images2.imgbox.com/15/1d/zbt1Ovjj_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/f2/33/5gjPyPw2_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a9/f2/wS4L4MBV_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/96/13/7B4uJhIi_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/60/34/GHqlOGwR_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-4-29<br> Few-shot Video-to-Video Synthesis<br> NeulrIPS2019<br> vid2vid的instance-level扩展。很复杂，主要是在vid2vid的生成路改进（也即除了光流路的另一路）。这里example会决定卷积核的参数，当一张example图时，E_F卷好多层，每一层过E_P得到卷积核参数，参数分为三个part，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            θ 
           
          
            S 
           
          
            l 
           
          
         
           , 
          
          
          
            θ 
           
          
            γ 
           
          
            l 
           
          
         
           , 
          
          
          
            θ 
           
          
            S 
           
          
            l 
           
          
         
        
          \theta_S^l, \theta_{\gamma}^l, \theta_S^l 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.23222em; vertical-align: -0.383108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -2.42467em; margin-left: -0.02778em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05764em;">S</span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.275331em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -2.453em; margin-left: -0.02778em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05556em;">γ</span></span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.383108em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -2.42467em; margin-left: -0.02778em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05764em;">S</span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.275331em;"><span class=""></span></span></span></span></span></span></span></span></span></span>。这里<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
           
           
             p 
            
           
             ^ 
            
           
          
            H 
           
          
            l 
           
          
         
        
          \hat{p}_H^l 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.12444em; vertical-align: -0.275331em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.69444em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault">p</span></span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.16666em;">^</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.19444em;"><span class=""></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -2.42467em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.08125em;">H</span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.275331em;"><span class=""></span></span></span></span></span></span></span></span></span></span>似乎是normalized features，最好看源代码。当example的数量K&gt;1时，也即一个人的一堆图片，采用一种attention方式，认真看结构图，这里attention会和每一个example对应的source有关，也会和当前帧的source有关！这个方式原文说和SPADE是一样的。另外，本文loss与vid2vid一致。<br> <img src="https://images2.imgbox.com/fd/5f/dkRHmZJX_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/cc/c8/9hWwpBgd_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a4/ce/rfgkcWqh_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/66/b9/pZrSm0MI_o.png" alt="在这里插入图片描述"><br> 链接：https://nvlabs.github.io/few-shot-vid2vid/web_gifs/face.gif</p> </li><li> <p>2020-4-29<br> Video-to-Video Synthesis<br> NeurlIPS2018<br> 成对数据下的视频到视频。看作是pix2pix的扩展真实图像的预测用了光流和直接预测两支。网络的输入包括前L-1帧的生成和带上当前时刻的前L帧的原图。三个生成模块，分别用来预测光流；生成和这两者之间的Mask；判别器CGAN有图像级别的，和一段T帧的固定序列级别的两部分。pix2pix中的ground truth约束被换成了预测光流场。街区数据库的训练分了前景和背景，还用了和pix2pix-HD中的multi modal 的方式。另外还有视频级别的FID和人脸数据库的特征点检测、对齐<br> vid2vid<br> <img src="https://images2.imgbox.com/64/f0/e1MFZve7_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/4c/c2/lsta5fbx_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/c4/d2/hC3QPHQQ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/2e/6d/szUP8XkZ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/66/b1/TspGqncq_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/2b/03/yeIvXQ3L_o.png" alt="在这里插入图片描述">链接：https://tcwang0509.github.io/vid2vid/paper_gifs/face.mp4</p> </li><li> <p>2020-4-9<br> Representation Learning by Rotating Your Faces （DR-GAN的期刊版本 )<br> Disentangled Representation Learning GAN for Pose-Invariant Face Recognition<br> CAPR17, PAMI19<br> 把GAN用于人脸转正，辅助判别。G包括Encoder和Decoder，从人脸解码出身份id，再拼上噪声和姿态项，生成人脸。判别器D判断r/f，姿态分类，身份分类。id直接当作embedding用于识别。用余弦角度度量相似度。很好的结合GAN的knowledge和判别任务的。另：几个改进（1）输入多张同id人脸图，Encoder除了生成编码还生成权重，最后id进行加权平均。这里权重也可以评估图片质量。（2）D和Encoder共同share参数。（3）D的身份分类和Encoder的id作用差不多，可以交替用于约束GAN。（4）实验非常全，评估指标和数据集都可以注意一下。<br> GAN和判别<br> <img src="https://images2.imgbox.com/73/03/vJ7dA1Ll_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/48/c6/8Om4VTDV_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/45/e7/7jNjTD3P_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/80/6e/xO7Rvl02_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b8/0c/pMbGINNz_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-3-19<br> DRIT++: Diverse Image-to-Image Translation via Disentangled Representations<br> IJCV2020 multi-domain multi-modal instance-level<br> DRIT的期刊版，非常强。这里全部总结下。对于两个域，共用content，style服从先验分布。（1）content用一个D对抗使其混淆；交换两次，对偶重构；自重构；style要来一次info回归；两个判别器保持真实；KL散度作用于style E。（2）对于多域；统一用一个G，D，style E和content E，用onehot编码，类似stargan。生成器和判别器ACGAN；content对抗、一次和二次重构，info回归、style KL散度仍用。多属性style E的输入还包含domain；（3）<strong>一个启发性的亮点，让不同style的图片距离尽量大，类似StarGAN v2.</strong>（公式截图）。<br> https://github.com/lzhbrian/image-to-image-papers IJCV<br> <img src="https://images2.imgbox.com/5b/9b/oHFy2eIn_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/dc/b1/p05FHHUQ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/db/ff/AAcKEeaW_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/15/58/bk7VayLN_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/70/b2/iXeTpghq_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/3b/cf/ib1u8ZKK_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-3-19<br> Multimodal Unsupervised Image-to-Image Translation<br> ECCV2018 MUNIT 两个域，multi-modal， instance-level<br> 以前读过，很经典的文章了。这里回顾总结下。（1）两个域，每个域解开style和content，style都服从先验分布，content共享。两个E，两个G，两个D。（2）loss很简洁，L1自重构；交换content后，content和style都生成再打开，回归一边，类似infogan；再gan保持真实。文章理论分析了达到最优后，两个域c和s都在同一个分布。（3）用了AdaIN<br> 经典文章<br> <img src="https://images2.imgbox.com/5b/a9/mIxXcwaY_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/51/63/XhAOlcwM_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/0b/19/OdPz3qIz_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/3a/ef/bZw9pQn6_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/d7/70/bmtlFi2a_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/c5/c9/6Q5u12MX_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/fc/56/sy5VTALw_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-3-18<br> Multi-mapping Image-to-Image Translation via Learning Disentanglement<br> Nips2019 DMIT disentangle，多域，multi-modal<br> <strong>很强的工作</strong>，<strong>Mark</strong>。（1）多域共享一个风格编码空间，和内容编码空间。这两个内容分别用两个编码器解出来。注意所有域share同样的编码器。（2）训练过程分为两个部分，D-Path部分，将一张图先解开，再合上。风格编码服从正态分布，VAE来一套。InfoGAN的回归也引入。为了让多域Content混淆在一起，<strong>引入了一种很神奇的CGAN</strong>，把一个域的label和其他域的content编码判真，该label和本域content判假。（好像是这样，可能不太对）这样会让不同域的content不断不断接近。（3）T-Path部分，给content，采样label和style，生成后解开。Infogan的回归约束content和style。CGAN让生成图加上类别并保持真实。<br> https://github.com/lzhbrian/image-to-image-papers 未分类（应该算是disentanglement）<br> <img src="https://images2.imgbox.com/5e/97/dCKS65uO_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/f7/d9/a7NLXzuc_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a9/a3/9XYezKOt_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/0e/5e/LFeuVs7v_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a0/3d/dLXPeFEa_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/79/1c/cIJHoOVe_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-3-18<br> Image-to-Image Translation with Multi-Path Consistency Regularization<br> IJCAI2019 多域互转. 人脸、艺术图、去雨三个数据集<br> 提出了multi-path consistency，也即从A-&gt;B和从A-&gt;C-&gt;B，要保持一致。把这个约束加在stargan和cyclegan上两种架构上。认为这个约束能减小noise，生成更一致的图片。为了做两个域，需要引入一个辅助域，例如去雨，可以把噪声图片当作中间域；去噪，可以把雨当作中间域。（这么搞真的靠谱嘛。。。因为引入了6个域）<br> https://github.com/lzhbrian/image-to-image-papers unsupervised multi-domain<br> <img src="https://images2.imgbox.com/7f/fa/qjHhDnL6_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/3b/49/n7iHYCzp_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-3-17<br> Augmented CycleGAN: Learning Many-to-Many Mappings from Unpaired Data<br> ICML2018 两个域互转，multi-modal AugCGAN<br> 在cyclegan基础上，引入标准高斯分布的noise，两个g都有。（1）为了让noise发挥作用，引入两个Encoder，输入一张A域和一张B域图，预测把A域转B域需要什么样的noise。这样可以合理地做图片cycle loss。（2）为了让E的预测符合先验，和高斯先验对抗。（3）另外，给一个B域noise，先从A域生成到B域图，然后用E预测B域noise，约束一致。原理同infogan。（4）如果有paired数据，可以从一个pair中预测noise，然后translation。是一个可选的有监督约束。（5）noise的注入方式为CIN（conditional IN），没有直接concatenate<br> https://github.com/lzhbrian/image-to-image-papers Unsupervised multi-domain<br> <img src="https://images2.imgbox.com/32/36/qEDJd3iP_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b2/8d/Tfl0FcJs_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/d7/16/DXDYkRhn_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/3c/a8/0zG14SSX_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/00/b2/3LMoFrAO_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/5b/f0/8KJUxLYd_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/de/9a/P2czcxzR_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-3-17<br> Attribute-Guided Face Generation Using Conditional CycleGAN<br> ECCV2018 早期，人脸编辑或身份编辑。模糊、清楚两个域 ConditionalCycleGAN<br> 大致上cyclegan。两个D和G，一个模糊域，一个清晰域。（1）为了属性编辑，从模糊到清晰，用了cgan的结构。反之没有，我觉得这里其实有问题。从模糊到清晰再到模糊的后半段cycle怎么保证生成属性和模糊一样？（2）为了身份编辑，用LightCNN提取256维身份编码，从模糊到清晰的编辑输入该信号。输出也用LightCNN过一遍，拿到身份编码的L1 identity loss. 一个亮点是该方法的人脸编辑看上去很instance，而且能做很多任务，包括人脸交换、人脸转正等。<br> https://github.com/lzhbrian/image-to-image-papers Unsupervised multi-domain<br> <img src="https://images2.imgbox.com/77/0c/qs6wGEfk_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/df/1f/EbSfdoxK_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/39/3f/2Qlek4yZ_o.png" alt="在这里插入图片描述"></p> </li><li> <p>2020-3-16<br> Semantic Image Synthesis with Spatially-Adaptive Normalization<br> CVPR2019 semantic map 2 image SPADE<br> 提出Spatially-Adaptive，在semantic mask上通过网络层得到均值和方差map（不是一个标量，也即和Contitional BN的区别），然后把它用于偏移noise上。另一个点是可以训一个encoder解出一张图片的noise充当style，实现instancle-level style reference。其他几乎同pix2pix-HD<br> https://github.com/lzhbrian/image-to-image-papers Supervised<br> （下图<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           i 
          
         
        
          i 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.65952em; vertical-align: 0em;"></span><span class="mord mathdefault">i</span></span></span></span></span>表示第<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           i 
          
         
        
          i 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.65952em; vertical-align: 0em;"></span><span class="mord mathdefault">i</span></span></span></span></span>层）<br> <img src="https://images2.imgbox.com/ed/ad/SGpgkSaX_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/6c/52/6PPUQiqK_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/92/cf/laUJK9hN_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/6f/5f/A6vPKOrW_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/09/e8/YVFALsqp_o.png" alt="在这里插入图片描述"></p> </li></ol>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4cd12cf9a08d2ab0e71bcffa6cbab01f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">http中get和post性能对比</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/661bb84288ff32d2b02d0b40542a054a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">python练习题（十三）：求s=a&#43;aa&#43;aaa&#43;aaaa&#43;aa...a的值</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>