<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>一文彻底搞懂SLAM技术 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="一文彻底搞懂SLAM技术" />
<meta property="og:description" content="什么是SLAM? SLAM (simultaneous localization and mapping),也称为CML (Concurrent Mapping and Localization), 即时定位与地图构建，或并发建图与定位。
问题可以描述为：将一个机器人放入未知环境中的未知位置，是否有办法让机器人一边逐步描绘出此环境完全的地图，同时一边决定机器人应该往哪个方向行进。
例如扫地机器人就是一个很典型的SLAM问题，所谓完全的地图（a consistent map）是指不受障碍行进到房间可进入的每个角落。
SLAM最早由Smith、Self和Cheeseman于1988年提出。
由于其重要的理论与应用价值，被很多学者认为是实现真正全自主移动机器人的关键。
当你来到一个陌生的环境时，为了迅速熟悉环境并完成自己的任务（比如找饭馆，找旅馆），你应当做以下事情：
a.用眼睛观察周围地标如建筑、大树、花坛等，并记住他们的特征（特征提取）
b.在自己的脑海中，根据双目获得的信息，把特征地标在三维地图中重建出来（三维重建）
c.当自己在行走时，不断获取新的特征地标，并且校正自己头脑中的地图模型（bundle adjustment or EKF）
d.根据自己前一段时间行走获得的特征地标，确定自己的位置（trajectory）
e.当无意中走了很长一段路的时候，和脑海中的以往地标进行匹配，看一看是否走回了原路（loop-closure detection）。实际这一步可有可无。
以上五步是同时进行的，因此是simultaneous localization and mapping
离不开这两类传感器 目前用在SLAM上的Sensor主要分两大类，激光雷达和摄像头。
这里面列举了一些常见的雷达和各种深度摄像头。激光雷达有单线多线之分，角分辨率及精度也各有千秋。
SICK、velodyne、Hokuyo以及国内的北醒光学、Slamtech是比较有名的激光雷达厂商。他们可以作为SLAM的一种输入形式。
这里展示的就是一种简单的2D SLAM。
这个小视频是宾大的教授kumar做的特别有名的一个demo，是在无人机上利用二维激光雷达做的SLAM。
而VSLAM则主要用摄像头来实现，摄像头品种繁多，主要分为单目、双目、单目结构光、双目结构光、ToF几大类。
他们的核心都是获取RGB和depth map(深度信息)。
简单的单目和双目（Zed、leapmotion）我这里不多做解释，我主要解释一下结构光和ToF。
最近流行的结构光和TOF 结构光原理的深度摄像机通常具有激光投射器、光学衍射元件（DOE）、红外摄像头三大核心器件。
这个图（下图）摘自primesense的专利。
可以看到primesense的doe是由两部分组成的，一个是扩散片，一个是衍射片。
先通过扩散成一个区域的随机散斑，然后复制成九份，投射到了被摄物体上。根据红外摄像头捕捉到的红外散斑，PS1080这个芯片就可以快速解算出各个点的深度信息。
这儿还有两款结构光原理的摄像头。
第一页它是由两幅十分规律的散斑组成，最后同时被红外相机获得，精度相对较高。但据说DOE成本也比较高。
还有一种比较独特的方案（最后一幅图），它采用mems微镜的方式，类似DLP投影仪，将激光器进行调频，通过微镜反射出去，并快速改变微镜姿态，进行行列扫描，实现结构光的投射。（产自ST，ST经常做出一些比较炫的黑科技）。
ToF（time of flight）也是一种很有前景的深度获取方法。
传感器发出经调制的近红外光，遇物体后反射，传感器通过计算光线发射和反射时间差或相位差，来换算被拍摄景物的距离，以产生深度信息。
类似于雷达，或者想象一下蝙蝠，softkinetic的DS325采用的就是ToF方案（TI设计的）。
但是它的接收器微观结构比较特殊，有2个或者更多快门，测ps级别的时间差，但它的单位像素尺寸通常在100um的尺寸，所以目前分辨率不高。
在有了深度图之后呢，SLAM算法就开始工作了，由于Sensor和需求的不同，SLAM的呈现形式略有差异。
大致可以分为激光SLAM（也分2D和3D）和视觉SLAM（也分Sparse、semiDense、Dense）两类，但其主要思路大同小异。
这个是Sparse（稀疏）的
这个偏Dense（密集）的
SLAM算法实现的4要素 SLAM算法在实现的时候主要要考虑以下4个方面吧：
地图表示问题，比如dense和sparse都是它的不同表达方式，这个需要根据实际场景需求去抉择
信息感知问题，需要考虑如何全面的感知这个环境，RGBD摄像头FOV通常比较小，但激光雷达比较大
数据关联问题，不同的sensor的数据类型、时间戳、坐标系表达方式各有不同，需要统一处理
定位与构图问题，就是指怎么实现位姿估计和建模，这里面涉及到很多数学问题，物理模型建立，状态估计和优化
其他的还有回环检测问题，探索问题（exploration），以及绑架问题（kidnapping）。
640-3
这个是一个比较有名的SLAM算法，这个回环检测就很漂亮。但这个调用了cuda，gpu对运算能力要求挺高，效果看起来比较炫。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/cf7deac4a60fdde723019b8aaa715fd3/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-08-05T20:43:26+08:00" />
<meta property="article:modified_time" content="2021-08-05T20:43:26+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">一文彻底搞懂SLAM技术</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3>什么是SLAM?</h3> 
<p><strong>SLAM</strong> (<code>simultaneous localization and mapping),也称为CML (Concurrent Mapping and Localization</code>), 即时定位与地图构建，或并发建图与定位。</p> 
<p>问题可以描述为：将一个机器人放入未知环境中的未知位置，是否有办法让机器人一边逐步描绘出此环境完全的地图，同时一边决定机器人应该往哪个方向行进。</p> 
<p>例如扫地机器人就是一个很典型的SLAM问题，所谓完全的地图（<code>a consistent map</code>）是指不受障碍行进到房间可进入的每个角落。</p> 
<p><strong>SLAM</strong>最早由Smith、Self和Cheeseman于1988年提出。</p> 
<p>由于其重要的理论与应用价值，被很多学者认为是实现真正全自主移动机器人的关键。</p> 
<p>当你来到一个陌生的环境时，为了迅速熟悉环境并完成自己的任务（比如找饭馆，找旅馆），你应当做以下事情：</p> 
<ul><li> <p>a.用<strong>眼睛观察周围</strong>地标如建筑、大树、花坛等，并记住他们的特征（特征提取）</p> </li><li> <p>b.在自己的脑海中，根据<strong>双目获得的信息</strong>，把特征地标在三维地图中重建出来（三维重建）</p> </li><li> <p>c.当自己在行走时，<strong>不断获取新的特征地标</strong>，并且校正自己头脑中的地图模型（<code>bundle adjustment or EKF</code>）</p> </li><li> <p>d.根据自己前一段时间行走<strong>获得的特征地标，确定自己的位置</strong>（<code>trajectory</code>）</p> </li><li> <p>e.当无意中走了很长一段路的时候，和脑海中的以往地标进行匹配，看一看是否走回了原路（<code>loop-closure detection</code>）。实际这一步可有可无。</p> </li></ul> 
<p>以上五步是同时进行的，因此是simultaneous localization and mapping</p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/46/1a/lY5x6Mx6_o.png"></p> 
<p></p> 
<p></p> 
<h3>离不开这两类传感器</h3> 
<p>目前用在SLAM上的Sensor主要分两大类，<strong>激光雷达</strong>和<strong>摄像头</strong>。</p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/16/12/RQ2hUh7o_o.png"></p> 
<p>这里面列举了一些常见的雷达和各种深度摄像头。激光雷达有单线多线之分，角分辨率及精度也各有千秋。</p> 
<p>SICK、velodyne、Hokuyo以及国内的北醒光学、Slamtech是比较有名的激光雷达厂商。他们可以作为SLAM的一种输入形式。</p> 
<p>这里展示的就是一种简单的2D SLAM。</p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/ed/22/0QwCATwW_o.png"></p> 
<p>这个小视频是宾大的教授kumar做的特别有名的一个demo，是在无人机上利用二维激光雷达做的SLAM。</p> 
<p></p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/2b/b0/szvnwoaf_o.gif"></p> 
<p></p> 
<p>而VSLAM则主要用摄像头来实现，摄像头品种繁多，主要分为<strong>单目</strong>、<strong>双目</strong>、<strong>单目结构光</strong>、<strong>双目结构光</strong>、<strong>ToF</strong>几大类。</p> 
<p>他们的核心都是获取RGB和depth map(深度信息)。</p> 
<p>简单的单目和双目（Zed、leapmotion）我这里不多做解释，我主要解释一下结构光和ToF。</p> 
<h3>最近流行的结构光和TOF</h3> 
<p>结构光原理的深度摄像机通常具有激光投射器、光学衍射元件（DOE）、红外摄像头三大核心器件。</p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/9f/f4/8tWviiS9_o.png"></p> 
<p>这个图（下图）摘自primesense的专利。</p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/75/89/MBdJTd5W_o.png"></p> 
<p>可以看到primesense的doe是由两部分组成的，一个是扩散片，一个是衍射片。</p> 
<p>先通过扩散成一个区域的随机散斑，然后复制成九份，投射到了被摄物体上。根据红外摄像头捕捉到的红外散斑，PS1080这个芯片就可以快速解算出各个点的深度信息。</p> 
<p>这儿还有两款结构光原理的摄像头。</p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/98/65/eouPm0Of_o.png"></p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/51/61/mTDtdmCW_o.png"></p> 
<p>第一页它是由两幅十分规律的散斑组成，最后同时被红外相机获得，精度相对较高。但据说DOE成本也比较高。</p> 
<p>还有一种比较独特的方案（最后一幅图），它采用mems微镜的方式，类似DLP投影仪，将激光器进行调频，通过微镜反射出去，并快速改变微镜姿态，进行行列扫描，实现结构光的投射。（产自ST，ST经常做出一些比较炫的黑科技）。</p> 
<p><strong>ToF</strong>（<code>time of flight</code>）也是一种很有前景的深度获取方法。</p> 
<p>传感器发出经调制的近红外光，遇物体后反射，传感器通过计算光线发射和反射时间差或相位差，来换算被拍摄景物的距离，以产生深度信息。</p> 
<p>类似于雷达，或者想象一下蝙蝠，<code>softkinetic</code>的<code>DS325</code>采用的就是ToF方案（TI设计的）。</p> 
<p>但是它的接收器微观结构比较特殊，有2个或者更多快门，测ps级别的时间差，但它的单位像素尺寸通常在100um的尺寸，所以目前分辨率不高。</p> 
<p>在有了深度图之后呢，SLAM算法就开始工作了，由于Sensor和需求的不同，SLAM的呈现形式略有差异。</p> 
<p>大致可以分为<strong>激光SLAM</strong>（也分2D和3D）和<strong>视觉SLAM</strong>（也分<code>Sparse</code>、<code>semiDense</code>、<code>Dense</code>）两类，但其主要思路大同小异。</p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/63/89/Jc2VJnSV_o.png"></p> 
<p>这个是Sparse（稀疏）的</p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/05/90/0JEKuKiY_o.gif"></p> 
<p>这个偏Dense（密集）的</p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/b9/6e/lbZONKyo_o.png"></p> 
<h3>SLAM算法实现的4要素</h3> 
<p>SLAM算法在实现的时候主要要考虑以下4个方面吧：</p> 
<ol><li> <p>地图表示问题，比如dense和sparse都是它的不同表达方式，这个需要根据实际场景需求去抉择</p> </li><li> <p>信息感知问题，需要考虑如何全面的感知这个环境，RGBD摄像头FOV通常比较小，但激光雷达比较大</p> </li><li> <p>数据关联问题，不同的sensor的数据类型、时间戳、坐标系表达方式各有不同，需要统一处理</p> </li><li> <p>定位与构图问题，就是指怎么实现位姿估计和建模，这里面涉及到很多数学问题，物理模型建立，状态估计和优化</p> </li></ol> 
<p>其他的还有回环检测问题，探索问题（exploration），以及绑架问题（kidnapping）。</p> 
<p><strong>640-3</strong></p> 
<p>这个是一个比较有名的SLAM算法，这个回环检测就很漂亮。但这个调用了cuda，gpu对运算能力要求挺高，效果看起来比较炫。</p> 
<p>以VSLAM举个栗子</p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/e4/a8/Tls3mSv9_o.png"></p> 
<p>我大概讲一种比较流行的VSLAM方法框架。</p> 
<p>整个SLAM大概可以分为前端和后端</p> 
<p><strong>前端</strong></p> 
<p>前端相当于VO（视觉里程计），研究帧与帧之间变换关系。</p> 
<p>首先提取每帧图像特征点，利用相邻帧图像，进行特征点匹配，然后利用RANSAC去除大噪声，然后进行匹配，得到一个pose信息（位置和姿态），同时可以利用IMU（Inertial measurement unit惯性测量单元）提供的姿态信息进行滤波融合后端则主要是对前端出结果进行优化，利用滤波理论（EKF、UKF、PF）、或者优化理论TORO、G2O进行树或者图的优化。最终得到最优的位姿估计。</p> 
<p><strong>后端</strong></p> 
<p>后端这边难点比较多，涉及到的数学知识也比较多，总的来说大家已经慢慢抛弃传统的滤波理论走向图优化去了。</p> 
<p>因为基于滤波的理论，滤波器稳度增长太快，这对于需要频繁求逆的EKF（扩展卡尔曼滤波器），PF压力很大。</p> 
<p>而基于图的SLAM，通常以keyframe（关键帧）为基础，建立多个节点和节点之间的相对变换关系，比如仿射变换矩阵，并不断地进行关键节点的维护，保证图的容量，在保证精度的同时，降低了计算量。</p> 
<p>列举几个目前比较有名的SLAM算法：<code>PTAM</code>,<code>MonoSLAM</code>, <code>ORB-SLAM</code>,<code>RGBD-SLAM</code>,<code>RTAB-SLAM</code>,<code>LSD-SLAM</code>。</p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/b3/c5/Djy5ZxsK_o.png"></p> 
<p style="text-align:center;">RGBD-SLAM</p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/24/53/VAtXw9si_o.png"></p> 
<p style="text-align:center;">MonoSLAM</p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/0f/e7/PjzqB2BN_o.png"></p> 
<p style="text-align:center;">LSD-SLAM</p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/66/31/QGIFg2Yf_o.gif"></p> 
<p></p> 
<p>所以大家如果想学习SLAM的话，各个高校提高的素材是很多的，比如宾大、MIT、ETH、香港科技大学、帝国理工等等都有比较好的代表作品，还有一个比较有前景的就是三维的机器视觉，普林斯顿大学的肖剑雄教授结合SLAM和Deep Learning做一些三维物体的分类和识别，实现一个对场景深度理解的机器人感知引擎。</p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/cd/ba/EvkI27KL_o.png"></p> 
<blockquote> 
 <p><code>http://robots.princeton.edu/talks/2016_MIT/RobotPerception.pdf</code></p> 
</blockquote> 
<p><strong>SLAM</strong>技术从最早的军事用途（核潜艇海底定位就有了SLAM的雏形）到今天，已经逐步走入人们的视野，扫地机器人的盛行更是让它名声大噪。</p> 
<p>同时基于三维视觉的VSLAM越来越显主流。在<strong>地面/空中机器人</strong>、<strong>VR/AR/MR</strong>、<strong>汽车/AGV自动驾驶</strong>等领域，都会得到深入的发展，同时也会出现越来越多的细分市场等待挖掘。</p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/3d/d3/Ia4AzIdv_o.png"></p> 
<h3>SLAM技术的应用领域</h3> 
<p>1）室内机器人 扫地机要算机器人里最早用到SLAM技术这一批了。</p> 
<p>国内的科沃斯、塔米扫地机通过用SLAM算法结合激光雷达或者摄像头的方法，让<strong>扫地机可以高效绘制室内地图</strong>，<strong>智能分析和规划扫地环境</strong>，从而成功让自己步入了智能导航的阵列。</p> 
<p>不过有意思的是，科沃斯引领时尚还没多久，一大帮懂Slam算法的扫地机厂商就开始陆陆续续地推出自己的智能导航，直到昨天雷锋网还看到一款智能扫地机新鲜出炉，而这追逐背后的核心，大家都知道就是SLAM技术的应用。</p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/60/4b/YSh8d0UD_o.gif"></p> 
<p>而另一个跟SLAM息息相关的室内移动机器人，因为目前市场定位和需求并不明确，我们目前只能在商场导购室内机器人和Buddy那样的demo视频里才能看到，国内<strong>Watchhhh Slam</strong>和<strong>Slam Tech</strong>两家公司都是做这方面方案提供的，以现实的观点看，现在室内移动机器人市场定位和需求没落地的时候，由方案商公司推动，商用室内移动机器人先行，这反而是一种曲线救国的发展方式。</p> 
<p>2）AR 目前基于SLAM技术开发的代表性产品有微软的Hololens，谷歌的Project Tango以及同样有名的Magic Leap，后者4月20号公布它的新一代水母版demo后，国内的AR公司更加看到了这个趋势。</p> 
<p>比如进化动力近期就公布了他们的SLAM demo, 用一个小摄像头实现VR头显空间定位，而易瞳去年10月雷锋网去试用新品的时候，就发现已经整合SLAM技术了，国内其他公司虽然没有正式公布，但我们可以肯定，他们都在暗暗研发这项技术，只等一个成熟的时机就会展现给大家。</p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/14/7a/tMmNITGL_o.png"></p> 
<p>进化动力CTO聂崇岭向雷锋网表示，如果用一个准确的说法</p> 
<p>很多VR应用需要用到SLAM技术，定位只是一个feature，路径记录、3D重构、地图构建都可以是SLAM技术的输出。</p> 
<p>3）无人机 国外的话，原来做 Google X Project Wing 无人机的创始人 MIT 机器人大牛 Nicholas Roy 的学生 Adam Bry 创办的 Skydio，挖来了 Georgia Tech 的 Slam 大牛教授 Frank Dellaert 做他们的首席科学家。</p> 
<p>国内大家非常熟悉的大疆精灵四避障用的双目视觉+超声波，一位大疆工程师徐枭涵在百度百家的撰文里坦率承认“<strong>P4里面呈现的主动避障功能就是一种非常非常典型的Slam的弱应用，无人机只需要知道障碍物在哪，就可以进行 Planning，并且绕开障碍物</strong>。</p> 
<p>当然Slam能做的事情远远不止这些，包括灾区救援，包括探洞，包括人机配合甚至集群，所有的关于无人机的梦想都建立在Slam之上，这是无人机能飞（具有定位，姿态确定以后）的时代以后，无人机最核心的技术。”</p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/e3/fc/cTbwatG0_o.png"></p> 
<p>而近期另一个号称刷爆美国朋友圈的hover camera无人机，因为其创始人的的计算机视觉背景，正式把SLAM技术应用进来了，在介绍他们无人机的主要产品技术时，提到了</p> 
<p>●SLAM（即时定位与地图构建）：通过感知自身周围环境来构建3D增量式地图，从而实现自主定位和导航。</p> 
<p>4）无人驾驶 因为Google无人驾驶车的科普，很多人都知道了基于激光雷达技术的Lidar Slam。</p> 
<p><strong>Lidar Slam是指利用激光雷达作为外部传感器，获取地图数据，使机器人实现同步定位与地图构建</strong>。</p> 
<p>虽然成本高昂，但目前为止是最稳定、最可靠、高性能的SLAM方式。</p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/4c/7c/F1WJj6wT_o.png"></p> 
<p>另外，2011 年，牛津大学Mobile Robotics Group 首次向公众展示他们的第一辆无人驾驶汽车野猫（Wildcat），这是一辆由 Bowler Wildcat 4X4 改装而成的车。汽车头顶的相机和激光能够搜集信息然后即时分析导航，已经成功通过了测试。</p> 
<p>2014 年，他们改装的一辆 Nissan 的 Leaf 也成功路测。</p> 
<p>Mobile Robotics Group主要研究领域是大规模的导航和对自然场景理解。</p> 
<p>据称，团队所拥有的技术非常牛逼，其复杂和先进性远远超过一般的同步定位与地图构建（SLAM）算法。</p> 
<p>可圈可点的是，对于无人驾驶技术，他们并没有使用 GPS 或者是嵌入式的基础设施（信标之类的），而是使用算法来导航，包括机器学习和概率推理来建立周围的地图等。</p> 
<p></p> 
<p>—— The End ——</p> 
<p></p> 
<p>推荐好文  点击蓝色字体即可跳转</p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzA5OTg3NTk4MA==&amp;mid=2247489408&amp;idx=1&amp;sn=43f3ce292bd3acb75cb2194260b278ca&amp;chksm=90fafa63a78d7375860ef6a2e46dfb4672331ccd6a909c5cdad0a1b106e8d03f97262dc4d197&amp;scene=21#wechat_redirect" rel="nofollow">☞</a> <a href="http://mp.weixin.qq.com/s?__biz=Mzg5MDU1OTgzMw==&amp;mid=2247488821&amp;idx=1&amp;sn=4130724f2bb8a44382c3cb46383a76b3&amp;chksm=cfdb9903f8ac101543ed2c6d32c2d9791af861bfeb8bdeae1d14b824e6e7f50d53ff3528ab8f&amp;scene=21#wechat_redirect" rel="nofollow">没想到靠股市，我差点成功逆袭</a></p> 
<p>☞ <a href="http://mp.weixin.qq.com/s?__biz=Mzg5MDU1OTgzMw==&amp;mid=2247488821&amp;idx=2&amp;sn=a1870a88c6598d01f59bf293dd283c68&amp;chksm=cfdb9903f8ac10159230b37b5be495e04e62b5f4ff68b388c8b7897ba99ac53bdebe041ac5e2&amp;scene=21#wechat_redirect" rel="nofollow">分享一下嵌入式 HarmonyOS 的学习思路</a> </p> 
<p>☞ <a href="http://mp.weixin.qq.com/s?__biz=Mzg5MDU1OTgzMw==&amp;mid=2247488757&amp;idx=1&amp;sn=d02099a88a47a0bc6e37f676ab9af9f1&amp;chksm=cfdb98c3f8ac11d5130c7dbae5aa55b0a691f2d60574e93c95e26c7bedf49db6276071537959&amp;scene=21#wechat_redirect" rel="nofollow">推荐一款我私藏已久的串口示波神器</a></p> 
<p>☞ <a href="http://mp.weixin.qq.com/s?__biz=Mzg5MDU1OTgzMw==&amp;mid=2247488704&amp;idx=2&amp;sn=5aa0a3f5dfe770f2fff6ee42918ac35a&amp;chksm=cfdb98f6f8ac11e069df6535b3583501cded828935ac1699cf9deb3f2cf7abe9f8251ebeee3f&amp;scene=21#wechat_redirect" rel="nofollow">一位老电子工程师的十年职场感悟</a></p> 
<p>☞ <a href="http://mp.weixin.qq.com/s?__biz=Mzg5MDU1OTgzMw==&amp;mid=2247488555&amp;idx=1&amp;sn=8bcf0fb8ab5835527ce1f989e9ac76c3&amp;chksm=cfdb981df8ac110b0d859b811ed630182222846ade957503657349ba511b4b15b07b9f445111&amp;scene=21#wechat_redirect" rel="nofollow">优雅地用宏实现环形缓冲区</a></p> 
<p></p> 
<p>欢迎<strong>转发、留言、点赞、分享</strong>给你的朋友，感谢您的支持！</p> 
<p></p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/ba/97/iEDArGpA_o.png"></p> 
<p><strong>长按识别二维码关注我</strong></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a52e6c4bb9a79dd86a632c4e0f1f463f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">苍蓝誓约服务器一直维护,《苍蓝誓约》11月26日09:30停服维护公告</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/cdffea1d0d557f0d522f745bd04cd58a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">rosdep install -y --from-paths src --ignore-src --rosdistro kinetic出错</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>