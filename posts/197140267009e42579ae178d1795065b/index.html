<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>写给初学者的YOLO目标检测 概述 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="写给初学者的YOLO目标检测 概述" />
<meta property="og:description" content="文章目录 什么是目标检测What is YOLO?为什么YOLO在目标检测领域如此流行？1. 速度快2. 高检测精度3. 更好的泛化性4. 开源 YOLO架构YOLO目标检测是如何工作的？残差块(Residual blocks)边界框回归（Bounding box regression）交并比 IoU (Intersection over Union)非极大值抑制（Non-Maximum Suppression） YOLO的应用场景1- 应用于工业领域医疗农业 安全监控 YOLO, YOLOv2, YOLO9000, YOLOv3, YOLOv4, YOLOR, YOLOX, YOLOv5, YOLOv6, YOLOv7比较YOLO/YOLOv1，起点YOLOv2或YOLO90001- 批量归一化(批标准化)2- 更高的输入分辨率3- 使用锚框的卷积层4- 维度聚类5- 细粒度特征 YOLOv3 - 渐进式改进1- 更好的边界框预测2- 更准确的类别预测3- 在不同尺度上更准确的预测 YOLOv4 - 目标检测的最佳速度和准确性YOLOR — You Only Look One Representation1- 预测对齐2- 目标检测的预测细化3- 多任务学习的规范表示 YOLOX - 2021年超越YOLO系列1- 高效分离的头部2- 强大的数据增强3- 无锚点系统4- SimOTA标签分配 YOLOv5YOLOv6 - 一种面向工业应用的单阶段物体检测框架YOLOv7 - 可训练的免费工具包为实时目标检测器设定了新的技术水平1- 架构层面2- 可训练的免费工具包 YOLOv8结果： 结论 本文主要介绍; YOLO（You Only Look Once） 目标检测的 优势、它在过去几年中的发展情况以及一些现实生活中的应用。 什么是目标检测 目标检测（Object detection）是计算机视觉中使用的一种技术，用于识别和定位图像或视频中的对象。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/197140267009e42579ae178d1795065b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-03T11:16:32+08:00" />
<meta property="article:modified_time" content="2023-05-03T11:16:32+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">写给初学者的YOLO目标检测 概述</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_5" rel="nofollow">什么是目标检测</a></li><li><a href="#What_is_YOLO_45" rel="nofollow">What is YOLO?</a></li><li><a href="#YOLO_55" rel="nofollow">为什么YOLO在目标检测领域如此流行？</a></li><li><ul><li><a href="#1__65" rel="nofollow">1. 速度快</a></li><li><a href="#2__69" rel="nofollow">2. 高检测精度</a></li><li><a href="#3__73" rel="nofollow">3. 更好的泛化性</a></li><li><a href="#4__77" rel="nofollow">4. 开源</a></li></ul> 
  </li><li><a href="#YOLO_82" rel="nofollow">YOLO架构</a></li><li><a href="#YOLO_102" rel="nofollow">YOLO目标检测是如何工作的？</a></li><li><ul><li><a href="#Residual_blocks_123" rel="nofollow">残差块(Residual blocks)</a></li><li><a href="#Bounding_box_regression_131" rel="nofollow">边界框回归（Bounding box regression）</a></li><li><a href="#_IoU_Intersection_over_Union_165" rel="nofollow">交并比 IoU (Intersection over Union)</a></li><li><a href="#NonMaximum_Suppression_183" rel="nofollow">非极大值抑制（Non-Maximum Suppression）</a></li></ul> 
  </li><li><a href="#YOLO_191" rel="nofollow">YOLO的应用场景</a></li><li><ul><li><a href="#1__195" rel="nofollow">1- 应用于工业领域</a></li><li><ul><li><a href="#_198" rel="nofollow">医疗</a></li><li><a href="#_204" rel="nofollow">农业</a></li></ul> 
   </li><li><a href="#_212" rel="nofollow">安全监控</a></li></ul> 
  </li><li><a href="#YOLO_YOLOv2_YOLO9000_YOLOv3_YOLOv4_YOLOR_YOLOX_YOLOv5_YOLOv6_YOLOv7_220" rel="nofollow">YOLO, YOLOv2, YOLO9000, YOLOv3, YOLOv4, YOLOR, YOLOX, YOLOv5, YOLOv6, YOLOv7比较</a></li><li><ul><li><a href="#YOLOYOLOv1_235" rel="nofollow">YOLO/YOLOv1，起点</a></li><li><a href="#YOLOv2YOLO9000_247" rel="nofollow">YOLOv2或YOLO9000</a></li><li><ul><li><a href="#1__259" rel="nofollow">1- 批量归一化(批标准化)</a></li><li><a href="#2__263" rel="nofollow">2- 更高的输入分辨率</a></li><li><a href="#3__267" rel="nofollow">3- 使用锚框的卷积层</a></li><li><a href="#4__270" rel="nofollow">4- 维度聚类</a></li><li><a href="#5__275" rel="nofollow">5- 细粒度特征</a></li></ul> 
   </li><li><a href="#YOLOv3___282" rel="nofollow">YOLOv3 - 渐进式改进</a></li><li><ul><li><a href="#1__287" rel="nofollow">1- 更好的边界框预测</a></li><li><a href="#2__291" rel="nofollow">2- 更准确的类别预测</a></li><li><a href="#3__295" rel="nofollow">3- 在不同尺度上更准确的预测</a></li></ul> 
   </li><li><a href="#YOLOv4___300" rel="nofollow">YOLOv4 - 目标检测的最佳速度和准确性</a></li><li><a href="#YOLOR%E2%80%8A%E2%80%8AYou_Only_Look_One_Representation_322" rel="nofollow">YOLOR — You Only Look One Representation</a></li><li><ul><li><a href="#1__336" rel="nofollow">1- 预测对齐</a></li><li><a href="#2__339" rel="nofollow">2- 目标检测的预测细化</a></li><li><a href="#3__343" rel="nofollow">3- 多任务学习的规范表示</a></li></ul> 
   </li><li><a href="#YOLOX__2021YOLO_359" rel="nofollow">YOLOX - 2021年超越YOLO系列</a></li><li><ul><li><a href="#1__365" rel="nofollow">1- 高效分离的头部</a></li><li><a href="#2__369" rel="nofollow">2- 强大的数据增强</a></li><li><a href="#3__373" rel="nofollow">3- 无锚点系统</a></li><li><a href="#4_SimOTA_377" rel="nofollow">4- SimOTA标签分配</a></li></ul> 
   </li><li><a href="#YOLOv5_382" rel="nofollow">YOLOv5</a></li><li><a href="#YOLOv6___408" rel="nofollow">YOLOv6 - 一种面向工业应用的单阶段物体检测框架</a></li><li><a href="#YOLOv7___435" rel="nofollow">YOLOv7 - 可训练的免费工具包为实时目标检测器设定了新的技术水平</a></li><li><ul><li><a href="#1__445" rel="nofollow">1- 架构层面</a></li><li><a href="#2__453" rel="nofollow">2- 可训练的免费工具包</a></li></ul> 
   </li><li><a href="#YOLOv8_469" rel="nofollow">YOLOv8</a></li><li><ul><li><a href="#_483" rel="nofollow">结果：</a></li></ul> 
  </li></ul> 
  </li><li><a href="#_491" rel="nofollow">结论</a></li></ul> 
</div> 
<br> 本文主要介绍; YOLO（You Only Look Once） 目标检测的 优势、它在过去几年中的发展情况以及一些现实生活中的应用。 
<p></p> 
<h2><a id="_5"></a>什么是目标检测</h2> 
<p>目标检测（Object detection）是计算机视觉中使用的一种技术，用于识别和定位图像或视频中的对象。</p> 
<p>图像定位是指使用边界框（bounding boxes）来识别一个或多个对象的正确位置的过程，这些边界框对应于围绕对象的矩形形状。</p> 
<p>这个过程有时会与图像分类或图像识别混淆，后者旨在将图像或图像中的对象预测为类别或类别之一。</p> 
<p>下面的插图对应于上述解释的计算机视觉技术。在图像中检测到的对象是“人”。</p> 
<p><img src="https://images2.imgbox.com/db/d9/HGbTiDgN_o.png" alt="img"></p> 
<p>在本文中，将首先了解目标检测的优势，然后介绍最先进的目标检测算法<strong>YOLO</strong>。</p> 
<p>在第二部分中，我们将更加关注<strong>YOLO</strong>算法及其工作原理。之后，我们将提供一些使用<strong>YOLO</strong>的实际应用。</p> 
<p>最后一节将解释<strong>YOLO</strong>从2015年到2020年的演变，然后总结下一步的步骤。</p> 
<h2><a id="What_is_YOLO_45"></a>What is YOLO?</h2> 
<p><strong>You Only Look Once (YOLO)</strong> 是一种最先进的<strong>实时目标检测</strong>算法，由<strong>Joseph Redmon</strong>、<strong>Santosh Divvala</strong>、<strong>Ross Girshick</strong>和<strong>Ali Farhadi</strong>于2015年在他们著名的研究论文“You Only Look Once: Unified, Real-Time Object Detection”中引入。</p> 
<p>YOLO 的核心思想就是把目标检测转变成一个回归问题，而不是分类任务。利用整张图作为网络的输入，仅仅经过单个卷积神经网络 (CNN)，得到bounding box（边界框） 的位置及其所属的类别。</p> 
<h2><a id="YOLO_55"></a>为什么YOLO在目标检测领域如此流行？</h2> 
<p>YOLO之所以领先竞争对手，原因包括：</p> 
<ul><li>速度快</li><li>检测精度高</li><li>良好的泛化能力</li><li>开源</li></ul> 
<h3><a id="1__65"></a>1. 速度快</h3> 
<p>YOLO之所以非常快，是因为它不涉及复杂的流程。它可以以每秒45帧的速度处理图像。此外，与其他实时系统相比，YOLO的平均精度（mAP）超过了两倍，使其成为实时处理的绝佳选择。从下面的图表中，我们可以看到YOLO的速度远远超过其他目标检测器，达到了91 FPS。<img src="https://images2.imgbox.com/ec/46/edYzuGmn_o.png" alt="YOLO Speed compared to other state-of-the-art object detectors"></p> 
<h3><a id="2__69"></a>2. 高检测精度</h3> 
<p>YOLO的准确性远远超过其他最先进的模型，几乎没有背景误差。</p> 
<h3><a id="3__73"></a>3. 更好的泛化性</h3> 
<p>特别是对于新版本的YOLO，本文稍后会讨论。通过这些改进，YOLO在新领域提供了更好的泛化性能，使其非常适合依赖快速和强大的目标检测应用程序。例如，<a href="https://www.researchgate.net/publication/337498273_Automatic_Detection_of_Melanoma_with_Yolo_Deep_Convolutional_Neural_Networks" rel="nofollow">《使用YOLO深度卷积神经网络自动检测黑色素瘤》</a>的研究表明，YOLOv1版本的平均精度最低，而YOLOv2和YOLOv3版本的平均精度更高。</p> 
<h3><a id="4__77"></a>4. 开源</h3> 
<p>将YOLO开源使得社区可以不断改进模型。这是YOLO在有限的时间内取得如此多改进的原因之一。</p> 
<h2><a id="YOLO_82"></a>YOLO架构</h2> 
<p>YOLO架构类似于GoogleNet。如下图所示，它总共有24个卷积层，<strong>四个最大池化层</strong>和<strong>两个全连接层</strong>。</p> 
<p><img src="https://images2.imgbox.com/a2/68/qys8v6fL_o.png" alt="YOLO Architecture from the original paper"></p> 
<p>该架构的工作方式如下：</p> 
<ul><li>将输入图像调整为448x448，然后通过卷积网络处理。</li><li>首先应用1x1卷积来减少通道数，然后是3x3卷积来生成一个立方体输出。</li><li>在内部使用的激活函数是ReLU，除了最后一层使用线性激活函数。</li><li>一些额外的技术，如批量归一化和dropout，分别对模型进行正则化并防止过拟合。</li></ul> 
<h2><a id="YOLO_102"></a>YOLO目标检测是如何工作的？</h2> 
<p>上一节已经了解了YOLO的架构，让我们简单介绍一下YOLO算法如何使用一个简单的用例来执行目标检测。</p> 
<p>想象一下，你构建了一个YOLO应用程序，可以从给定的图像中检测出球员和足球。但是如何向某个人，特别是非专业人士解释这个过程呢？</p> 
<p><img src="https://images2.imgbox.com/38/21/rUMrMLav_o.png" alt="YOLO Object Detection Image by Jeffrey F Lin on Unsplash"></p> 
<p>该算法基于以下四种方法进行操作：</p> 
<ul><li>残差块(Residual blocks)</li><li>边界框回归（Bounding box regression）</li><li>交并比 IoU (Intersection over Union)</li><li>非极大值抑制（Non-Maximum Suppression）</li></ul> 
<p>让我们更详细地了解每一种方法。</p> 
<h3><a id="Residual_blocks_123"></a>残差块(Residual blocks)</h3> 
<p>这一步首先将原始图像(A)划分为NxN个具有相等形状的网格单元，其中N在我们的案例中为4，如右侧的图像所示。网格中的每个单元负责定位和预测其覆盖的对象的类别，以及概率（probability）/置信度值（confidence value.）。</p> 
<p><img src="https://images2.imgbox.com/5d/d1/DNey0vEM_o.png" alt="Application of grid cells to the original image"></p> 
<h3><a id="Bounding_box_regression_131"></a>边界框回归（Bounding box regression）</h3> 
<p>下一步是确定与图像中的所有对象相对应的边界框，可以有与给定图像中的对象数量相同的边界框。YOLO使用以下格式的单个回归模块确定这些边界框的属性，其中Y是每个边界框的最终向量表示。</p> 
<p><code>Y = [pc，bx，by，bh，bw，c1，c2]</code></p> 
<p>这在模型的训练阶段尤为重要。</p> 
<ul><li>pc对应于包含对象的网格的概率分数。例如，所有红色网格的概率分数都将大于零。右侧的图像是简化版本，因为每个黄色单元格的概率为零（insignificant）。</li></ul> 
<p><img src="https://images2.imgbox.com/cb/f2/axNqiA8x_o.png" alt="Identification of significant and insignificant grids"></p> 
<ul><li> <p>bx和by是边界框的中心坐标，相对于包围它的网格单元。</p> </li><li> <p>bh和bw是边界框的高度和宽度，相对于包围它的网格单元。</p> </li><li> <p>c1和c2对应于两个类别Player和Ball。你可以根据你的应用需求拥有任意数量的类别。</p> </li></ul> 
<p>为了更好地理解，让我们更仔细地观察右下角的球员。</p> 
<p><img src="https://images2.imgbox.com/01/72/8LkUdl0B_o.png" alt="Bounding box regression identification"></p> 
<h3><a id="_IoU_Intersection_over_Union_165"></a>交并比 IoU (Intersection over Union)</h3> 
<p>通常情况下，一张图像中的单个物体可能有多个网格框作为预测结果，但并非所有网格框都是相关的。IOU（介于0和1之间的值）的目标是舍弃这些无关的网格框，只保留相关的网格框。其逻辑如下：</p> 
<ul><li> <p>用户定义其IOU选择阈值，例如0.5。</p> </li><li> <p>然后，YOLO计算每个网格单元的IOU，即交集面积除以并集面积。</p> </li><li> <p>最后，它忽略IOU ≤ 阈值的网格单元的预测结果，并考虑IOU&gt;阈值的网格单元。</p> </li></ul> 
<p>下面是将网格选择过程应用于左下角物体的示例。我们可以观察到，该物体最初有两个网格框候选，最终只选择了“Grid 2”。</p> 
<p><img src="https://images2.imgbox.com/f0/ea/oHdL1Gxt_o.png" alt="Process of selecting the best grids for prediction"></p> 
<h3><a id="NonMaximum_Suppression_183"></a>非极大值抑制（Non-Maximum Suppression）</h3> 
<p>设置IOU的阈值并不总是足够的，因为一个物体可能有多个框与阈值的IOU值相同，如果保留所有这些框，可能会包含噪声。这就是我们可以使用NMS来仅保留具有最高检测概率分数的框的原因。</p> 
<h2><a id="YOLO_191"></a>YOLO的应用场景</h2> 
<p>YOLO目标检测在我们日常生活中有不同的应用。在这一部分中，我们将涵盖以下领域的一些应用：医疗保健、农业、安全监控和自动驾驶汽车。</p> 
<h3><a id="1__195"></a>1- 应用于工业领域</h3> 
<p>目标检测已经被引入到许多实际的工业领域，如医疗保健和农业。让我们通过具体的例子来了解每一个领域。</p> 
<h4><a id="_198"></a>医疗</h4> 
<p>在医疗领域，特别是在手术中，由于患者之间的生物多样性，实时定位器官可能具有挑战性。Kidney Recognition in CT使用YOLOv3来帮助在计算机断层扫描（CT）中定位二维和三维肾脏。</p> 
<h4><a id="_204"></a>农业</h4> 
<p>人工智能和机器人在现代农业中扮演着重要角色。收割机器人是基于视觉的机器人，用于代替手工采摘水果和蔬菜。在这个领域中最好的模型之一使用了YOLO。在<a href="https://www.nature.com/articles/s41598-021-81216-5" rel="nofollow">《基于修改后的YOLOv3框架的番茄检测》</a>中，作者描述了他们如何使用YOLO来识别不同类型的水果和蔬菜，以实现高效收获。</p> 
<p><img src="https://images2.imgbox.com/36/8b/WkzNPf9O_o.png" alt="Comparison of YOLO-tomato models"></p> 
<h3><a id="_212"></a>安全监控</h3> 
<p>在安全监控领域，虽然物体检测技术被广泛应用，但这并不是唯一的应用。在 COVID-19 疫情期间，YOLOv3 被用来估计人们之间的社交距离违规情况。你可以从<a href="https://pubmed.ncbi.nlm.nih.gov/33163330/" rel="nofollow">《基于深度学习的 COVID-19 社交距离监测框架》</a>进一步了解这个话题。</p> 
<h2><a id="YOLO_YOLOv2_YOLO9000_YOLOv3_YOLOv4_YOLOR_YOLOX_YOLOv5_YOLOv6_YOLOv7_220"></a>YOLO, YOLOv2, YOLO9000, YOLOv3, YOLOv4, YOLOR, YOLOX, YOLOv5, YOLOv6, YOLOv7比较</h2> 
<p>自从2015年YOLO首次发布以来，它已经通过不同版本不断发展。在本节中，我们将了解每个版本之间的区别。</p> 
<p><img src="https://images2.imgbox.com/5f/4b/1boNPvVq_o.png" alt="YOLO Timeframe 2015 to 2022"></p> 
<h3><a id="YOLOYOLOv1_235"></a>YOLO/YOLOv1，起点</h3> 
<p>YOLO的第一个版本由于快速高效地识别物体的能力而成为目标检测的改变者。</p> 
<p>然而，像许多其他解决方案一样，YOLO的第一个版本也有其自身的局限性：</p> 
<ul><li>它难以检测图像组中的较小图像，例如体育场中的一组人。这是因为YOLO架构中的每个网格都设计用于单个对象检测。</li><li>然后，YOLO无法成功检测新的或不寻常的形状。</li><li>最后，用于近似检测性能的损失函数对小型和大型边界框的误差进行相同处理，从而创建错误的定位。</li></ul> 
<h3><a id="YOLOv2YOLO9000_247"></a>YOLOv2或YOLO9000</h3> 
<p>YOLOv2是在2016年创建的，旨在使YOLO模型<a href="https://arxiv.org/abs/1612.08242" rel="nofollow">更好、更快、更强</a>。</p> 
<p>改进包括但不限于使用<a href="https://paperswithcode.com/method/darknet-19" rel="nofollow">Darknet-19</a>作为新的架构、批量归一化、更高的输入分辨率、具有锚点的卷积层、维度聚类和细粒度特征。</p> 
<p>1-批量归一化，添加批量归一化层将性能提高</p> 
<h4><a id="1__259"></a>1- 批量归一化(批标准化)</h4> 
<p>添加批量归一化层可以提高2%的mAP性能。这种批量归一化包括正则化效果，防止过拟合。</p> 
<h4><a id="2__263"></a>2- 更高的输入分辨率</h4> 
<p>YOLOv2直接使用更高分辨率的448×448输入，而不是224×224，这使得模型调整其滤波器以在更高分辨率的图像上表现更好。在ImageNet数据上训练10个时期后，这种方法将准确性提高了4%的mAP。</p> 
<h4><a id="3__267"></a>3- 使用锚框的卷积层</h4> 
<p>YOLOv2简化了问题，用锚框替换完全连接层，而不是像YOLOv1那样预测物体边界框的精确坐标。这种方法略微降低了准确性，但将模型召回率提高了7％，为改进提供了更大的空间。</p> 
<h4><a id="4__270"></a>4- 维度聚类</h4> 
<p>上述提到的锚框是由YOLOv2使用k = 5的k-means维度聚类自动发现的，而不是手动选择。这种新颖的方法在模型的召回率和精度之间提供了良好的折衷。</p> 
<p>为了更好地理解k-means维度聚类，请查看我们的Python中使用scikit-learn的K-Means聚类和R中的K-Means聚类教程。它们深入探讨了使用Python和R进行k-means聚类的概念。</p> 
<h4><a id="5__275"></a>5- 细粒度特征</h4> 
<p>YOLOv2预测生成13x13的特征图，这对于大型物体检测当然足够了。但是，对于更细的物体检测，可以通过将26×26×512特征图转换为13×13×2048特征图并与原始特征串联来修改架构。这种方法将模型性能提高了1％。</p> 
<h3><a id="YOLOv3___282"></a>YOLOv3 - 渐进式改进</h3> 
<p>在YOLOv2的基础上进行了渐进式改进，创建了YOLOv3。</p> 
<p>主要的变化包括一个新的网络架构：<a href="https://paperswithcode.com/method/darknet-53" rel="nofollow">Darknet-53</a>。这是一个106层的神经网络，具有上采样网络和残差块。与YOLOv2的骨干网络<a href="https://arxiv.org/abs/1612.08242" rel="nofollow">Darknet-19</a>相比，它更大、更快、更准确。这种新的架构在许多方面都有益处：</p> 
<h4><a id="1__287"></a>1- 更好的边界框预测</h4> 
<p>YOLOv3使用逻辑回归模型为每个边界框预测物体得分。</p> 
<h4><a id="2__291"></a>2- 更准确的类别预测</h4> 
<p>与YOLOv2中使用的softmax不同，引入了独立的逻辑分类器来准确预测边界框的类别。这在面对具有重叠标签的更复杂领域时非常有用（例如，人→足球运动员）。使用softmax会限制每个框只有一个类别，这并不总是正确的。</p> 
<h4><a id="3__295"></a>3- 在不同尺度上更准确的预测</h4> 
<p>YOLOv3对每个位置在输入图像中进行三次不同尺度的预测，以帮助从前一层进行上采样。这种策略允许获得细粒度和更有意义的语义信息，以获得更高质量的输出图像。</p> 
<h3><a id="YOLOv4___300"></a>YOLOv4 - 目标检测的最佳速度和准确性</h3> 
<p>与所有先前版本和其他最先进的目标检测器相比，这个版本的YOLO具有最佳的目标检测速度和准确性。</p> 
<p>下面的图像显示，与YOLOv3相比，YOLOv4的速度提高了10％，与FPS相比提高了12％。</p> 
<p><img src="https://images2.imgbox.com/6a/f1/W2jbbCE2_o.png" alt="YOLOv4 Speed compared to YOLOv3"></p> 
<p>YOLOv4是专门为生产系统设计的，并针对并行计算进行了优化。</p> 
<p>YOLOv4架构的主干是<a href="https://paperswithcode.com/method/cspdarknet53" rel="nofollow">CSPDarknet53</a>，这是一个包含29个卷积层的网络，具有3 x 3滤波器和大约2760万个参数。</p> 
<p>与YOLOv3相比，该架构增加了以下信息以实现更好的物体检测：</p> 
<ul><li>空间金字塔池化（SPP）块显着增加了<strong>感受野</strong>，分离了最相关的上下文特征，并不影响网络速度。</li><li>YOLOv4使用PANet而不是YOLOv3中使用的特征金字塔网络（FPN）来进行来自不同检测级别的参数聚合。</li><li>数据增强使用拼贴技术，将四个训练图像组合在一起，并采用自适应对抗性训练方法。</li><li>使用遗传算法执行最佳超参数选择。</li></ul> 
<blockquote> 
 <p>感受野: 卷积神经网络每一层输出的特征图（feature map）上的像素点映射回输入图像上的区域大小。通俗点的解释是，特征图上一点，相对于原图的大小，也是卷积神经网络特征所能看到输入图像的区域</p> 
</blockquote> 
<h3><a id="YOLOR%E2%80%8A%E2%80%8AYou_Only_Look_One_Representation_322"></a>YOLOR — You Only Look One Representation</h3> 
<p>YOLOR是一个多任务统一网络，它基于显式和隐式知识方法的组合统一网络。</p> 
<p><img src="https://images2.imgbox.com/49/34/EVnAzYJA_o.png" alt="YOLOR unified network architecture"></p> 
<p><strong>显性知识和隐性知识</strong></p> 
<p>显性知识是指正常或有意识的学习。而隐性学习则是指通过经验在潜意识中进行的学习。</p> 
<p>将这两种技术结合起来，YOLOR能够基于三个过程创建更强大的架构：（1）特征对齐，（2）目标检测的预测对齐，以及（3）多任务学习的规范表示。</p> 
<h4><a id="1__336"></a>1- 预测对齐</h4> 
<p>这种方法在每个特征金字塔网络（FPN）的特征图中引入了隐式表示，可以将精度提高约0.5％。</p> 
<h4><a id="2__339"></a>2- 目标检测的预测细化</h4> 
<p>通过向网络的输出层添加隐式表示，可以对模型预测进行细化。</p> 
<h4><a id="3__343"></a>3- 多任务学习的规范表示</h4> 
<p>执行多任务训练需要在所有任务共享的损失函数上执行联合优化。这个过程可能会降低模型的整体性能，而在模型训练期间集成规范表示可以缓解这个问题。</p> 
<p>从下面的图表中，我们可以看到YOLOR在<a href="https://viso.ai/deep-learning/yolor/#:~:text=vision.%20On%20the-,MS%20COCO,-dataset%2C%20the%20mean" rel="nofollow">MS COCO</a>数据上实现了与其他模型相比的最先进的推理速度。</p> 
<p><img src="https://images2.imgbox.com/3d/76/uhd73HxP_o.png" alt="YOLOR vs YOLOv4"></p> 
<h3><a id="YOLOX__2021YOLO_359"></a>YOLOX - 2021年超越YOLO系列</h3> 
<p>本文使用YOLOv3的修改版作为基线，并以Darknet-53作为其骨干网络。</p> 
<p><a href="https://arxiv.org/abs/2107.08430" rel="nofollow">《YOLOX: Exceeding YOLO Series in 2021》</a>一文中，YOLOX提供了以下四个关键特性，以创建比旧版本更好的模型。</p> 
<h4><a id="1__365"></a>1- 高效分离的头部</h4> 
<p>在之前的YOLO版本中使用的联合头部被证明会降低模型的性能。YOLOX使用了分离的头部，可以将分类和定位任务分开，从而提高了模型的性能。</p> 
<h4><a id="2__369"></a>2- 强大的数据增强</h4> 
<p>将Mosaic和<a href="https://paperswithcode.com/method/mixup" rel="nofollow">MixUp</a>集成到数据增强方法中，显著提高了YOLOX的性能。</p> 
<h4><a id="3__373"></a>3- 无锚点系统</h4> 
<p>基于锚点的算法在内部执行聚类，这会增加推理时间。在YOLOX中去除了锚点机制，减少了每个图像的预测数量，并显著提高了推理时间。</p> 
<h4><a id="4_SimOTA_377"></a>4- SimOTA标签分配</h4> 
<p>作者引入了SimOTA，一种更强大的标签分配策略，而不是使用交并比（IoU）方法。SimOTA不仅减少了训练时间，还避免了额外的超参数问题，从而实现了最先进的结果。此外，它将检测mAP提高了3％。</p> 
<h3><a id="YOLOv5_382"></a>YOLOv5</h3> 
<p>相较于其他版本，YOLOv5没有发表研究论文，是第一个在Pytorch中实现而非Darknet的YOLO版本。</p> 
<p>由Glenn Jocher于2020年6月发布，YOLOv5与YOLOv4类似，使用<a href="https://paperswithcode.com/method/cspdarknet53" rel="nofollow">CSPDarknet53</a>作为其架构的主干。发布包括五种不同的模型大小：YOLOv5s（最小）、YOLOv5m、YOLOv5l和YOLOv5x（最大）。</p> 
<p>YOLOv5架构中的一个主要改进是集成了Focus层，由单个层表示，通过替换YOLOv3的前三层来创建。该集成减少了层数和参数数量，并且在不对mAP产生重大影响的情况下增加了前向和后向速度。</p> 
<p>以下插图比较了YOLOv4和YOLOv5s之间的训练时间。</p> 
<p><img src="https://images2.imgbox.com/70/d2/RARJJPQ3_o.png" alt="YOLOv4 vs YOLOv5 Training Time"></p> 
<h3><a id="YOLOv6___408"></a>YOLOv6 - 一种面向工业应用的单阶段物体检测框架</h3> 
<p>由中国电商公司美团推出的 YOLOv6 (MT-YOLOv6) 框架专门针对<a href="https://arxiv.org/pdf/2209.02976.pdf" rel="nofollow">工业应用</a>进行了硬件友好的高效设计和高性能的优化。</p> 
<p>该框架是用 Pytorch 编写的，虽然不是官方 YOLO 的一部分，但其主干受到了原始单阶段 YOLO 架构的启发而被命名为 YOLOv6。</p> 
<p>相对于先前的 YOLOv5，YOLOv6 在硬件友好的主干和颈部设计、高效的解耦头部和更有效的训练策略方面引入了三项重大改进。</p> 
<p>如下所示，YOLOv6 在 COCO 数据集上的准确性和速度方面相对于先前的 YOLO 版本都有了显著提高。</p> 
<p><img src="https://images2.imgbox.com/e3/0e/fpKwhxhn_o.png" alt="YOLO Model Comparison"></p> 
<ul><li> <p>YOLOv6-N在NVIDIA Tesla T4 GPU上的吞吐量为1234 FPS，达到了35.9%的AP。</p> </li><li> <p>YOLOv6-S在869 FPS的速度下，达到了43.3%的AP，创造了新的最优结果。</p> </li><li> <p>YOLOv6-M和YOLOv6-L在相同的推理速度下分别达到了更好的准确性表现，分别为49.5%和52.3%。</p> </li></ul> 
<h3><a id="YOLOv7___435"></a>YOLOv7 - 可训练的免费工具包为实时目标检测器设定了新的技术水平</h3> 
<p>YOLOv7于2022年7月发布，文章名为“<a href="https://arxiv.org/pdf/2207.02696.pdf" rel="nofollow">训练免费工具包为实时目标检测器设定了新的技术水平</a>”。这个版本在目标检测领域取得了重大进展，其在准确性和速度方面均超过了以前的所有模型。</p> 
<p><img src="https://images2.imgbox.com/da/d7/fXK7abmc_o.png" alt="YOLOV7 VS Competitors"></p> 
<p>YOLOv7在其(1)架构和(2)可训练免费工具包层面上进行了重大改变：</p> 
<h4><a id="1__445"></a>1- 架构层面</h4> 
<p>YOLOv7通过集成扩展高效层聚合网络（E-ELAN）来改进其架构，从而使模型学习更多样化的特征，以便更好地进行学习。</p> 
<p>此外，YOLOv7通过连接其派生模型的架构，如YOLOv4、Scaled YOLOv4和YOLO-R，来扩展其架构，从而使模型满足不同推理速度的需求。</p> 
<p><img src="https://images2.imgbox.com/88/e0/9F8rN2Bi_o.png" alt="YOLO Compound Scaling Depth"></p> 
<h4><a id="2__453"></a>2- 可训练的免费工具包</h4> 
<p>“免费工具包”一词指的是提高模型准确性而不增加训练成本的方法，这就是为什么YOLOv7不仅提高了推理速度，还提高了检测准确性的原因。</p> 
<p>YOLOv8 是一种尖端的、最先进的 (SOTA) 模型，它建立在以前成功的 YOLO 版本的基础上，并引入了新功能和改进以进一步提高性能和灵活性。</p> 
<p>与其他 YOLO 的比较 [来源：Ultralytics ]<br> 它使用无锚检测和新的卷积层来使预测更加准确。</p> 
<p>V8 不同版本对比【来源：Roboflow】<br> 结果：<br> YOLO 8 在 RF100 上得到的结果比其他版本有所改进。</p> 
<h3><a id="YOLOv8_469"></a>YOLOv8</h3> 
<p>YOLOv8是一种尖端的、最先进的 (SOTA) 模型，它建立在以前成功的 YOLO 版本的基础上，并引入了新功能和改进以进一步提高性能和灵活性。</p> 
<p><img src="https://images2.imgbox.com/ac/4d/QcRA3DzG_o.png" alt="img"></p> 
<p>它使用<strong>无锚检测</strong>和新的卷积层来使预测更加准确。</p> 
<p><img src="https://images2.imgbox.com/ed/c3/sfyLuKRu_o.png" alt="img"></p> 
<h4><a id="_483"></a>结果：</h4> 
<p>YOLO 8 在 RF100 上得到的结果比其他版本有所改进。</p> 
<p><img src="https://images2.imgbox.com/a9/26/ot5N9fCn_o.png" alt="img"></p> 
<h2><a id="_491"></a>结论</h2> 
<p>本文介绍了YOLO与其他最先进的目标检测算法相比的优势，以及其从2015年到2020年的演变，并突出了其优点。</p> 
<p>考虑到YOLO的快速发展，毫无疑问它将在很长一段时间内保持目标检测领域的领导地位。</p> 
<p><strong>下一步将是将YOLO算法应用于实际案例。</strong></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6cd2f8ecd792fb439f599fb63a3afe5a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">ARM处理器的指令集（3）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/745129e7e5f84c8e687a8c30c3dc2394/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">idea修改 项目代码，浏览器页面不生效 解决方案</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>