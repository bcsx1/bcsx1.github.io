<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【时间序列】时序分析实战之SARIMA、Linear model... - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【时间序列】时序分析实战之SARIMA、Linear model..." />
<meta property="og:description" content="1 数据准备 链接：https://pan.baidu.com/s/1SpDi0oT-6jqKmAT4JwC0TA
提取码：6ylx
数据：ads.csv currency.csv
代码：time_series.py
首先引入相关的 statsmodels，包含统计模型函数（时间序列）。
# 引入相关的统计包 import warnings # 忽略警告 warnings.filterwarnings(&#39;ignore&#39;) import numpy as np # 矢量和矩阵 import pandas as pd # 表格和数据操作 import matplotlib.pyplot as plt import seaborn as sns from dateutil.relativedelta import relativedelta # 有风格地处理日期 from scipy.optimize import minimize # 函数优化 import statsmodels.formula.api as smf # 统计与经济计量 import statsmodels.tsa.api as smt import scipy.stats as scs from itertools import product from tqdm import tqdm_notebook import statsmodels." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/97f3ad0f49aa9ad2159d517d77bbdafc/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-09-27T18:00:00+08:00" />
<meta property="article:modified_time" content="2021-09-27T18:00:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【时间序列】时序分析实战之SARIMA、Linear model...</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <h3>1 数据准备</h3> 
 <p><code>链接：https://pan.baidu.com/s/1SpDi0oT-6jqKmAT4JwC0TA</code></p> 
 <p><code>提取码：6ylx</code></p> 
 <p><code>数据：ads.csv currency.csv</code></p> 
 <p><code>代码：time_series.py</code></p> 
 <p>首先引入相关的 statsmodels，包含统计模型函数（时间序列）。</p> 
 <pre class="has"><code class="language-go"># 引入相关的统计包
import warnings  # 忽略警告
warnings.filterwarnings('ignore')

import numpy as np  # 矢量和矩阵
import pandas as pd  # 表格和数据操作
import matplotlib.pyplot as plt
import seaborn as sns
from dateutil.relativedelta import relativedelta  # 有风格地处理日期
from scipy.optimize import minimize  # 函数优化
import statsmodels.formula.api as smf  # 统计与经济计量
import statsmodels.tsa.api as smt
import scipy.stats as scs
from itertools import product
from tqdm import tqdm_notebook
import statsmodels.api as sm</code></pre> 
 <p>用真实的手机游戏数据作为样例，研究每小时观看的广告数和每日所花的游戏币。</p> 
 <pre class="has"><code class="language-go"># 1 如真实的手机游戏数据，将调查每小时观看的广告和每天花费的游戏币
ads = pd.read_csv(r'./test/ads.csv', index_col=['Time'], parse_dates=['Time'])
currency = pd.read_csv(r'./test/currency.csv', index_col=['Time'], parse_dates=['Time'])</code></pre> 
 <h3>2 稳定性</h3> 
 <p>建模前，先来了解一下稳定性（stationarity)。</p> 
 <p>如果一个过程是平稳的，这意味着它不会随时间改变其统计特性，如<strong>均值和方差</strong>等等。</p> 
 <p>方差的恒常性称为同方差，<strong>协方差函数</strong>不依赖于时间，它只取决于观测值之间的距离。</p> 
 <p>非平稳过程是指分布参数或者分布规律随时间发生变化。也就是说，非平稳过程的统计特征是时间的函数（随时间变化）。</p> 
 <p>下面的红色图表不是平稳的：</p> 
 <ul><li><p>平均值随时间增加</p></li></ul> 
 <img src="https://images2.imgbox.com/c5/0e/ZVNuZZNl_o.png" alt="d772016fbbaca979ff82b2738b31046a.png">- 方差随时间变化 
 <img src="https://images2.imgbox.com/cd/1c/IMCDVs72_o.png" alt="f66addb4419fbc4c1d8c5c945b51912b.png">- 随着时间的增加，距离变得越来越近。因此，协方差不是随时间而恒定的 
 <img src="https://images2.imgbox.com/d1/f7/sJUet3Zw_o.png" alt="d07f31e0d59502b4679ba76fb993a5f7.png"> 
 <p><strong>为什么平稳性如此重要呢?</strong>通过假设未来的统计性质与目前观测到的统计性质不会有什么不同，可以很容易对平稳序列进行预测。</p> 
 <p>大多数的时间序列模型，以这样或那样的方式，试图预测那些属性(例如均值或方差)。</p> 
 <p>如果原始序列不是平稳的，那么未来的预测将是错误的。</p> 
 <p>大多数时间序列都是非平稳的，但可以(也应该)改变这一点。</p> 
 <p><strong>平稳时间序列的类型：</strong></p> 
 <ul><li><p>平稳过程（stationary process）：产生平稳观测序列的过程。</p></li><li><p>平稳模型（stationary model）：描述平稳观测序列的模型。</p></li><li><p>趋势平稳（trend stationary）：不显示趋势的时间序列。</p></li><li><p>季节性平稳（seasonal stationary）：不表现出季节性的时间序列。</p></li><li><p>严格平稳（strictly stationary）：平稳过程的数学定义，特别指观测值的联合分布不受时移的影响。</p></li></ul> 
 <p>若时序中有明显的趋势和季节性，则对这些成分建模，将它们从观察中剔除，然后用残差训练建模。</p> 
 <p><strong>平稳性检查方法（可以检查观测值和残差）：</strong></p> 
 <ul><li><p>看图：绘制时序图，看是否有明显的趋势或季节性，如绘制频率图，看是否呈现高斯分布（钟形曲线）。</p></li><li><p>概括统计：看不同季节的数据或随机分割或检查重要的差分，如将数据分成两部分，计算各部分的均值和方差，然后比较是否一样或同一范围内</p></li><li><p>统计测试：选用统计测试检查是否有趋势和季节性</p></li></ul> 
 <p>若时序的均值和方差相差过大，则有可能是非平稳序列，此时可以对观测值取对数，将指数变化转变为线性变化。然后再次查看取对数后的观测值的均值和方差以及频率图。</p> 
 <p>上面前两种方法常常会欺骗使用者，因此更好的方法是用统计测试 sm.tsa.stattools.adfuller()。</p> 
 <p>接下来将学习如何检测稳定性，从白噪声开始。</p> 
 <pre class="has"><code class="language-go"># 5经济计量方法（Econometric approach)
# ARIMA 属于经济计量方法
# 创建平稳序列
white_noise = np.random.normal(size=1000)
with plt.style.context('bmh'):
    plt.figure(figsize=(15,5))
    plt.plot(white_noise)</code></pre> 
 <img src="https://images2.imgbox.com/d9/3c/JH09LKI8_o.png" alt="ec9e8797bc933ef8bfd9034397111edf.png"> 
 <p>标准正态分布产生的过程是平稳的，在0附近振荡，偏差为1。现在，基于这个过程将生成一个新的过程，其中每个后续值都依赖于前一个值。</p> 
 <pre class="has"><code class="language-go">def plotProcess(n_samples=1000,rho=0):
    x=w=np.random.normal(size=n_samples)
    for t in range(n_samples):
        x[t] = rho*x[t-1]+w[t]

    with plt.style.context('bmh'):
        plt.figure(figsize=(10,5))
        plt.plot(x)
        plt.title('Rho {}\n Dickey-Fuller p-value: {}'.format(rho,round(sm.tsa.stattools.adfuller(x)[1],3)))

#-------------------------------------------------------------------------------------
for rho in [0,0.6,0.9,1]:
    plotProcess(rho=rho)</code></pre> 
 <img src="https://images2.imgbox.com/82/ea/ypImXUdk_o.png" alt="2bbc4454dcddb7f8512ade7ccbe2875f.png"> 
 <img src="https://images2.imgbox.com/ac/30/vhptFvbS_o.png" alt="f394c2daa56b415b1f9b4a5d2133654c.png"> 
 <img src="https://images2.imgbox.com/3f/b4/quShbyzx_o.png" alt="f20d3ffd3e3a9dd44b87ea3f8ae0394a.png"> 
 <img src="https://images2.imgbox.com/51/04/Dl46dXoz_o.png" alt="af48a741579c5249f27b6d694263b6de.png"> 
 <p>第一张图与静止白噪声一样。</p> 
 <p>第二张图，ρ增加至0.6，大的周期出现，但整体是静止的。</p> 
 <p>第三张图，偏离0均值，但仍然在均值附近震荡。</p> 
 <p>第四张图，ρ= 1，有一个随机游走过程即非平稳时间序列。当达到临界值时， 不返回其均值。从两边减去 ，得到  ，左边的表达式被称为<strong>一阶差分</strong>。</p> 
 <p>如果 ρ= 1，那么一阶差分等于平稳白噪声 。这是<strong>Dickey-Fuller时间序列平稳性测试</strong>(测试单位根的存在)背后的主要思想。</p> 
 <p>如果<strong>可以用一阶差分从非平稳序列中得到平稳序列，称这些序列为1阶积分</strong>。<strong>该检验的零假设是时间序列是非平稳的</strong>，在前三个图中被拒绝，在最后一个图中被接受。</p> 
 <p>1阶差分并不总是得到一个平稳的序列，因为这个过程可能是 d 阶的积分，d &gt; 1阶的积分(有多个单位根)。在这种情况下，使用增广的Dickey-Fuller检验，它一次检查多个滞后时间。</p> 
 <p>可以使用不同的方法来去除非平稳性：各种顺序差分、趋势和季节性去除、平滑以及转换如Box-Cox或对数转换。</p> 
 <h3>3 SARIMA</h3> 
 <p>接下来开始建立ARIMA模型，在建模之前需要将非平稳时序转换为平稳时序。</p> 
 <h4>3.1 去除非稳定性</h4> 
 <pre class="has"><code class="language-go"># 摆脱非平稳性，建立SARIMA（Getting rid of non-stationarity and building SARIMA）

def tsplot(y,lags=None,figsize=(12,7),style='bmh'):
    """
    Plot time series, its ACF and PACF, calculate Dickey-Fuller test
    y:timeseries
    lags:how many lags to include in ACF,PACF calculation
    """
     if not isinstance(y, pd.Series):
         y = pd.Series(y)

     with plt.style.context(style):
         fig = plt.figure(figsize=figsize)
         layout=(2,2)
         ts_ax = plt.subplot2grid(layout, (0,0), colspan=2)
         acf_ax = plt.subplot2grid(layout, (1,0))
         pacf_ax = plt.subplot2grid(layout, (1,1))

         y.plot(ax=ts_ax)
         p_value = sm.tsa.stattools.adfuller(y)[1]
         ts_ax.set_title('Time Series Analysis Plots\n Dickey-Fuller: p={0:.5f}'.format(p_value))
         smt.graphics.plot_acf(y,lags=lags, ax=acf_ax)
         smt.graphics.plot_pacf(y,lags=lags, ax=pacf_ax)
         plt.tight_layout()

#-------------------------------------------------------------------------------------
tsplot(ads.Ads, lags=60)</code></pre> 
 <img src="https://images2.imgbox.com/b1/f6/p4lZJL1D_o.png" alt="802ac5a388ae8a7ac8fc62c7555e883c.png"> 
 <p>从图中可以看出，Dickey-Fuller检验拒绝了单位根存在的原假设（p=0）；序列是平稳的，没有明显的趋势，所以均值是常数，方差很稳定。</p> 
 <p>唯一剩下的是季节性，必须在建模之前处理它。可以通过<strong>季节差分</strong>去除季节性，即序列本身减去一个滞后等于季节周期的序列。</p> 
 <pre class="has"><code class="language-go">ads_diff = ads.Ads-ads.Ads.shift(24) # 去除季节性
tsplot(ads_diff[24:], lags=60)

ads_diff = ads_diff - ads_diff.shift(1) # 去除趋势
tsplot(ads_diff[24+1:], lags=60) # 最终图</code></pre> 
 <img src="https://images2.imgbox.com/5b/41/HrHwUW6E_o.png" alt="82790eedaa02e7d0ca72457837080b47.png"> 
 <img src="https://images2.imgbox.com/c6/19/9Cqq5Q5y_o.png" alt="bdc97c71fe2683c8334ad583694fdf3d.png"> 
 <p>第一张图中，随着季节性的消失，自回归好多了，但是仍存在太多显著的滞后，需要删除。首先使用一阶差分，用滞后1从自身中减去时序。</p> 
 <p>第二张图中，通过季节差分和一阶差分得到的序列在0周围震荡。Dickey-Fuller试验表明，ACF是平稳的，显著峰值的数量已经下降，可以开始建模了。</p> 
 <h4>3.2 建 SARIMA 模型</h4> 
 <p>SARIMA：Seasonal Autoregression Integrated Moving Average model。</p> 
 <p>是简单自回归移动平均的推广，并增加了积分的概念。</p> 
 <ul><li><p><strong>AR</strong>(p): 利用一个观测值和一些滞后观测值之间的依赖关系的模型。模型中的最大滞后称为p。要确定初始p，需要查看PACF图并找到最大的显著滞后，在此之后大多数其他滞后都变得不显著。</p></li><li><p><strong>I</strong>(d): 利用原始观测值的差值(如观测值减去上一个时间步长的观测值)使时间序列保持平稳。这只是使该系列固定所需的非季节性差分的数量。在例子中它是1，因为使用了一阶差分。</p></li><li><p><strong>MA</strong>(q):利用观测值与滞后观测值的移动平均模型残差之间的相关性的模型。目前的误差取决于前一个或前几个，这被称为q。初始值可以在ACF图上找到，其逻辑与前面相同</p></li></ul> 
 <p>每个成分都对应着相应的参数。</p> 
 <p>SARIMA(p,d,q)(P,D,Q,s) 模型需要选择趋势和季节的超参数。</p> 
 <p><strong>趋势参数</strong>，趋势有三个参数，与ARIMA模型的参数一样：</p> 
 <ul><li><p>p: 模型中包含的滞后观测数，也称滞后阶数。</p></li><li><p>d: 原始观测值被差值的次数，也称为差值度。</p></li><li><p>q:移动窗口的大小，也叫移动平均的阶数</p></li></ul> 
 <p><strong>季节参数</strong>：</p> 
 <ul><li><p>S(s):负责季节性，等于单个季节周期的时间步长</p></li><li><p>P:模型季节分量的自回归阶数，可由PACF推导得到。但是需要看一下显著滞后的次数，是季节周期长度的倍数。如果周期等于24，看到24和48的滞后在PACF中是显著的，这意味着初始P应该是2。P=1将利用模型中第一次季节偏移观测，如t-(s*1)或t-24；P=2，将使用最后两个季节偏移观测值t-(s * 1)， t-(s * 2)</p></li><li><p>Q:使用ACF图实现类似的逻辑</p></li><li><p>D:季节性积分的阶数（次数）。这可能等于1或0，取决于是否应用了季节差分。</p></li></ul> 
 <blockquote> 
  <p>A seasonal ARIMA model uses differencing at a lag equal to the number of seasons (s) to remove additive seasonal effects. As with lag 1 differencing to remove a trend, the lag s differencing introduces a moving average term. The seasonal ARIMA model includes autoregressive and moving average terms at lag s. — Page 142, Introductory Time Series with R, 2009.</p> 
 </blockquote> 
 <p><strong>可以通过分析ACF和PACF图来选择趋势参数，查看最近时间步长的相关性（如1，2，3）。</strong></p> 
 <p><strong>同样，可以分析ACF和PACF图，查看季节滞后时间步长的相关性来指定季节模型参数的值。</strong></p> 
 <p>现在知道了如何设置初始参数，查看最终图并设置参数。</p> 
 <p>上面倒数第一张图就是最终图：</p> 
 <ul><li><p>p:最可能是4，因为它是PACF上最后一个显著的滞后，在这之后，大多数其他的都不显著。</p></li><li><p>d:为1，因为我们计算了一阶差分</p></li><li><p>q:应该在4左右，就像ACF上看到的那样</p></li><li><p>P:可能是2，因为24和48的滞后对PACF有一定的影响</p></li><li><p>D:为1，因为计算了季节差分</p></li><li><p>Q:可能1，ACF的第24个滞后显著，第48个滞后不显著。</p></li></ul> 
 <p>下面看不同参数的模型表现如何：</p> 
 <pre class="has"><code class="language-go"># 建模 SARIMA
# setting initial values and some bounds for them
ps = range(2,5)
d=1
qs=range(2,5)
Ps=range(0,2)
D=1
Qs=range(0,2)
s=24 #season length

# creating list with all the possible combinations of parameters
parameters=product(ps,qs,Ps,Qs)
parameters_list = list(parameters)
print(parameters)
print(parameters_list)
print(len(parameters_list))
# 36</code></pre> 
 <pre class="has"><code class="language-go">def optimizeSARIMA(parameters_list, d,D,s):
    """
    Return dataframe with parameters and corresponding AIC
    parameters_list:list with (p,q,P,Q) tuples
    d:integration order in ARIMA model
    D:seasonal integration order
    s:length of season
    """
    results = []
    best_aic = float('inf')

    for param in tqdm_notebook(parameters_list):
        # we need try-exccept because on some combinations model fails to converge
        try:
            model = sm.tsa.statespace.SARIMAX(ads.Ads, order=(param[0], d,param[1]),
                                              seasonal_order=(param[2], D,param[3], s)).fit(disp=-1)
        except:
            continue
        aic = model.aic
        # saving best model, AIC and parameters
        if aic&lt;best_aic:
            best_model = model
            best_aic = aic
            best_param = param
        results.append([param, model.aic])

    result_table = pd.DataFrame(results)
    result_table.columns = ['parameters', 'aic']
    # sorting in ascending order, the lower AIC is - the better
    result_table = result_table.sort_values(by='aic',ascending=True).reset_index(drop=True)

    return result_table</code></pre> 
 <pre class="has"><code class="language-go">%%time
result_table = optimizeSARIMA(parameters_list, d,D,s)

result_table.head()

# set the parameters that give the lowerst AIC
p,q,P,Q = result_table.parameters[0]
best_model = sm.tsa.statespace.SARIMAX(ads.Ads, order=(p,d,q),seasonal_order=(P,D,Q,s)).fit(disp=-1)
print(best_model.summary()) # 打印拟合模型的摘要。这总结了所使用的系数值以及对样本内观测值的拟合技巧。

# inspect the residuals of the model
tsplot(best_model.resid[24+1:], lags=60)</code></pre> 
 <pre class="has"><code class="language-go">parameters          aic
0  (2, 3, 1, 1)  3888.642174
1  (3, 2, 1, 1)  3888.763568
2  (4, 2, 1, 1)  3890.279740
3  (3, 3, 1, 1)  3890.513196
4  (2, 4, 1, 1)  3892.302849

                                Statespace Model Results
==========================================================================================
Dep. Variable:                                Ads   No. Observations:                  216
Model:             SARIMAX(2, 1, 3)x(1, 1, 1, 24)   Log Likelihood               -1936.321
Date:                            Sun, 08 Mar 2020   AIC                           3888.642
Time:                                    23:06:23   BIC                           3914.660
Sample:                                09-13-2017   HQIC                          3899.181
                                     - 09-21-2017                                         
Covariance Type:                              opg
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
ar.L1          0.7913      0.270      2.928      0.003       0.262       1.321
ar.L2         -0.5503      0.306     -1.799      0.072      -1.150       0.049
ma.L1         -0.7316      0.262     -2.793      0.005      -1.245      -0.218
ma.L2          0.5651      0.282      2.005      0.045       0.013       1.118
ma.L3         -0.1811      0.092     -1.964      0.049      -0.362      -0.000
ar.S.L24       0.3312      0.076      4.351      0.000       0.182       0.480
ma.S.L24      -0.7635      0.104     -7.361      0.000      -0.967      -0.560
sigma2      4.574e+07   5.61e-09   8.15e+15      0.000    4.57e+07    4.57e+07
===================================================================================
Ljung-Box (Q):                       43.70   Jarque-Bera (JB):                10.56
Prob(Q):                              0.32   Prob(JB):                         0.01
Heteroskedasticity (H):               0.65   Skew:                            -0.28
Prob(H) (two-sided):                  0.09   Kurtosis:                         4.00
===================================================================================
Warnings:
[1] Covariance matrix calculated using the outer product of gradients (complex-step).
[2] Covariance matrix is singular or near-singular, with condition number 8.82e+31. Standard errors may be unstable.</code></pre> 
 <img src="https://images2.imgbox.com/4a/c7/89fVlOov_o.png" alt="fb0ffb7e2a19950599253f74a3b19311.png"> 
 <p>很明显，残差是平稳的，不存在明显的自相关。可用我们的模型来预测。</p> 
 <pre class="has"><code class="language-go">def plotSARIMA(series, model, n_steps):
    """
    plot model vs predicted values
    series:dataset with timeseries
    model:fitted SARIMA model
    n_steps:number of steps to predict in the future
    """
    # adding model values
    data = series.copy()
    data.columns = ['actual']
    data['arima_model']=model.fittedvalues
    # making a shift on s+d steps, because these values were unobserved by the model due the differentiating
    data['arima_model'][:s+d]=np.nan

    # forecasting on n_steps forward
    forecast = model.predict(start=data.shape[0],end=data.shape[0]+n_steps)
    forecast = data.arima_model.append(forecast)
    # calculate error, again having shifted on s+d steps from the beginning
    error = mean_absolute_percentage_error(data['actual'][s+d:], data['arima_model'][s+d:])

    plt.figure(figsize=(15,7))
    plt.title('Mean Absolute Percentage Error: {0:.2f}%'.format(error))
    plt.plot(forecast,color='r',label='model')
    plt.axvspan(data.index[-1],forecast.index[-1], alpha=0.5,color='lightgrey')
    plt.plot(data.actual,label='actual')
    plt.legend()
    plt.grid(True)

#-------------------------------------------------------------------------------------
plotSARIMA(ads, best_model, 50)</code></pre> 
 <img src="https://images2.imgbox.com/0f/cd/eLfGtbUq_o.png" alt="ea639d67d052bfa99710bfefb7af80de.png"> 
 <p>最后，得到了非常充分的预测。模型平均误差是3.94%，非常好。但准备数据、使序列平稳和选择参数的总成本可能不值得这么精确。</p> 
 <p>这一过程的步骤总结如下：</p> 
 <ul><li><p>模式识别：使用图表和汇总统计数据来确定趋势、季节性和自回归元素，从而了解差分的大小和所需的滞后的大小。</p></li><li><p>参数估计：利用拟合程序求出回归模型的系数。</p></li><li><p>模型检查：利用剩余误差的图和统计检验来确定模型没有捕捉到的时间结构的数量和类型。</p></li></ul> 
 <h3>4 线性模型</h3> 
 <p>通常，在工作中必须以快速、好作为指导原则来建立模型。</p> 
 <p>一些模型不能拿来就用，因为它们需要太多的数据准备时间(如SARIMA)，或者需要对新数据进行频繁的训练(SARIMA)，或者很难调参(SARIMA)。因此，从现有的时间序列中选择几个特征并构建一个简单的线性回归模型，或者一个随机森林，通常要容易得多。</p> 
 <p>这种方法没有理论支持，并且打破了几个假设(例如高斯-马尔科夫定理，尤其是误差不相关的情况下)，但是在实践中非常有用，经常在机器学习竞赛中使用。</p> 
 <h4>4.1 特征提取</h4> 
 <p>模型需要特征，但只有一维的时间序列。<strong>可以提取那些特征？</strong></p> 
 <ul><li><p><strong>时间序列窗口统计的滞后</strong>：一个窗口中序列的最大值/最小值、一个窗口中值、窗口方差等。</p></li><li><p><strong>日期和时间特征</strong>：一小时的第几分钟，一天的第几小时，一周的第几天等等</p></li><li><p><strong>特别的活动</strong>：将其表示为布尔特征(注意，这种方式可能失去预测的速度)。让我们运行一些方法，看看我们可以从ads时间序列数据中提取什么。</p></li></ul> 
 <p>接下来用这些方法提取特征。</p> 
 <h5>4.1.1 滞后特征</h5> 
 <p>将序列向后移动 n个时间点，得到一个特征列，其中时间序列的当前值与其在时间 t-n 时的值一致。</p> 
 <p>若进行一个 1 延迟移位，并对该特征训练模型，则该模型能够在观察到该序列的当前状态后提前预测1步。</p> 
 <p>增加滞后，比如增加到6，将允许模型提前6步做出预测，将使用6步前观察到的数据。</p> 
 <p>如果在这段未观察到的时间内，有什么东西从根本上改变了这个序列，那么模型就不会捕捉到这些变化，并且会返回带有很大误差的预测。因此，在初始滞后选择过程中，必须在最优预测质量与预测长度之间找到一个平衡点。</p> 
 <pre class="has"><code class="language-go"># 其他模型：线性...
# Creating a copy of the initial dataframe to make various transformations
data = pd.DataFrame(ads.Ads.copy())
data.columns=['y']

# Adding the lag of the target variable from 6 setps back up to 24
for i in range(6,25):
    data['lag_{}'.format(i)]=data.y.shift(i)

# take a look at the new dataframe
data.tail(7)</code></pre> 
 <pre class="has"><code class="language-go">y     lag_6    ...       lag_23    lag_24
Time                                     ...
2017-09-21 17:00:00  151790  132335.0    ...     146215.0  139515.0
2017-09-21 18:00:00  155665  146630.0    ...     142425.0  146215.0
2017-09-21 19:00:00  155890  141995.0    ...     123945.0  142425.0
2017-09-21 20:00:00  123395  142815.0    ...     101360.0  123945.0
2017-09-21 21:00:00  103080  146020.0    ...      88170.0  101360.0
2017-09-21 22:00:00   95155  152120.0    ...      76050.0   88170.0
2017-09-21 23:00:00   80285  151790.0    ...      70335.0   76050.0
[7 rows x 20 columns]</code></pre> 
 <pre class="has"><code class="language-go">from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score

# for time-series cross-validation set 5 folds
tscv = TimeSeriesSplit(n_splits=5)

def timeseries_train_test_split(X,y,test_size):
    """
    Perform train-test split with respect to time series structure
    """
    # get the index after which test set starts
    test_index = int(len(X)*(1-test_size))
    X_train = X.iloc[:test_index]
    y_train = y.iloc[:test_index]
    X_test = X.iloc[test_index:]
    y_test = y.iloc[test_index:]
    return X_train,X_test,y_train,y_test

#-------------------------------------------------------------------------------------
y = data.dropna().y
X = data.dropna().drop(['y'],axis=1)

# reserve 30% of data for testing
X_train, X_test, y_train, y_test =timeseries_train_test_split(X,y,test_size=0.3)

#-------------------------------------------------------------------------------------
# machine learning in two lines
lr = LinearRegression()
lr.fit(X_train, y_train)

#-------------------------------------------------------------------------------------
def plotModelResults(model, X_train=X_train, X_test=X_test, plot_intervals=False, plot_anomalies=False):
    """
    Plot modelled vs fact values, prediction intervals and anomalies
    """
    prediction = model.predict(X_test)

    plt.figure(figsize=(15,7))
    plt.plot(prediction,'g',label='prediction', linewidth=2.0)
    plt.plot(y_test.values, label='actual', linewidth=2.0)

    if plot_intervals:
        cv = cross_val_score(model, X_train, y_train, cv=tscv, scoring='neg_mean_absolute_error')
        mae = cv.mean() *(-1)
        deviation = cv.std()

        scale=1.96
        lower = prediction-(mae + scale * deviation)
        upper = prediction + (mae + scale *deviation)

        plt.plot(lower, 'r--', label='upper bond / lower bond', alpha=0.5)
        plt.plot(upper, 'r--', alpha=0.5)

    if plot_anomalies:
        anomalies = np.array([np.nan]*len(y_test))
        anomalies[y_test&lt;lower] = y_test[y_test&lt;lower]
        anomalies[y_test&gt;upper] = y_test[y_test&gt;upper]
        plt.plot(anomalies, 'o', markersize=10, label='Anomalies')

    error = mean_absolute_percentage_error(prediction, y_test)
    plt.title('Mean absolute percentage error {0:.2f}%'.format(error))
    plt.legend(loc='best')
    plt.tight_layout()
    plt.grid(True)


def plotCoefficients(model):
    """
    PLots sorted coefficient values of the model
    """

    coefs = pd.DataFrame(model.coef_, X_train.columns)
    print()
    coefs.columns = ['coef']
    coefs['abs'] = coefs.coef.apply(np.abs)
    coefs = coefs.sort_values(by='abs', ascending=False).drop(['abs'],axis=1)

    plt.figure(figsize=(15, 7))
    coefs.coef.plot(kind='bar')
    plt.grid(True, axis='y')
    plt.hlines(y=0,xmin=0, xmax=len(coefs), linestyles='dashed')

#-------------------------------------------------------------------------------------
plotModelResults(lr, plot_intervals=True)
plotCoefficients(lr)</code></pre> 
 <img src="https://images2.imgbox.com/45/0c/eSY04QcU_o.png" alt="8394acf8e8d3f6dd8f9564ccbc4b8321.png"> 
 <img src="https://images2.imgbox.com/d7/7f/iSWA3ugt_o.png" alt="6ee3dffb3c749207ac37de884d2b9ce6.png"> 
 <p>简单的滞后和线性回归的预测在质量方面与SARIMA差不多。有许多不必要的特征，稍后将进行特征选择。</p> 
 <p>接下来提取时间特征。</p> 
 <h5>4.1.2 时间特征</h5> 
 <pre class="has"><code class="language-go"># 提取时间特征 hour、day of week、is_weekend
data.index = pd.to_datetime(data.index)
data['hour'] = data.index.hour
data['weekday'] = data.index.weekday
data['is_weekend'] = data.weekday.isin([5,6])*1
data.tail()

#-------------------------------------------------------------------------------------
# 可视化特征
plt.figure(figsize=(16,5))
plt.title('Encoded features')
data.hour.plot()
data.weekday.plot()
data.is_weekend.plot()
plt.grid(True)</code></pre> 
 <pre class="has"><code class="language-go">y     lag_6     ...      weekday  is_weekend
Time                                      ...
2017-09-21 19:00:00  155890  141995.0     ...            3           0
2017-09-21 20:00:00  123395  142815.0     ...            3           0
2017-09-21 21:00:00  103080  146020.0     ...            3           0
2017-09-21 22:00:00   95155  152120.0     ...            3           0
2017-09-21 23:00:00   80285  151790.0     ...            3           0
[5 rows x 23 columns]</code></pre> 
 <img src="https://images2.imgbox.com/60/32/m5y7w7h9_o.png" alt="e65da79f7350daebb0dffc098f986b68.png"> 
 <p>因为现在的变量中有不同的尺度，滞后特征有数千，分类特征有数十，我们需要将它们转换成相同的尺度，以探索特征的重要性，然后是正则化。</p> 
 <h4>4.2 归一化</h4> 
 <pre class="has"><code class="language-go"># 特征的尺度不一样，需要归一化

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

y = data.dropna().y
X = data.dropna().drop(['y'], axis=1)
X_train, X_test, y_train, y_test =timeseries_train_test_split(X,y,test_size=0.3)
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

lr = LinearRegression()
lr.fit(X_train_scaled, y_train)

#-------------------------------------------------------------------------------------
plotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True)
plotCoefficients(lr)</code></pre> 
 <img src="https://images2.imgbox.com/a4/80/Ogg9RadG_o.png" alt="3e14cce6867e1c04c3ae2c45c9037bbf.png"> 
 <img src="https://images2.imgbox.com/f7/19/mx9kxCpm_o.png" alt="16a01c052e7f00e6e84029d0f47d5a54.png"> 
 <h4>4.3 目标编码</h4> 
 <pre class="has"><code class="language-go">def code_mean(data, cat_feature, real_feature):
    """
    Returns a dictionary where keys are unique categories of the cat_feature,
    and values are means over real_feature
    """
    return dict(data.groupby(cat_feature)[real_feature].mean())

average_hour = code_mean(data, 'hour', 'y')
plt.figure(figsize=(7,5))
plt.title('Hour averages')
pd.DataFrame.from_dict(average_hour, orient='index')[0].plot()
plt.grid(True)</code></pre> 
 <img src="https://images2.imgbox.com/10/ad/COsdIXHf_o.png" alt="df4eea208bec290a5786a7ffe94dc518.png"> 
 <p>把所有的变换放在一个函数中：</p> 
 <pre class="has"><code class="language-go"># 将所有的数据准备结合到一起
def prepareData(series, lag_start, lag_end, test_size, target_encoding=False):
    """
    series: pd.DataFrame or dataframe with timeseries
    lag_start: int, initial step back in time to slice target variable
               example-lag_start=1 means that the model will see yesterday's values to predict today
    lag_end: int, finial step back in time to slice target variable
             example-lag_end=4 means that the model will see up to 4 days back in time to predict today
    test_size:float, size of the test dataset after train/test split as percentage of dataset
    target_encoding:boolean, if True -  add target averages to dataset
    """

    # copy of the initial dataset
    data = pd.DataFrame(series.copy())
    data.columns = ['y']

    # lags of series
    for i in range(lag_start, lag_end):
        data['lag_{}'.format(i)]=data.y.shift(i)

    # datatime features
    data.index = pd.to_datetime(data.index)
    data['hour'] = data.index.hour
    data['weekday'] =data.index.weekday
    data['is_weekend']=data.weekday.isin([5,6])*1

    if target_encoding:
        # calculate averages on train set only
        test_index = int(len(data.dropna())*(1-test_size))
        data['weekday_average'] = list(map(code_mean(data[:test_index], 'weekday', 'y').get, data.weekday))
        data['hour_average'] = list(map(code_mean(data[:test_index], 'hour', 'y').get, data.hour))

        # drop encoded variables
        data.drop(['hour', 'weekday'], axis=1, inplace=True)

    # train-test split
    y = data.dropna().y
    X = data.dropna().drop(['y'], axis=1)
    X_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=test_size)

    return X_train, X_test, y_train, y_test

#-------------------------------------------------------------------------------------
X_train, X_test, y_train, y_test = prepareData(ads.Ads, lag_start=6, lag_end=25, test_size=0.3, target_encoding=True)

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

lr = LinearRegression()
lr.fit(X_train_scaled, y_train)

plotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True, plot_anomalies=True)
plotCoefficients(lr)
# plt.xticks(rotation=45, fontsize=7)</code></pre> 
 <img src="https://images2.imgbox.com/98/0b/fmpVbVdM_o.png" alt="8d8fcc52e038e9b15f7c622c92cefafd.png"> 
 <img src="https://images2.imgbox.com/93/ef/6nD8dwzz_o.png" alt="9b9822411e9db458ecb1aa3d4f959731.png">从图中可以看出，存在过拟合。Hour_average在训练数据集中系数过大，以至于模型决定都集中在它上面。结果，预测的质量下降了。 
 <p>这个问题可以用多种方法解决：例如，我们可以计算目标编码而不是整个训练集，而是针对某个窗口。这样，来自最后一个观察到的窗口的编码将很可能更好地描述当前的序列状态。或者，可以手动删除它，因为我们确信在这种情况下它只会使事情变得更糟。</p> 
 <pre class="has"><code class="language-go">X_train, X_test, y_train, y_test = prepareData(ads.Ads, lag_start=6, lag_end=25, test_size=0.3, target_encoding=False)

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)</code></pre> 
 <h4>4.4 正则化</h4> 
 <p>并不是所有的特征都是有用的，有些可能会导致过拟合，有些应该被删除。</p> 
 <p>除了人工检查，还可以采用正则化。</p> 
 <p>两个有名的正则化回归模型是岭回归和套索回归<sup>[2]</sup>。它们都给损失函数增加了一些约束。</p> 
 <p>在岭回归的情况下，这些约束是系数乘正则化系数的平方和。一个特征的系数越大，损失就越大。因此，尽量优化模型，同时保持系数相当低。L2正则化的结果，将有更高的偏差和更低的方差，因此模型泛化性能更好。</p> 
 <p>第二个回归模型是Lasso回归，它将损失函数添加到系数的绝对值中，而不是平方。因此，在优化过程中，不重要的特征的系数可能变成零，从而允许自动选择特征。这种正则化类型称为L1。</p> 
 <p>首先，确定有要删除的特征，并且数据具有高度相关的特征。</p> 
 <pre class="has"><code class="language-go"># 若过拟合，对特征进行正则化
# 先看一下特征之间的相关性
y = data.dropna().y
X = data.dropna().drop(['y'], axis=1)
X_train, X_test, y_train, y_test =timeseries_train_test_split(X,y,test_size=0.3)
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

plt.figure(figsize=(10,8))
sns.heatmap(X_train.corr())</code></pre> 
 <img src="https://images2.imgbox.com/69/07/Ru9uN6cz_o.png" alt="bd93d8bfc0e5f519f8795eb4b81fa8f8.png"> 
 <pre class="has"><code class="language-go"># 开始正则化
# 岭回归
from sklearn.linear_model import LassoCV, RidgeCV

ridge = RidgeCV(cv=tscv)
ridge.fit(X_train_scaled,y_train)
plotModelResults(ridge, X_train=X_train_scaled,X_test=X_test_scaled, plot_intervals=True, plot_anomalies=True)
plotCoefficients(ridge)</code></pre> 
 <img src="https://images2.imgbox.com/b2/3e/Sma8l5F6_o.png" alt="b40ebd71c38a48e791ef1e0645b58fe6.png"> 
 <img src="https://images2.imgbox.com/3b/3e/LqvbVJ8b_o.png" alt="815959cc1d3bf58c3f75adfc82d179de.png"> 
 <p>可以清楚地看到，随着它们在模型中的重要性下降，一些系数正越来越接近于零(尽管它们从未真正达到零)。</p> 
 <pre class="has"><code class="language-go"># 套索回归
lasso = LassoCV(cv=tscv)
lasso.fit(X_train_scaled,y_train)
plotModelResults(lasso, X_train=X_train_scaled,X_test=X_test_scaled, plot_intervals=True, plot_anomalies=True)
plotCoefficients(lasso)</code></pre> 
 <img src="https://images2.imgbox.com/cb/33/TXKjraJY_o.png" alt="27b2a55754d708f4fdc6e23203ab22d7.png"> 
 <img src="https://images2.imgbox.com/98/7b/yljLQwF9_o.png" alt="88a6a7e588436d4c236008a8d1fa926a.png"> 
 <p>套索回归被证明是更保守的。它从最重要的特征中去除了23的延迟，并完全去掉了5个特征，这提高了预测的质量。</p> 
 <p>下面尝试用XGBoost建模。</p> 
 <h3>5 Boosting</h3> 
 <pre class="has"><code class="language-go"># 预测模型 Boosting
from xgboost import XGBRegressor

xgb = XGBRegressor()
xgb.fit(X_train_scaled, y_train)
plotModelResults(xgb, X_train=X_train_scaled,X_test=X_test_scaled, plot_intervals=True, plot_anomalies=True)</code></pre> 
 <img src="https://images2.imgbox.com/6d/60/xYLHNPcf_o.png" alt="f26dcf3ca335d371f66391d9c7b8224a.png"> 
 <p>这是到目前为止测试过的所有模型中测试集上误差最小的。但是，这具有欺骗性。得到时间序列数据，不适合先用xgboost。<br></p> 
 <p>通常，与线性模型相比，基于树的模型处理数据趋势的能力较差。在这种情况下，您必须先停止使用序列，或者使用一些技巧来实现这个魔术。</p> 
 <p>理想情况下，您可以使该系列平稳，然后使用XGBoost。例如，可以使用线性模型预测趋势，然后加上xgboost的预测以获得最终预测。</p> 
 <h4>参考资料</h4> 
 <p>[1]</p> 
 <p>时间序列分析和预测: <em>https://blog.csdn.net/mengjizhiyou/article/details/82683448</em></p> 
 [2] 
 <p>岭回归和套索回归: <em>https://blog.csdn.net/mengjizhiyou/article/details/103048479</em></p> 
 <p style="text-align:left;"><strong>建议阅读：</strong><br></p> 
 <p style="text-align:left;"><strong><a href="https://blog.csdn.net/qq_33431368/article/details/119745444">高考失利之后，属于我的大学本科四年</a><br></strong></p> 
 <p style="text-align:left;"><a href="http://mp.weixin.qq.com/s?__biz=MzA4ODUxNjUzMQ==&amp;mid=2247484945&amp;idx=1&amp;sn=d3b302099848ad3549a395750b9da2bf&amp;chksm=9029b4cda75e3ddb7cb31a55ed528f386b3adcba3a077d6344f4c868f19bac1185a58cb262f8&amp;scene=21#wechat_redirect" rel="nofollow"><strong>【资源分享】对于时间序列，你所能做的一切.</strong></a><br></p> 
 <p style="text-align:left;"><a href="http://mp.weixin.qq.com/s?__biz=MzA4ODUxNjUzMQ==&amp;mid=2247484711&amp;idx=1&amp;sn=c4187946d58e277a45512d3146d4f062&amp;chksm=9029b7fba75e3eed7d9e647994cae086ee483c1e55d386a92d385e3648f0ce715097f2c33a62&amp;scene=21#wechat_redirect" rel="nofollow"><strong>【时空序列预测第一篇】什么是时空序列问题？这类问题主要应用了哪些模型？主要应用在哪些领域？</strong></a><br></p> 
 <p style="text-align:left;"><a href="http://mp.weixin.qq.com/s?__biz=MzA4ODUxNjUzMQ==&amp;mid=2247490839&amp;idx=1&amp;sn=d9d0a9fad6efb225a92deafd80a2838d&amp;chksm=9029afcba75e26dd76bd9263b4cf22a9ef3e1538f90008bc64867f52a7b2705bdeaf526d4b37&amp;scene=21#wechat_redirect" rel="nofollow"><strong>【AI蜗牛车出品】手把手AI项目、时空序列、时间序列、白话机器学习、pytorch修炼</strong></a></p> 
 <pre class="has"><code class="language-go">公众号：AI蜗牛车

保持谦逊、保持自律、保持进步



个人微信
备注：昵称+学校/公司+方向
如果没有备注不拉群！
拉你进AI蜗牛车交流群</code></pre> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c177468aeafb334adde832ef559b8ae7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">VSCode中文显示及中文乱码解决</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f7f8c2fb57d6ed4ab2f2846563cee443/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">关于mysql的“&#43;0”操作</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>