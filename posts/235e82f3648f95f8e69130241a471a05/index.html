<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ByteTrack翻译 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="ByteTrack翻译" />
<meta property="og:description" content="ByteTrack：通过关联每个检测框进行多目标跟踪 摘要 多目标跟踪（MOT）旨在估计视频中物体的边界框和身份。大多数方法通过关联分数高于阈值的检测框来获得身份。检测分数低的物体，例如被遮挡的物体，被简单地扔掉，这带来了不可忽视的真实物体丢失和碎片轨迹。为了解决这个问题，我们提出了一种简单、有效和通用的关联方法，通过关联几乎每个检测框而不是只关联高分的检测框来进行跟踪。对于低分检测框，我们利用它们与 tracklets 的相似性来恢复真实对象并过滤掉背景检测。当应用于 9 个不同的最先进的跟踪器时，我们的方法在 IDF1 分数上实现了从 1 到 10 分的持续改进。为了提出 MOT 的 state-of-theart 性能，我们设计了一个简单而强大的跟踪器，命名为 ByteTrack。我们首次在单 V100 GPU 上以 30 FPS 运行速度在 MOT17 的测试集上实现了 80.3 MOTA、77.3 IDF1 和 63.1 HOTA。ByteTrack 还在 MOT20、HiEve 和 BDD100K 跟踪基准上实现了最先进的性能。源代码、带有部署版本的预训练模型和应用于其他跟踪器的教程在 https://github.com/ifzhang/ByteTrack 发布。
图 1. MOT17 测试集上不同跟踪器的 MOTA-IDF1-FPS 比较。横轴为FPS（运行速度），纵轴为MOTA，圆的半径为IDF1。我们的 ByteTrack 在 MOT17 测试集上以 30 FPS 的运行速度达到 80.3 MOTA、77.3 IDF1，优于所有之前的跟踪器。详情见表 4。
1. 简介 合理的就是真实的；真实的就是合理的。 —— G.W.F. 黑格尔
基于检测的跟踪是当前最有效的多目标跟踪方法。由于视频中的复杂场景，检测器容易做出不完美的预测。最先进的 MOT 方法 [1-3, 6, 12, 18, 45, 59, 70, 72, 85] 需要处理检测框中的真阳性/假阳性权衡，以消除低置信度检测框 [4, 40]。但是，消除所有低置信度检测框是否正确？我们的回答是否定的：正如黑格尔所说：“合理的就是真实的；真实的才是合理的。”低置信度检测框有时指示对象的存在，例如被遮挡的物体。过滤掉这些对象会导致 MOT 出现不可逆转的错误，并带来不可忽略的缺失检测和碎片化轨迹。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/235e82f3648f95f8e69130241a471a05/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-05-04T16:43:29+08:00" />
<meta property="article:modified_time" content="2022-05-04T16:43:29+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ByteTrack翻译</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2 style="text-align:center;">ByteTrack：通过关联每个检测框进行多目标跟踪</h2> 
<p><img alt="" height="112" src="https://images2.imgbox.com/2f/f3/tAjOJ2BN_o.png" width="1087"></p> 
<h3 style="text-align:center;">摘要</h3> 
<p>         多目标跟踪（MOT）旨在估计视频中物体的边界框和身份。大多数方法通过关联分数高于阈值的检测框来获得身份。检测分数低的物体，例如被遮挡的物体，被简单地扔掉，这带来了不可忽视的真实物体丢失和碎片轨迹。为了解决这个问题，我们提出了一种简单、有效和通用的关联方法，通过关联几乎每个检测框而不是只关联高分的检测框来进行跟踪。对于低分检测框，我们利用它们与 tracklets 的相似性来恢复真实对象并过滤掉背景检测。当应用于 9 个不同的最先进的跟踪器时，我们的方法在 IDF1 分数上实现了从 1 到 10 分的持续改进。为了提出 MOT 的 state-of-theart 性能，我们设计了一个简单而强大的跟踪器，命名为 ByteTrack。我们首次在单 V100 GPU 上以 30 FPS 运行速度在 MOT17 的测试集上实现了 80.3 MOTA、77.3 IDF1 和 63.1 HOTA。ByteTrack 还在 MOT20、HiEve 和 BDD100K 跟踪基准上实现了最先进的性能。源代码、带有部署版本的预训练模型和应用于其他跟踪器的教程在 https://github.com/ifzhang/ByteTrack 发布。</p> 
<p class="img-center"><img alt="" height="387" src="https://images2.imgbox.com/60/5f/FvL10EF5_o.png" width="538"></p> 
<blockquote> 
 <p> 图 1. MOT17 测试集上不同跟踪器的 MOTA-IDF1-FPS 比较。横轴为FPS（运行速度），纵轴为MOTA，圆的半径为IDF1。我们的 ByteTrack 在 MOT17 测试集上以 30 FPS 的运行速度达到 80.3 MOTA、77.3 IDF1，优于所有之前的跟踪器。详情见表 4。</p> 
</blockquote> 
<h3>1. 简介</h3> 
<p>合理的就是真实的；真实的就是合理的。 —— G.W.F. 黑格尔</p> 
<p>        基于检测的跟踪是当前最有效的多目标跟踪方法。由于视频中的复杂场景，检测器容易做出不完美的预测。最先进的 MOT 方法 [1-3, 6, 12, 18, 45, 59, 70, 72, 85] 需要处理检测框中的真阳性/假阳性权衡，以消除低置信度检测框 [4, 40]。但是，消除所有低置信度检测框是否正确？我们的回答是否定的：正如黑格尔所说：“合理的就是真实的；真实的才是合理的。”低置信度检测框有时指示对象的存在，例如被遮挡的物体。过滤掉这些对象会导致 MOT 出现不可逆转的错误，并带来不可忽略的缺失检测和碎片化轨迹。</p> 
<p>        图 2 (a) 和 (b) 显示了这个问题。在第 t1 帧中，我们初始化了三个不同的 tracklet，因为它们的分数都高于 0.5。但是，在第 t2 帧和第 t3 帧发生遮挡时，红色 tracklet 对应的检测分数变低，即从 0.8 到 0.4，然后从 0.4 到 0.1。</p> 
<p class="img-center"><img alt="" height="478" src="https://images2.imgbox.com/e6/70/Kv07o09x_o.png" width="502"></p> 
<blockquote> 
 <p> 图 2. 关联每个检测框的方法示例。 (a) 显示所有检测框及其分数。(b) 显示了通过先前方法获得的轨迹，这些方法关联了分数高于阈值的检测框，即 0.5。相同的框颜色代表相同的身份。(c) 显示了通过我们的方法获得的轨迹。虚线框表示使用卡尔曼滤波器预测的之前轨迹框。两个低分检测框基于大 IoU 与之前的 tracklets 正确匹配。</p> 
</blockquote> 
<p> 这些检测框被阈值机制消除，红色轨迹相应地消失。然而，如果我们将每个检测框都考虑在内，则会立即引入更多误报，例如，图 2 (a) 的帧 t3 中最右边的框。据我们所知，MOT 中很少有方法 [30, 63] 能够处理这种检测困境。</p> 
<p>        在本文中，我们发现tracklets 的相似性为区分低分检测框中的对象和背景提供了强有力的线索。如图2（c）所示，两个低分检测框通过运动模型的预测框与轨迹匹配，从而正确地恢复了目标。同时，由于没有匹配的 tracklet，背景框被移除。</p> 
<p>        为了在匹配过程中充分利用从高分到低分的检测框，我们提出了一种简单有效的关联方法BYTE，将每个检测框命名为tracklet的一个基本单元，在计算机程序中作为字节，我们的跟踪方法对每个详细的检测框进行估值。我们首先根据运动相似性或外观相似性将高分检测框与轨迹匹配。同样[6]，我们采用卡尔曼滤波器[29]来预测轨迹在新帧中的位置。相似度可以通过预测框和检测框的IoU或Re-ID特征距离来计算。图2（b）正是第一次匹配后的结果。然后，我们使用相同的运动相似性在未匹配的轨迹（即红色框中的轨迹）和低分数检测框之间执行第二次匹配。图 2 (c) 显示了第二次匹配后的结果。检测分数低的被遮挡人与前一个轨迹正确匹配，并且背景（在图像的右侧）被移除。</p> 
<p>        作为目标检测与关联的综合课题，MOT 的理想解决方案绝不是检测器和以下关联；此外，精心设计它们的连接区域也很重要。BYTE 的创新在于检测和关联的交界处，低分检测框是提升两者的桥梁。受益于这种集成创新，当 BYTE 应用于 9 个不同的最先进的跟踪器时，包括基于 Re-ID 的跟踪器 [33,47,69,85]，基于运动的跟踪器 [71, 89]，基于链的很总起 [48] 和基于注意力的 跟踪器[59, 80]，几乎所有指标都取得了显着改进，包括 MOTA、IDF1 分数和ID 切换。例如，我们将 CenterTrack [89] 的 MOTA 从 66.1 增加到 67.4，IDF1 从 64.2 增加到 74.0，并将 MOT17 的半验证集上的 ID 从 528 减少到 144。</p> 
<p>        为了推动 MOT 的最新性能，我们提出了一个简单而强大的跟踪器，名为 ByteTrack。我们采用最近的高性能检测器 YOLOX [24] 来获取检测框并将它们与我们提出的 BYTE 相关联。在MOT挑战中，ByteTrack在MOT17[44]和MOT20[17]中均排名第一，在MOT17的V100 GPU上以30 FPS的运行速度实现了80.3 MOTA、77.3 IDF1和63.1 HOTA，在更拥挤的MOT20上实现了77.8 MOTA、75.2 IDF1和61.3 HOTA。ByteTrack 还在 HiEve [37] 和 BDD100K [79] 跟踪基准上实现了最先进的性能。我们希望 ByteTrack 的效率和简单性能够使其在社交计算等实际应用中具有吸引力。</p> 
<h3>2. 相关工作</h3> 
<h4>2.1 MOT 中的目标检测</h4> 
<p>        目标检测是计算机视觉中最活跃的课题之一，是多目标跟踪的基础。MOT17 数据集 [44] 提供了由 DPM [22]、Faster R-CNN [50] 和 SDP [77] 等流行检测器获得的检测结果。大量的方法 [3, 9, 12, 14, 28, 74, 91] 专注于基于这些给定的检测结果来提高跟踪性能。</p> 
<p><strong>        通过检测进行跟踪。</strong>随着目标检测的快速发展[10、23、26、35、49、50、58、60]，越来越多的方法开始利用更强大的检测器来获得更高的跟踪性能。单阶段目标检测器 RetinaNet [35] 开始被 [39, 48] 等几种方法采用。CenterNet [90] 是大多数方法 [63, 65, 67, 71, 85, 87, 89] 采用的最流行的检测器，因为它简单高效。YOLO 系列检测器 [8, 49] 也因其在准确性和速度上的出色平衡而被大量方法 [15, 33, 34, 69] 采用。这些方法中的大多数直接使用单个图像上的检测框进行跟踪。然而，正如视频对象检测方法 [41, 62] 所指出的，当视频序列中发生遮挡或运动模糊时，缺失检测和非常低评分检测的数量开始增加。因此，通常利用前一帧的信息来增强视频检测性能。</p> 
<p><strong>        通过跟踪进行检测。</strong>也可以采用跟踪来帮助获得更准确的检测框。一些方法 [12-15, 53, 91] 利用单目标跟踪 (SOT) [5] 或卡尔曼滤波器 [29] 来预测跟踪帧在下一帧中的位置，并将预测框与检测框融合以增强检测结果。其他方法 [34, 86] 利用前一帧中的跟踪框来增强下一帧的特征表示。最近，基于 Transformer 的 [20, 38, 64, 66] 检测器 [11, 92] 被多种方法 [42, 59, 80] 采用，因为它具有在帧之间传播框的强大能力。我们的方法也利用 tracklets 的相似性来增强检测框的可靠性。</p> 
<p>        在通过各种检测器获得检测框后，大多数MOT方法[33、39、47、59、69、71、85]仅将高分检测框保持一个阈值，即0.5，并将这些框用作数据关联的输入。这是因为低分检测框包含许多损害跟踪性能的背景。然而，我们观察到许多被遮挡的物体可以被正确检测到但分数很低。为了减少丢失的检测并保持轨迹的持久性，我们保留所有检测框并在每个检测框之间关联。</p> 
<h4>2.2.数据关联</h4> 
<p>        数据关联是多目标跟踪的核心，它首先计算tracklet和检测框之间的相似度，并根据相似度利用不同的策略进行匹配。</p> 
<p>        <strong>相似度指标。</strong>位置、运动和外观是关联的有用线索。SORT[6]以一种非常简单的方式将位置和运动线索结合起来。它首先采用卡尔曼滤波器 [29] 来预测轨迹在新帧中的位置，然后计算检测框和预测框之间的 IoU 作为相似度。最近的一些方法[59、71、89]设计网络来学习目标运动，并在相机运动较大或帧速率较低的情况下获得更稳健的结果。位置和运动相似度在短距离匹配中是准确的。外观相似性有助于长期匹配。在长时间被遮挡后，可以使用外观相似性重新识别目标。外观相似度可以通过 Re-ID 特征的余弦相似度来衡量。DeepSORT [70] 采用独立的 Re-ID 模型从检测框中提取外观特征。最近，联合检测和 Re-ID 模型 [33, 39, 47, 69, 84, 85] 因其简单和高效而变得越来越流行。</p> 
<p>        <strong>匹配策略。</strong>在相似度计算之后，匹配策略为对象分配身份。这可以通过匈牙利算法 [31] 或贪心分配 [89] 来完成。SORT [6] 通过一次匹配将检测框与轨迹匹配。DeepSORT [70] 提出了一种级联匹配策略，该策略首先将检测框与最新的轨迹匹配，然后与丢失的轨迹匹配。MOTDT [12]首先利用外观相似度进行匹配，然后利用 IoU 相似度来匹配未匹配的轨迹。QDTrack [47]通过双向softmax操作将外观相似度转化为概率，并采用最近邻搜索来完成匹配。注意机制[64]可以直接在帧之间传播框并隐式执行关联。最近的方法如 [42, 80] 提出了跟踪查询以在以下帧中找到跟踪对象的位置。匹配是在注意力交互过程中隐式执行的，不使用匈牙利算法。</p> 
<p>        所有这些方法都集中在如何设计更好的关联方法上。然而，我们认为检测框的使用方式决定了数据关联的上限，我们关注的是如何在匹配过程中充分利用从高分到低分的检测框。</p> 
<h3>3.BYTE</h3> 
<p>        我们提出了一种简单、有效和通用的数据关联方法，BYTE。与之前只保留高分检测框的方法[33、47、69、85]不同，我们保留了几乎每个检测框，并将它们分为高分和低分。我们首先将高分检测框与轨迹相关联。一些 tracklet 无法匹配是因为它们与合适的高分检测框不匹配，这通常发生在发生遮挡、运动模糊或大小变化时。然后，我们将低分检测框和这些不匹配的轨迹关联起来，以恢复低分检测框中的对象并同时过滤掉背景。</p> 
<p class="img-center"><img alt="" height="808" src="https://images2.imgbox.com/38/0f/ndNvSaHc_o.png" width="460"></p> 
<p> BYTE 的伪代码如算法 1 所示。</p> 
<p>        BYTE 的输入是视频序列 V，以及对象检测器 Det。我们还设置了检测分数阈值 <img alt="\tau" class="mathcode" src="https://images2.imgbox.com/c0/d2/PdQdAGGj_o.png">。BYTE 的输出是视频的轨迹 T，每个轨迹包含边界框和每一帧中对象的标识。对于视频中的每一帧，我们使用检测器 Det 预测检测框和分数。我们根据检测分数阈值 <img alt="\tau" class="mathcode" src="https://images2.imgbox.com/32/59/F6tPS0rO_o.png"> 将所有检测框分为 <img alt="D_{high}" class="mathcode" src="https://images2.imgbox.com/5a/f7/hPq2Ndm6_o.png"> 和 <img alt="D_{low}" class="mathcode" src="https://images2.imgbox.com/ac/9c/CYg4JUrV_o.png"> 两部分。对于得分高于 <img alt="\tau" class="mathcode" src="https://images2.imgbox.com/b8/47/z8J8usEd_o.png"> 的检测框，我们将它们放入高分检测框 <img alt="D_{high}" class="mathcode" src="https://images2.imgbox.com/99/1f/Rqih56HY_o.png">中。对于分数低于 <img alt="\tau" class="mathcode" src="https://images2.imgbox.com/3e/55/JVz4S2Ws_o.png"> 的检测框，我们将它们放入低分检测框 <img alt="D_{low}" class="mathcode" src="https://images2.imgbox.com/91/36/JkNF6BtW_o.png"> （算法 1 中的第 3 到 13 行）。</p> 
<p>        在分离出低分检测框和高分检测框后，我们采用卡尔曼滤波器来预测 T 中每个轨迹在当前帧中的新位置（算法 1 中的第 14 到 16 行）。第一个关联是在高分检测框 <img alt="D_{high}" class="mathcode" src="https://images2.imgbox.com/6c/ed/QAJimK7c_o.png"> 和所有轨迹 T（包括丢失的轨道 <img alt="T_{lost}" class="mathcode" src="https://images2.imgbox.com/36/c0/gjgDFknF_o.png">）之间执行的。相似度#1 可以通过 IoU 或检测框 ​​​​  <img alt="D_{high}" class="mathcode" src="https://images2.imgbox.com/ce/9b/Kzcpbdaa_o.png">  与轨迹T的预测框之间的 Re-ID 特征距离来计算。然后，我们采用匈牙利算法[31]完成基于相似度的匹配。我们将未匹配的检测框保留在 <img alt="D_{remain}" class="mathcode" src="https://images2.imgbox.com/5b/72/kTXl1AYJ_o.png"> 中，将未匹配的轨迹保留在 <img alt="T_{remain}" class="mathcode" src="https://images2.imgbox.com/f4/c1/g2kA4qGx_o.png"> 中（算法 1 中的第 17 到 19 行）。</p> 
<p>        BYTE 非常灵活，可以兼容其他不同的关联方式。例如，当 BYTE 与 FairMOT [85] 结合时，在算法 1 中将 Re-ID 特征添加到 * first association * 中，其他相同。在实验中，我们将 BYTE 应用于 9 个不同的最先进的跟踪器，并在几乎所有指标上取得了显著的改进。</p> 
<p>        在第一次关联之后，在低分检测框 <img alt="D_{low}" class="mathcode" src="https://images2.imgbox.com/b5/b9/C0FK5RWm_o.png"> 和剩余轨迹 <img alt="T_{remain}" class="mathcode" src="https://images2.imgbox.com/ac/46/UkrznZig_o.png"> 之间执行第二次关联。我们将不匹配的轨迹保留在 <img alt="T_{re-remain}" class="mathcode" src="https://images2.imgbox.com/0b/dd/a1AguaBJ_o.png"> 中，并删除所有不匹配的低分检测框，因为我们将它们视为背景。 （算法 1 中的第 20 到 21 行）。我们发现在第二个关联中单独使用 IoU 作为 Similarity#2 很重要，因为低分检测框通常包含严重的遮挡或运动模糊，并且外观特征不可靠。因此，当将 BYTE 应用于其他基于 Re-ID 的跟踪器 [47、69、85] 时，我们在第二个关联中不采用外观相似性。</p> 
<p>        关联后，不匹配的轨迹将从 tracklets 中删除。为简单起见，我们没有在算法 1 中列出轨迹重生的过程 [12, 70, 89]。实际上，长期关联需要保留轨迹的身份。对于第二次关联后未匹配的轨迹 <img alt="T_{re-reamin}" class="mathcode" src="https://images2.imgbox.com/cb/a4/maU0hwxC_o.png">，我们将它们放入 <img alt="T_{lost}" class="mathcode" src="https://images2.imgbox.com/7b/21/TFnLQVGd_o.png">。对于 <img alt="T_{lost}" class="mathcode" src="https://images2.imgbox.com/9e/b0/EwDRJsy8_o.png">中的每个轨迹，只有当它存在超过一定数量的帧，即 30 帧时，我们才会将其从轨迹 T 中删除。否则，我们保留 <img alt="T_{lost}" class="mathcode" src="https://images2.imgbox.com/12/75/feD9wwKU_o.png">（the lost tracks）在轨迹 T 中。（算法 1 中的第 22 行）。最后，我们在第一次关联后从未匹配的高分检测框 <img alt="D_{remain}" class="mathcode" src="https://images2.imgbox.com/00/b8/QApV7Oig_o.png"> 中初始化新的轨迹。（算法 1 中的第 23 到 27 行）。每个单独帧的输出是当前帧中轨迹 T 的边界框和标识。请注意，我们不输出 <img alt="T_{lost}" class="mathcode" src="https://images2.imgbox.com/4b/20/H41Rc8S0_o.png">的边界框和标识。</p> 
<p>         为了提出 MOT 的最新性能，我们通过为高性能检测器 YOLOX [24] 配备我们的关联方法 BYTE，设计了一个简单而强大的跟踪器，名为 ByteTrack。</p> 
<h3>4. 实验</h3> 
<h4>4.1 环境</h4> 
<p><strong>        数据集。</strong>我们根据“私人检测”协议在 MOT17 [44] 和 MOT20 [17] 数据集上评估 BYTE 和 ByteTrack。两个数据集都包含训练集和测试集，没有验证集。对于消融研究，我们使用 MOT17 训练集中每个视频的前半部分进行训练，后半部分用于验证 [89]。我们在 CrowdHuman 数据集 [55] 和 MOT17 半训练集 [59, 71, 80, 89] 的组合上进行训练。在 MOT17 的测试集上进行测试时，我们在 [33、69、85] 之后添加了 Cityperson [82] 和 ETHZ [21] 进行训练。我们还在 HiEve [37] 和 BDD100K [79] 数据集上测试了 ByteTrack。HiEve 是一个以人为中心的大规模数据集，专注于拥挤和复杂的事件。BDD100K 是最大的驾驶视频数据集，MOT 任务的数据集拆分为 1400 个用于训练的视频、200 个用于验证的视频和 400 个用于测试的视频。它需要跟踪8个类别的对象，并包含大量相机运动的情况。</p> 
<p>      <strong>  指标。</strong>我们使用 CLEAR 指标 [4]，包括 MOTA、FP、FN、ID 等，IDF1 [51] 和 HOTA [40] 来评估跟踪性能的不同方面。MOTA 是基于 FP、FN 和 ID 计算的。考虑到 FP 和 FN 的数量大于 ID，MOTA 更注重检测性能。IDF1 评估身份保持能力，更关注关联性能。HOTA 是一个最近提出的指标，它明确地平衡了执行准确检测、关联和定位的效果。对于 BDD100K 数据集，有一些多类指标，例如 mMOTA 和 mIDF1。mMOTA / mIDF1 是通过平均所有类的 MOTA / IDF1 来计算的。</p> 
<p>        <strong>实现细节。</strong>对于 BYTE，除非另有说明，否则默认检测分数阈值 <img alt="\tau" class="mathcode" src="https://images2.imgbox.com/79/0e/KEAqhz5J_o.png"> 为 0.6。对于 MOT17、MOT20 和 HiEve 的基准评估，我们仅使用 IoU 作为相似度指标。在线性分配步骤中，如果检测框和轨迹框之间的 IoU 小于 0.2，则匹配将被拒绝。对于丢失的 tracklets，我们将其保留 30 帧以防再次出现。对于 BDD100K，我们使用 UniTrack [68] 作为 Re-ID 模型。在消融研究中，我们使用 FastReID [27] 来提取 MOT17 的 Re-ID 特征。</p> 
<p>         对于 ByteTrack，检测器是 YOLOX [24]，其中 YOLOX-X 作为主干，COCO 预训练模型 [36] 作为初始化权重。对于 MOT17，在 MOT17、CrowdHuman、Cityperson 和 ETHZ 的组合上的训练计划是 80 个 epoch。对于 MOT20 和 HiEve，我们只添加 CrowdHuman 作为额外的训练数据。对于 BDD100K，我们不使用额外的训练数据，只训练 50 个 epoch。多尺度训练时，输入图像大小为 1440×800，最短边范围为 576 到 1024。数据增强包括 Mosaic [8] 和 Mixup [81]。该模型在 8 个 NVIDIA Tesla V100 GPU 上进行训练，批量大小为 48。优化器是 SGD，权重衰减为 5 × <img alt="10^{-4}" class="mathcode" src="https://images2.imgbox.com/3e/73/1x1kPGxG_o.png">，动量为 0.9。初始学习率为 <img alt="10^{-3}" class="mathcode" src="https://images2.imgbox.com/8c/a7/XbdQg929_o.png">，有 1 个 epoch 预热和余弦退火计划。总培训时间约为12小时。在 [24] 之后，FPS 在单个 GPU 上使用 FP16 精度 [43] 和 1 的批量大小进行测量。</p> 
<h4>4.2. BYTE 的消融研究</h4> 
<p>        <strong>相似性分析。</strong>我们为 BYTE 的第一个关联和第二个关联选择不同类型的相似性。结果如表1所示。我们可以看到，IoU 或 Re-ID 都是 MOT17 上 Similarity#1 的不错选择。IoU 实现了更好的 MOTA 和 ID，而 Re-ID 实现了更高的 IDF1。在 BDD100K 上，Re-ID 在第一次关联中取得了比 IoU 更好的结果。这是因为 BDD100K 包含较大的相机运动，并且注释处于低帧率，导致运动提示失败。在两个数据集的第二个关联中使用 IoU 作为 Similarity#2 很重要，因为低分检测框通常包含严重的遮挡或运动模糊，因此 Re-ID 特征不可靠。从表 1 中我们可以发现，使用 IoU 作为 Similarity#2 与 Re-ID 相比增加了大约 1.0 MOTA，这表明低分检测框的 Re-ID 特征不可靠。</p> 
<p><img alt="" height="172" src="https://images2.imgbox.com/50/a4/V6SPVTYz_o.png" width="919"></p> 
<blockquote> 
 <p> 表 1. 在 MOT17 和 BDD100K 验证集上 BYTE 的第一个关联和第二个关联中使用的不同类型相似性度量的比较。最佳结果以粗体显示。</p> 
</blockquote> 
<p><strong>        与其他关联方法的比较。</strong>我们在 MOT17 和 BDD100K 的验证集上将 BYTE 与其他流行的关联方法进行比较，包括 SORT [6]、DeepSORT [70] 和 MOTDT [12]。结果如表2所示。</p> 
<p> <img alt="" height="203" src="https://images2.imgbox.com/2a/53/rM31gST6_o.png" width="924"></p> 
<blockquote> 
 <p> 表 2. MOT17 和 BDD100K 验证集上不同数据关联方法的比较。最佳结果以粗体显示。</p> 
</blockquote> 
<p>         SORT 可以看作是我们的基线方法，因为这两种方法都只采用卡尔曼滤波器来预测物体运动。我们可以发现，BYTE 将 SORT 的 MOTA 指标从 74.6 提高到 76.6，IDF1 从 76.9 提高到 79.3，并将 ID 从 291 降低到 159。这突出了低分检测框的重要性，并证明了BYTE从低分检测框恢复对象框的能力。</p> 
<p>        DeepSORT利用额外的Re ID模型来增强长期关联。我们惊奇地发现，与 DeepSORT 相比，BYTE 也有额外的收益。这表明当检测框足够准确时，一个简单的卡尔曼滤波器可以执行长期关联并获得更好的 IDF1 和 ID。<span style="color:#956fe7;">我们注意到，在严重遮挡的情况下，Re-ID 特征很容易受到攻击，并可能导致身份转换，相反，运动模型表现得更可靠。</span></p> 
<p><span style="color:#956fe7;">        </span>MOTDT 将运动引导框传播结果与检测结果相结合，以将不可靠的检测结果与 tracklets 相关联。虽然有着相似的动机，但 MOTDT 还是远远落后于 BYTE。我们解释说 MOTDT 使用传播框作为 tracklet 框，这可能导致跟踪中的定位漂移。<span style="color:#956fe7;">相反，BYTE 使用低分检测框来重新关联那些不匹配的轨迹，因此轨迹框更准确。</span></p> 
<p><span style="color:#956fe7;">        </span>表 2 还显示了 BDD100K 数据集上的结果。 BYTE 也大大优于其他关联方法。卡尔曼滤波器在自动驾驶场景中失效，其主要原因是 SORT、DeepSORT 和 MOTDT 性能低下所导致。因此，我们不在 BDD100K 上使用卡尔曼滤波器。其他现成的 ReID 模型极大地提高了 BYTE 在 BDD100K 上的性能。</p> 
<p>         <strong>检测分数阈值的鲁棒性。</strong>检测分数阈值 <img alt="\tau _{high}" class="mathcode" src="https://images2.imgbox.com/00/1f/hG9zSYmK_o.png"> 是一个敏感的超参数，在多目标跟踪任务中需要仔细调整。我们将其从 0.2 更改为 0.8，并比较 BYTE 和 SORT 的 MOTA 和 IDF1 得分。结果如图 3 所示。从结果中我们可以看出，BYTE 比 SORT 对检测分数阈值的鲁棒性更强。这是因为 BYTE 中的第二个关联恢复了分数低于 <img alt="\tau _{high}" class="mathcode" src="https://images2.imgbox.com/54/e0/YmBP92lD_o.png"> 的对象，因此无论 <img alt="\tau _{high}" class="mathcode" src="https://images2.imgbox.com/77/31/t5eDCO0d_o.png"> 的变化如何，几乎都会考虑每个检测框。</p> 
<p></p> 
<p class="img-center"><img alt="" height="198" src="https://images2.imgbox.com/91/e9/UdeQ9bMe_o.png" width="443"></p> 
<blockquote> 
 <p> 图 3. BYTE 和 SORT 在不同检测分数阈值下的性能比较。结果来自 MOT17 的验证集。</p> 
</blockquote> 
<p>         <strong>低分检测框分析。</strong>为了证明 BYTE 的有效性，我们收集了 BYTE 获得的低分框中的 TP 和 FP 的数量。我们使用 MOT17 的半训练集和 CrowdHuman 对 MOT17 的半验证集进行训练和评估。首先，我们保留所有得分范围从 <img alt="\tau _{low}" class="mathcode" src="https://images2.imgbox.com/c6/8b/vtD0vTfn_o.png"> 到 <img alt="\tau _{high}" class="mathcode" src="https://images2.imgbox.com/8b/ca/yfcdpUlW_o.png"> 的低分检测框，并使用地面真值注释对 TP 和 FP 进行分类。然后，我们从低分检测框中选择 BYTE 获得的跟踪结果。每个序列的结果如图 4 所示。我们可以看到，BYTE 从低分检测框中获得的 TP 明显多于 FP，即使某些序列（即 MOT17-02）在所有检测框中都有更多的 FP。如表 2 所示，获得的 TP 显着增加了 MOTA 从 74.6 到 76.6。</p> 
<p class="img-center"><img alt="" height="193" src="https://images2.imgbox.com/b0/cc/auPxiG8p_o.png" width="476"></p> 
<blockquote> 
 <p> 图 4. 所有低分检测框与 BYTE 获得的低分跟踪框的 TP 和 FP 数量比较。结果来自 MOT17 的验证集。</p> 
</blockquote> 
<p>         <strong>其他跟踪器上的应用。</strong>我们将 BYTE 应用于 9 种不同的最先进的跟踪器，包括 JDE [69]、CSTrack [33]、FairMOT [85]、TraDes [71]、QDTrack [47]、CenterTrack [89]、Chained-Tracker [ 48]、TransTrack [59] 和 MOTR [80]。在这些跟踪器中，JDE、CSTrack、FairMOT、TraDes 采用了运动和 ReID 相似度的组合。QDTrack 仅采用 Re-ID 相似度。CenterTrack 和 TraDes 通过学习网络预测运动相似度。Chained-Tracker 采用链式结构，同时输出两个连续帧的结果，并通过 IoU 关联到同一帧中。TransTrack 和 MOTR 采用注意机制在帧之间传播框。他们的结果显示在表 3 中每个跟踪器的第一行。为了评估 BYTE 的有效性，我们设计了两种不同的模式来将 BYTE 应用于这些跟踪器。</p> 
<p></p> 
<p class="img-center"><img alt="" height="542" src="https://images2.imgbox.com/3f/e2/phskgmvm_o.png" width="465"></p> 
<blockquote> 
 <p>表 3. 在 MOT17 验证集上将 BYTE 应用于 9 个不同的最先进跟踪器的结果。 “K”是卡尔曼滤波器的缩写。绿色是至少 +1.0 点的改进。 </p> 
</blockquote> 
<ul><li>第一种模式是将BYTE插入到不同tracker的原始关联方法中，如表3中每个tracker结果的第二行所示。以 FairMOT [85] 为例，在原始关联完成后，我们选择所有不匹配的轨迹，并将它们与算法 1 中*第二关联*之后的低分检测框关联。请注意，对于低分对象，Re-ID 特征不可靠，因此我们仅采用运动预测后检测框和 tracklet 框之间的 IoU 作为相似度。我们没有将BYTE的第一种模式应用到Chained-Tracker，因为我们发现在链式结构中很难实现。</li><li>第二种模式是直接使用这些跟踪器的检测框，并使用算法1中的整个过程进行关联，如表3中每个跟踪器的结果的第三行所示。</li></ul> 
<p>        我们可以看到，在这两种模式下，BYTE 都能对包括 MOTA、IDF1 和 IDs 在内的几乎所有指标带来稳定的提升。例如，BYTE 将 CenterTrack 增加 1.3 MOTA 和 9.8 IDF1，Chained-Tracker 增加 1.9 MOTA 和 5.8 IDF1，TransTrack 增加 1.2 MOTA 和 4.1 IDF1。表 3 中的结果表明 BYTE 具有很强的泛化能力，可以很容易地应用于现有的跟踪器以获得性能增益。</p> 
<h4>4.3 基准评估</h4> 
<p>        我们分别在表 4、表 5 和表 6 中将 ByteTrack 与在私有检测协议下的 MOT17、MOT20 和 HiEve 的测试集上的最先进的跟踪器进行了比较。所有结果均直接从官方 MOT Challenge 评估服务器和 Human in Events 服务器获得。</p> 
<p>        <strong>MOT17。</strong>ByteTrack 在 MOT17 排行榜的所有跟踪器中排名第一。它不仅达到了最佳精度（即 80.3 MOTA、77.3 IDF1 和 63.1 HOTA），而且运行速度最高（30 FPS）。它大大优于第二性能跟踪器 [76]（即 +3.3 MOTA、+5.3 IDF1 和 +3.4 HOTA）。此外，我们使用的训练数据少于许多高性能方法，例如 [33、34、54、65、85]（29K 图像与 73K 图像）。值得注意的是，与其他额外采用 Re-ID 相似性或注意机制的方法 [33、47、59、67、80、85] 相比，我们仅在关联步骤中利用了最简单的相似度计算方法 Kalman 滤波器。所有这些都表明 ByteTrack 是一个简单而强大的跟踪器。</p> 
<p>        <strong> MOT20。</strong>与 MOT17 相比，MOT20 具有更多的拥挤场景和遮挡案例。在 MOT20 的测试集中，图像中的平均行人数为 170。ByteTrack 在 MOT20 排行榜上的所有跟踪器中也排名第一，并且在几乎所有指标上都大大优于其他跟踪器。例如，它将 MOTA 从 68.6 增加到 77.8，IDF1 从 71.4 增加到 75.2，并将 ID 从 4209 减少 71% 到 1223。值得注意的是，ByteTrack 实现了极低的身份切换，这进一步表明关联每个检测框在遮挡情况下非常有效。</p> 
<p>         <strong>Human in Events。</strong>与 MOT17 和 MOT20 相比，HiEve 包含更复杂的事件和更多样化的摄像机视图。我们在 CrowdHuman 数据集和 HiEve 的训练集上训练 ByteTrack。ByteTrack 在 HiEve 排行榜上的所有跟踪器中也排名第一，并且大大优于其他最先进的跟踪器。例如，它将 MOTA 从 40.9 增加到 61.3，将 IDF1 从 45.1 增加到 62.9。优异的结果表明 ByteTrack 对复杂场景具有鲁棒性。</p> 
<p>       <strong>  BDD100K。</strong>BDD100K 是自动驾驶场景中的多类别跟踪数据集。挑战包括低帧率和大的相机运动。我们利用来自 UniTrack [68] 的简单 ResNet-50 ImageNet 分类模型来提取 Re-ID 特征并计算外观相似度。ByteTrack 在 BDD100K 排行榜上排名第一。它将验证集的 mMOTA 从 36.6 提高到 45.5，将测试集的 mMOTA 从 35.5 提高到 40.1，这表明 ByteTrack 也可以应对自动驾驶场景中的挑战。</p> 
<p class="img-center"><img alt="" height="461" src="https://images2.imgbox.com/e5/c8/wjQgcCHG_o.png" width="457"></p> 
<blockquote> 
 <p> 表 4. MOT17 测试集上“私有检测器”协议下最先进方法的比较。最佳结果以粗体显示。 MOT17 包含丰富的场景，一半的序列是用摄像机运动捕捉的。 ByteTrack 在 MOT17 排行榜上的所有跟踪器中排名第一，并且在几乎所有指标上都大大超过了第二个 ReMOT。它也是所有跟踪器中运行速度最快的。</p> 
</blockquote> 
<p></p> 
<p class="img-center"><img alt="" height="261" src="https://images2.imgbox.com/12/18/5jRYtKqL_o.png" width="451"></p> 
<blockquote> 
 <p> 表 5. MOT20 测试集上“私有检测器”协议下最先进方法的比较。最佳结果以粗体显示。 MOT20中的场景比MOT17中的场景拥挤得多。 ByteTrack 在 MOT20 排行榜上的所有跟踪器中排名第一，并且在所有指标上都大大优于第二名 SOTMOT。它也是所有跟踪器中运行速度最快的。</p> 
</blockquote> 
<p></p> 
<p class="img-center"><img alt="" height="175" src="https://images2.imgbox.com/54/b3/Ha7rPTmy_o.png" width="453"></p> 
<blockquote> 
 <p> 表 6. HiEve 测试集上“私有检测器”协议下最先进方法的比较。最佳结果以粗体显示。 HiEve 的事件比 MOT17 和 MOT20 更复杂。 ByteTrack 在 HiEve 排行榜上的所有跟踪器中排名第一，并且在所有指标上都大大超过了第二名的 CenterTrack。</p> 
</blockquote> 
<h3>5. 结论 </h3> 
<p>         我们提出了一种简单而有效的数据关联方法 BYTE 用于多对象跟踪。BYTE 可以很容易地应用于现有的跟踪器并实现一致的改进。我们还提出了一个强大的跟踪器 ByteTrack，它在 MOT17 测试集上以 30 FPS 实现了 80.3 MOTA、77.3 IDF1 和 63.1 HOTA，在排行榜上的所有跟踪器中排名第一。ByteTrack 因其准确的检测性能和关联低分检测框的帮助而对遮挡非常鲁棒。它还阐明了如何充分利用检测结果来增强多目标跟踪。我们希望 ByteTrack 的高精度、快速和简单性能够使其在实际应用中具有吸引力。</p> 
<p class="img-center"><img alt="" height="220" src="https://images2.imgbox.com/f9/31/A270LeO8_o.png" width="760"></p> 
<blockquote> 
 <p> 表 7. BDD100K 测试集上最先进方法的比较。最佳结果以粗体显示。 ByteTrack 在 BDD100K 排行榜上的所有跟踪器中排名第一，并且在大多数指标上都大大优于第二名 QDTrack。</p> 
</blockquote> 
<p><strong> A. 边界框注解</strong></p> 
<p><strong>        </strong>我们注意到 MOT17 [44] 需要边界框 [89] 覆盖整个身体，即使对象被遮挡或部分超出图像。但是，YOLOX 的默认实现会裁剪图像区域内的检测框。为了避免图像边界周围的错误检测结果，我们在数据预处理和标签分配方面修改了 YOLOX。在数据预处理和数据增强过程中，我们不会裁剪图像内的边界框。我们只删除数据增强后完全在图像之外的框。在 SimOTA 标签分配策略中，正样本需要围绕物体中心，而全身框的中心可能位于图像之外，因此我们将物体的中心裁剪在图像内部。</p> 
<p>        MOT20 [17]、HiEve [37] 和 BDD100K 将图像内的边界框注释剪辑在其中，因此我们只使用 YOLOX 的原始设置。</p> 
<p><strong>B. 轻模型的跟踪性能</strong></p> 
<p>        我们使用轻检测模型比较了 BYTE 和 DeepSORT [70]。我们使用具有不同主干的 YOLOX [24] 作为我们的检测器。所有模型都在 CrowdHuman 和 MOT17 的一半训练集上进行训练。多尺度训练时，输入图像大小为 1088×608，最短边范围为 384 到 832。结果如表8所示。我们可以看到，与 DeepSORT 相比，BYTE 在 MOTA 和 IDF1 上带来了稳定的改进，这表明 BYTE 对检测性能具有鲁棒性。值得注意的是，在使用 YOLOX-Nano 作为主干时，BYTE 带来的 MOTA 比 DeepSORT 高出 3 个点，这使得它在实际应用中更具吸引力。</p> 
<p class="img-center"><img alt="" height="178" src="https://images2.imgbox.com/35/fe/CCoDoT5r_o.png" width="438"></p> 
<blockquote> 
 <p> 表 8. BYTE 和 DeepSORT 在 MOT17 验证集上使用光检测模型的比较。</p> 
</blockquote> 
<p><strong>C. ByteTrack 的消融研究</strong></p> 
<p><strong>        速度与速度准确性。</strong>我们在推理过程中使用不同大小的输入图像来评估 ByteTrack 的速度和准确性。所有实验都使用相同的多尺度训练。结果如表9所示。推理期间的输入大小范围从 512 × 928 到 800 × 1440。检测器的运行时间从 17.9 ms 到 30.0 ms，关联时间在 4.0 ms 左右。ByteTrack 可以达到 75.0 MOTA 45.7 FPS 运行速度和 76.6 MOTA 29.6 FPS 运行速度，在实际应用中具有优势。</p> 
<p>        <strong>训练数据。</strong>我们使用不同的训练数据组合在 MOT17 的半验证集上评估 ByteTrack。结果如表10所示。仅使用 MOT17 的一半训练集时，性能达到 75.8 MOTA，已经优于大多数方法。这是因为我们使用了强大的增强功能，例如 Mosaic [8] 和 Mixup [81]。当进一步添加 CrowdHuman、Cityperson 和 ETHZ 进行训练时，我们可以达到 76.7 MOTA 和 79.7 IDF1。IDF1 的巨大改进源于 CrowdHuman 数据集可以增强检测器识别被遮挡的人，因此，使卡尔曼滤波器生成更平滑的预测并增强跟踪器的关联能力。对训练数据的实验表明，ByteTrack并不渴求数据。与需要超过 7 个数据源 [19、21、44、55、73、82、88] 才能实现高性能的先前方法 [33、34、65、85] 相比，这对于实际应用来说是一个很大的优势。</p> 
<p class="img-center"><img alt="" height="146" src="https://images2.imgbox.com/51/c1/RsdI5Yyn_o.png" width="421"></p> 
<blockquote> 
 <p> 表 9. MOT17 验证集上不同输入大小的比较。总运行时间是检测时间和关联时间的组合。最佳结果以粗体显示。</p> 
</blockquote> 
<p class="img-center"><img alt="" height="122" src="https://images2.imgbox.com/78/c7/Sy7GjHzW_o.png" width="436"></p> 
<blockquote> 
 <p> 表 10. MOT17 验证集上不同训练数据的比较。 “MOT17”是 MOT17 半训练集的缩写。 “CH”是 CrowdHuman 数据集的缩写。 “CE”是 Cityperson 和 ETHZ 数据集的缩写。最佳结果以粗体显示。</p> 
</blockquote> 
<p><strong>D.轨迹插值</strong></p> 
<p>        我们注意到在 MOT17 中有一些完全被遮挡的行人，其在地面实况注释中的可见比率为 0。由于几乎不可能通过视觉线索检测到它们，我们通过轨迹插值获得这些对象。</p> 
<p>        假设我们有一个 tracklet T，它的 tracklet box 由于从帧 t1 到 t2 的遮挡而丢失。T 在第 t1 帧的 tracklet box 是 <img alt="B_{t_{1}}" class="mathcode" src="https://images2.imgbox.com/fb/92/vxzUxMYt_o.png"> ∈ <img alt="\mathbb{R}^{4}" class="mathcode" src="https://images2.imgbox.com/48/35/HkrL6fz5_o.png">，其中包含边界框的左上角和右下角坐标。令 <img alt="B_{t_{2}}" class="mathcode" src="https://images2.imgbox.com/90/59/domrx5kz_o.png"> 表示 T 在第 t2 帧的 tracklet box。我们设置了一个超参数 σ，表示我们执行 tracklet 插值的最大间隔，这意味着当 t2 − t1 ≤ σ, 时执行 tracklet 插值。tracklet T 在帧 t 的插值框可以计算如下：</p> 
<p style="text-align:center;"><img alt="B_{t}=B_{t_{1}}+(B_{t_{2}}-B_{t_{1}})\frac{t-t_{1}}{t_{2}-t_{1}}," class="mathcode" src="https://images2.imgbox.com/0b/0b/ZMftbQ33_o.png">                                  (1)</p> 
<p>其中 <img alt="t_{1}&lt;t&lt;t_{2}" class="mathcode" src="https://images2.imgbox.com/be/ee/XtUg2F8k_o.png">。</p> 
<p>如表 11 所示，当 σ 为 20 时，tracklet 插值可以将 MOTA 从 76.6 提高到 78.3，IDF1 从 79.3 提高到 80.2。轨迹插值是一种有效的后处理方法来获得那些完全被遮挡的物体的框。我们在私有检测协议下的 MOT17 [44]、MOT20 [17] 和 HiEve [37] 的测试集中使用 tracklet 插值。</p> 
<p class="img-center"><img alt="" height="147" src="https://images2.imgbox.com/5a/68/i3pkXOn9_o.png" width="426"></p> 
<blockquote> 
 <p> 表 11. MOT17 验证集上不同插值区间的比较。最佳结果以粗体显示。</p> 
</blockquote> 
<p><strong>E. MOTChallenge 的公开检测结果</strong></p> 
<p>        我们在公共检测协议下在 MOT17 [44] 和 MOT20 [17] 的测试集上评估 ByteTrack。遵循 Tracktor [3] 和 CenterTrack [89] 中的公共检测过滤策略，我们仅在其与公共检测框的 IoU 大于 0.8 时才初始化新轨迹。我们不使用公共检测协议下的轨迹插值。如表 12 所示，ByteTrack 在 MOT17 上大大优于其他方法。例如，它在 MOTA 上比 SiamMOT 高出 1.5 分，在 IDF1 上高出 6.7 分。表 13 显示了 MOT20 上的结果。ByteTrack 的表现也大大优于现有结果。例如，它在 MOTA 上比 TMOH [57] 高 6.9 分，在 IDF1 上高出 9.0 分，在 HOTA 上高出 7.5 分，并将身份转换减少了四分之三。公共检测协议下的结果进一步表明了我们的关联方法 BYTE 的有效性。</p> 
<p class="img-center"><img alt="" height="275" src="https://images2.imgbox.com/36/2f/Jbk0zKsv_o.png" width="432"></p> 
<blockquote> 
 <p> 表 12. MOT17 测试集上“公共检测器”协议下最先进方法的比较。最佳结果以粗体显示。</p> 
</blockquote> 
<p class="img-center"><img alt="" height="178" src="https://images2.imgbox.com/2f/c0/AscIZaJM_o.png" width="433"></p> 
<blockquote> 
 <p> 表 13. MOT20 测试集上“公共检测器”协议下最先进方法的比较。最佳结果以粗体显示。</p> 
</blockquote> 
<p><strong>F. 可视化结果。</strong></p> 
<p>        我们在图 5 中展示了一些 ByteTrack 能够处理的困难案例的可视化结果。困难案例包括遮挡（即 MOT17-02、MOT1704、MOT17-05、MOT17-09、MOT17-13）、运动模糊（即 MOT17-10、MOT17-13）和小物体（即 MOT1713）。中间帧中带有红色三角形的行人检测分数较低，这是通过我们的关联方法 BYTE 获得的。低分框不仅减少了缺失检测的数量，而且对长期关联也起着重要作用。从所有这些困难的案例中我们可以看出，ByteTrack 没有带来任何身份转换，有效地保留了身份。</p> 
<p class="img-center"><img alt="" height="848" src="https://images2.imgbox.com/40/e8/1tVEoGaz_o.png" width="486"></p> 
<blockquote> 
 <p> 图 5. ByteTrack 的可视化结果。我们从 MOT17 的验证集中选择了 6 个序列，并展示了 ByteTrack 处理遮挡和运动模糊等困难情况的有效性。黄色三角形代表高分框，红色三角形代表低分框。相同的框颜色代表相同的身份。</p> 
</blockquote> 
<p></p> 
<p>References见原文，文章类型填翻译的话需要作者许可，未经许可，仅学习，不获利，侵删。</p> 
<p>原文链接：<a class="link-info" href="http://arxiv.org/abs/2110.06864" rel="nofollow" title="http://arxiv.org/abs/2110.06864">http://arxiv.org/abs/2110.06864</a> </p> 
<p>代码链接：<a class="link-info" href="https://github.com/ifzhang/ByteTrack" title="https://github.com/ifzhang/ByteTrack">https://github.com/ifzhang/ByteTrack</a></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3cbf6779e0114c847ef8c91b2be1d24a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">漏洞发现“小迪安全课堂笔记”系统，web，app，服务探针利用修复</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/107f288de37bcbc7f0d13c615f027aae/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">N: 无法安全地用该源进行更新，所以默认禁用该源。【树莓派 ubuntu 软件源问题】E: 无法下载 http://mirrors.tuna.tsinghua.edu.cn/ubuntu/dists/</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>