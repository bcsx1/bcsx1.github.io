<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>TricorNet: A Hybrid Temporal Convolutional  and Recurrent Network for Video Action Segmentation  翻译 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="TricorNet: A Hybrid Temporal Convolutional  and Recurrent Network for Video Action Segmentation  翻译" />
<meta property="og:description" content="本博文是对论文的翻译，如有不准确，请在评论中指出 TricorNet: A Hybrid Temporal Convolutional and Recurrent Network for Video Action Segmentation
一种用于视频动作分割的时间卷积和递归混合网络
关键词：hybrid temporal convolutional and recurrent network 混合时间卷积与递归网络
文章主要介绍了一种混合时间卷积与递归网络，它具有编码解码结构：编码器由一层时间卷积核组成，捕捉不同动作的局部运动变化；解码器是一种递归神经网络的层次结构，能够在编码阶段之后学习和记忆长期的动作依赖关系。其新能要优于现有网络。（论文发表时间在2017-5）
1介绍
目前大多数的动作分割方法[27，20，7]都使用卷积神经网络提取的特征。例如：Two-stream CNNS [19] or local 3D ConvNets [24]，
过去方法的优点：在编解码网络中，分别采用一维时间卷积核和反卷积核的层次结构，它们的模型在捕捉局部运动方面是有效的，并且在各种动作分割数据集中达到了最先进的性能。
过去方法的缺点：一个明显的缺点是它无法捕获视频中由于固定大小而产生的不同动作的长期依赖关系。 例如：典型的制作热狗的视频中，挤出番茄酱的动作通常发生在同时拿面包和香肠之后。
另外，一个扩展的时间卷积网络，类似于用于语音处理的WavNet[25]，也是在[10]中测试的。但表现更差，这进一步说明了视频和语音数据之间存在着差异。尽管它们都被表示为序列特征。
为了克服上述限制，我们提出了一种新的时间卷积和递归混合网络(TricorNet)，它既关注局部运动的变化，又关注长期的动作依赖关系，视频动作分割建模。TricorNet使用帧级特征作为编解码结构的输入。在我们的例子中，编码器是一个时间卷积网络，由一维卷积核组成，观察到卷积核善于编码局部运动变化；解码器是递归神经网络的分层结构，双向短时记忆网络(Bi-LSTMS)[6]，它能够在编码过程之后学习和记忆长期的动作依赖关系。我们的网络很简单，但是非常有效，可以处理不同时间的动作，并对不同动作之间的依赖关系进行建模。
2 Related Work 相关工作
对于动作分割，现有的许多作品都是以帧级特征作为输入，然后在整个视频序列上建立时态模型。
杨等人。[27]提出了一种关注lstm网络来模拟固定长度窗口中输入帧特征的依赖关系。
黄等人。[7]考虑无监督的行为标号问题。
辛格等人。[ 20 ]提出了一种细粒度动作检测任务的多流双向递归神经网络。
Lea等人。[10]提出了两种时间卷积网络用于动作分割和检测。
我们的模型设计受到了[20]和[10]的启发，并在实验中与它们进行了比较。
Lea等人。[11]引入spatial CNN 和 a spatiotemporal CNN；后者是一种端到端的方法，用来从帧中对整个序列进行建模。在这里，我们使用它们的spatial CNN的输出特性作为自己的TricorNet的输入，并与spatial CNN的结果进行了比较。
Richard等人。[18]使用一种统计语言模型，着重于对不同长度的视频片段进行定位和分类。
Kuehne等人。[9]提出了一种基于稠密轨迹特征的Hidden Markov模型的一种端到端的生成行为分割的方法。
另一个相关的领域是行动检测。
Peng 和 Schmid[17]提出了一个双流R-CNN探测行动。
杨等人[28]介绍了一个基于强化学习的行动检测框架。
李等人。[14]提出联合分类-回归递归神经网络，用于从三维骨骼数据中检测人体行为。
这些方法主要适用于单动作、短视频。
最近的工作[29]关注到行动检测和依赖 untrimmed （未修剪和不受限制）的YouTube视频。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/dedad3fdbe55a7fdc62fd9c57fef3f80/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-03-12T14:47:38+08:00" />
<meta property="article:modified_time" content="2018-03-12T14:47:38+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">TricorNet: A Hybrid Temporal Convolutional  and Recurrent Network for Video Action Segmentation  翻译</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>本博文是对论文的翻译，如有不准确，请在评论中指出</h2> 
<p><strong></strong></p> 
<p><strong>TricorNet: A Hybrid Temporal Convolutional  and Recurrent Network for Video Action Segmentation</strong></p> 
<p><strong>一种用于视频动作分割的时间卷积和递归混合网络</strong></p> 
<p><strong>关键词：</strong>hybrid temporal convolutional and recurrent network  <span style="font-family:'宋体';">混合时间卷积与递归网络</span></p> 
<p> </p> 
<p> <span style="font-family:'宋体';">文章主要介绍了一种混合时间卷积与递归网络，它具有编码解码结构：编码器由一层时间卷积核组成，捕捉不同动作的局部运动变化；解码器是一种递归神经网络的层次结构，能够在编码阶段之后学习和记忆长期的动作依赖关系。其新能要优于现有网络。</span><span style="font-family:'宋体';">（论文发表时间在</span>2017-5<span style="font-family:'宋体';">）</span></p> 
<p> </p> 
<p><strong>1<span style="font-family:'宋体';">介绍</span></strong></p> 
<p><span style="font-family:'宋体';">目前大多数的动作分割方法</span>[27<span style="font-family:'宋体';">，</span><span style="font-family:Calibri;">20</span><span style="font-family:'宋体';">，</span><span style="font-family:Calibri;">7]</span><span style="font-family:'宋体';">都使用卷积神经网络提取的特征。例如：</span><span style="font-family:Calibri;">Two-stream CNNS [19] or local 3D ConvNets [24]</span><span style="font-family:'宋体';">，</span></p> 
<p><strong>过去方法的优点：</strong>在编解码网络中，分别采用一维时间卷积核和反卷积核的层次结构，它们的模型在捕捉局部运动方面是有效的，并且在各种动作分割数据集中达到了最先进的性能。</p> 
<p><strong>过去方法的缺点：</strong><span style="font-family:'宋体';">一个明显的缺点是它无法捕获视频中由于固定大小而产生的不同动作的长期依赖关系。</span>  <span style="font-family:'宋体';">例如：典型的制作热狗的视频中，挤出番茄酱的动作通常发生在同时拿面包和香肠之后。</span></p> 
<p><span style="font-family:'宋体';">另外，一个扩展的时间卷积网络，类似于用于语音处理的</span>WavNet[25]<span style="font-family:'宋体';">，也是在</span><span style="font-family:Calibri;">[10]</span><span style="font-family:'宋体';">中测试的。但表现更差，这进一步说明了视频和语音数据之间存在着差异。尽管它们都被表示为序列特征。</span></p> 
<p>为了克服上述限制，<span style="color:rgb(255,0,0);"><span style="font-family:'宋体';">我们提出了一种新的时间卷积和递归混合网络</span>(TricorNet)<span style="font-family:'宋体';">，</span></span><span style="font-family:'宋体';">它既关注局部运动的变化，又关注长期的动作依赖关系，视频动作分割建模。</span>TricorNet<span style="font-family:'宋体';">使用帧级特征作为编解码结构的输入。在我们的例子中，编码器是一个时间卷积网络，由一维卷积核组成，观察到卷积核善于编码局部运动变化；解码器是递归神经网络的分层结构，双向短时记忆网络</span><span style="font-family:Calibri;">(Bi-LSTMS)[6]</span><span style="font-family:'宋体';">，它能够在编码过程之后学习和记忆长期的动作依赖关系。我们的网络很简单，但是非常有效，</span><span style="color:rgb(255,0,0);">可以处理不同时间的动作，并对不同动作之间的依赖关系进行建模。</span></p> 
<p> </p> 
<p><strong>2 Related Work  <span style="font-family:'宋体';">相关工作</span></strong></p> 
<p>对于动作分割，现有的许多作品都是以帧级特征作为输入，然后在整个视频序列上建立时态模型。</p> 
<p><span style="font-family:'宋体';">杨等人。</span>[27]<span style="font-family:'宋体';">提出了一种关注</span><span style="font-family:Calibri;">lstm</span><span style="font-family:'宋体';">网络来模拟固定长度窗口中输入帧特征的依赖关系。</span></p> 
<p><span style="font-family:'宋体';">黄等人。</span>[7]<span style="font-family:'宋体';">考虑无监督的行为标号问题。</span></p> 
<p><span style="font-family:'宋体';">辛格等人。</span>[ 20 ]<span style="font-family:'宋体';">提出了一种细粒度动作检测任务的多流双向递归神经网络。</span></p> 
<p>Lea<span style="font-family:'宋体';">等人。</span><span style="font-family:Calibri;">[10]</span><span style="font-family:'宋体';">提出了两种时间卷积网络用于动作分割和检测。</span></p> 
<p><span style="font-family:'宋体';">我们的模型设计受到了</span>[20]<span style="font-family:'宋体';">和</span><span style="font-family:Calibri;">[10]</span><span style="font-family:'宋体';">的启发，并在实验中与它们进行了比较。</span></p> 
<p> </p> 
<p>Lea<span style="font-family:'宋体';">等人。</span><span style="font-family:Calibri;">[11]</span><span style="font-family:'宋体';">引入</span><span style="font-family:Calibri;">spatial CNN </span><span style="font-family:'宋体';">和 </span><span style="font-family:Calibri;">a spatiotemporal CNN</span><span style="font-family:'宋体';">；后者是一种端到端的方法，用来从帧中对整个序列进行建模。在这里，我们使用它们的</span><span style="font-family:Calibri;">spatial CNN</span><span style="font-family:'宋体';">的输出特性作为自己的</span><span style="font-family:Calibri;">TricorNet</span><span style="font-family:'宋体';">的输入，并与</span><span style="font-family:Calibri;">spatial CNN</span><span style="font-family:'宋体';">的结果进行了比较。</span></p> 
<p>Richard<span style="font-family:'宋体';">等人。</span><span style="font-family:Calibri;">[18]</span><span style="font-family:'宋体';">使用一种统计语言模型，着重于对不同长度的视频片段进行定位和分类。</span></p> 
<p>Kuehne<span style="font-family:'宋体';">等人。</span><span style="font-family:Calibri;">[9]</span><span style="font-family:'宋体';">提出了一种基于稠密轨迹特征的</span><span style="font-family:Calibri;">Hidden Markov</span><span style="font-family:'宋体';">模型的一种端到端的生成行为分割的方法。</span></p> 
<p> </p> 
<p>另一个相关的领域是行动检测。</p> 
<p>Peng <span style="font-family:'宋体';">和 </span><span style="font-family:Calibri;">Schmid[17]</span><span style="font-family:'宋体';">提出了一个双流</span><span style="font-family:Calibri;">R-CNN</span><span style="font-family:'宋体';">探测行动。</span></p> 
<p><span style="font-family:'宋体';">杨等人</span>[28]<span style="font-family:'宋体';">介绍了一个基于强化学习的行动检测框架。</span></p> 
<p><span style="font-family:'宋体';">李等人。</span>[14]<span style="font-family:'宋体';">提出联合分类</span><span style="font-family:Calibri;">-</span><span style="font-family:'宋体';">回归递归神经网络，用于从三维骨骼数据中检测人体行为。</span></p> 
<p>这些方法主要适用于单动作、短视频。</p> 
<p><span style="font-family:'宋体';">最近的工作</span>[29]<span style="font-family:'宋体';">关注到行动检测和依赖 </span><span style="font-family:Calibri;">untrimmed </span>（未修剪和不受限制）<span style="font-family:'宋体';">的</span>YouTube<span style="font-family:'宋体';">视频。</span></p> 
<p>但本课题不在本文研究范围之内。</p> 
<p> </p> 
<p><strong>3<span style="font-family:'宋体';">模型</span></strong></p> 
<p>TricorNet<span style="font-family:'宋体';">的输入是一组帧级视频特征，例如，</span><span style="font-family:Calibri;">cnn</span><span style="font-family:'宋体';">对给定视频的每一帧的输出。</span></p> 
<p> <img src="https://images2.imgbox.com/c0/5f/FAqfdnIV_o.png" alt=""></p> 
<p><span style="color:rgb(127,127,127);"><span style="font-family:'宋体';">图</span>2<span style="font-family:'宋体';">：这是</span><span style="font-family:Calibri;">TricorNet</span><span style="font-family:'宋体';">的总框架。编码器网络由一层时间卷积核组成，能够有效地捕捉局部运动变化。解码器网络由一层依靠长期动作的</span><span style="font-family:Calibri;">Bi-LSTMs</span><span style="font-family:'宋体';">组成。</span></span></p> 
<p> </p> 
<p><strong>3.1<span style="font-family:'宋体';">时间卷积和递归网络</span></strong></p> 
<p><span style="font-family:'宋体';"></span></p> 
<p><span style="font-family:'宋体';"></span></p> 
<p><span style="font-family:'宋体';">图</span>2<span style="font-family:'宋体';">大致描述了</span><span style="font-family:Calibri;">TricorNet</span><span style="font-family:'宋体';">的一般框架。该</span><span style="font-family:Calibri;">TricorNet</span><span style="font-family:'宋体';">具有一组编码解码器结构。编码器和解码器网络都由</span><span style="font-family:Calibri;">K</span><span style="font-family:'宋体';">层组成。我们将编码层定义为<img src="https://images2.imgbox.com/ed/64/M6DB8pWr_o.png" alt=""></span>，我们将解码层定义为<img src="https://images2.imgbox.com/6a/df/LMyFmPmJ_o.png" alt="">，i=1,2,......k<span style="font-family:'宋体';">，在编码器和解码器之间的是中间层<img src="https://images2.imgbox.com/16/38/xv10ouEb_o.png" alt=""></span><span style="font-family:'宋体';">。这里，</span>K<span style="font-family:'宋体';">是一个超参数，它可以根据数据集中的视频数据的大小和外观来转换。一个大的</span><span style="font-family:Calibri;">K</span><span style="font-family:'宋体';">表示网络很深，通常需要更多的数据来训练。从经验角度来看，我们对所有的实验中设置</span> K = 2<span style="font-family:'宋体';">。</span></p> 
<span style="font-family:'宋体';"></span> 
<span style="font-family:'宋体';"></span> 
<p> </p> 
<p>在解码层，每个<img src="https://images2.imgbox.com/4f/bc/d0eLmnWM_o.png" alt="">层都是<u><span style="font-family:'宋体';">由</span>(<span style="font-family:'宋体';">一维</span><span style="font-family:Calibri;">)</span><span style="font-family:'宋体';">时间卷积</span><span style="font-family:'宋体';">、非线性激活函数</span>E = f(<span style="font-family:'宋体';">·</span><span style="font-family:Calibri;">)</span><span style="font-family:'宋体';">、最大池化跨越时间组成</span></u>。在编码层L<sub>E</sub><sup><span style="font-family:'宋体';">（</span>i<span style="font-family:'宋体';">）</span></sup><span style="font-family:'宋体';">上使用</span>F<sub>i</sub>指定的一系列卷积滤波器，我们定义了卷积滤波器的集合<img src="https://images2.imgbox.com/d1/52/VfzxtxaO_o.png" alt="">，并给出了相应的偏置向量<img src="https://images2.imgbox.com/94/dc/87jAwN1O_o.png" alt="">。<span style="font-family:'宋体';">给定前一编码层在</span>pooling E<sup>(i-1)</sup><span style="font-family:'宋体';">后的输出，我们计算当前层<img src="https://images2.imgbox.com/5d/6f/jEOCpQhG_o.png" alt=""></span> 的激活函数：</p> 
<p> <img src="https://images2.imgbox.com/c6/24/ZkCjQnzG_o.png" alt=""></p> 
<p align="center"> </p> 
<p><span style="font-family:'宋体';">其中</span>*<span style="font-family:'宋体';">表示一维卷积算子。注意，<img src="https://images2.imgbox.com/67/76/bwWk1TDC_o.png" alt=""></span>是输入帧级特征向量的集合。卷积核的长度是另一个超参数。长度越长，感受野越大，但也会减少相邻时间步长之间的差别。我们在文章的第四部分给出了最佳做法。</p> 
<p> </p> 
<p><span style="font-family:'宋体';">这里，中间层</span>L<sub>mid</sub><span style="font-family:'宋体';">是池化之后最后一个编码层</span>E<sup>(K)</sup><span style="font-family:'宋体';">的输出，并将其作为译码器网络的输入端。解码部分的结构与编码部分相比是一个保留的层次结构，也是由</span>K<span style="font-family:'宋体';">个层组成。我们使用</span><span style="font-family:Calibri;">(Bi-LSTM)[6]</span><span style="font-family:'宋体';">单元来模拟远程动作依赖关系，并使用上采样来解码帧级标签。因此，</span><u>每一层的<img src="https://images2.imgbox.com/21/2f/IYxYrP1f_o.png" alt=""><span style="font-family:'宋体';">在解码器网络是结合了上采样和</span>Bi- LSTM</u>。</p> 
<p> </p> 
<p><span style="font-family:'宋体';">通常，递归神经网络使用隐藏状态表示</span>h=(h1<span style="font-family:'宋体';">，</span><span style="font-family:Calibri;">h2</span><span style="font-family:'宋体';">，…，</span><span style="font-family:Calibri;">ht)</span><span style="font-family:'宋体';">将输入向量</span><span style="font-family:Calibri;">x=(x1</span><span style="font-family:'宋体';">，</span><span style="font-family:Calibri;">x2</span><span style="font-family:'宋体';">，…，</span><span style="font-family:Calibri;">xt)</span><span style="font-family:'宋体';">映射到输出序列</span><span style="font-family:Calibri;">y=(y1</span><span style="font-family:'宋体';">，</span><span style="font-family:Calibri;">y2</span><span style="font-family:'宋体';">，…，</span><span style="font-family:Calibri;">yt)</span><span style="font-family:'宋体';">。在</span><span style="font-family:Calibri;">LSTM</span><span style="font-family:'宋体';">单元的条件中，它通过以下公式更新其隐藏状态：</span></p> 
<p align="center"><img src="https://images2.imgbox.com/67/ea/GlIMZvDo_o.png" alt=""> </p> 
<p> </p> 
<p><span style="font-family:'宋体';">其中</span>σ<span style="font-family:Calibri;">(</span><span style="font-family:'宋体';">·</span><span style="font-family:Calibri;">)</span><span style="font-family:'宋体';">是</span><span style="font-family:Calibri;">S</span><span style="font-family:'宋体';">型激活函数，</span><span style="font-family:Calibri;">tanh(</span><span style="font-family:'宋体';">·</span><span style="font-family:Calibri;">)</span><span style="font-family:'宋体';">是双曲正切激活函数，</span><span style="font-family:Calibri;">i</span><sub>t</sub><span style="font-family:'宋体';">、</span>f<sub>t</sub><span style="font-family:'宋体';">、</span>o<sub>t</sub><span style="font-family:'宋体';">和</span>c<sub>t</sub><span style="font-family:'宋体';">是各自地输入门、遗忘门、输出门和记忆单元激活向量。这里，</span>W<sub>s</sub><span style="font-family:'宋体';">和</span>b<sub>s</sub>是权重和偏差。</p> 
<p>Bi-LSTM<span style="font-family:'宋体';">层包含两个</span><span style="font-family:Calibri;">LSTM</span><span style="font-family:'宋体';">：一个在时间上前进，一个倒退。输出是两个方向的结果的连合。</span></p> 
<p><span style="font-family:'宋体';">在</span>TricorNet<span style="font-family:'宋体';">中，我们使用隐藏状态</span><span style="font-family:Calibri;">H</span><sup>(I)</sup>的更新序列作为每个解码层<img src="https://images2.imgbox.com/5e/7e/y5336liR_o.png" alt=""><span style="font-family:'宋体';">的输出。我们使用</span>H<sub>i</sub><span style="font-family:'宋体';">来指定单个</span>LSTM<span style="font-family:'宋体';">层中隐藏状态的数量。因此，对于<img src="https://images2.imgbox.com/db/36/y9de7qbx_o.png" alt=""></span><span style="font-family:'宋体';">层，每一时间步骤的输出维数为</span>2H<sub>i</sub><span style="font-family:'宋体';">，作为正向和反向</span>LSTM<span style="font-family:'宋体';">的级联。整个解码部分的输出将是一个矩阵<img src="https://images2.imgbox.com/6b/f8/77FBngjY_o.png" alt=""></span><span style="font-family:'宋体';">，这意味着在每个时间步骤</span>t=1<span style="font-family:'宋体';">，</span><span style="font-family:Calibri;">2</span><span style="font-family:'宋体';">，…，</span><span style="font-family:Calibri;">T</span><span style="font-family:'宋体';">，我们有一个</span><span style="font-family:Calibri;">2</span>* h<sub>k</sub>维向量D<sub>t</sub>（是最后一个解码层<img src="https://images2.imgbox.com/8d/27/cu7cNdcp_o.png" alt="">的输出）。</p> 
<p> </p> 
<p><span style="font-family:'宋体';">最后，我们有一个跨时间的</span>Softmax<span style="font-family:'宋体';">层来计算帧的标记在每个时间步骤</span><span style="font-family:Calibri;">t</span><span style="font-family:'宋体';">中接受</span><span style="font-family:Calibri;">c</span><span style="font-family:'宋体';">操作类之一的概率，它是由：</span></p> 
<p align="center"><img src="https://images2.imgbox.com/81/d1/n3bZyTYE_o.png" alt=""> </p> 
<p>其中<img src="https://images2.imgbox.com/b6/0e/UEptZKjz_o.png" alt=""><span style="font-family:'宋体';">是</span>c<span style="font-family:'宋体';">类在时间步长</span><span style="font-family:Calibri;">t</span><span style="font-family:'宋体';">上的输出概率向量，</span><span style="font-family:Calibri;">D</span><sub>t</sub><span style="font-family:'宋体';">是时间步长</span>t<span style="font-family:'宋体';">的译码器输出，</span><span style="font-family:Calibri;">W</span><sub>d</sub><span style="font-family:'宋体';">是权，</span>B<sub>d</sub>是偏置项。</p> 
<p align="justify"> </p> 
<p> </p> 
<p> </p> 
<p><strong>3.2<span style="font-family:'宋体';">模型的变化</span></strong></p> 
<p><span style="font-family:'宋体';">为了找到时间卷积层和</span>Bi-LSTM<span style="font-family:'宋体';">层之间最好的组合结构，本文测试了三种不同的模型设计。在第四节中给出了不同模型的测试。</span></p> 
<p> <img src="https://images2.imgbox.com/03/8c/4Lln5bWl_o.png" alt=""></p> 
<p><strong>TricorNet<span style="font-family:'宋体';">：</span></strong>这个模型的想法是通过不同的时间卷积层来进行编码，通过不同的Bi-LSTM<span style="font-family:'宋体';">来进行解码，来学习不同长时间活动依赖关系的水平。</span><strong></strong></p> 
<p><strong>TricorNet (high)<span style="font-family:'宋体';">：</span></strong>TriorNet(high)<span style="font-family:'宋体';">只在中间层<img src="https://images2.imgbox.com/b5/a8/xc2szhkp_o.png" alt=""></span><span style="font-family:'宋体';">部署</span>Bi-LSTM<span style="font-family:'宋体';">单元，后者是编码器和解码器之间的一层。编码层<img src="https://images2.imgbox.com/0e/c0/fXJkF1MW_o.png" alt=""></span>和解码层<img src="https://images2.imgbox.com/a1/ed/Ev3uqcuk_o.png" alt=""><span style="font-family:'宋体';">都使用了时间卷积核。灵感来源于使用</span>Bi-LSTM<span style="font-family:'宋体';">在抽象级别上对序列依赖进行建模，其中信息是高度压缩的，同时保持本地的编码和解码。当动作标签很粗糙时和预期的表现一样良好。</span><strong></strong></p> 
<p><strong>TricorNet (low)<span style="font-family:'宋体';">：</span></strong>TriorNet(low)<span style="font-family:'宋体';">只在<img src="https://images2.imgbox.com/1e/61/9IUZOb6A_o.png" alt=""></span><span style="font-family:'宋体';">层部署</span>Bi-LSTM<span style="font-family:'宋体';">单元，这是解码器的最后一层。它对编码层<img src="https://images2.imgbox.com/a5/de/SKQ0KRd6_o.png" alt=""></span>和解码层<img src="https://images2.imgbox.com/c9/8f/5s9peDAq_o.png" alt=""><span style="font-family:'宋体';">使用时间卷积核，其中在解码器中</span>i&lt;k<span style="font-family:'宋体';">。出发点是是使用 </span><span style="font-family:Calibri;">Bi-LSTM</span><span style="font-family:'宋体';">对唯一细节解码。对于操作标签是细粒度的情况，最好集中在低级别学习依赖项，在低级别上，信息压缩较少。</span></p> 
<p> </p> 
<p>3.3<span style="font-family:'宋体';">实现细节</span></p> 
<p><span style="font-family:'宋体';">在本工作中，所有</span>TricorNets<span style="font-family:'宋体';">的一些超参数是固定的，并在所有实验中使用。在编码部分，使用的</span><span style="font-family:Calibri;">max pooling</span><span style="font-family:'宋体';">宽度为</span><span style="font-family:Calibri;">2</span><span style="font-family:'宋体';">。每个时间卷积层<img src="https://images2.imgbox.com/92/b1/hRK3DZxK_o.png" alt=""></span><span style="font-family:'宋体';">具有</span>32+32i<span style="font-family:'宋体';">个滤波器。在解码部分，上采样是由每进入两次的简单重复实现。</span><span style="font-family:Calibri;">2 H</span><sub>i</sub><span style="font-family:'宋体';">给出了每个</span>LSTM<span style="font-family:'宋体';">层<img src="https://images2.imgbox.com/06/34/1PlHKr2q_o.png" alt=""></span><span style="font-family:'宋体';">的潜伏状态。我们使用归一化校正线性单元</span>[10]<span style="font-family:'宋体';">作为所有时间卷积层的激活函数，其定义为：</span></p> 
<p align="center"><img src="https://images2.imgbox.com/e9/a4/MGBGqqes_o.png" alt=""> </p> 
<p align="justify"><img src="https://images2.imgbox.com/7a/7d/dcNirUhc_o.png" alt="">是层中的最大激活值，<img src="https://images2.imgbox.com/71/82/2kzYhlh5_o.png" alt="">。</p> 
<p align="justify"><span style="font-family:'宋体';">在我们的实验中，模型仅使用目标数据集的训练集从头开始进行培训。使用随机梯度下降和</span>ADAM[8]<span style="font-family:'宋体';">阶跃更新的分类交叉熵损失来学习权值和参数。我们还在卷积层之间和</span><span style="font-family:Calibri;">Bi-LSTM</span><span style="font-family:'宋体';">层之间添加空间</span><span style="font-family:Calibri;">dropout</span><span style="font-family:'宋体';">算法。这些模型是用</span><span style="font-family:Calibri;">Keras[1]</span><span style="font-family:'宋体';">和</span><span style="font-family:Calibri;">TensorFlow</span><span style="font-family:'宋体';">实现的。</span></p> 
<p> </p> 
<p>实验和评估部分略</p> 
<p> </p> 
<p>References</p> 
<p>[1]  F. Chollet. keras. https://github.com/fchollet/keras, 2015.</p> 
<p>[2] J. Cross and L. Huang. Incremental parsing with minimal features using bi-directional lstm. In Association</p> 
<p>for Computational Linguistics, 2016.</p> 
<p>[3] P. Das, C. Xu, R. Doell, and J. J. Corso. A thousand frames in just a few words: lingual description of</p> 
<p>videos through latent topics and sparse object stitching. In IEEE Conference on Computer Vision and</p> 
<p>Pattern Recognition, 2013.</p> 
<p>[4] A. Fathi, X. Ren, and J. M. Rehg. Learning to recognize objects in egocentric activities. In Computer</p> 
<p>Vision and Pattern Recognition (CVPR), 2011 IEEE Conference On, pages 3281<span style="font-family:'宋体';">–</span><span style="font-family:Calibri;">3288. IEEE, 2011.</span></p> 
<p>[5] Y. Gao, S. S. Vedula, C. E. Reiley, N. Ahmidi, B. Varadarajan, H. C. Lin, L. Tao, L. Zappella, B. Béjar,</p> 
<p>D. D. Yuh, et al. Jhu-isi gesture and skill assessment working set (jigsaws): A surgical activity dataset for</p> 
<p>human motion modeling. In MICCAI Workshop: M2CAI, volume 3, 2014.</p> 
<p>[6] A. Graves, S. Fernández, and J. Schmidhuber. Bidirectional lstm networks for improved phoneme</p> 
<p>classification and recognition. Artificial Neural Networks: Formal Models and Their Applications<span style="font-family:'宋体';">–</span><span style="font-family:Calibri;">ICANN</span></p> 
<p>2005, pages 753<span style="font-family:'宋体';">–</span><span style="font-family:Calibri;">753, 2005.</span></p> 
<p>[7]  D.-A. Huang, L. Fei-Fei, and J. C. Niebles. Connectionist temporal modeling for weakly supervised action</p> 
<p>labeling. In European Conference on Computer Vision, 2016.</p> 
<p>[8] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p> 
<p>[9] H. Kuehne, J. Gall, and T. Serre. An end-to-end generative framework for video segmentation and</p> 
<p>recognition. In Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on, pages 1<span style="font-family:'宋体';">–</span><span style="font-family:Calibri;">8.</span></p> 
<p>IEEE, 2016.</p> 
<p>[10] C. Lea, M. D. Flynn, R. Vidal, A. Reiter, and G. D. Hager. Temporal convolutional networks for action</p> 
<p>segmentation and detection. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.</p> 
<p>[11] C. Lea, A. Reiter, R. Vidal, and G. D. Hager. Segmental spatiotemporal cnns for fine-grained action</p> 
<p>segmentation. In European Conference on Computer Vision, pages 36<span style="font-family:'宋体';">–</span><span style="font-family:Calibri;">52. Springer, 2016.</span></p> 
<p>[12] C. Lea, R. Vidal, and G. D. Hager. Learning convolutional action primitives for fine-grained action</p> 
<p>recognition. In Robotics and Automation (ICRA), 2016 IEEE International Conference on, pages 1642<span style="font-family:'宋体';">–</span></p> 
<p>1649. IEEE, 2016.</p> 
<p>[13] C. Lea, R. Vidal, A. Reiter, and G. D. Hager. Temporal convolutional networks: A unified approach to</p> 
<p>action segmentation. In Computer Vision<span style="font-family:'宋体';">–</span><span style="font-family:Calibri;">ECCV 2016 Workshops, pages 47</span><span style="font-family:'宋体';">–</span><span style="font-family:Calibri;">54. Springer, 2016.</span></p> 
<p>[14]  Y. Li, C. Lan, J. Xing, W. Zeng, C. Yuan, and J. Liu. Online human action detection using joint</p> 
<p>classification-regression recurrent neural networks. In European Conference on Computer Vision, 2016.</p> 
<p>[15] P. Mettes, J. C. van Gemert, and C. G. Snoek. Spot on: Action localization from pointly-supervised</p> 
<p>proposals. In European Conference on Computer Vision, 2016.</p> 
<p>[16]  H. Noh, S. Hong, and B. Han. Learning deconvolution network for semantic segmentation. In IEEE</p> 
<p>International Conference on Computer Vision, 2015.</p> 
<p>[17] X. Peng and C. Schmid. Multi-region two-stream r-cnn for action detection. In European Conference on</p> 
<p>Computer Vision, 2016.</p> 
<p>[18] A. Richard and J. Gall. Temporal action detection using a statistical language model. In Proceedings of the</p> 
<p>IEEE Conference on Computer Vision and Pattern Recognition, pages 3131<span style="font-family:'宋体';">–</span><span style="font-family:Calibri;">3140, 2016.</span></p> 
<p>[19] K. Simonyan and A. Zisserman. Two-stream convolutional networks for action recognition in videos. In</p> 
<p>Advances in Neural Information Processing Systems, 2014.</p> 
<p>[20] B. Singh, T. K. Marks, M. Jones, O. Tuzel, and M. Shao. A multi-stream bi-directional recurrent neural</p> 
<p>network for fine-grained action detection. In IEEE Conference on Computer Vision and Pattern Recognition,</p> 
<p>2016.</p> 
<p>[21] S. Singh, C. Arora, and C. Jawahar. First person action recognition using deep learned descriptors. In</p> 
<p>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2620<span style="font-family:'宋体';">–</span><span style="font-family:Calibri;">2628,</span></p> 
<p>2016.</p> 
<p>[22] S. Stein and S. J. McKenna. Combining embedded accelerometers with computer vision for recognizing</p> 
<p>food preparation activities. In Proceedings of the 2013 ACM international joint conference on Pervasive</p> 
<p>and ubiquitous computing, pages 729<span style="font-family:'宋体';">–</span><span style="font-family:Calibri;">738. ACM, 2013.</span></p> 
<p>[23]  L. Tao, L. Zappella, G. D. Hager, and R. Vidal. Surgical gesture segmentation and recognition. In</p> 
<p>International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 339<span style="font-family:'宋体';">–</span></p> 
<p>346. Springer, 2013.</p> 
<p>[24]  D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri. Learning spatiotemporal features with 3d</p> 
<p>convolutional networks. In IEEE International Conference on Computer Vision, 2015.</p> 
<p>9</p> 
<p>[25]  A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior,</p> 
<p>and K. Kavukcuoglu. Wavenet: A generative model for raw audio. Technical report, arXiv:1609.03499,</p> 
<p>2016.</p> 
<p>[26] L. Wang, Y. Qiao, and X. Tang. Action recognition with trajectory-pooled deep-convolutional descriptors.</p> 
<p>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4305<span style="font-family:'宋体';">–</span><span style="font-family:Calibri;">4314,</span></p> 
<p>2015.</p> 
<p>[27] S. Yeung, O. Russakovsky, N. Jin, M. Andriluka, G. Mori, and L. Fei-Fei. Every moment counts: Dense</p> 
<p>detailed labeling of actions in complex videos. Technical report, arXiv:1507.05738, 2015.</p> 
<p>[28]  S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei. End-to-end learning of action detection from frame</p> 
<p>glimpses in videos. In IEEE Conference on Computer Vision and Pattern Recognition, 2016.</p> 
<p>[29]  L. Zhou, C. Xu, and J. J. Corso. Procnets: Learning to segment procedures in untrimmed and unconstrained</p> 
<p>videos. Technical report, arXiv:1703.09788, 2017</p> 
<p></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c62244849457d26e82c04d2e71309e99/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">vue的多标签页实现</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0cc4a06a40f2d26bf98136df71bdd854/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">《Sequential Short-Text Classification with Neural Networks》读书笔记</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>