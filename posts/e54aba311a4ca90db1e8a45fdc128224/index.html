<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>循环神经网络（RNN） - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="循环神经网络（RNN）" />
<meta property="og:description" content="1.NN &amp; RNN 在神经网络从原理到实现一文中已经比较详细地介绍了神经网络，下面用一张图直观地比较NN与RNN地不同。从图1中可以看出，RNN比NN多了指向自己的环，即图1中的7,8,9,10节点及其连线。图2显示RNN展开的网络结构。
在传统的神经网络中，我们假设所有的输入（包括输出）之间是相互独立的。对于很多任务来说，这是一个非常糟糕的假设。如果你想预测一个序列中的下一个词，你最好能知道哪些词在它前面。RNN之所以循环的，是因为它针对系列中的每一个元素都执行相同的操作，每一个操作都依赖于之前的计算结果。换一种方式思考，可以认为RNN记忆了到当前为止已经计算过的信息。理论上，RNN可以利用任意长的序列信息，但实际中只能回顾之前的几步。
图1 图2 2.demo Character Language Model，通过预测下一个字符，从而产生整篇文章。代码：min-char-rnn.py
3.前向传播 图3 在图3中 4.后向传播 一共有T个时刻，参数求导如下：
对t时刻求导，需要用到前面k=1到t的信息：
[k,t]之间的求导，可以转换为[k&#43;1,t]对前一时刻求导的乘积：
其中,对前一时刻进行求导：
所以，最终参数更新公式如下：
4.简单应用 from __future__ import print_function from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense, Activation from keras.layers import SimpleRNN from keras.initializations import normal, identity from keras.optimizers import RMSprop from keras.utils import np_utils batch_size = 32 nb_classes = 10 nb_epochs = 200 hidden_units = 100 learning_rate = 1e-6 clip_norm = 1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/e54aba311a4ca90db1e8a45fdc128224/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2016-12-05T17:03:49+08:00" />
<meta property="article:modified_time" content="2016-12-05T17:03:49+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">循环神经网络（RNN）</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3 id="1nn-rnn"><strong><font color="#0099ff" size="5" face="黑体">1.NN &amp; RNN</font></strong></h3> 
<p><font size="3">在<a href="http://blog.csdn.net/a819825294/article/details/53393837">神经网络从原理到实现</a>一文中已经比较详细地介绍了神经网络，下面用一张图直观地比较NN与RNN地不同。从图1中可以看出，RNN比NN多了指向自己的环，即图1中的7,8,9,10节点及其连线。图2显示RNN展开的网络结构。</font></p> 
<p><font size="3">在传统的神经网络中，我们假设所有的输入（包括输出）之间是相互独立的。对于很多任务来说，这是一个非常糟糕的假设。如果你想预测一个序列中的下一个词，你最好能知道哪些词在它前面。RNN之所以循环的，是因为它针对系列中的每一个元素都执行相同的操作，每一个操作都依赖于之前的计算结果。换一种方式思考，可以认为RNN记忆了到当前为止已经计算过的信息。理论上，RNN可以利用任意长的序列信息，但实际中只能回顾之前的几步。</font></p> 
<p></p> 
<center> 
 <img src="https://images2.imgbox.com/d3/66/w4XWuC1x_o.png" alt="" title=""> 
 <br> 
 <center> 
  <font size="4">图1</font> 
 </center> 
</center> 
<p></p> 
<p></p> 
<center> 
 <img src="https://images2.imgbox.com/fd/c5/lgHBFHWP_o.png" alt="" title=""> 
 <br> 
 <center> 
  <font size="4">图2</font> 
 </center> 
</center> 
<p></p> 
<h3 id="2demo"><strong><font color="#0099ff" size="5" face="黑体">2.demo</font></strong></h3> 
<p><font size="4" color="009900">Character Language Model，通过预测下一个字符，从而产生整篇文章。代码：<a href="https://gist.github.com/karpathy/d4dee566867f8291f086" rel="nofollow">min-char-rnn.py</a></font></p> 
<p></p> 
<center> 
 <img src="https://images2.imgbox.com/91/66/rVq7Nkwf_o.png" alt="" title=""> 
</center> 
<p></p> 
<h3 id="3前向传播"><strong><font color="#0099ff" size="5" face="黑体">3.前向传播</font></strong></h3> 
<p></p> 
<center> 
 <img src="https://images2.imgbox.com/44/da/XBbhTU2u_o.png" alt="" title=""> 
 <br> 
 <center> 
  <font size="4">图3</font> 
 </center> 
</center> 
<p></p> 
<p><font size="3">在图3中 <br> <img src="https://images2.imgbox.com/1d/9d/yTvPwmXz_o.png" alt="" title=""> <br> <img src="https://images2.imgbox.com/db/d5/eoo3lrfq_o.png" alt="" title=""></font></p> 
<h3 id="4后向传播"><strong><font color="#0099ff" size="5" face="黑体">4.后向传播</font></strong></h3> 
<p><font size="3">一共有T个时刻，参数求导如下：</font></p> 
<p><img src="https://images2.imgbox.com/fa/a4/6KDNXz3x_o.png" alt="" title=""></p> 
<p><font size="3">对t时刻求导，需要用到前面k=1到t的信息：</font></p> 
<p><img src="https://images2.imgbox.com/01/ec/YuNaFyZA_o.png" alt="" title=""></p> 
<p><font size="3">[k,t]之间的求导，可以转换为[k+1,t]对前一时刻求导的乘积：</font></p> 
<p><img src="https://images2.imgbox.com/2c/1e/xSbGISXY_o.png" alt="" title=""></p> 
<p><font size="3">其中,对前一时刻进行求导：</font></p> 
<p><img src="https://images2.imgbox.com/62/b6/KJjXifaO_o.png" alt="" title=""></p> 
<p><font size="3">所以，最终参数更新公式如下：</font></p> 
<p><img src="https://images2.imgbox.com/27/2d/AsDMOHUl_o.png" alt="" title=""></p> 
<h3 id="4简单应用"><strong><font color="#0099ff" size="5" face="黑体">4.简单应用</font></strong></h3> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-keyword">from</span> __future__ <span class="hljs-keyword">import</span> print_function

<span class="hljs-keyword">from</span> keras.datasets <span class="hljs-keyword">import</span> mnist
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential
<span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Dense, Activation
<span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> SimpleRNN
<span class="hljs-keyword">from</span> keras.initializations <span class="hljs-keyword">import</span> normal, identity
<span class="hljs-keyword">from</span> keras.optimizers <span class="hljs-keyword">import</span> RMSprop
<span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> np_utils

batch_size = <span class="hljs-number">32</span>
nb_classes = <span class="hljs-number">10</span>
nb_epochs = <span class="hljs-number">200</span>
hidden_units = <span class="hljs-number">100</span>

learning_rate = <span class="hljs-number">1e-6</span>
clip_norm = <span class="hljs-number">1.0</span>

<span class="hljs-comment"># the data, shuffled and split between train and test sets</span>
(X_train, y_train), (X_test, y_test) = mnist.load_data()

X_train = X_train.reshape(X_train.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)
X_test = X_test.reshape(X_test.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)
X_train = X_train.astype(<span class="hljs-string">'float32'</span>)
X_test = X_test.astype(<span class="hljs-string">'float32'</span>)
X_train /= <span class="hljs-number">255</span>
X_test /= <span class="hljs-number">255</span>
print(<span class="hljs-string">'X_train shape:'</span>, X_train.shape)
print(X_train.shape[<span class="hljs-number">0</span>], <span class="hljs-string">'train samples'</span>)
print(X_test.shape[<span class="hljs-number">0</span>], <span class="hljs-string">'test samples'</span>)

<span class="hljs-comment"># convert class vectors to binary class matrices</span>
Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)

print(<span class="hljs-string">'Evaluate IRNN...'</span>)
model = Sequential()
model.add(SimpleRNN(output_dim=hidden_units,
                    init=<span class="hljs-keyword">lambda</span> shape, name: normal(shape, scale=<span class="hljs-number">0.001</span>, name=name),
                    inner_init=<span class="hljs-keyword">lambda</span> shape, name: identity(shape, scale=<span class="hljs-number">1.0</span>, name=name),
                    activation=<span class="hljs-string">'relu'</span>,
                    input_shape=X_train.shape[<span class="hljs-number">1</span>:]))
model.add(Dense(nb_classes))
model.add(Activation(<span class="hljs-string">'softmax'</span>))
rmsprop = RMSprop(lr=learning_rate)
model.compile(loss=<span class="hljs-string">'categorical_crossentropy'</span>,
              optimizer=rmsprop,
              metrics=[<span class="hljs-string">'accuracy'</span>])

model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epochs,
          verbose=<span class="hljs-number">1</span>, validation_data=(X_test, Y_test))

scores = model.evaluate(X_test, Y_test, verbose=<span class="hljs-number">0</span>)
print(<span class="hljs-string">'IRNN test score:'</span>, scores[<span class="hljs-number">0</span>])
print(<span class="hljs-string">'IRNN test accuracy:'</span>, scores[<span class="hljs-number">1</span>])
</code></pre> 
<h3 id="5rnn还可以做什么"><strong><font color="#0099ff" size="5" face="黑体">5.RNN还可以做什么</font></strong></h3> 
<p><font size="4" color="009900">语言模型和文本生成</font></p> 
<p><font size="3">给定一个词的序列，我们想预测在前面的词确定之后，每个词出现的概率。语言模型可以度量一个句子出现的可能性，这可以作为机器翻译的一个重要输入（因为出现概率高的句子通常是正确的）。能预测下一个词所带来的额外效果是我们得到了一个生成模型，这可以让我们通过对输出概率采样来生成新的文本。根据训练数据的具体内容，我们可以生成任意东西。在语言模型中，输入通常是词的序列（编码成one hot向量），输出是预测得到的词的序列。在训练网络是，设置，因为我们想要的时刻的输出是下一个词。</font></p> 
<p>关于语言模型和文本生成的研究论文：</p> 
<ul><li><a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf" rel="nofollow">Recurrent neural network based language model</a></li><li><a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf" rel="nofollow">EXTENSIONS OF RECURRENT NEURAL NETWORK LANGUAGE MODEL</a></li><li><a href="http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Sutskever_524.pdf" rel="nofollow">Generating Text with Recurrent Neural Networks</a></li></ul> 
<p><font size="4" color="009900">机器翻译</font></p> 
<p><font size="3">机器翻译与语言模型相似，输入是源语言中的一个词的序列（例如，德语），输出是目标语言（例如，英语）的一个词的序列。一个关键不同点在于在接收到了完整的输入后才会开始输出，因为我们要翻译得到的句子的第一个词可能需要前面整个输入序列的信息。</font></p> 
<p></p> 
<center> 
 <img src="https://images2.imgbox.com/d2/1b/SMkfuv2C_o.png" alt="" title=""> 
</center> 
<p></p> 
<p><font size="3">关于机器翻译的研究论文：</font></p> 
<ul><li><a href="http://www.aclweb.org/anthology/P14-1140.pdf" rel="nofollow">A Recursive Recurrent Neural Network for Statistical Machine Translation</a></li><li><a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="nofollow">Sequence to Sequence Learning with Neural Networks</a></li><li><a href="http://research.microsoft.com/en-us/um/people/gzweig/Pubs/EMNLP2013RNNMT.pdf" rel="nofollow">Joint Language and Translation Modeling with Recurrent Neural Networks</a></li></ul> 
<h3 id="6扩展"><strong><font color="#0099ff" size="5" face="黑体">6.扩展</font></strong></h3> 
<p>（1）<a href="http://geek.csdn.net/news/detail/84745" rel="nofollow">百度PRNN：增强GPU伸缩性，RNN训练最高提速30倍</a> <br> （2）<a href="https://github.com/baidu-research/persistent-rnn">PRNN github</a> <br> （3）<a href="https://arxiv.org/pdf/1503.01007v4.pdf" rel="nofollow">Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets</a> <br> （4）<a href="https://github.com/facebook/Stack-RNN">Facebook Stack-RNN github</a></p> 
<p>参考文献 <br> （1）<a href="https://arxiv.org/pdf/1504.00941v2.pdf" rel="nofollow">A Simple Way to Initialize Recurrent Networks of Rectiﬁed Linear Units </a> <br> （2）<a href="https://keras.io/initializations/" rel="nofollow">https://keras.io/initializations/</a> <br> （3）<a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" rel="nofollow">http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/</a> <br> （4）Deep Learning for NLP <br> （5）<a href="https://github.com/karpathy/karpathy.github.io/blob/master/assets/rnn/charseq.jpeg">https://github.com/karpathy/karpathy.github.io/blob/master/assets/rnn/charseq.jpeg</a></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c3fe726b0e166fdd692d2723d9d4a95b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Springboot 之 解决IDEA读取properties配置文件的中文乱码问题</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8bd55a366b5f50fe297fb1b6e7a11dc8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">jenkins配置多job执行</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>