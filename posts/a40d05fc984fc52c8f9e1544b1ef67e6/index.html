<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>k8s功能介绍和常用命令 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="k8s功能介绍和常用命令" />
<meta property="og:description" content="一、Node篇 kubectl get nodes 查看所有node信息
kubectl get nodes -owide 查看所有node的详细信息
kubectl get node -o yaml 查看所有node的yaml文件
kubectl get node master01 -o yaml 查看master01的yaml文件
kubectl get node --show-labels 查看所有node的label信息
kubectl describe node node01 |grep -i taint -A3 查看node01的污点配置
kubectl taint nodes k8s-node01 ssd=true:PreferNoSchedule 给node01配置污点
二、deployment篇 2.1 创建deployment，命名为nginx kubectl create deployment nginx --image=nginx:1.15.2 2.2 查看deployment，包括IP kubectl get deploy -owide kubectl scale deploy {deployment的名称} -n xiyu --replicas=4 缩扩容副本数 kubectl describe -f pod-diff-nodes.yaml 通过查看文件创建deployment的具体信息 备注： NAME： Deployment名称 READY：Pod的状态，已经Ready的个数 UP-TO-DATE：已经达到期望状态的被更新的副本数 AVAILABLE：已经可以用的副本数 AGE：显示应用程序运行的时间 CONTAINERS：容器名称 IMAGES：容器的镜像 SELECTOR：管理的Pod的标签 2." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/a40d05fc984fc52c8f9e1544b1ef67e6/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-11T16:40:21+08:00" />
<meta property="article:modified_time" content="2022-04-11T16:40:21+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">k8s功能介绍和常用命令</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="Node_1"></a>一、Node篇</h3> 
<p>kubectl get nodes 查看所有node信息<br> kubectl get nodes -owide 查看所有node的详细信息<br> kubectl get node -o yaml 查看所有node的yaml文件<br> kubectl get node master01 -o yaml 查看master01的yaml文件<br> kubectl get node --show-labels 查看所有node的label信息</p> 
<p>kubectl describe node node01 |grep -i taint -A3 查看node01的污点配置<br> kubectl taint nodes k8s-node01 ssd=true:PreferNoSchedule 给node01配置污点</p> 
<h3><a id="deployment_11"></a>二、deployment篇</h3> 
<h4><a id="21%09deploymentnginx_12"></a>2.1 创建deployment，命名为nginx</h4> 
<pre><code class="prism language-shell">kubectl create deployment nginx --image<span class="token operator">=</span>nginx:1.15.2

</code></pre> 
<h4><a id="22%09deploymentIP_17"></a>2.2 查看deployment，包括IP</h4> 
<pre><code class="prism language-shell">kubectl get deploy -owide
kubectl scale deploy <span class="token punctuation">{<!-- --></span>deployment的名称<span class="token punctuation">}</span> -n xiyu --replicas<span class="token operator">=</span><span class="token number">4</span>  缩扩容副本数
kubectl describe -f pod-diff-nodes.yaml  通过查看文件创建deployment的具体信息
</code></pre> 
<pre><code class="prism language-shell">备注：
NAME： Deployment名称
READY：Pod的状态，已经Ready的个数
UP-TO-DATE：已经达到期望状态的被更新的副本数
AVAILABLE：已经可以用的副本数
AGE：显示应用程序运行的时间
CONTAINERS：容器名称
IMAGES：容器的镜像
SELECTOR：管理的Pod的标签
</code></pre> 
<h4><a id="23%09deploymentyaml_34"></a>2.3 将deployment配置导出yaml格式文件</h4> 
<pre><code class="prism language-shell">kubectl create deploy nginx --try-run<span class="token operator">=</span>client --image<span class="token operator">=</span>nginx:1.15.2 <span class="token operator">&gt;</span>nginx_deployment.yaml
</code></pre> 
<h4><a id="24%09deploymentpod_40"></a>2.4 deployment更新，升级pod镜像</h4> 
<pre><code class="prism language-shell">kubectl <span class="token builtin class-name">set</span> image deploy nginx <span class="token assign-left variable">nginx</span><span class="token operator">=</span>nginx:1.15.3 –record
</code></pre> 
<h4><a id="25%09deployment_45"></a>2.5 deployment回滚</h4> 
<pre><code class="prism language-shell">kubectl rollout <span class="token function">history</span> deploy nginx
</code></pre> 
<h4><a id="26%09deployment_50"></a>2.6 deployment的暂停和恢复</h4> 
<pre><code class="prism language-shell">kubectl rollout pause deployment nginx
kubectl <span class="token builtin class-name">set</span> image deploy nginx <span class="token assign-left variable">nginx</span><span class="token operator">=</span>nginx:1.15.3 --record
kubectl <span class="token builtin class-name">set</span> resources deploy nginx -c nginx --limits<span class="token operator">=</span>cpu<span class="token operator">=</span>200m,memory<span class="token operator">=</span>128Mi --requests<span class="token operator">=</span>cpu<span class="token operator">=</span>10m,memory<span class="token operator">=</span>16Mi  //添加内存cpu配置
kubectl get deploy nginx -oyaml
kubectl rollout resume deploy nginx
</code></pre> 
<h3><a id="statefulset_60"></a>三、statefulset篇</h3> 
<p>用statefulset部署pod，会按照顺序创建，如pod名字为nginx，副本数为3，那么pod名称会自动生成nginx-0，nginx-1，nginx-2，并且容器的hostname也为nginx-0，nginx-1，nginx-2<br> 1）创建statefulset前必须要创建service<br> 2）statefulset 按照顺序启动，前一个正常，才会继续下一个，删除的话为倒序<br> 3）用yaml文件创建，一个service和pod</p> 
<h4><a id="31__68"></a>3.1 扩容和缩容</h4> 
<pre><code class="prism language-shell">kubectl scale --replicas<span class="token operator">=</span><span class="token number">2</span> qingchen nginx  扩容或者缩容
kubectl get pod --show-lables 查看标签
kubectl get pod -A --show-lables 查看标签
kubectl get pod -l <span class="token assign-left variable">app</span><span class="token operator">=</span>nginx -w  查看指定的标签，监视资源变化
</code></pre> 
<h3><a id="deamonset_78"></a>四、deamonset篇</h3> 
<pre><code class="prism language-shell">kubectl get po -owide 
kubectl label node01 node02 <span class="token assign-left variable">ds</span><span class="token operator">=</span>ture  //给node打个标签
</code></pre> 
<p>备注：yaml文件中spec块中 nodeSelector: ds： true 这样创建的deamonset就会自动选择node</p> 
<pre><code class="prism language-shell">kubectl rollout <span class="token function">history</span> ds nginx 查看回滚记录   
</code></pre> 
<p>如果想要一个node中自动部署deamonset，可以直接给node 设置一个标签</p> 
<pre><code class="prism language-shell">kubectl label node03 <span class="token assign-left variable">ds</span><span class="token operator">=</span>ture  //给node打个标签
kubectl get pod -owide
</code></pre> 
<h3><a id="label_95"></a>五、label篇</h3> 
<pre><code class="prism language-shell">kubectl get pod -A --show-labels                查看所有namespace下的所有标签
kubectl get pod -A -l <span class="token assign-left variable">app</span><span class="token operator">=</span>busybox               查看所有namespace下的指定的标签
kubectl get pod -n kube-public -l <span class="token assign-left variable">app</span><span class="token operator">=</span>busybox   查看指定namespace中的标签
kubectl label pod busybox <span class="token assign-left variable">app</span><span class="token operator">=</span>busybox -n kube-public      配置pod的标签为app<span class="token operator">=</span>busybox
kubectl label pod busybox app- -n kube-public   删除app<span class="token operator">=</span>busybox的标签，用app-减号 

kubectl get pod -n -A -l <span class="token string">'k8s-app in (metrices-server,kubernetes-dashboard)'</span> 过滤多个标签
kubectl get pod -A -l version<span class="token operator">!=</span>v1 过滤不等于version<span class="token operator">=</span>v1的标签

kubectl get <span class="token function">node</span> --show-labels  查看所有node节点标签
kubectl label nodes <span class="token operator">&lt;</span>node-name<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>label-key<span class="token operator">&gt;=</span><span class="token operator">&lt;</span>label-value<span class="token operator">&gt;</span>   给node添加标签
kubectl label nodes <span class="token operator">&lt;</span>node-name<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>node-name <span class="token operator">&gt;</span><span class="token operator">&lt;</span>label-key<span class="token operator">&gt;=</span><span class="token operator">&lt;</span>label-value<span class="token operator">&gt;</span>   给多个node添加标签
kubectl label nodes <span class="token operator">&lt;</span>node-name<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>label-key<span class="token operator">&gt;</span>-   删除标签
kubectl label nodes <span class="token operator">&lt;</span>node-name<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>label-key<span class="token operator">&gt;=</span><span class="token operator">&lt;</span>label-value<span class="token operator">&gt;</span> --overwrite  修改标签
</code></pre> 
<h3><a id="volumes_115"></a>六、volumes篇</h3> 
<h4><a id="61___116"></a>6.1 常用命令</h4> 
<ul><li>kubectl get pv</li><li>kubectl get pvc</li></ul> 
<h4><a id="62_emptyDir_120"></a>6.2 emptyDir</h4> 
<p>删除pod，emptyDir会随之删除，只是做容器间共享数据用，无法持久化数据</p> 
<pre><code class="prism language-shell">apiVersion: v1
kind: Pod
metadata:
  name:test-pod
spec:
  containers:
  - image: k8s.gcr.io/test/webserver
    name: test-container
    volumeMounts:
    - mountpath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir:<span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
    
</code></pre> 
<h4><a id="63_hostPath_140"></a>6.3 hostPath</h4> 
<p>共享pod宿主机的文件或者文件夹，pod迁移后无法正常使用，因此很少被使用</p> 
<pre><code class="prism language-shell">apiVersion: v1
kind: Pod
metadata:
  name:test-pod2
spec:
  containers:
  - image: k8s.gcr.io/test/webserver
    name: test-container2
    volumeMounts:
    - mountpath: /cache2
      name: cache-volume2
  volumes:
  - name: cache-volume2
    hostPath:
      path: /data
      type: Directory
</code></pre> 
<h4><a id="64_NFS_161"></a>6.4 NFS</h4> 
<p>需要安装nfs-unils，一般生产环境下NFS共享并无法实现高可用保障，如果是公有云环境下，建议使用阿里云NAS存储，协议与NFS兼容</p> 
<pre><code class="prism language-shell">NFS服务端：yum <span class="token function">install</span> nfs* rpcbind -y
NFS客户端：yum <span class="token function">install</span> nfs-utils -y 
NFS服务端创建共享目录 <span class="token function">mkdir</span> /data/k8s/ -p
NFS服务端配置 <span class="token function">vim</span> /etc/exports 添加一下内容
/data/k8s. *<span class="token punctuation">(</span>rw,sync,no_subtree_check,no_root_squash<span class="token punctuation">)</span>
NFS服务端配置加载 exportfs -r
NFS服务端服务重启 systemctl restart nfs rpcbind
NFS服务端口号查看 <span class="token function">netstat</span> -anlp <span class="token operator">|</span><span class="token function">grep</span> nfs
客户端挂载：mount -fs nfs <span class="token number">10.1</span>.1.1:/data/k8s /mnt/ 
<span class="token number">10.1</span>.1.1是nfs服务端的IP
</code></pre> 
<p>https://blog.csdn.net/m0_46327721/article/details/108006037<br> 先配置好nfs共享服务后，再配置pod</p> 
<pre><code class="prism language-shell">apiVersion: v1
kind: Pod
metadata:
  name:test-pod3
spec:
  containers:
  - image: k8s.gcr.io/test/webserver
    name: test-container3
    volumeMounts:
    - mountpath: /mnt
      name: nfs-volume
  volumes:
  - name: nfs-volume
    nfs:
      server:192.168.1.100
      path: /data/nfs/test-dp
</code></pre> 
<h4><a id="65_k8s_PV__PVC_196"></a>6.5 k8s持久化存储 PV &amp; PVC</h4> 
<p>有些问题使用volume是无法解决的，比如<br> 1）数据卷不再被挂载了，数据如何处理？<br> 2）只读挂载？<br> 3）只能一个pod挂载<br> 4）限制某个pod使用10G空间</p> 
<p>PersistentVolume：简称PV，由k8s管理员设置的存储，可以配置ceph, NFS, GlusterFS等常用存储配置，相对于volume，它提供了更多功能，比如生命周期管理，大小限制，PV分为静态和动态，PV是没有命名空间限制</p> 
<p>PersistentVolumeClaim：简称PVC，是对存储PV的请求，配置需要什么类型的PV，PV无法直接使用，需要配置成PVC才可以，PVC是有命名空间限制</p> 
<h4><a id="66_PV_208"></a>6.6 PV的访问策略和回收策略</h4> 
<p>1）ReadWriteOnce：可以被单个节点以读写模式挂载，命令行中可缩写为RWO<br> 2）ReadOnlyMany：可以被多个节点以只读模式挂载，命令行中可缩写为ROX<br> 3）ReadWriteMany：可以被多个节点以读写模式挂载，命令行中可缩写为RWX</p> 
<p>1）Retain：保留，允许手动回收，当删除pvc时，pv仍保留，可以手动回收<br> 2）Recycle：回收，如果Volume插件支持，Recycle策略对卷支持rm -rf清理PV，为创建新的pvc做准备，但该策略将来被废弃，目前只有NFS HostPath支持<br> 3）Delete：删除，如果Volume插件支持，删除pvc会同时删除pv，动态卷默认delete模式，目前支持delete的存储后端包括AWS EBS ,GCE PD ,Azure Disk 等</p> 
<ul><li>可以通过persistentVolumeReclaimPolicy：Recyle 字段配置</li></ul> 
<h4><a id="67_PVyaml_220"></a>6.7 PV的yaml案例</h4> 
<pre><code class="prism language-shell">apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv001
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: slow
  mountOptions:
    - hard
    - <span class="token assign-left variable">nfsvers</span><span class="token operator">=</span><span class="token number">4.1</span>
  nfs:
    path: /tmp
    server:172.17.0.2
</code></pre> 
<font size="1" color="red"> 备注： </font> 
<font size="1" color="red"> volumeMode：卷的模式，目前支持Filesystem（文件系统）和block（块），其中block需要后端存储支持，默认为文件系统 </font> 
<font size="1" color="red"> accessModes：该PV的访问模式 </font> 
<font size="1" color="red"> storageClassName：PV的类，一个特定类型的PV只能绑定到特定类别的PVC，用来绑定PVC使用的名字 </font> 
<font size="1" color="red"> mountOptions：非必须，新版本中已废弃 </font> 
<font size="1" color="red"> nfs：NFS服务配置，包括两个选项，path：nfs上的共享目录，server：nfs的IP地址 </font> 
<ul><li>文件存储：一些数据被多个节点使用，实现方式：NFS NAS FTP等（生产环境中不建议使用NFS，建议使用有高可用的功能的NAS Ceph等）</li><li>块存储：一些数据只能被一个节点使用，或者将一块裸盘整个挂载使用，比如数据库 redis等，实现方式：Ceph, GlusterFS 公有云等</li><li>对象存储：由程序代码直接实现的一种存储方式，云原生应用无状态常用的实现方式，实现方式：一般是复核S3协议的云存储，比如AWS的S3存储，Minio等</li></ul> 
<h4><a id="68_NASNFSPV_266"></a>6.8 创建NAS或者NFS类型的PV</h4> 
<p>1、先创建NFS服务端和客户端（已完成，请忽略）<br> 2、创建yaml</p> 
<pre><code class="prism language-shell">apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv001
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: slow
  mountOptions:
    - hard
    - <span class="token assign-left variable">nfsvers</span><span class="token operator">=</span><span class="token number">4.1</span>
  nfs:
    path: /tmp
    server:172.17.0.2
</code></pre> 
<p>pv的几种status</p> 
<pre><code class="prism language-shell">Available:可用，还未被PVC绑定资源
Bound：已绑定，被PVC绑定
Released：已释放，pvc被删除，但资源还未被重新使用
Faild：失败，自动回收失败（Recycle策略中存在的一种）
</code></pre> 
<h4><a id="69_hostPathPV_296"></a>6.9 创建hostPath类型的PV</h4> 
<p>2、创建yaml</p> 
<pre><code class="prism language-shell">apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv002
  labels:
    type:local
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: hostpath
  hostPath:
    path:<span class="token string">"/mnt/data"</span>
</code></pre> 
<p>备注：path是指宿主机上的目录路径，与nas和nfs的唯一不同的地方就是后端存储的地方</p> 
<pre><code class="prism language-shell">  hostPath:
    path:<span class="token string">"/mnt/data"</span>
</code></pre> 
<h4><a id="610_PVCPV_324"></a>6.10 创建PVC，绑定到PV</h4> 
<ul><li>单独创建PVC</li></ul> 
<pre><code class="prism language-shell">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-test  //随便写
spec:
  storageClassName: slow   //与PV一致
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage:3Gi
</code></pre> 
<p>注意：slow为pv的storageClassName的名称，accessModes要跟pv的模式一致，容量要小于pv的大小</p> 
<ul><li>常用命令<br> kubectl create -f pvc-nfs.yaml<br> kubectl get -f pvc-nfs.yaml<br> kubect get pv pv-nfs</li></ul> 
<h4><a id="611_PODPVC_347"></a>6.11 创建POD中绑定PVC</h4> 
<p>创建yaml为pvc-nfs-pod.yaml</p> 
<pre><code class="prism language-shell">apiVersion: v1
kind: Pod
metadata:
  name: pvc-pod
spec:
  volumes:
    - name:pvc-storage    <span class="token comment">#volumes的名称</span>
      persistentVolumeClaim:
        claimName:nfs-pvc-claim  <span class="token comment">#pvc的名称</span>
  containers:
    - name: pvc-container
      image: nginx
      ports:
        - containerPort: <span class="token number">80</span>
          name:<span class="token string">"http-server"</span>
      volumeMounts:
        - mountPath:<span class="token string">"/usr/share/nginx/html"</span>
          name: pvc-storage
</code></pre> 
<ul><li> <p>常用命令<br> kubectl create -f pvc-nfs-pod.yaml<br> kubectl get pod<br> kubectl get pvc<br> 进入pod中执行<br> df -Th</p> </li><li> <p>测试<br> 1）写入数据，查看宿主机和pod中是否同步<br> 2）删除pod，查看数据是否还在</p> </li></ul> 
<h4><a id="612_deploymentPVC_382"></a>6.12 创建deployment绑定PVC</h4> 
<ul><li>创建文件pvc-deployment.yaml</li></ul> 
<pre><code class="prism language-shell">apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx
  name:nginx
  namespace: default
spec:
  replicas: <span class="token number">2</span>
  revisionHistoryLimit: <span class="token number">10</span>
  selector:
    matchLabels:
      app: nginx
  strategy:
    rollingUpdate:
      maxSurge: <span class="token number">1</span>
      maxUnavailable: <span class="token number">0</span>
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx
    spec:
      volumes:
        - name:pvc-storage    <span class="token comment">#volumes的名称</span>
          persistentVolumeClaim:
            claimName:nfs-pvc-claim  <span class="token comment">#pvc的名称</span>
      containers:
      - name:nginx
        image: nginx
        imagePullPolicy: IfNotPresent
        volumeMounts:
        - mountPath:"/usr/share/nginx/html
          name: task-pv-storage
               
</code></pre> 
<ul><li>常用命令<br> kubectl get po<br> kubectl exec -it nginx-xxxxx-xx – bash 进入pod内</li></ul> 
<h4><a id="613_pvc_426"></a>6.13 pvc创建和报错分析</h4> 
<ul><li> <p>PVC一直pending<br> 1）PVC的空间大小大于PV的大小<br> 2）PVC的storageClassName没有与PV一致<br> 3）PVC的accessModes与PV不一致</p> </li><li> <p>挂载PVC的pod一直处于pending<br> 1）PVC没有创建成功<br> 2）PVC和pod不在同一个Namespace</p> </li></ul> 
<h3><a id="Job_436"></a>七、Job篇</h3> 
<h4><a id="71_Job_437"></a>7.1 Job</h4> 
<ul><li>常用命令<br> kubectl create -f job.yaml<br> kubectl delete -f job.yaml<br> kubectl get pod<br> kubectl get job<br> kubectl logs -f echo-xxxx 查看执行日志</li></ul> 
<p>创建job.yaml</p> 
<pre><code class="prism language-shell">apiVersion: batch/v1
kind: Job
metadata:
  labels:
    job-name: <span class="token builtin class-name">echo</span>
  name: <span class="token builtin class-name">echo</span>
  namespace: default
spec:
  backoffLimit: <span class="token number">4</span>
  completions: <span class="token number">1</span>
  parallelism: <span class="token number">1</span>
  template:
    spec:
      containers:
      - command:
        - <span class="token builtin class-name">echo</span>
        - Hello,job
        image: registry.cn-beijing.aliyuncs.com/dotbalo/busybox
        imagePullPolicy: Always
        name: <span class="token builtin class-name">echo</span>
</code></pre> 
<p>备注：<br> backoofLimit: 如果任务失败，多少次不再执行<br> completions: 多少个pod执行成功，认为任务是成功的<br> parallelism: 并行执行任务的数量（如果大于未完成的数，会多次创建）<br> ttlSecondsAfterFinishe：job结束后自动清理，0表示结束后立即删除，不设置则不会清除</p> 
<h4><a id="72_CronJob_474"></a>7.2 CronJob</h4> 
<p>CronJob用于以时间为基准周期性地执行任务，这些自动化任务和运行在Linux或UNIX系统上的CronJob一样。CronJob对于创建定期和重复任务非常有用，例如执行备份任务、周期性调度程序接口、发送电子邮件等。</p> 
<p>对于Kubernetes 1.8以前的版本，需要添加–runtime-config=batch/v2alpha1=true参数至APIServer中，然后重启APIServer和Controller Manager用于启用API，对于1.8以后的版本无须修改任何参数，可以直接使用，本节的示例基于1.8以上的版本。</p> 
<ul><li> <p>创建CronJob有两种方式，一种是直接使用kubectl创建，一种是使用yaml文件创建。</p> </li><li> <p>使用kubectl创建CronJob的命令如下：</p> </li></ul> 
<pre><code class="prism language-shell">kubectl run hello --schedule<span class="token operator">=</span><span class="token string">"*/1 * * * *"</span> --restart<span class="token operator">=</span>OnFailure --image<span class="token operator">=</span>busybox -- /bin/sh -c <span class="token string">"date; echo Hello from the Kubernetes cluster"</span>
</code></pre> 
<p>对应的yaml文件如下：</p> 
<pre><code class="prism language-shell">apiVersion: batch/v1beta1  <span class="token comment">#1.21版本以上  改为batch/v1</span>
kind: CronJob
metadata:
  name: hello
spec:
  schedule: <span class="token string">"*/1 * * * *"</span>
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            args:
            - /bin/sh
            - -c
            - <span class="token function">date</span><span class="token punctuation">;</span> <span class="token builtin class-name">echo</span> Hello from the Kubernetes cluster
          restartPolicy: OnFailure
</code></pre> 
<ul><li>创建一个每分钟执行一次、打印当前时间和Hello from the Kubernetes cluster的计划任务。<br> 查看创建的CronJob：</li></ul> 
<pre><code class="prism language-shell">kubectl get cj
NAME    SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
hello   */1 * * * *   False     <span class="token number">0</span>        <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>          5s
</code></pre> 
<ul><li>等待1分钟可以查看执行的任务（Jobs）：</li></ul> 
<pre><code class="prism language-shell">$ kubectl get <span class="token function">jobs</span>
NAME               COMPLETIONS   DURATION   AGE
hello-1558779360   <span class="token number">1</span>/1           23s        32s
</code></pre> 
<ul><li>CronJob每次调用任务的时候会创建一个Pod执行命令，执行完任务后，Pod状态就会变成Completed，如下所示：</li></ul> 
<pre><code class="prism language-shell">$ kubectl get po 
NAME                          READY   STATUS      RESTARTS   AGE
hello-1558779360-jcp4r        <span class="token number">0</span>/1     Completed   <span class="token number">0</span>          37s
</code></pre> 
<ul><li>可以通过logs查看Pod的执行日志：</li></ul> 
<pre><code class="prism language-shell">$ kubectl logs -f hello-1558779360-jcp4r 
Sat May <span class="token number">25</span> <span class="token number">10</span>:16:23 UTC <span class="token number">2019</span>
Hello from the Kubernetes cluster
</code></pre> 
<ul><li>如果要删除CronJob，直接使用delete即可：</li></ul> 
<pre><code class="prism language-shell">kubectl delete cronjob hello
</code></pre> 
<ul><li>定义一个CronJob的yaml文件如下：</li></ul> 
<pre><code class="prism language-shell">apiVersion: batch/v1beta1
kind: CronJob
metadata:
  labels:
    run: hello
  name: hello
  namespace: default
spec:
  concurrencyPolicy: Allow
  failedJobsHistoryLimit: <span class="token number">1</span>
  jobTemplate:
    metadata:
    spec:
      template:
        metadata:
          labels:
            run: hello
        spec:
          containers:
          - args:
            - /bin/sh
            - -c
            - <span class="token function">date</span><span class="token punctuation">;</span> <span class="token builtin class-name">echo</span> Hello from the Kubernetes cluster
            image: registry.cn-beijing.aliyuncs.com/dotbalo/busybox
            imagePullPolicy: Always
            name: hello
            resources: <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
          restartPolicy: OnFailure
          securityContext: <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
  schedule: <span class="token string">'*/1 * * * *'</span>
  successfulJobsHistoryLimit: <span class="token number">3</span>
  suspend: <span class="token boolean">false</span>
</code></pre> 
<ul><li>参数说明：</li></ul> 
<blockquote> 
 <p>apiVersion: batch/v1beta1 #1.21+ batch/v1<br> schedule：调度周期，和Linux一致，分别是分时日月周。<br> restartPolicy：重启策略，和Pod一致。<br> concurrencyPolicy：并发调度策略。可选参数如下：<br> – Allow：允许同时运行多个任务。<br> – Forbid：不允许并发运行，如果之前的任务尚未完成，新的任务不会被创建。<br> – Replace：如果之前的任务尚未完成，新的任务会替换的之前的任务。<br> suspend：如果设置为true，则暂停后续的任务，默认为false。<br> successfulJobsHistoryLimit：保留多少已完成的任务，按需配置。<br> failedJobsHistoryLimit：保留多少失败的任务。</p> 
</blockquote> 
<p>相对于Linux上的计划任务，Kubernetes的CronJob更具有可配置性，并且对于执行计划任务的环境只需启动相对应的镜像即可。比如，如果需要Go或者PHP环境执行任务，就只需要更改任务的镜像为Go或者PHP即可，而对于Linux上的计划任务，则需要安装相对应的执行环境。此外，Kubernetes的CronJob是创建Pod来执行，更加清晰明了，查看日志也比较方便。可见，Kubernetes的CronJob更加方便和简单。</p> 
<h4><a id="73__590"></a>7.3 初始化容器</h4> 
<p>7.3.1 初始化容器介绍<br> <img src="https://images2.imgbox.com/48/45/edl8I3Gr_o.png" alt="在这里插入图片描述"></p> 
<ul><li> <p>初始化容器和poststart区别，PostStart：依赖主应用的环境，而且并不一定先于Command运行，InitContainer：不依赖主应用的环境，可以有更高的权限和更多的工具，一定会在主应用启动之前完成</p> </li><li> <p>另外，初始化容器和普通容器区别，Init 容器与普通的容器非常像，除了如下几点：<br> 它们总是运行到完成；上一个运行完成才会运行下一个；<br> 如果 Pod 的 Init 容器失败，Kubernetes 会不断地重启该 Pod，直到 Init 容器成功为止，但是Pod 对应的 restartPolicy 值为 Never，Kubernetes 不会重新启动 Pod。<br> Init 容器不支持 lifecycle、livenessProbe、readinessProbe 和 startupProbe</p> </li></ul> 
<p>7.3.2 创建初始化容器</p> 
<p>创建init.yaml文件，用来测试初始化容器的整个工作过程，使用deployment部署pod，定义三个pod副本，定义两个初始化容器init-touch和echo，并且为了演示效果，将echo加入for循环和sleep100秒，</p> 
<blockquote> 
 <p>kubectl create -f init.yaml<br> kubectl get pod -n kube-public<br> kubectl describe pod test-init-xxxxx -c echo -n kube-public 查看指定容器目前执行的命令<br> kubectl logs -f init.yaml</p> 
</blockquote> 
<pre><code class="prism language-shell">apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: test-init
  name: test-init
  namespace: kube-public
spec:
  replicas: <span class="token number">3</span>
  selector:
    matchLabels:
      app: test-init
  template:
    metadata:
      labels:
        app: test-init
    spec:
      volumes:
      - name: data
        emptyDir: <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
      initContainers:
      - command:
        - <span class="token function">sh</span>
        - -c
        - <span class="token function">touch</span> /mnt/test-init.txt
        image: nginx
        imagePullPolicy: IfNotPresent
        name: init-touch
        volumeMounts:
        - name: data
          mountPath: /mnt
      - command:
        - <span class="token function">sh</span>
        - -c
        - <span class="token keyword">for</span> <span class="token for-or-select variable">i</span> <span class="token keyword">in</span> <span class="token variable"><span class="token variable">`</span><span class="token function">seq</span> <span class="token number">1</span> <span class="token number">100</span><span class="token variable">`</span></span><span class="token punctuation">;</span> <span class="token keyword">do</span> <span class="token builtin class-name">echo</span> <span class="token variable">$i</span><span class="token punctuation">;</span> <span class="token function">sleep</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token keyword">done</span>
        image: nginx
        imagePullPolicy: IfNotPresent
        name: <span class="token builtin class-name">echo</span>
      containers:
      - image: nginx
        imagePullPolicy: IfNotPresent
        name: test-init
        volumeMounts:
        - name: data
          mountPath: /mnt
</code></pre> 
<h4><a id="74__661"></a>7.4 临时容器</h4> 
<p>什么是临时容器？<br> 临时容器：一种特殊的容器，该容器在现有 Pod 中临时运行，以便完成用户发起的操作，例如故障排查。 你会使用临时容器来检查服务，而不是用它来构建应用程序。<br> 临时容器与其他容器的不同之处在于，它们缺少对资源或执行的保证，并且永远不会自动重启， 因此不适用于构建应用程序。 临时容器使用与常规容器相同的 ContainerSpec 节来描述，但许多字段是不兼容和不允许的。</p> 
<p>Pod 是 Kubernetes 应用程序的基本构建块。 由于 Pod 是一次性且可替换的，因此一旦 Pod 创建，就无法将容器加入到 Pod 中。 取而代之的是，通常使用 Deployment 以受控的方式来删除并替换 Pod。</p> 
<p>有时有必要检查现有 Pod 的状态。例如，对于难以复现的故障进行排查。 在这些场景中，可以在现有 Pod 中运行临时容器来检查其状态并运行任意命令。</p> 
<p>临时容器没有端口配置，因此像 ports，livenessProbe，readinessProbe 这样的字段是不允许的。<br> Pod 资源分配是不可变的，因此 resources 配置是不允许的。<br> 有关允许字段的完整列表，请参见 EphemeralContainer 参考文档。<br> 临时容器是使用 API 中的一种特殊的 ephemeralcontainers 处理器进行创建的， 而不是直接添加到 pod.spec 段，因此无法使用 kubectl edit 来添加一个临时容器。</p> 
<p>与常规容器一样，将临时容器添加到 Pod 后，将不能更改或删除临时容器。</p> 
<p>临时容器的用途<br> 当由于容器崩溃或容器镜像不包含调试工具而导致 kubectl exec 无用时， 临时容器对于交互式故障排查很有用。</p> 
<p>尤其是，Distroless 镜像 允许用户部署最小的容器镜像，从而减少攻击面并减少故障和漏洞的暴露。 由于 distroless 镜像不包含 Shell 或任何的调试工具，因此很难单独使用 kubectl exec 命令进行故障排查。</p> 
<p>使用临时容器时，启用 进程名字空间共享很有帮助，可以查看其他容器中的进程。</p> 
<h3><a id="TaintToleration_687"></a>八、污点和容忍篇（Taint和Toleration）</h3> 
<p>设计理念：Taint在一类服务器上打上污点，让不能容忍这个污点的Pod不能部署在打了污点的服务器上。Toleration是让Pod容忍节点上配置的污点，可以让一些需要特殊配置的Pod能够调用到具有污点和特殊配置的节点上。<br> 官方文档：https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/</p> 
<p>污点：作用在节点上（相当于锁）<br> 容忍：作用在pod上（相当于钥匙）</p> 
<font color="red" size="1"> 注意：若是想要pod落到指定的某一个node中，需要在pod的yaml中配置nodeSelector，并且pod必须与nodes的label一致，然后还需要pod配置可以容忍node的污点。 如果单纯的只配置了容忍，未配置label，那么pod的容忍未必会起到作用。 </font> 
  
<h4><a id="81_taint_700"></a>8.1 taint污点</h4> 
<p>创建一个污点（一个节点可以有多个污点）：<br> kubectl taint nodes NODE_NAME TAINT_KEY=TAINT_VALUE:EFFECT</p> 
<pre><code class="prism language-shell">	kubectl taint nodes k8s-node01 <span class="token assign-left variable">ssd</span><span class="token operator">=</span>true:PreferNoSchedule
	kubectl taint nodes k8s-node01 <span class="token assign-left variable">ssd</span><span class="token operator">=</span>true:NoExecute（此时会驱逐没有容忍该污点的Pod）
	kubectl taint nodes k8s-node01 <span class="token assign-left variable">ssd</span><span class="token operator">=</span>true:NoSchedule
	kubectl label <span class="token function">node</span> k8s-node01 <span class="token assign-left variable">ssd</span><span class="token operator">=</span>true
</code></pre> 
<blockquote> 
 <p>NoSchedule：禁止调度到该节点，已经在该节点上的Pod不受影响<br> NoExecute：禁止调度到该节点，如果不符合这个污点，会立马被驱逐（或在一段时间后，配置tolerationSeconds: 6000）<br> PreferNoSchedule：尽量避免将Pod调度到指定的节点上，如果没有更合适的节点，可以部署到该节点</p> 
</blockquote> 
<ul><li>说明，配置6000秒后驱使pod</li></ul> 
<pre><code class="prism language-shell">tolerations:
- key: <span class="token string">"taintKey"</span>
  operator: <span class="token string">"Exists"</span>
  effect: <span class="token string">"NoExecute"</span>   
  tolerationSeconds: <span class="token number">6000</span>
</code></pre> 
<h4><a id="82_Toleration_723"></a>8.2 Toleration容忍</h4> 
<p>创建一个pod的 toleration.yaml，Toleration是配置在pod中的，与taint不一样，taint是定义在node中的</p> 
<pre><code class="prism language-shell">apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: <span class="token builtin class-name">test</span>
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    ssd: <span class="token string">"true"</span>
  tolerations:
  - key: <span class="token string">"ssd"</span>
    operator: <span class="token string">"Exists"</span>
</code></pre> 
<p>另外，toleration有四种模式</p> 
<ul><li>第一种：完全匹配</li></ul> 
<pre><code class="prism language-shell">tolerations:
- key: <span class="token string">"taintKey"</span>
  operator: <span class="token string">"Equal"</span>
  value: <span class="token string">"taintValue"</span>
  effect: <span class="token string">"NoSchedule"</span>
</code></pre> 
<ul><li>第二种：不完全匹配（这种模式下，只配置key，不配置value）</li></ul> 
<pre><code class="prism language-shell">tolerations:
- key: <span class="token string">"taintKey"</span>
  operator: <span class="token string">"Exists"</span>
  effect: <span class="token string">"NoSchedule"</span>
</code></pre> 
<ul><li>第三种：大范围匹配（这种模式下，不推荐key为内置Taint）</li></ul> 
<pre><code class="prism language-shell">tolerations:
- key: <span class="token string">"taintKey"</span>
  operator: <span class="token string">"Exists"</span>
</code></pre> 
<ul><li>第四种：匹配所有（不推荐）</li></ul> 
<pre><code class="prism language-shell">tolerations:
- operator: <span class="token string">"Exists"</span>
</code></pre> 
<h4><a id="83__774"></a>8.3 常用命令</h4> 
<blockquote> 
 <p>1）创建一个污点（一个节点可以有多个污点）：<br> #kubectl taint nodes NODE_NAME TAINT_KEY=TAINT_VALUE:EFFECT<br> 比如：<br> kubectl taint nodes k8s-node01 ssd=true:PreferNoSchedule<br> 2）查看一个节点的污点：<br> kubectl get node k8s-node01 -o go-template --template {<!-- -->{.spec.taints}}<br> kubectl describe node k8s-node01 | grep Taints -A 10<br> 3）删除污点（和label类似）：<br> 基于Key删除： kubectl taint nodes k8s-node01 ssd-<br> 基于Key+Effect删除： kubectl taint nodes k8s-node01 ssd:PreferNoSchedule-<br> 4）修改污点（Key和Effect相同）：<br> kubectl taint nodes k8s-node01 ssd=true:PreferNoSchedule --overwrite</p> 
</blockquote> 
<h4><a id="84__788"></a>8.4 内置污点</h4> 
<p>内置污点为pod创建成功后，自动生成的tolerations，是为了在机器宕机或者网络不可达后，节点自动采取迁移或者其他动作来规避故障<br> 使用以下命令可以查看：</p> 
<pre><code class="prism language-shell">kubectl get pod nginx -o yaml
</code></pre> 
<blockquote> 
 <p>node.kubernetes.io/not-ready：节点未准备好，相当于节点状态Ready的值为False。<br> node.kubernetes.io/unreachable：Node Controller访问不到节点，相当于节点状态Ready的值为Unknown。node.kubernetes.io/out-of-disk：节点磁盘耗尽。<br> node.kubernetes.io/memory-pressure：节点存在内存压力。<br> node.kubernetes.io/disk-pressure：节点存在磁盘压力。<br> node.kubernetes.io/network-unavailable：节点网络不可达。<br> node.kubernetes.io/unschedulable：节点不可调度。<br> node.cloudprovider.kubernetes.io/uninitialized：如果Kubelet启动时指定了一个外部的cloudprovider，它将给当前节点添加一个Taint将其标记为不可用。在cloud-controller-manager的一个controller初始化这个节点后，Kubelet将删除这个Taint。</p> 
</blockquote> 
<h3><a id="Affinity_804"></a>九、亲和力篇（Affinity）</h3> 
<h4><a id="91__805"></a>9.1 使用背景</h4> 
<ul><li>部署pod时，优先选择对应的标签，如果不存在，则部署到其他节点</li><li>部署pod时，部署到标签一和标签二的节点上，但需要优先部署到标签一的节点上</li><li>同一个pod多个副本，尽量或者必须部署到同一个节点上</li><li>相互有关联的pod，必须部署到同一个节点上，降低网络延迟</li><li>同一个项目的应用，尽量部署到不同的节点上，保证业务的高可用性</li></ul> 
<h4><a id="92__812"></a>9.2 亲和力分类</h4> 
<blockquote> 
 <p>NodeAffinity：节点亲和力 / 反亲和力<br> 1）分为硬亲和力-required（必须满足）和软亲和力-preferred（尽量满足）<br> 2）作用：</p> 
</blockquote> 
<blockquote> 
 <p>PodAffinity：Pod亲和力<br> 1）分为硬亲和力-required（必须满足）和软亲和力-preferred（尽量满足）<br> 2）作用：</p> 
</blockquote> 
<blockquote> 
 <p>PodAntiAffinity：Pod反亲和力<br> 1）分为硬亲和力-required（必须满足）和软亲和力-preferred（尽量满足）<br> 2）作用：为了规避所有的pod副本都部署到同一个节点，尽可能按照不同node均衡打散，或者按照机房和可用区打散</p> 
</blockquote> 
<h4><a id="93__826"></a>9.3 节点亲和力配置</h4> 
<p>创建nodeAffinity.yaml文件</p> 
<pre><code class="prism language-shell">apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: ipaddress
            operator: In
            values:
            - <span class="token number">172</span>-16-27-35
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: <span class="token number">1</span>
        preference:
          matchExpressions:
          - key: another-node-label-key
            operator: In
            values:
            - another-node-label-value
  containers:
  - name: with-node-affinity
    image: nginx
</code></pre> 
<p>注意：</p> 
<pre><code class="prism language-shell">nodeSelectorTerms 下的配置其一满足就可以 
matchExpressions下的配置项都要符合才可以 
requiredDuringSchedulingIgnoredDuringExecution：硬亲和力配置
nodeSelectorTerms：节点选择器配置，可以配置多个matchExpressions（满足其一），每个matchExpressions下可以配置多个key、value类型的选择器（都需要满足），其中values可以配置多个（满足其一）

preferredDuringSchedulingIgnoredDuringExecution：软亲和力配置
weight：	软亲和力的权重，权重越高优先级越大，范围1-100
preference：软亲和力配置项，和weight同级，可以配置多个，matchExpressions和硬亲和力一致
operator：标签匹配的方式 In：相当于key <span class="token operator">=</span> value的形式 NotIn：相当于key <span class="token operator">!=</span> value的形式
Exists：节点存在label的key为指定的值即可，不能配置values字段
DoesNotExist：节点不存在label的key为指定的值即可，不能配置values字段 
Gt：大于value指定的值
Lt：小于value指定的值
</code></pre> 
<font size="3" color="red"> 说明：可同时配置硬亲和力和软亲和力 </font> 
<h4><a id="94_Pod_876"></a>9.4 Pod亲和力和反亲和力配置</h4> 
<p>创建pod-affinity.yaml</p> 
<pre><code class="prism language-shell">apiVersion: v1
kind: Pod
metadata:
  name: with-pod-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
        topologyKey: failure-domain.beta.kubernetes.io/zone
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: <span class="token number">100</span>
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security
              operator: In
              values:
              - S2
          namespaces:
          - default
          topologyKey: failure-domain.beta.kubernetes.io/zone
  containers:
  - name: with-pod-affinity
    image: nginx

</code></pre> 
<p>注意：</p> 
<pre><code class="prism language-shell">labelSelector：Pod选择器配置，可以配置多个，匹配的是pod的标签，非node的标签 
matchExpressions：和节点亲和力配置一致，pod中只能配置一个
operator：配置和节点亲和力一致，但是没有Gt和Lt   
topologyKey：匹配的拓扑域的key，也就是节点上label的key，key和value相同的为同一个域，可以用于标注不同的机房和地区
Namespaces: 和哪个命名空间的Pod进行匹配，为空为当前命名空间
</code></pre> 
<font size="3" color="red"> 说明：可同时配置node亲和pod亲和力以及pod反亲和力 </font> 
<h4><a id="95__923"></a>9.5 案例实践</h4> 
<ul><li>案例一</li></ul> 
<p>同一个业务的pod副本，部署到不同的node中<br> 创建一个deployment，vim pod-diff-nodes.yaml</p> 
<pre><code class="prism language-shell">apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: must-be-diff-nodes
  name: must-be-diff-nodes
  namespace: kube-public
spec:
  replicas: <span class="token number">3</span>
  selector:
    matchLabels:
      app: must-be-diff-nodes
  template:
    metadata:
      labels:
        app: must-be-diff-nodes
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - must-be-diff-nodes
            topologyKey: kubernetes.io/hostname
      containers:
      - image: nginx
        imagePullPolicy: IfNotPresent
        name: must-be-diff-nodes
</code></pre> 
<ul><li>案例二</li></ul> 
<p>将pod调度到符合标签ssd=true的node节点上，并且不调度到gpu=true的node节点上，如果两者都不满足，则调度到type=physical节点上。preferredDuringSchedulingIgnoredDuringExecution代表尽可能，不强制</p> 
<p>1、首先，对master和node节点打上对应的标签</p> 
<pre><code class="prism language-shell"><span class="token comment">#kubectl label node master01 master02 master03 ssd=true</span>
<span class="token comment">#kubectl label node master01 master02 master03 gpu=true</span>
<span class="token comment">#kubectl label node node01 ssd=true</span>
<span class="token comment">#kubectl label node node02 type=physical</span>
<span class="token comment">#kubectl get nodes --show-labels |grep ssd  //检查配置</span>
</code></pre> 
<p>2、创建pod-affinity-ssd.yaml</p> 
<pre><code class="prism language-shell">apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: prefer-ssd
  name: prefer-ssd
  namespace: kube-public
spec:
  replicas: <span class="token number">3</span>
  selector:
    matchLabels:
      app: prefer-ssd
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: prefer-ssd
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
              - key: ssd
                operator: In
                values:
                - <span class="token string">"true"</span>
              - key: gpu
                operator: NotIn
                values:
                - <span class="token string">"true"</span>
            weight: <span class="token number">100</span>
          - preference:
              matchExpressions:
              - key: <span class="token builtin class-name">type</span>
                operator: In
                values:
                - physical
            weight: <span class="token number">10</span>
      containers:
      - env:
        - name: TZ
          value: Asia/Shanghai
        - name: <span class="token environment constant">LANG</span>
          value: C.UTF-8
        image: nginx
        imagePullPolicy: IfNotPresent
        name: prefer-ssd


说明：node中有一个label满足条件，便可以调度上去
</code></pre> 
<p>3、将node01的ssd=true标签删除，重新创建pod，观察pod会被调度到哪个节点中（正常应该会是node02）</p> 
<h4><a id="96_TopologyKey_1034"></a>9.6 拓扑域（TopologyKey）</h4> 
<ul><li>什么时拓扑域</li></ul> 
<p>pod亲和性调度需要各个相关的pod对象运行于"同一位置"， 而反亲和性调度则要求他们不能运行于"同一位置"，</p> 
<p>这里指定“同一位置” 是通过 topologyKey 来定义的，topologyKey 对应的值是 node 上的一个标签名称，比如各别节点zone=A标签，各别节点有zone=B标签，pod affinity topologyKey定义为zone，那么调度pod的时候就会围绕着A拓扑，B拓扑来调度，而相同拓扑下的node就为“同一位置”。</p> 
<p>可以理解成，它主要针对宿主机，相当于对宿主机进行区域的划分。用label进行判断，不同的key和不同的value是属于不同的拓扑域</p> 
<p>实验目的，将5个节点分为三个拓扑域，分别是beijing、shanghai、hangzhou<br> master01、master02 为region=beijing<br> master03、node01 为region=shanghai<br> node02 为region=hangzhou<br> 然后创建3个副本的pod，观察是否分别位于三个region上</p> 
<p>1、首先为节点打标签</p> 
<pre><code class="prism language-shell"><span class="token comment">#kubectl label node master01 master02 region=beijing</span>
<span class="token comment">#kubectl label node master03 node01 region=shanghai</span>
<span class="token comment">#kubectl label node node03 region=hangzhou</span>
</code></pre> 
<p>2、创建pod ，vim diff-zone.yaml</p> 
<pre><code class="prism language-shell">apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: must-be-diff-zone
  name: must-be-diff-zone
  namespace: kube-public
spec:
  replicas: <span class="token number">3</span>
  selector:
    matchLabels:
      app: must-be-diff-zone
  template:
    metadata:
      labels:
        app: must-be-diff-zone
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - must-be-diff-zone
            topologyKey: region
      containers:
      - image: nginx
        imagePullPolicy: IfNotPresent
        name: must-be-diff-zone
</code></pre> 
<p>3、上面yaml中，是通过region来定义区分不同的拓扑域</p> 
<p>注意：如果副本数超过域的数量，就会一直pending，解决该问题需要按照条件调整域的逻辑定义</p> 
<h3><a id="_1093"></a>十、资源配额</h3> 
<h3><a id="RBAC_1095"></a>十一、RBAC</h3> 
<p>https://kubernetes.io/zh/docs/reference/access-authn-authz/rbac/</p> 
<h4><a id="111___RBAC__1098"></a>11.1 使用 RBAC 鉴权</h4> 
<p>基于角色（Role）的访问控制（RBAC）是一种基于组织中用户的角色来调节控制对 计算机或网络资源的访问的方法。</p> 
<p>RBAC 鉴权机制使用 rbac.authorization.k8s.io API 组 来驱动鉴权决定，允许你通过 Kubernetes API 动态配置策略。</p> 
<p>要启用 RBAC，在启动 API 服务器 时将 --authorization-mode 参数设置为一个逗号分隔的列表并确保其中包含 RBAC。</p> 
<pre><code>kube-apiserver --authorization-mode=Example,RBAC --&lt;其他选项&gt; --&lt;其他选项&gt;
</code></pre> 
<h4><a id="112_API__1107"></a>11.2 API 对象</h4> 
<p>RBAC API 声明了四种 Kubernetes 对象：</p> 
<blockquote> 
 <p>Role<br> ClusterRole<br> RoleBinding<br> ClusterRoleBinding。</p> 
</blockquote> 
<h4><a id="113_Role__ClusterRole_1114"></a>11.3 Role 和 ClusterRole</h4> 
<p>RBAC 的 Role 或 ClusterRole 中包含一组代表相关权限的规则。 这些权限是纯粹累加的（不存在拒绝某操作的规则）。</p> 
<p>Role 总是用来在某个名字空间内设置访问权限；在你创建Role时，你必须指定该Role所属的名字空间。</p> 
<p>与之相对，ClusterRole则是一个集群作用域的资源。这两种资源的名字不同（Role 和 ClusterRole）是因为 Kubernetes 对象要么是名字空间作用域的，要么是集群作用域的， 不可两者兼具。</p> 
<p>ClusterRole的用法</p> 
<ul><li>定义对某名字空间域对象的访问权限，并将在各个名字空间内完成授权；</li><li>为名字空间作用域的对象设置访问权限，并跨所有名字空间执行授权；</li><li>为集群作用域的资源定义访问权限。</li></ul> 
<p>如果你希望在名字空间内定义角色，应该使用 Role； 如果你希望定义集群范围的角色，应该使用 ClusterRole</p> 
<p>Role 示例<br> 下面是一个位于 “default” 名字空间的 Role 的示例，可用来授予对 pods 的读访问权限：</p> 
<pre><code class="prism language-shell">apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">]</span> <span class="token comment"># "" 标明 core API 组</span>
  resources: <span class="token punctuation">[</span><span class="token string">"pods"</span><span class="token punctuation">]</span>
  verbs: <span class="token punctuation">[</span><span class="token string">"get"</span>, <span class="token string">"watch"</span>, <span class="token string">"list"</span><span class="token punctuation">]</span>
</code></pre> 
<p>ClusterRole 示例<br> ClusterRole 可以和 Role 相同完成授权。 因为 ClusterRole 属于集群范围，所以它也可以为以下资源授予访问权限：</p> 
<ul><li>集群范围资源（比如 节点（Node））</li><li>非资源端点（比如 /healthz）</li><li>跨名字空间访问的名字空间作用域的资源（如 Pods）</li></ul> 
<p>比如，你可以使用 ClusterRole 来允许某特定用户执行 kubectl get pods --all-namespaces<br> 下面是一个 ClusterRole 的示例，可用来为任一特定名字空间中的 Secret 授予读访问权限， 或者跨名字空间的访问权限（取决于该角色是如何绑定的）：</p> 
<pre><code class="prism language-shell">apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  <span class="token comment"># "namespace" 被忽略，因为 ClusterRoles 不受名字空间限制</span>
  name: secret-reader
rules:
- apiGroups: <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">]</span>
  <span class="token comment"># 在 HTTP 层面，用来访问 Secret 对象的资源的名称为 "secrets"</span>
  resources: <span class="token punctuation">[</span><span class="token string">"secrets"</span><span class="token punctuation">]</span>
  verbs: <span class="token punctuation">[</span><span class="token string">"get"</span>, <span class="token string">"watch"</span>, <span class="token string">"list"</span><span class="token punctuation">]</span>
</code></pre> 
<h4><a id="114_RoleBinding__ClusterRoleBinding_1165"></a>11.4 RoleBinding 和 ClusterRoleBinding</h4> 
<p>角色绑定（Role Binding）是将角色中定义的权限赋予一个或者一组用户。 它包含若干 主体（用户、组或服务账户）的列表和对这些主体所获得的角色的引用。 RoleBinding 在指定的名字空间中执行授权，而 ClusterRoleBinding 在集群范围执行授权。</p> 
<p>一个 RoleBinding 可以引用同一的名字空间中的任何 Role。 或者，一个 RoleBinding 可以引用某 ClusterRole 并将该 ClusterRole 绑定到 RoleBinding 所在的名字空间。 如果你希望将某 ClusterRole 绑定到集群中所有名字空间，你要使用 ClusterRoleBinding。</p> 
<p>RoleBinding 或 ClusterRoleBinding 对象的名称必须是合法的 路径区段名称。</p> 
<p>RoleBinding 示例<br> 下面的例子中的 RoleBinding 将 “pod-reader” Role 授予在 “default” 名字空间中的用户 “jane”。 这样，用户 “jane” 就具有了读取 “default” 名字空间中 pods 的权限。</p> 
<pre><code class="prism language-shell">apiVersion: rbac.authorization.k8s.io/v1
<span class="token comment">#此角色绑定允许 "jane" 读取 "default" 名字空间中的 Pods</span>
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
<span class="token comment">#你可以指定不止一个“subject（主体）”</span>
- kind: User
  name: jane <span class="token comment"># "name" 是区分大小写的</span>
  apiGroup: rbac.authorization.k8s.io
roleRef:
  <span class="token comment"># "roleRef" 指定与某 Role 或 ClusterRole 的绑定关系</span>
  kind: Role <span class="token comment"># 此字段必须是 Role 或 ClusterRole</span>
  name: pod-reader     <span class="token comment"># 此字段必须与你要绑定的 Role 或 ClusterRole 的名称匹配</span>
  apiGroup: rbac.authorization.k8s.io
</code></pre> 
<p>RoleBinding 也可以引用 ClusterRole，以将对应 ClusterRole 中定义的访问权限授予 RoleBinding 所在名字空间的资源。这种引用使得你可以跨整个集群定义一组通用的角色， 之后在多个名字空间中复用。</p> 
<p>例如，尽管下面的 RoleBinding 引用的是一个 ClusterRole，“dave”（这里的主体， 区分大小写）只能访问 “development” 名字空间中的 Secrets 对象，因为 RoleBinding 所在的名字空间（由其 metadata 决定）是 “development”。</p> 
<pre><code class="prism language-shell">apiVersion: rbac.authorization.k8s.io/v1
<span class="token comment">#此角色绑定使得用户 "dave" 能够读取 "development" 名字空间中的 Secrets</span>
<span class="token comment">#你需要一个名为 "secret-reader" 的 ClusterRole</span>
kind: RoleBinding
metadata:
  name: read-secrets
  <span class="token comment"># RoleBinding 的名字空间决定了访问权限的授予范围。</span>
  <span class="token comment"># 这里隐含授权仅在 "development" 名字空间内的访问权限。</span>
  namespace: development
subjects:
- kind: User
  name: dave <span class="token comment"># 'name' 是区分大小写的</span>
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io
</code></pre> 
<p>ClusterRoleBinding 示例<br> 要跨整个集群完成访问权限的授予，你可以使用一个 ClusterRoleBinding。 下面的 ClusterRoleBinding 允许 “manager” 组内的所有用户访问任何名字空间中的 Secrets。</p> 
<pre><code class="prism language-shell">apiVersion: rbac.authorization.k8s.io/v1
<span class="token comment">#此集群角色绑定允许 “manager” 组中的任何人访问任何名字空间中的 secrets</span>
kind: ClusterRoleBinding
metadata:
  name: read-secrets-global
subjects:
- kind: Group
  name: manager <span class="token comment"># 'name' 是区分大小写的</span>
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io
</code></pre> 
<p>创建了绑定之后，你不能再修改绑定对象所引用的 Role 或 ClusterRole。 试图改变绑定对象的 roleRef 将导致合法性检查错误。 如果你想要改变现有绑定对象中 roleRef 字段的内容，必须删除重新创建绑定对象。</p> 
<p>这种限制有两个主要原因：<br> 针对不同角色的绑定是完全不一样的绑定。要求通过删除/重建绑定来更改 roleRef, 这样可以确保要赋予绑定的所有主体会被授予新的角色（而不是在允许或者不小心修改 了 roleRef 的情况下导致所有现有主体未经验证即被授予新角色对应的权限）。<br> 将 roleRef 设置为不可以改变，这使得可以为用户授予对现有绑定对象的 update 权限， 这样可以让他们管理主体列表，同时不能更改被授予这些主体的角色。</p> 
<h4><a id="115__1244"></a>11.5 对资源的引用</h4> 
<p>在 Kubernetes API 中，大多数资源都是使用对象名称的字符串表示来呈现与访问的。 例如，对于 Pod 应使用 “pods”。 RBAC 使用对应 API 端点的 URL 中呈现的名字来引用资源。 有一些 Kubernetes API 涉及 子资源（subresource），例如 Pod 的日志。 对 Pod 日志的请求看起来像这样：</p> 
<blockquote> 
 <p>GET /api/v1/namespaces/{namespace}/pods/{name}/log</p> 
</blockquote> 
<p>在这里，pods 对应名字空间作用域的 Pod 资源，而 log 是 pods 的子资源。 在 RBAC 角色表达子资源时，使用斜线（/）来分隔资源和子资源。 要允许某主体读取 pods 同时访问这些 Pod 的 log 子资源，你可以这么写：</p> 
<pre><code class="prism language-shell">apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-and-pod-logs-reader
rules:
- apiGroups: <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">]</span>
  resources: <span class="token punctuation">[</span><span class="token string">"pods"</span>, <span class="token string">"pods/log"</span><span class="token punctuation">]</span>
  verbs: <span class="token punctuation">[</span><span class="token string">"get"</span>, <span class="token string">"list"</span><span class="token punctuation">]</span>
</code></pre> 
<p>对于某些请求，也可以通过 resourceNames 列表按名称引用资源。 在指定时，可以将请求限定为资源的单个实例。 下面的例子中限制可以 “get” 和 “update” 一个名为 my-configmap 的 ConfigMap：</p> 
<pre><code class="prism language-shell">apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: configmap-updater
rules:
- apiGroups: <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">]</span>
  <span class="token comment"># 在 HTTP 层面，用来访问 ConfigMap 的资源的名称为 "configmaps"</span>
  resources: <span class="token punctuation">[</span><span class="token string">"configmaps"</span><span class="token punctuation">]</span>   
  resourceNames: <span class="token punctuation">[</span><span class="token string">"my-configmap"</span><span class="token punctuation">]</span>   <span class="token comment">#要指定名称，不指定的话对所有的cm都又权限</span>
  verbs: <span class="token punctuation">[</span><span class="token string">"update"</span>, <span class="token string">"get"</span><span class="token punctuation">]</span>
</code></pre> 
<p>说明：<br> 你不能使用资源名字来限制 create 或者 deletecollection 请求。 对于 create 请求而言，这是因为在鉴权时可能还不知道新对象的名字。 如果你使用 resourceName 来限制 list 或者 watch 请求， 客户端必须在它们的 list 或者 watch 请求里包含一个与指定的 resourceName 匹配的 metadata.name 字段选择器。 例如，kubectl get configmaps --field-selector=metadata.name=my-configmap</p> 
<h4><a id="116__Role_1279"></a>11.6 Role案例</h4> 
<p>以下示例均为从 Role 或 ClusterRole 对象中截取出来，我们仅展示其 rules 部分。</p> 
<p>允许读取在核心 API 组下的 “Pods”：</p> 
<pre><code class="prism language-shell">rules:
- apiGroups: <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">]</span>
  <span class="token comment"># 在 HTTP 层面，用来访问 Pod 的资源的名称为 "pods"</span>
  resources: <span class="token punctuation">[</span><span class="token string">"pods"</span><span class="token punctuation">]</span>
  verbs: <span class="token punctuation">[</span><span class="token string">"get"</span>, <span class="token string">"list"</span>, <span class="token string">"watch"</span><span class="token punctuation">]</span>
</code></pre> 
<p>允许读/写在 “extensions” 和 “apps” API 组中的 Deployment（在 HTTP 层面，对应 URL 中资源部分为 “deployments”）：</p> 
<pre><code class="prism language-shell">rules:
- apiGroups: <span class="token punctuation">[</span><span class="token string">"extensions"</span>, <span class="token string">"apps"</span><span class="token punctuation">]</span>
  resources: <span class="token punctuation">[</span><span class="token string">"deployments"</span><span class="token punctuation">]</span>
  verbs: <span class="token punctuation">[</span><span class="token string">"get"</span>, <span class="token string">"list"</span>, <span class="token string">"watch"</span>, <span class="token string">"create"</span>, <span class="token string">"update"</span>, <span class="token string">"patch"</span>, <span class="token string">"delete"</span><span class="token punctuation">]</span>
</code></pre> 
<p>允许读取核心 API 组中的 “pods” 和读/写 “batch” 或 “extensions” API 组中的 “jobs”：</p> 
<pre><code class="prism language-shell">rules:
- apiGroups: <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">]</span>
  resources: <span class="token punctuation">[</span><span class="token string">"pods"</span><span class="token punctuation">]</span>
  verbs: <span class="token punctuation">[</span><span class="token string">"get"</span>, <span class="token string">"list"</span>, <span class="token string">"watch"</span><span class="token punctuation">]</span>
- apiGroups: <span class="token punctuation">[</span><span class="token string">"batch"</span>, <span class="token string">"extensions"</span><span class="token punctuation">]</span>
  resources: <span class="token punctuation">[</span><span class="token string">"jobs"</span><span class="token punctuation">]</span>
  verbs: <span class="token punctuation">[</span><span class="token string">"get"</span>, <span class="token string">"list"</span>, <span class="token string">"watch"</span>, <span class="token string">"create"</span>, <span class="token string">"update"</span>, <span class="token string">"patch"</span>, <span class="token string">"delete"</span><span class="token punctuation">]</span>
</code></pre> 
<p>允许读取名称为 “my-config” 的 ConfigMap（需要通过 RoleBinding 绑定以 限制为某名字空间中特定的 ConfigMap）：</p> 
<pre><code class="prism language-shell">rules:
- apiGroups: <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">]</span>
  resources: <span class="token punctuation">[</span><span class="token string">"configmaps"</span><span class="token punctuation">]</span>
  resourceNames: <span class="token punctuation">[</span><span class="token string">"my-config"</span><span class="token punctuation">]</span>
  verbs: <span class="token punctuation">[</span><span class="token string">"get"</span><span class="token punctuation">]</span>
</code></pre> 
<p>允许读取在核心组中的 “nodes” 资源（因为 Node 是集群作用域的，所以需要 ClusterRole 绑定到 ClusterRoleBinding 才生效）：</p> 
<pre><code class="prism language-shell">rules:
- apiGroups: <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">]</span>
  resources: <span class="token punctuation">[</span><span class="token string">"nodes"</span><span class="token punctuation">]</span>
  verbs: <span class="token punctuation">[</span><span class="token string">"get"</span>, <span class="token string">"list"</span>, <span class="token string">"watch"</span><span class="token punctuation">]</span>
</code></pre> 
<p>允许针对非资源端点 /healthz 和其子路径上发起 GET 和 POST 请求 （必须在 ClusterRole 绑定 ClusterRoleBinding 才生效）：</p> 
<pre><code class="prism language-shell">rules:
  - nonResourceURLs: <span class="token punctuation">[</span><span class="token string">"/healthz"</span>, <span class="token string">"/healthz/*"</span><span class="token punctuation">]</span> <span class="token comment"># nonResourceURL 中的 '*' 是一个全局通配符</span>
    verbs: <span class="token punctuation">[</span><span class="token string">"get"</span>, <span class="token string">"post"</span><span class="token punctuation">]</span>
</code></pre> 
<h4><a id="117_RoleBinding__1337"></a>11.7 RoleBinding 示例</h4> 
<p>下面示例是 RoleBinding 中的片段，仅展示其 subjects 的部分。</p> 
<p>对于名称为 alice@example.com 的用户：</p> 
<pre><code class="prism language-shell">subjects:
- kind: User
  name: <span class="token string">"alice@example.com"</span>
  apiGroup: rbac.authorization.k8s.io
</code></pre> 
<p>对于名称为 frontend-admins 的用户组：</p> 
<pre><code class="prism language-shell">subjects:
- kind: Group
  name: <span class="token string">"frontend-admins"</span>
  apiGroup: rbac.authorization.k8s.io
</code></pre> 
<p>对于 kube-system 名字空间中的默认服务账户：</p> 
<pre><code class="prism language-shell">subjects:
- kind: ServiceAccount
  name: default
  namespace: kube-system
</code></pre> 
<p>对于任何名称空间中的 “qa” 组中所有的服务账户：</p> 
<pre><code class="prism language-shell">subjects:
- kind: Group
  name: system:serviceaccounts:qa
  apiGroup: rbac.authorization.k8s.io
</code></pre> 
<p>对于 “development” 名称空间中 “dev” 组中的所有服务帐户：</p> 
<pre><code class="prism language-shell">subjects:
- kind: Group
  name: system:serviceaccounts:dev
  apiGroup: rbac.authorization.k8s.io
  namespace: development
</code></pre> 
<p>对于在任何名字空间中的服务账户：</p> 
<pre><code class="prism language-shell">subjects:
- kind: Group
  name: system:serviceaccounts
  apiGroup: rbac.authorization.k8s.io
</code></pre> 
<p>对于所有已经过认证的用户：</p> 
<pre><code class="prism language-shell">subjects:
- kind: Group
  name: system:authenticated
  apiGroup: rbac.authorization.k8s.io
</code></pre> 
<p>对于所有未通过认证的用户：</p> 
<pre><code class="prism language-shell">subjects:
- kind: Group
  name: system:unauthenticated
  apiGroup: rbac.authorization.k8s.io
</code></pre> 
<p>对于所有用户：</p> 
<pre><code class="prism language-shell">subjects:
- kind: Group
  name: system:authenticated
  apiGroup: rbac.authorization.k8s.io
- kind: Group
  name: system:unauthenticated
  apiGroup: rbac.authorization.k8s.io
</code></pre> 
<h3><a id="_1420"></a>二十、账号相关</h3> 
<p>kubectl get sa //serviceaccount<br> <code>创建角色-绑定角色-授权-创建账号-测试</code><br> https://blog.csdn.net/cr7258/article/details/114274628</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5428debff66560d09bfd00940e6c1200/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Windows10 修改Docker镜像的存储位置</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/17dcf534399cd22efcb9539f2ab66749/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">docker部署达梦流程记录（DM7和DM8）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>