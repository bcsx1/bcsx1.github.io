<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>FCN论文翻译（一） - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="FCN论文翻译（一）" />
<meta property="og:description" content="FCN论文翻译（一） 目录
用于语义分割的完全卷积网络
1、介绍
2、相关工作
前言：这篇文章是对FCN原文进行翻译，这篇翻译是第一和第二节，剩下的接下来会持续更新。论文原文：https://arxiv.org/pdf/1605.06211.pdf 用于语义分割的完全卷积网络 Jonathan Long∗ Evan Shelhamer∗ Trevor Darrell UC Berkeley
{jonlong,shelhamer,trevor}@cs.berkeley.edu
摘要：卷积网络是一种强大的视觉模型，它能产生特征层次结构。我们发现，卷积网络本身，经过端到端、像素到像素的训练，在语义分割方面超过了目前的水平。我们的关键洞察是建立“完全卷积”网络，该网络接收任意大小的输入，并通过有效的推理和学习产生相应大小的输出。我们定义并详细描述了完全卷积网络的空间，解释了它们在空间密集型预测任务中的应用，以及与先前模型的联系。我们将当代分类网络（AlexNet[19]、VGG网[31]和GoogLeNet[32]）改编为完全卷积网络，并通过微调[4]将它们的学习表示转移到分割任务中。然后，我们定义了一种新的架构，它将来自深层、粗糙层的语义信息与来自浅层、精细层的外观信息相结合，以产生精确而详细的分割。我们的全卷积网络实现了PASCAL VOC（2012年平均IU为62.2%）的20%相对改进，NYUDv2和SIFT流的分割达到了最新的水平，而对于典型图像，推理需要不到五分之一秒。
1、介绍 卷积网络正在推动识别技术的进步。卷积神经网络不仅改进了整体图像分类[19，31，32]，而且在结构化输出的局部任务上也取得了进展。其中包括边界盒目标检测[29,12,17]、部分和关键点预测[39,24]以及局部对应[24,9]方面的进展。
从粗推断到精推断过程中的下一步自然是在每个像素处进行预测。先前的方法已经使用卷积神经网络进行语义分割[27，2，8，28，16，14，11]，其中每个像素都用其封闭对象或区域的类别进行标记，但这项工作解决了一些缺点。
图1.完全卷积网络可以有效地学习对每像素任务（如语义分割）进行密集预测。
我们证明了一个完全卷积网络（FCN），经过训练的端到端，像素到像素的语义分割在没有进一步机器学习的情况下超过了最先进的水平。据我们所知，这是第一个端到端训练FCNs的工作（1）用于像素预测（2）来自有监督的预训练。现有网络的完全卷积版本预测任意大小输入的密集输出。学习和推理都是通过密集的前向计算和反向传播来实现的。在网络中，上采样层通过池化层在网络中实现像素级的预测和学习。
这种方法是有效的，无论是渐进的和绝对的，并排除了需要做其他工作的复杂性。分块训练很常见[27,2,8,28,11]，但缺乏完全卷积训练的效率。我们的方法没有使用预处理和后处理的复杂度，包括超像素[8，16]、建议[16，14]，或随机场或局部分类器的事后细化[8，16]。我们的模型通过将分类网重新解释为完全卷积的，并从学习的表示中进行微调，从而将最近在分类方面取得的成功[19，31，32]转化为密集预测。相比之下，以前的工作在没有监督的预训练的情况下应用了小型卷积神经网络[8，28，27]。
语义分割面临着语义和位置之间的内在张力：全局信息决定什么，而局部信息决定哪里。深层特征层次在局部到全局金字塔中联合编码位置和语义。在第4.2节中，我们定义了一个新颖的“跳过”架构，将深度的、粗糙的语义信息和浅层的、精细的外观信息结合起来(见图3)。
在下一节中，我们将回顾有关深度分类网络、模糊神经网络和使用卷积神经网络进行语义分割的最新方法。以下部分解释FCN设计和密集预测权衡，介绍我们的网络内上采样和多层组合架构，并描述我们的实验框架。最后，我们展示了PASCAL VOC 2011-2、NYUDv2和SIFT Flow的最新研究结果。
2、相关工作 我们的方法借鉴了深度网络在图像分类[19,31,32]和迁移学习[4,38]方面的最新成功。迁移首先在各种视觉识别任务上演示[4,38]，然后在检测上演示，在混合提议分类器模型中的实例和语义分割上演示[12,16,14]。我们现在重新构建和微调分类网络，以指导语义分割的密集预测。我们绘制了FCN的空间图，并在此框架中定位了以前的模型，包括历史模型和近期模型。
完全卷积网络 据我们所知，将卷积网络扩展到任意大小的输入的想法最早出现在Matan等人的[25]中，他们将经典的LeNet[21]扩展为识别数字字符串。 由于他们的网络仅限于一维输入字符串，Matan等人使用Viterbi解码获得他们的输出。Wolf和Platt[37]将卷积网络输出扩展为四角检测分数的二维地图。宁等。 [27]定义了一个卷积网络，用完全卷积推理对线虫组织进行粗分类。 在当今的多层网络中，也已经开发了完全卷积计算。Sermanet等人[29]对滑动窗口进行检测，Pinheiro和Collobert [28]对语义进行分割，Eigen等人[5]对图像进行还原，可以进行完全卷积推理。完全卷积训练很少见，但Tompson等[35]有效地用于学习端到端零件检测器和空间模型以进行姿势估计，尽管它们没有阐述或分析这种方法。 或者，He等人[17]丢弃了分类网络的非卷积部分，制成了特征提取器。他们将提案和空间金字塔池相结合，以产生用于分类的局部，固定长度的特征。尽管这种混合模型快速有效，但无法端对端学习
卷积网络的密集预测 最近的一些工作已经将卷积应用于密集的预测问题，包括Ning等人[27]，Farabet等人[8]，Pinheiro和Collobert [28]的语义分割。Ciresan等人[2]的电子显微镜边界预测，以及Ganin和Lempitsky的混合神经网络/最近邻模型[11]预测自然图像。以及Eigen等人[5，6]进行的图像恢复和深度估计。 这些方法的共同要素包括 :
限制能力和接受领域的小模型； 逐批训练[27、2、8、28、11]； 通过超像素投影，随机场正则化，滤波或局部分类进行后处理[8、2、11]； 由OverFeat [29]引入的用于密集输出[28，11]的输入移位和输出隔行； 多尺度金字塔处理[8、28、11]； 饱和tanh非线性[8，5，28]和集合[2，11]， 而我们的方法没有这种机制。 但是，我们确实从FCN的角度研究了分批训练3.4和“移位和缝合”密集输出3.2。 我们还讨论了网络内上采样3.3，其中Eigen等人[6]的全连接预测是一个特例。 与这些现有方法不同，我们使用图像分类作为监督的预训练来适应和扩展深度分类体系结构，并进行全面卷积微调，以从整个图像输入和整个图像基础事实中简单有效地学习。
Hariharan等人[16]和Gupta等人[14]同样将深层分类网适应于语义分割，但是在混合提议分类器模型中却是如此。 这些方法通过对边界框和/或区域建议进行采样以进行检测，语义分割和实例分割来微调R-CNN系统[12]。 两种方法都不是端到端学习的。
他们分别在PASCAL VOC分割和NYUDv2分割上获得了最新的结果，因此我们在第5节中直接将我们独立的端到端FCN与它们的语义分割结果进行比较。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/0366a835ce35b9e1f4d994cecd98c8b5/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-05-26T15:19:51+08:00" />
<meta property="article:modified_time" content="2021-05-26T15:19:51+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">FCN论文翻译（一）</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>FCN论文翻译（一）</h2> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="%E7%94%A8%E4%BA%8E%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%9A%84%E5%AE%8C%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C-toc" style="margin-left:0px;"><a href="#%E7%94%A8%E4%BA%8E%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%9A%84%E5%AE%8C%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C" rel="nofollow">用于语义分割的完全卷积网络</a></p> 
<p style="margin-left:0px;"><a href="#1%E3%80%81%E4%BB%8B%E7%BB%8D" rel="nofollow">1、介绍</a></p> 
<p style="margin-left:0px;"><a href="#2%E3%80%81%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C" rel="nofollow">2、相关工作</a></p> 
<h3 style="margin-left:0px;">前言：这篇文章是对FCN原文进行翻译，这篇翻译是第一和第二节，剩下的接下来会持续更新。论文原文：<a href="https://arxiv.org/pdf/1605.06211.pdf" rel="nofollow">https://arxiv.org/pdf/1605.06211.pdf</a></h3> 
<h2 id="%E7%94%A8%E4%BA%8E%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%9A%84%E5%AE%8C%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C">用于语义分割的完全卷积网络</h2> 
<p>Jonathan Long∗ Evan Shelhamer∗ Trevor Darrell UC Berkeley</p> 
<p>    {jonlong,shelhamer,trevor}@cs.berkeley.edu</p> 
<p> </p> 
<p>摘要：卷积网络是一种强大的视觉模型，它能产生特征层次结构。我们发现，卷积网络本身，经过端到端、像素到像素的训练，在语义分割方面超过了目前的水平。我们的关键洞察是建立“完全卷积”网络，该网络接收任意大小的输入，并通过有效的推理和学习产生相应大小的输出。我们定义并详细描述了完全卷积网络的空间，解释了它们在空间密集型预测任务中的应用，以及与先前模型的联系。我们将当代分类网络（AlexNet[19]、VGG网[31]和GoogLeNet[32]）改编为完全卷积网络，并通过微调[4]将它们的学习表示转移到分割任务中。然后，我们定义了一种新的架构，它将来自深层、粗糙层的语义信息与来自浅层、精细层的外观信息相结合，以产生精确而详细的分割。我们的全卷积网络实现了PASCAL VOC（2012年平均IU为62.2%）的20%相对改进，NYUDv2和SIFT流的分割达到了最新的水平，而对于典型图像，推理需要不到五分之一秒。</p> 
<h3 id="1%E3%80%81%E4%BB%8B%E7%BB%8D">1、介绍</h3> 
<p>    卷积网络正在推动识别技术的进步。卷积神经网络不仅改进了整体图像分类[19，31，32]，而且在结构化输出的局部任务上也取得了进展。其中包括边界盒目标检测[29,12,17]、部分和关键点预测[39,24]以及局部对应[24,9]方面的进展。</p> 
<p>    从粗推断到精推断过程中的下一步自然是在每个像素处进行预测。先前的方法已经使用卷积神经网络进行语义分割[27，2，8，28，16，14，11]，其中每个像素都用其封闭对象或区域的类别进行标记，但这项工作解决了一些缺点。</p> 
<p>                                                                                                             <img alt="" height="416" src="https://images2.imgbox.com/e3/61/w14LZOFs_o.png" width="851"></p> 
<p>                                                                                                                                 图1.完全卷积网络可以有效地学习对每像素任务（如语义分割）进行密集预测。</p> 
<p>    我们证明了一个完全卷积网络（FCN），经过训练的端到端，像素到像素的语义分割在没有进一步机器学习的情况下超过了最先进的水平。据我们所知，这是第一个端到端训练FCNs的工作（1）用于像素预测（2）来自有监督的预训练。现有网络的完全卷积版本预测任意大小输入的密集输出。学习和推理都是通过密集的前向计算和反向传播来实现的。在网络中，上采样层通过池化层在网络中实现像素级的预测和学习。</p> 
<p>    这种方法是有效的，无论是渐进的和绝对的，并排除了需要做其他工作的复杂性。分块训练很常见[27,2,8,28,11]，但缺乏完全卷积训练的效率。我们的方法没有使用预处理和后处理的复杂度，包括超像素[8，16]、建议[16，14]，或随机场或局部分类器的事后细化[8，16]。我们的模型通过将分类网重新解释为完全卷积的，并从学习的表示中进行微调，从而将最近在分类方面取得的成功[19，31，32]转化为密集预测。相比之下，以前的工作在没有监督的预训练的情况下应用了小型卷积神经网络[8，28，27]。</p> 
<p>    语义分割面临着语义和位置之间的内在张力：全局信息决定什么，而局部信息决定哪里。深层特征层次在局部到全局金字塔中联合编码位置和语义。在第4.2节中，我们定义了一个新颖的“跳过”架构，将深度的、粗糙的语义信息和浅层的、精细的外观信息结合起来(见图3)。</p> 
<p>    在下一节中，我们将回顾有关深度分类网络、模糊神经网络和使用卷积神经网络进行语义分割的最新方法。以下部分解释FCN设计和密集预测权衡，介绍我们的网络内上采样和多层组合架构，并描述我们的实验框架。最后，我们展示了PASCAL VOC 2011-2、NYUDv2和SIFT Flow的最新研究结果。</p> 
<h3 id="2%E3%80%81%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C">2、相关工作</h3> 
<p>    我们的方法借鉴了深度网络在图像分类[19,31,32]和迁移学习[4,38]方面的最新成功。迁移首先在各种视觉识别任务上演示[4,38]，然后在检测上演示，在混合提议分类器模型中的实例和语义分割上演示[12,16,14]。我们现在重新构建和微调分类网络，以指导语义分割的密集预测。我们绘制了FCN的空间图，并在此框架中定位了以前的模型，包括历史模型和近期模型。</p> 
<p>    <em><strong>完全卷积网络</strong></em>  据我们所知，将卷积网络扩展到任意大小的输入的想法最早出现在Matan等人的[25]中，他们将经典的LeNet[21]扩展为识别数字字符串。 由于他们的网络仅限于一维输入字符串，Matan等人使用Viterbi解码获得他们的输出。Wolf和Platt[37]将卷积网络输出扩展为四角检测分数的二维地图。宁等。 [27]定义了一个卷积网络，用完全卷积推理对线虫组织进行粗分类。 </p> 
<p>    在当今的多层网络中，也已经开发了完全卷积计算。Sermanet等人[29]对滑动窗口进行检测，Pinheiro和Collobert [28]对语义进行分割，Eigen等人[5]对图像进行还原，可以进行完全卷积推理。完全卷积训练很少见，但Tompson等[35]有效地用于学习端到端零件检测器和空间模型以进行姿势估计，尽管它们没有阐述或分析这种方法。 或者，He等人[17]丢弃了分类网络的非卷积部分，制成了特征提取器。他们将提案和空间金字塔池相结合，以产生用于分类的局部，固定长度的特征。尽管这种混合模型快速有效，但无法端对端学习</p> 
<p>   <em><strong>卷积网络的密集预测  </strong></em>最近的一些工作已经将卷积应用于密集的预测问题，包括Ning等人[27]，Farabet等人[8]，Pinheiro和Collobert [28]的语义分割。Ciresan等人[2]的电子显微镜边界预测，以及Ganin和Lempitsky的混合神经网络/最近邻模型[11]预测自然图像。以及Eigen等人[5，6]进行的图像恢复和深度估计。 这些方法的共同要素包括 :</p> 
<ol><li>  限制能力和接受领域的小模型；</li><li>  逐批训练[27、2、8、28、11]；</li><li>  通过超像素投影，随机场正则化，滤波或局部分类进行后处理[8、2、11]；</li><li>  由OverFeat [29]引入的用于密集输出[28，11]的输入移位和输出隔行；</li><li>  多尺度金字塔处理[8、28、11]；</li><li>  饱和tanh非线性[8，5，28]和集合[2，11]，</li></ol> 
<p>而我们的方法没有这种机制。 但是，我们确实从FCN的角度研究了分批训练3.4和“移位和缝合”密集输出3.2。 我们还讨论了网络内上采样3.3，其中Eigen等人[6]的全连接预测是一个特例。 与这些现有方法不同，我们使用图像分类作为监督的预训练来适应和扩展深度分类体系结构，并进行全面卷积微调，以从整个图像输入和整个图像基础事实中简单有效地学习。</p> 
<p>    Hariharan等人[16]和Gupta等人[14]同样将深层分类网适应于语义分割，但是在混合提议分类器模型中却是如此。 这些方法通过对边界框和/或区域建议进行采样以进行检测，语义分割和实例分割来微调R-CNN系统[12]。 两种方法都不是端到端学习的。</p> 
<p>    他们分别在PASCAL VOC分割和NYUDv2分割上获得了最新的结果，因此我们在第5节中直接将我们独立的端到端FCN与它们的语义分割结果进行比较。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/764468755b1fca79bf3e91942a13b130/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">android7.1.1版本更新,ColorOS安卓7.1.1升级包</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4a4a62fd82a149a340032bd4faa6d561/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">android:scrollbars代码控制,android:scrollbars属性和弹出键盘的问题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>