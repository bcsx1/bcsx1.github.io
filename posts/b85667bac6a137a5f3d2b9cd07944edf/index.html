<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>随机森林计算特征重要性_随机森林中计算特征重要性的3种方法 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="随机森林计算特征重要性_随机森林中计算特征重要性的3种方法" />
<meta property="og:description" content="随机森林计算特征重要性
The feature importance describes which features are relevant. It can help with a better understanding of the solved problem and sometimes lead to model improvement by utilizing feature selection. In this post, I will present 3 ways (with code) to compute feature importance for the Random Forest algorithm from scikit-learn package (in Python).
功能重要性描述了哪些功能是相关的。 它可以帮助您更好地了解已解决的问题，有时还可以利用特征选择来改进模型。 在这篇文章中，我将介绍3种方法(使用代码)，以scikit-learn包(在Python中)为随机森林算法计算功能重要性。 内置随机森林重要性 (Built-in Random Forest Importance) The Random Forest algorithm has built-in feature importance which can be computed in two ways:" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/b85667bac6a137a5f3d2b9cd07944edf/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-09-07T14:37:45+08:00" />
<meta property="article:modified_time" content="2020-09-07T14:37:45+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">随机森林计算特征重要性_随机森林中计算特征重要性的3种方法</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <article style="font-size: 16px;"> 
 <p>随机森林计算特征重要性</p> 
 <div> 
  <section> 
   <div> 
    <div> 
     <p>The feature importance describes which features are relevant. It can help with a better understanding of the solved problem and sometimes lead to model improvement by utilizing feature selection. In this post, I will present 3 ways (with code) to compute feature importance for the Random Forest algorithm from <code>scikit-learn</code> package (in Python).</p> 
     <p> 功能重要性描述了哪些功能是相关的。 它可以帮助您更好地了解已解决的问题，有时还可以利用特征选择来改进模型。 在这篇文章中，我将介绍3种方法(使用代码)，以<code>scikit-learn</code>包(在Python中)为随机森林算法计算功能重要性。 </p> 
     <h2> 内置随机森林重要性 <span style="font-weight: bold;">(</span>Built-in Random Forest Importance<span style="font-weight: bold;">)</span></h2> 
     <p>The Random Forest algorithm has built-in feature importance which can be computed in two ways:</p> 
     <p> 随机森林算法具有内置的特征重要性，可以通过两种方式计算： </p> 
     <ul><li><p><strong>Gini importance</strong> (or mean decrease impurity), which is computed from the Random Forest structure. Let’s look at how the Random Forest is constructed. It is a set of Decision Trees. Each Decision Tree is a set of internal nodes and leaves. In the internal node, the selected feature is used to make a decision on how to divide the data set into two separate sets with similar responses within. The features for internal nodes are selected with some criterion, which for classification tasks can be Gini impurity or information gain, and for regression is variance reduction. We can measure how each feature decreases the impurity of the split (the feature with the highest decrease is selected for internal node). For each feature, we can collect how on average it decreases the impurity. The average over all trees in the forest is the measure of the feature importance. This method is available in <code><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_" rel="noopener nofollow noopener noreferrer" target="_blank">scikit-learn</a></code><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_" rel="noopener nofollow noopener noreferrer" target="_blank"> the implementation</a> of the Random Forest (for both classifier and regressor). It is worth mentioning that in this method, we should look at the relative values of the computed importances. This biggest advantage of this method is the speed of computation - all needed values are computed during the Radom Forest training. The drawback of the method is a tendency to prefer (select as important) numerical features and categorical features with high cardinality. What is more, in the case of correlated features it can select one of the features and neglect the importance of the second one (which can lead to wrong conclusions).</p><p> <strong>基尼重要性</strong> (或平均减少杂质)，由随机森林结构计算得出。 让我们看看随机森林是如何构建的。 它是一组决策树。 每个决策树都是一组内部节点和叶子。 在内部节点中，所选功能用于决定如何将数据集分为两个单独的集合，并在其中进行类似的响应。 内部节点的特征按某些标准选择，对于分类任务可以是基尼杂质或信息增益，对于回归是方差减少。 我们可以测量每个特征如何减少分割的杂质(为内部节点选择具有最大减少的特征)。 对于每个功能，我们可以收集平均如何减少杂质。 森林中所有树木的平均值是特征重要性的度量。 <code><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_" rel="noopener nofollow noopener noreferrer" target="_blank">scikit-learn</a></code>可以使用此方法来<code><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_" rel="noopener nofollow noopener noreferrer" target="_blank">scikit-learn</a></code>随机森林<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_" rel="noopener nofollow noopener noreferrer" target="_blank">的实现</a> (对于分类器和回归器)。 值得一提的是，在这种方法中，我们应该查看计算出的重要性的相对值。 此方法的最大优点是计算速度-在Radom Forest训练期间计算所有需要的值。 该方法的缺点是倾向于倾向于(选择作为重要的)具有高基数的数字特征和分类特征。 而且，在具有相关特征的情况下，它可以选择特征之一，而忽略第二个特征的重要性(这可能导致错误的结论)。 </p></li><li><p><strong>Mean Decrease Accuracy</strong> — is a method of computing the feature importance on permuted out-of-bag (OOB) samples based on a mean decrease in the accuracy. This method is not implemented in the <code>scikit-learn</code> package. The very similar to this method is permutation-based importance described below in this post.</p><p> <strong>平均降低准确性</strong> -是一种基于<strong>准确性的平均降低</strong>来计算置换袋装(OOB)样本的特征重要性的方法。 <code>scikit-learn</code>包中未实现此方法。 与此方法非常相似的是下文中介绍的基于置换的重要性。 </p></li></ul> 
     <p>I will show how to compute feature importance for the Random Forest with <code>scikit-learn</code> package and Boston dataset (house price regression task).</p> 
     <p> 我将展示如何使用<code>scikit-learn</code>软件包和Boston数据集(房价回归任务)来计算随机森林的特征重要性。 </p> 
     <pre><code class="has"><em># Let's load the packages</em><strong>import</strong> <strong>numpy</strong> <strong>as</strong> np<strong>import</strong> <strong>pandas</strong> <strong>as</strong> pd<strong>from</strong> <strong>sklearn.datasets</strong> <strong>import</strong> load_boston<strong>from</strong> <strong>sklearn.model_selection</strong> <strong>import</strong> train_test_split<strong>from</strong> <strong>sklearn.ensemble</strong> <strong>import</strong> RandomForestRegressor<strong>from</strong> <strong>sklearn.inspection</strong> <strong>import</strong> permutation_importance<strong>import</strong> <strong>shap</strong><strong>from</strong> <strong>matplotlib</strong> <strong>import</strong> pyplot <strong>as</strong> plt<br>plt.rcParams.update({'figure.figsize': (12.0, 8.0)})<br>plt.rcParams.update({'font.size': 14})</code></pre> 
     <p>Load the data set and split for training and testing.</p> 
     <p> 加载数据集并拆分以进行训练和测试。 </p> 
     <pre><code class="has">boston = load_boston()<br>X = pd.DataFrame(boston.data, columns=boston.feature_names)<br>y = boston.target<br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=12)</code></pre> 
     <p>Fit the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor" rel="noopener nofollow noopener noreferrer" target="_blank">Random Forest Regressor</a> with 100 Decision Trees:</p> 
     <p> 使<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor" rel="noopener nofollow noopener noreferrer" target="_blank">随机森林回归器</a>具有100个决策树： </p> 
     <pre><code class="has">rf = RandomForestRegressor(n_estimators=100)<br>rf.fit(X_train, y_train)</code></pre> 
     <p>To get the feature importances from the Random Forest model use the <code>feature_importances_</code> argument:</p> 
     <p> 要从“随机森林”模型中获取要素重要性，请使用<code>feature_importances_</code>参数： </p> 
     <pre><code class="has">rf.feature_importances_array([0.04054781, 0.00149293, 0.00576977, 0.00071805, 0.02944643,<br>       0.25261155, 0.01969354, 0.05781783, 0.0050257 , 0.01615872,<br>       0.01066154, 0.01185997, 0.54819617])</code></pre> 
     <p>Let’s plot the importances (chart will be easier to interpret than values).</p> 
     <p> 让我们来画出重要性(图表比值更容易解释)。 </p> 
     <pre><code class="has">plt.barh(boston.feature_names, rf.feature_importances_)</code></pre> 
     <p>To have an even better chart, let’s sort the features, and plot again:</p> 
     <p> 为了获得更好的图表，让我们对特征进行排序，然后再次绘图： </p> 
     <pre><code class="has">sorted_idx = rf.feature_importances_.argsort()<br>plt.barh(boston.feature_names[sorted_idx], rf.feature_importances_[sorted_idx])<br>plt.xlabel("Random Forest Feature Importance")</code></pre> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/f4/9c/KVgBOJqi_o.png" width="749" height="488" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
     </figure> 
     <h2> 基于排列的重要性 <span style="font-weight: bold;">(</span>Permutation-based Importance<span style="font-weight: bold;">)</span></h2> 
     <p>The permutation-based importance can be used to overcome drawbacks of default feature importance computed with mean impurity decrease. It is implemented in <code>scikit-learn</code> as <code><a href="https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance" rel="noopener nofollow noopener noreferrer" target="_blank">permutation_importance</a></code> method. As arguments it requires trained model (can be any model compatible with <code>scikit-learn</code> API) and validation (test data). This method will randomly shuffle each feature and compute the change in the model's performance. The features which impact the performance the most are the most important one.</p> 
     <p> 基于置换的重要性可用于克服使用平均杂质减少计算出的默认特征重要性的缺点。 它在<code>scikit-learn</code>作为<code><a href="https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance" rel="noopener nofollow noopener noreferrer" target="_blank">permutation_importance</a></code>方法实现。 作为参数，它需要训练有素的模型(可以是与<code>scikit-learn</code> API兼容的任何模型)和验证(测试数据)。 该方法将随机地对每个功能进行随机排序，并计算模型性能的变化。 最影响性能的功能是最重要的功能。 </p> 
     <p>Permutation importance can be easily computed:</p> 
     <p> 排列重要性很容易计算： </p> 
     <pre><code class="has">perm_importance = permutation_importance(rf, X_test, y_test)</code></pre> 
     <p>To plot the importance:</p> 
     <p> 绘制重要性： </p> 
     <pre><code class="has">sorted_idx = perm_importance.importances_mean.argsort()<br>plt.barh(boston.feature_names[sorted_idx], perm_importance.importances_mean[sorted_idx])<br>plt.xlabel("Permutation Importance")</code></pre> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/0b/33/wP5G75jR_o.png" width="749" height="488" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
     </figure> 
     <p>The permutation-based importance is computationally expensive. The permutation-based method can have problems with highly-correlated features, it can report them as unimportant.</p> 
     <p> 基于排列的重要性在计算上很昂贵。 基于置换的方法可能会遇到功能高度相关的问题，可以将其报告为不重要的。 </p> 
     <h2> 从SHAP值计算重要性 <span style="font-weight: bold;">(</span>Compute Importance from SHAP Values<span style="font-weight: bold;">)</span></h2> 
     <p>The <a href="https://github.com/slundberg/shap" target="_blank" rel="noopener nofollow noopener noreferrer">SHAP</a> interpretation can be used (it is model-agnostic) to compute the feature importances from the Random Forest. It is using the Shapley values from game theory to estimate how does each feature contributes to the prediction. It can be easily installed ( <code>pip install shap</code>) and used with <code>scikit-learn</code> Random Forest:</p> 
     <p> 可以使用<a href="https://github.com/slundberg/shap" target="_blank" rel="noopener nofollow noopener noreferrer">SHAP</a>解释(与模型无关)来计算随机森林中的特征重要性。 它使用博弈论中的Shapley值来估计每个特征如何对预测做出贡献。 它可以轻松安装( <code>pip install shap</code> )并与<code>scikit-learn</code>随机森林一起使用： </p> 
     <pre><code class="has">explainer = shap.TreeExplainer(rf)<br>shap_values = explainer.shap_values(X_test)</code></pre> 
     <p>To plot feature importance as the horizontal bar plot we need to use <code>summary_plot</code> the method:</p> 
     <p> 要将要素重要性绘制为水平条形图，我们需要使用<code>summary_plot</code>方法： </p> 
     <pre><code class="has">shap.summary_plot(shap_values, X_test, plot_type="bar")</code></pre> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div style="text-align: center;"> 
          <img alt="Image for post" src="https://images2.imgbox.com/7b/8f/vnAncFpR_o.png" width="522" height="412" style="outline: none;"> 
         </div> 
        </div> 
       </div> 
      </div> 
     </figure> 
     <p>The feature importance can be plotted with more details, showing the feature value:</p> 
     <p> 可以使用更多细节绘制特征重要性，以显示特征值： </p> 
     <pre><code class="has">shap.summary_plot(shap_values, X_test)</code></pre> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div style="text-align: center;"> 
          <img alt="Image for post" src="https://images2.imgbox.com/2d/1c/wvdfJvW7_o.png" width="517" height="416" style="outline: none;"> 
         </div> 
        </div> 
       </div> 
      </div> 
     </figure> 
     <p>The computing feature importances with SHAP can be computationally expensive. However, it can provide more information like decision plots or dependence plots.</p> 
     <p> SHAP对计算功能的重要性在计算上可能很昂贵。 但是，它可以提供更多信息，例如决策图或依赖图。 </p> 
     <h2> 摘要 <span style="font-weight: bold;">(</span>Summary<span style="font-weight: bold;">)</span></h2> 
     <p>The 3 ways to compute the feature importance for the <code>scikit-learn</code> Random Forest was presented:</p> 
     <p> 提出了三种计算<code>scikit-learn</code>随机森林特征重要性的方法： </p> 
     <ul><li>built-in feature importance<p class="nodelete"></p> 内置功能的重要性 </li><li>permutation-based importance<p class="nodelete"></p> 基于置换的重要性 </li><li>computed with SHAP values<p class="nodelete"></p> 用SHAP值计算 </li></ul> 
     <p>In my opinion, it is always good to check all methods and compare the results. I’m using permutation and SHAP based methods in MLJAR’s AutoML open-source package <code><a href="https://github.com/mljar/mljar-supervised" target="_blank" rel="noopener nofollow noopener noreferrer">mljar-supervised</a></code>. I'm using them because they are model-agnostic and works well with algorithms not from <code>scikit-learn</code>: Xgboost, Neural Networks (keras+tensorflow), LigthGBM, CatBoost.</p> 
     <p> 我认为，检查所有方法并比较结果总是好的。 我在MLJAR的AutoML开源软件包<code><a href="https://github.com/mljar/mljar-supervised" target="_blank" rel="noopener nofollow noopener noreferrer">mljar-supervised</a></code>使用基于置换和SHAP的方法。 我之所以使用它们，是因为它们与模型无关，并且可以很好地与<code>scikit-learn</code>算法配合使用：Xgboost，神经网络(keras + tensorflow)，LigthGBM，CatBoost。 </p> 
     <h2> 重要笔记 <span style="font-weight: bold;">(</span>Important Notes<span style="font-weight: bold;">)</span></h2> 
     <ul><li>The more accurate model is, the more trustworthy computed importance is.<p class="nodelete"></p> 模型越准确，计算出的重要性就越值得信赖。 </li><li>The computed importances describe how important features are for the machine learning model. It is an approximation of how important features are in the data<p class="nodelete"></p> 计算出的重要性描述了机器学习模型的重要特征。 这是数据中重要特征的近似值 </li></ul> 
    </div> 
   </div> 
  </section> 
  <section> 
   <div> 
    <div> 
     <p>The <a href="https://github.com/mljar/mljar-supervised" target="_blank" rel="noopener nofollow noopener noreferrer">mljar-supervised</a> is an open-source Automated Machine Learning (AutoML) Python package that works with tabular data. It is designed to save time for a data scientist. It abstracts the common way to preprocess the data, construct the machine learning models, and perform hyper-parameters tuning to find the best model. It is no black-box as you can see exactly how the ML pipeline is constructed (with a detailed Markdown report for each ML model).</p> 
     <p> 受<a href="https://github.com/mljar/mljar-supervised" target="_blank" rel="noopener nofollow noopener noreferrer">mljar监督的</a>是可处理表格数据的开源自动机器学习(AutoML)Python软件包。 它旨在为数据科学家节省时间。 它抽象了预处理数据，构建机器学习模型以及执行超参数调整以找到最佳模型的通用方法。 这不是黑盒子，因为您可以确切地看到ML管道的构造方式(每个ML模型都有详细的Markdown报告)。 </p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/a3/fe/6WQ0LazE_o.png" width="1159" height="975" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        The example report generated with a mljar-supervised AutoML package. 
      </figcaption> 
      <figcaption>
        使用mljar监督的AutoML软件包生成的示例报告。 
      </figcaption> 
     </figure> 
    </div> 
   </div> 
  </section> 
  <section> 
   <div> 
    <div> 
     <p><em>Originally published at </em><a href="https://mljar.com/blog/feature-importance-in-random-forest/" rel="noopener nofollow noopener noreferrer" target="_blank"><em>https://mljar.com</em></a><em> on June 29, 2020.</em></p> 
     <p> <em>最初于</em> <em>2020年6月29日</em> <em>发布在</em> <a href="https://mljar.com/blog/feature-importance-in-random-forest/" rel="noopener nofollow noopener noreferrer" target="_blank"><em>https://mljar.com</em></a> <em>上。</em> </p> 
    </div> 
   </div> 
  </section> 
 </div> 
 <blockquote> 
  <p>翻译自: <a href="https://towardsdatascience.com/the-3-ways-to-compute-feature-importance-in-the-random-forest-96c86b49e6d4" rel="nofollow">https://towardsdatascience.com/the-3-ways-to-compute-feature-importance-in-the-random-forest-96c86b49e6d4</a></p> 
 </blockquote> 
 <p>随机森林计算特征重要性</p> 
</article>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8fb3ea020e766ad18a713031efba799e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Java集合--ConcurrentHashMap</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/16eeb1cea0d9567727862eb27bd49842/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Element-ui input 输入框限制只能输入数字的问题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>