<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>基于FFMPEG的RTP推流H264和AAC文件 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="基于FFMPEG的RTP推流H264和AAC文件" />
<meta property="og:description" content="在本文中主要讲如何用FFMPEG编写RTP的推流程序和打视音频时间戳上的问题 PS：文中代码基于LINUX
一. 文件的打开和输出流的打开 用avformat_open_input分别打开视音频文件，用avformat_alloc_output_context2打开输出的RTP流，注意，这里用的选项是rtp_mpegts，代表的是传输的视音频数据会打包成TS流的形式进行发送。rtp一个端口只能传输一路数据，所以我这里开了两个rtp端口，分别传输视音频数据。
avformat_open_input(&amp;ifmt_ctx_v, in_filename_v, 0, 0)； avformat_open_input(&amp;ifmt_ctx_a, in_filename_a, 0, 0)； avformat_alloc_output_context2(ofmt_ctx, NULL, &#34;rtp_mpegts&#34;, NULL); avformat_alloc_output_context2(ofmt_ctx&#43;1, NULL, &#34;rtp_mpegts&#34;, NULL); 二. 读取视音频数据并打上时间戳 av_compare_ts两个不同时基的时间戳，谁的比较大。 通过av_compare_ts决定要读取音频数据还是视频数据，当音频时间戳比较大的时候就读取视频数据，当视频时间戳比较大的时候就读取音频时间戳。
1. 视频时间戳 AVRational time_base = in_stream-&gt;time_base;//{ 1, 1000 }; AVRational r_framerate1 = in_stream-&gt;r_frame_rate; AVRational time_base_q = { 1, AV_TIME_BASE }; //Duration between 2 frames (us) int64_t calc_duration = (double)(AV_TIME_BASE) / av_q2d(r_framerate1); //内部时间戳 pkt.pts = av_rescale_q(vframe_index*calc_duration, time_base_q, time_base); pkt.dts = pkt.pts; pkt.duration = av_rescale_q(calc_duration, time_base_q, time_base); pkt." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/a2413861230a7ca31fbec9ea8907603e/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2017-04-26T20:17:36+08:00" />
<meta property="article:modified_time" content="2017-04-26T20:17:36+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">基于FFMPEG的RTP推流H264和AAC文件</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>在本文中主要讲如何用FFMPEG编写RTP的推流程序和打视音频时间戳上的问题 <br> PS：文中代码基于LINUX</p> 
<h3 id="一-文件的打开和输出流的打开">一. 文件的打开和输出流的打开</h3> 
<p>用avformat_open_input分别打开视音频文件，用avformat_alloc_output_context2打开输出的RTP流，注意，这里用的选项是rtp_mpegts，代表的是传输的视音频数据会打包成TS流的形式进行发送。rtp一个端口只能传输一路数据，所以我这里开了两个rtp端口，分别传输视音频数据。</p> 
<pre><code>avformat_open_input(&amp;ifmt_ctx_v, in_filename_v, 0, 0)；
avformat_open_input(&amp;ifmt_ctx_a, in_filename_a, 0, 0)；
avformat_alloc_output_context2(ofmt_ctx, NULL, "rtp_mpegts", NULL);
avformat_alloc_output_context2(ofmt_ctx+1, NULL, "rtp_mpegts", NULL);
</code></pre> 
<h3 id="二-读取视音频数据并打上时间戳">二. 读取视音频数据并打上时间戳</h3> 
<p>av_compare_ts两个不同时基的时间戳，谁的比较大。 <br> 通过av_compare_ts决定要读取音频数据还是视频数据，当音频时间戳比较大的时候就读取视频数据，当视频时间戳比较大的时候就读取音频时间戳。</p> 
<h6 id="1-视频时间戳">1. 视频时间戳</h6> 
<pre><code>AVRational time_base = in_stream-&gt;time_base;//{ 1, 1000 };  
AVRational r_framerate1 = in_stream-&gt;r_frame_rate;
AVRational time_base_q = { 1, AV_TIME_BASE };  
//Duration between 2 frames (us)  
int64_t calc_duration = (double)(AV_TIME_BASE) / av_q2d(r_framerate1);  //内部时间戳 
pkt.pts = av_rescale_q(vframe_index*calc_duration, time_base_q, time_base);
pkt.dts = pkt.pts; 
pkt.duration = av_rescale_q(calc_duration, time_base_q, time_base);
pkt.pos = -1;
pkt.pts = av_rescale_q_rnd(pkt.pts, in_stream-&gt;time_base, out_stream-&gt;time_base, (enum AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX));  
pkt.dts = av_rescale_q_rnd(pkt.dts, in_stream-&gt;time_base, out_stream-&gt;time_base, (enum AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX));  
pkt.duration = av_rescale_q(pkt.duration, in_stream-&gt;time_base, out_stream-&gt;time_base);  
pkt.pos = -1;  
</code></pre> 
<p>av_q2d(a) ——————— 把AVRatioal结构a转换成double的函数 <br> av_rescale_q(a，b，c) ——– 把a从b时基转换成c时基 <br> 上面的一大堆代码其实是在不同的时基上大时间戳后，进行转换。</p> 
<p>首先是这段代码</p> 
<pre><code>AVRational time_base1=in_stream-&gt;time_base; 
AVRational time_base_q = { 1, AV_TIME_BASE }; 
double frame_size = (double)in_stream-&gt;codec-&gt;frame_size;
double sample_rate = (double)in_stream-&gt;codec-&gt;sample_rate;
//Duration between 2 frames (us)  
int64_t calc_duration=(double)(AV_TIME_BASE) * (frame_size/sample_rate);
pkt.pts = av_rescale_q(aframe_index*calc_duration, time_base_q, time_base1);
pkt.dts = pkt.pts; 
pkt.duration = av_rescale_q(calc_duration, time_base_q, time_base1);
pkt.pos = -1;
</code></pre> 
<p>在视频文件时钟的时基上打时间戳，然后进行时间戳的转换，把时间戳转换为RTP的时基单位。</p> 
<pre><code>pkt.pts = av_rescale_q_rnd(pkt.pts, in_stream-&gt;time_base, out_stream-&gt;time_base, (enum AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX));  
pkt.dts = av_rescale_q_rnd(pkt.dts, in_stream-&gt;time_base, out_stream-&gt;time_base, (enum AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX));  
pkt.duration = av_rescale_q(pkt.duration, in_stream-&gt;time_base, out_stream-&gt;time_base);  
pkt.pos = -1;  
</code></pre> 
<p>减去那些时钟转化的步骤其实就是</p> 
<pre><code>pkt.pts = (double)frame_index * 1000 / av_q2d(ifmt_ctx-&gt;streams[videoindex]-&gt;r_frame_rate);
pkt.dts = pkt.pts;
pkt.duration = 1000 / av_q2d(ifmt_ctx-&gt;streams[videoindex]-&gt;r_frame_rate);
pkt.pos = -1; 
</code></pre> 
<p>也就是视频时间戳的公式是：pts = frame_index++ *(1000/fps)；</p> 
<h6 id="2-音频时间戳">2. 音频时间戳</h6> 
<p>音频时间戳的公式就是： <br> 1. 计算两帧之间的时间间隔：duration = frame_size / frame_rate * 1000 (这是以毫秒为单位) <br> 2.frame_size是一帧音频有多少采样点，frame_rate是音频的采样率，也就是一秒有多少采样点 <br> 3.frame_size/frame_rate也就是一帧的时长，经过单位转化成毫秒</p> 
<pre><code>AVRational time_base1=in_stream-&gt;time_base; 
AVRational time_base_q = { 1, AV_TIME_BASE }; 
double frame_size = in_stream-&gt;codec-&gt;frame_size;
double sample_rate = in_stream-&gt;codec-&gt;sample_rate;
 //Duration between 2 frames (us)  
int64_t calc_duration=(double)(AV_TIME_BASE) * (frame_size/sample_rate);
pkt.pts = av_rescale_q(aframe_index*calc_duration, time_base_q, time_base1);
pkt.dts = pkt.pts; 
pkt.duration = av_rescale_q(calc_duration, time_base_q, time_base1);
pkt.pos = -1;
</code></pre> 
<h3 id="vlc播放的sdp问题">VLC播放的SDP问题</h3> 
<p>本人程序用的SDP文件如下：</p> 
<pre><code>v=0  
c=IN IP4 192.168.1.121 
t=0 0  
a=tool:libavformat 57.25.100 
m=video 6666 RTP/AVP 33    
a=rtpmap:33 MP2T   
m=video 6668 RTP/AVP 33
</code></pre> 
<p>c=IN IP4 192.168.1.121 这一行要根据自己的IP地址进行修改。</p> 
<p>整体的源代码：</p> 
<pre><code>#include &lt;stdio.h&gt;  
#include &lt;libavformat/avformat.h&gt;  
#include &lt;libavutil/mathematics.h&gt;  
#include &lt;libavutil/time.h&gt; 
#include &lt;libavcodec/avcodec.h&gt;


int main(int argc, char* argv[])  
{  
    AVOutputFormat *ofmt = NULL;  
    //输入对应一个AVFormatContext，输出对应一个AVFormatContext  
    //（Input AVFormatContext and Output AVFormatContext）  
    AVFormatContext *ifmt_ctx_a = NULL, *ifmt_ctx_v = NULL, *ifmt_ctx, 
                    *ofmt_ctx[2];  
    AVPacket pkt,pkt1;
    const char *in_filename_a, *in_filename_v, 
                *out_filename_a, *out_filename_v;  
    int ret, i;  
    int videoindex=-1, audioindex=-1;  
    int vframe_index=0, aframe_index = 0,  index = 0; 
    int64_t start_time=0;  
    int64_t cur_pts_v=0, cur_pts_a=0;  
    in_filename_v = "test.h264";//输入视频文件
    in_filename_a = "test.aac";//输入音频文件

    out_filename_v = "rtp://192.168.1.121:6666";//视频端口
    out_filename_a = "rtp://192.168.1.121:6668";//音频端口                                                                   
    av_register_all();  
    //Network  
    avformat_network_init();  
    //视频输入结构体初始化 
    if ((ret = avformat_open_input(&amp;ifmt_ctx_v, in_filename_v, 0, 0)) &lt; 0) {  
        printf( "Could not open input file.");  
        goto end;  
    }  
    if ((ret = avformat_find_stream_info(ifmt_ctx_v, 0)) &lt; 0) {  
        printf( "Failed to retrieve input stream information");  
        goto end;  
    }  

    //音频输入结构体初始化
    if ((ret = avformat_open_input(&amp;ifmt_ctx_a, in_filename_a, 0, 0)) &lt; 0) {  
        printf( "Could not open input file.");  
        goto end;  
    }  
    if ((ret = avformat_find_stream_info(ifmt_ctx_a, 0)) &lt; 0) {  
        printf( "Failed to retrieve input stream information");  
        goto end;  
    }  


    /**********打印输入文件信息**********/
    av_dump_format(ifmt_ctx_v, 0, in_filename_v, 0);    
    av_dump_format(ifmt_ctx_a, 1, in_filename_a, 0); 
    /************************************/


    //rtp输出结构体初始化
    avformat_alloc_output_context2(ofmt_ctx, NULL, "rtp_mpegts", NULL);
    avformat_alloc_output_context2(ofmt_ctx+1, NULL, "rtp_mpegts", NULL);


    if (!ofmt_ctx) {  
        printf( "Could not create output context\n");  
        ret = AVERROR_UNKNOWN;  
        goto end;  
    }  

    //视频部分
    for (i = 0; i &lt; ifmt_ctx_v-&gt;nb_streams; i++) {  
        //根据输入流创建输出流
        AVStream *in_stream = ifmt_ctx_v-&gt;streams[i];
        AVStream *out_stream = avformat_new_stream(ofmt_ctx[0], in_stream-&gt;codec-&gt;codec);  
        if (!out_stream) {  
            printf( "Failed allocating output stream\n");  
            ret = AVERROR_UNKNOWN;  
            goto end;  
        }    
        //复制AVCodecContext的设置
        ret = avcodec_copy_context(out_stream-&gt;codec, in_stream-&gt;codec);  
        if (ret &lt; 0) {  
            printf( "Failed to copy context from input to output stream codec context\n");  
            goto end;  
        }  
        out_stream-&gt;codec-&gt;codec_tag = 0;  
        if (ofmt_ctx[0]-&gt;oformat-&gt;flags &amp; AVFMT_GLOBALHEADER)  
            out_stream-&gt;codec-&gt;flags |= CODEC_FLAG_GLOBAL_HEADER;  
    }  
    //音频部分
    for (i = 0; i &lt; ifmt_ctx_a-&gt;nb_streams; i++) {  
        //根据输入流创建输出流
        AVStream *in_stream = ifmt_ctx_a-&gt;streams[i];
        AVStream *out_stream = avformat_new_stream(ofmt_ctx[1], in_stream-&gt;codec-&gt;codec);  
        if (!out_stream) {  
            printf( "Failed allocating output stream\n");  
            ret = AVERROR_UNKNOWN;  
            goto end;  
        }    
        //复制AVCodecContext的设置 
        ret = avcodec_copy_context(out_stream-&gt;codec, in_stream-&gt;codec);  
        if (ret &lt; 0) {  
            printf( "Failed to copy context from input to output stream codec context\n");  
            goto end;  
        }  
        out_stream-&gt;codec-&gt;codec_tag = 0;  
        if (ofmt_ctx[1]-&gt;oformat-&gt;flags &amp; AVFMT_GLOBALHEADER)  
            out_stream-&gt;codec-&gt;flags |= CODEC_FLAG_GLOBAL_HEADER;  
    }  

    //打印输出的信息格式
    av_dump_format(ofmt_ctx[0], 0, out_filename_v, 1); 
    av_dump_format(ofmt_ctx[1], 1, out_filename_a, 1);

    //打开输出URL（Open output URL）  
    if (!((*ofmt_ctx)-&gt;oformat-&gt;flags &amp; AVFMT_NOFILE)) {  
        ret = avio_open(&amp;(ofmt_ctx[0])-&gt;pb, out_filename_v, AVIO_FLAG_WRITE);  
        if (ret &lt; 0) {  
            printf( "Could not open output URL '%s'", out_filename_v);  
            goto end;  
        }
    }  
    if (!((*ofmt_ctx)-&gt;oformat-&gt;flags &amp; AVFMT_NOFILE)) {  
        ret = avio_open(&amp;(ofmt_ctx[1])-&gt;pb, out_filename_a, AVIO_FLAG_WRITE);  
        if (ret &lt; 0) {  
            printf( "Could not open output URL '%s'", out_filename_a);  
            goto end;  
        }
    }  

    //写文件头（Write file header）  
    ret= avformat_write_header(ofmt_ctx[0], NULL);
    ret= avformat_write_header(ofmt_ctx[1], NULL);

    if (ret &lt; 0) {  
        printf( "Error occurred when opening output URL\n");  
        goto end;  
    } 

    start_time=av_gettime();  
    while (1) {  
        AVStream *in_stream, *out_stream;  
        videoindex = 0;
        audioindex = 0;

        //Get an AVPacket  
        if(av_compare_ts(cur_pts_v,ifmt_ctx_v-&gt;streams[0]-&gt;time_base,cur_pts_a,ifmt_ctx_a-&gt;streams[0]-&gt;time_base) &lt;= 0){  
            ifmt_ctx=ifmt_ctx_v;
            index = 0;
            if(av_read_frame(ifmt_ctx, &amp;pkt) &gt;= 0){  
                do{  
                    in_stream  = ifmt_ctx-&gt;streams[0];  
                    out_stream = ofmt_ctx[index]-&gt;streams[0];  

                    if(pkt.stream_index==index){  

                        AVRational time_base = in_stream-&gt;time_base;//{ 1, 1000 };  
                        AVRational r_framerate1 = in_stream-&gt;r_frame_rate;
                        AVRational time_base_q = { 1, AV_TIME_BASE };  
                        //Duration between 2 frames (us)  
                        int64_t calc_duration = (double)(AV_TIME_BASE) / av_q2d(r_framerate1);  //内部时间戳 
                        pkt.pts = av_rescale_q(vframe_index*calc_duration, time_base_q, time_base);
                        pkt.dts = pkt.pts; 
                        pkt.duration = av_rescale_q(calc_duration, time_base_q, time_base);
                        pkt.pos = -1;
                        vframe_index++;  

                        cur_pts_v=pkt.pts;  
                        //发送延时Important:Delay
                        {
                            AVRational time_base=ifmt_ctx-&gt;streams[index]-&gt;time_base;
                            AVRational time_base_q={1,AV_TIME_BASE};  
                            int64_t pts_time = av_rescale_q(pkt.dts, time_base, time_base_q);  
                            int64_t now_time = av_gettime() - start_time;  
                            if (pts_time &gt; now_time)  
                                av_usleep(pts_time - now_time);
                        }

                        break;  
                    }  
                }while(av_read_frame(ifmt_ctx, &amp;pkt) &gt;= 0);  
            }
            else{  
                break;  
            }  
        }
        else{  
            ifmt_ctx=ifmt_ctx_a;  
            index = 1;  
            if(av_read_frame(ifmt_ctx, &amp;pkt) &gt;= 0){  
                do{  
                    in_stream  = ifmt_ctx-&gt;streams[0];  
                    out_stream = ofmt_ctx[index]-&gt;streams[0];  

                    if(pkt.stream_index==0){  
                        //FIX：No PTS  
                        //Simple Write PTS  
                        AVRational time_base1=in_stream-&gt;time_base; 
                        AVRational time_base_q = { 1, AV_TIME_BASE }; 
                        double frame_size = (double)in_stream-&gt;codec-&gt;frame_size;
                        double sample_rate = (double)in_stream-&gt;codec-&gt;sample_rate;
                        //Duration between 2 frames (us)  
                        int64_t calc_duration=(double)(AV_TIME_BASE) * (frame_size/sample_rate);

                        pkt.pts = av_rescale_q(aframe_index*calc_duration, time_base_q, time_base1);
                        pkt.dts = pkt.pts; 
                        pkt.duration = av_rescale_q(calc_duration, time_base_q, time_base1);
                        pkt.pos = -1;

                        aframe_index++;  
                        cur_pts_a=pkt.pts;  
                        break;  
                    }  
                }while(av_read_frame(ifmt_ctx, &amp;pkt) &gt;= 0);  
            }else{  
                break;  
            }  

        } 

        //Convert PTS/DTS  
        pkt.pts = av_rescale_q_rnd(pkt.pts, in_stream-&gt;time_base, out_stream-&gt;time_base, (enum AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX));  
        pkt.dts = av_rescale_q_rnd(pkt.dts, in_stream-&gt;time_base, out_stream-&gt;time_base, (enum AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX));  
        pkt.duration = av_rescale_q(pkt.duration, in_stream-&gt;time_base, out_stream-&gt;time_base);  
        pkt.pos = -1;  

        if(index == 0)
            printf("Write video Packet. size:%5d\tpts:%lld\n",pkt.size,pkt.pts);  
        if(index == 1)
            printf("Write audio Packet. size:%5d\tpts:%lld\n",pkt.size,pkt.pts);  
        if (av_interleaved_write_frame(ofmt_ctx[index], &amp;pkt) &lt; 0) {  
            printf( "Error muxing packet\n");  
            break;  
        }  
        av_free_packet(&amp;pkt);  
    }
    //写文件尾（Write file trailer）  


    av_write_trailer(ofmt_ctx[0]);
    av_write_trailer(ofmt_ctx[1]);


end:  
    avformat_close_input(&amp;ifmt_ctx_v);
    avformat_close_input(&amp;ifmt_ctx_a);
    /* close output */  
    if (ofmt_ctx[0] &amp;&amp; !(ofmt_ctx[0]-&gt;oformat-&gt;flags &amp; AVFMT_NOFILE))  
        avio_close((ofmt_ctx[0])-&gt;pb);  
        avformat_free_context(ofmt_ctx[0]);  
    if (ofmt_ctx[1] &amp;&amp; !(ofmt_ctx[1]-&gt;oformat-&gt;flags &amp; AVFMT_NOFILE))  
        avio_close((ofmt_ctx[1])-&gt;pb);  
        avformat_free_context(ofmt_ctx[1]);  
    if (ret &lt; 0 &amp;&amp; ret != AVERROR_EOF) {  
        printf( "Error occurred.\n");  
        return -1;  
    }  
    return 0;  
} 
</code></pre>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/cebd021e5ca2fc9abf5392529453f6fb/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">FindBoost.cmake</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/56d1a5916f8a76f3c965a41bf9f96653/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">wifi信号扫描探测软件</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>