<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>knn实战：如何对手写数字进行识别？ - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="knn实战：如何对手写数字进行识别？" />
<meta property="og:description" content="在 Python 的 sklearn 工具包中有 KNN 算法。KNN 既可以做分类器，也可以做回归。
如果是做分类，你需要引用：
from sklearn.neighbors import KNeighborsClassifier 如果是做回归，你需要引用：
from sklearn.neighbors import KNeighborsRegressor 如何在 sklearn 中创建 KNN 分类器： 我们使用构造函数 KNeighborsClassifier(n_neighbors=5, weights=‘uniform’，algorithm=‘auto’, leaf_size=30)，这里有几个比较主要的参数：
n_neighbors 即 KNN 中的 K 值，代表的是邻居的数量。
k小过拟合，k大欠拟合；一般默认使用5
weights weights=uniform
代表所有邻居的权重相同 weights=distance
代表权重是距离的倒数，即与距离成反比自定义函数你可以自定义不同距离所对应的权重。 algorithm
algorithm=auto
根据数据的情况自动选择适合的算法，默认情况选择 auto algorithm=kd_tree
也叫作 KD 树，是多维空间的数据结构，方便对关键数据进行检索；不过 KD 树适用于维度少的情况，一般维数不超过 20，如果维数大于 20 之后，效率反而会下降； algorithm=ball_tree
也叫作球树，它和 KD 树一样都是多维空间的数据结果，不同于 KD 树，球树更适用于维度大的情况； algorithm=brute
也叫作暴力搜索，它和 KD 树不同的地方是在于采用的是线性扫描，，而不是通过构造树结构进行快速检索。 leaf_size
代表构造 KD 树或球树时的叶子数，默认是 30，调整 leaf_size 会影响到树的构造和搜索速度。 创建完 KNN 分类器之后，我们就可以输入训练集对它进行训练，这里我们使用 fit() 函数，传入训练集中的样本特征矩阵和分类标识，会自动得到训练好的 KNN 分类器。然后可以使用 predict() 函数来对结果进行预测，这里传入测试集的特征矩阵可以得到测试集的预测分类结果。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/6e07605a681d691e0e46338b9b9d0fc6/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-05-19T18:48:06+08:00" />
<meta property="article:modified_time" content="2019-05-19T18:48:06+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">knn实战：如何对手写数字进行识别？</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>在 Python 的 sklearn 工具包中有 KNN 算法。KNN 既可以做分类器，也可以做回归。</p> 
<p>如果是做分类，你需要引用：</p> 
<pre class="has"><code class="hljs language-python">from sklearn.neighbors import KNeighborsClassifier
</code></pre> 
<p>如果是做回归，你需要引用：</p> 
<pre class="has"><code class="hljs language-python">from sklearn.neighbors import KNeighborsRegressor

</code></pre> 
<p> </p> 
<p> </p> 
<h2>如何在 sklearn 中创建 KNN 分类器：</h2> 
<p>我们使用构造函数 KNeighborsClassifier(n_neighbors=5, weights=‘uniform’，algorithm=‘auto’, leaf_size=30)，这里有几个比较主要的参数：</p> 
<p> </p> 
<table border="1" cellpadding="1" cellspacing="1" style="width:500px;"><tbody><tr><td>n_neighbors</td><td colspan="2" rowspan="1"> <p>即 KNN 中的 K 值，代表的是邻居的数量。</p> <p>k小过拟合，k大欠拟合；一般默认使用5</p> </td></tr><tr><td colspan="1" rowspan="3">weights</td><td> <p>weights=uniform</p> <p> </p> </td><td>代表所有邻居的权重相同</td></tr><tr><td> <p>weights=distance</p> <p> </p> </td><td>代表权重是距离的倒数，即与距离成反比</td></tr><tr><td>自定义函数</td><td>你可以自定义不同距离所对应的权重。</td></tr><tr><td colspan="1" rowspan="4"> <p>algorithm</p> <p> </p> </td><td> <p>algorithm=auto</p> <p> </p> </td><td>根据数据的情况自动选择适合的算法，默认情况选择 auto</td></tr><tr><td> <p>algorithm=kd_tree</p> <p> </p> </td><td>也叫作 KD 树，是多维空间的数据结构，方便对关键数据进行检索；不过 KD 树适用于维度少的情况，一般维数不超过 20，如果维数大于 20 之后，效率反而会下降；</td></tr><tr><td> <p>algorithm=ball_tree</p> <p> </p> <p> </p> </td><td>也叫作球树，它和 KD 树一样都是多维空间的数据结果，不同于 KD 树，球树更适用于维度大的情况；</td></tr><tr><td> <p>algorithm=brute</p> <p> </p> </td><td>也叫作暴力搜索，它和 KD 树不同的地方是在于采用的是线性扫描，，而不是通过构造树结构进行快速检索。</td></tr><tr><td> <p>leaf_size</p> <p> </p> </td><td colspan="2" rowspan="1">代表构造 KD 树或球树时的叶子数，默认是 30，调整 leaf_size 会影响到树的构造和搜索速度。</td></tr></tbody></table> 
<p>创建完 KNN 分类器之后，我们就可以输入训练集对它进行训练，这里我们使用 fit() 函数，传入训练集中的样本特征矩阵和分类标识，会自动得到训练好的 KNN 分类器。然后可以使用 predict() 函数来对结果进行预测，这里传入测试集的特征矩阵可以得到测试集的预测分类结果。</p> 
<h2>如何用 KNN 对手写数字进行识别分类</h2> 
<p>手写数字数据集是个非常有名的用于图像识别的数据集。数字识别的过程就是将这些图片与分类结果 0-9 一一对应起来。</p> 
<p>完整的手写数字数据集 MNIST 里面包括了 60000 个训练样本，以及 10000 个测试样本。如果你学习深度学习的话，MNIST 基本上是你接触的第一个数据集。</p> 
<p>今天我们用 sklearn <strong>自带的手写数字数据集</strong>做 KNN 分类，它只包括了 1797 幅数字图像，每幅图像大小是 8*8 像素。</p> 
<p>训练分三个阶段：</p> 
<p>1、加载数据集；本地导入，线上调取，自带数据集；在这里，我们使用自带数据集；</p> 
<p>2、准备阶段：可视化数据描述：样本量多少，图像长啥样，输入输出特征；数据处理：缺失值处理，异常值处理、特征工程构造；</p> 
<p>3、分类阶段：通过训练可以得到分类器，然后用测试集进行准确率的计算。</p> 
<p> </p> 
<pre class="has"><code class="hljs language-python">#加载库
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_digits
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt


</code></pre> 
<pre class="has"><code class="hljs language-python"># 加载数据
digits = load_digits()
data = digits.data
# 数据探索
print(data.shape)
# 查看第一幅图像
print(digits.images[0])
# 第一幅图像代表的数字含义
print(digits.target[0])
# 将第一幅图像显示出来
plt.gray()
plt.imshow(digits.images[0])
plt.show()
</code></pre> 
<p>运行结果：</p> 
<p><img alt="" class="has" height="489" src="https://images2.imgbox.com/ff/f1/UQXsMmZP_o.png" width="731"></p> 
<p>我们对原始数据集中的第一幅进行数据可视化，可以看到图像是个 8*8 的像素矩阵，上面这幅图像是一个“0”，从训练集的分类标注中我们也可以看到分类标注为“0”。</p> 
<p>sklearn 自带的手写数字数据集一共包括了 1797 个样本，每幅图像都是 8*8 像素的矩阵。因为并没有专门的测试集，所以我们需要对数据集做划分，划分成训练集和测试集。</p> 
<p>因为 KNN 算法和距离定义相关，我们需要对数据进行规范化处理，采用 Z-Score 规范化：</p> 
<pre class="has"><code class="hljs language-python"># 分割数据，将 25% 的数据作为测试集，其余作为训练集（你也可以指定其他比例的数据作为训练集）
train_x, test_x, train_y, test_y = train_test_split(data, digits.target, test_size=0.25, random_state=33)
# 采用 Z-Score 规范化
ss = preprocessing.StandardScaler()
train_ss_x = ss.fit_transform(train_x)
test_ss_x = ss.transform(test_x)</code></pre> 
<blockquote> 
 <p>75%数据作为训练集；train_x作为训练集的输入特征值矩阵，train_y作为训练集的输出特征值；</p> 
 <p>对训练集与测试集中的输入特征值进行z评分归一化；（记住一定要对测试集输入特征进行同样的处理！以后谈论的变换默认都是对输入特征进行！）</p> 
 <p>fit_transform是fit和transform两个函数都执行一次。所以ss是进行了fit拟合的。只有在fit拟合之后，才能进行transform<br> 在进行test的时候，我们已经在train的时候fit过了，所以直接transform即可。<br> 另外，如果我们没有fit，直接进行transform会报错，因为需要先fit拟合，才可以进行transform。</p> 
</blockquote> 
<p> </p> 
<p>然后我们构造一个 KNN 分类器 knn，把训练集的数据传入构造好的 knn，并通过测试集进行结果预测，与测试集的结果进行对比，得到 KNN 分类器准确率，</p> 
<pre class="has"><code class="hljs language-python"># 创建 KNN 分类器
knn = KNeighborsClassifier() 
knn.fit(train_ss_x, train_y) 
predict_y = knn.predict(test_ss_x) 
print("KNN 准确率: %.4lf" % accuracy_score(predict_y, test_y))</code></pre> 
<blockquote> 
 <p>knn.fit(训练集输入特征,训练集输出特征）</p> 
 <p>knn.predict(测试集输入特征)=模型输出值</p> 
 <p>accquary_score（knn.predict(测试集输入特征)，测试集输出特征)</p> 
</blockquote> 
<p>KNN 准确率: 0.9756</p> 
<p>我们选用之前学过的几个模型，进行预测：</p> 
<pre class="has"><code class="hljs language-python"># 创建 SVM 分类器
svm = SVC()
svm.fit(train_ss_x, train_y)
predict_y=svm.predict(test_ss_x)
print('SVM 准确率: %0.4lf' % accuracy_score(predict_y, test_y))
# 采用 Min-Max 规范化
mm = preprocessing.MinMaxScaler()
train_mm_x = mm.fit_transform(train_x)
test_mm_x = mm.transform(test_x)
# 创建 Naive Bayes 分类器
mnb = MultinomialNB()
mnb.fit(train_mm_x, train_y) 
predict_y = mnb.predict(test_mm_x) 
print(" 多项式朴素贝叶斯准确率: %.4lf" % accuracy_score(predict_y, test_y))
# 创建 CART 决策树分类器
dtc = DecisionTreeClassifier()
dtc.fit(train_mm_x, train_y) 
predict_y = dtc.predict(test_mm_x) 
print("CART 决策树准确率: %.4lf" % accuracy_score(predict_y, test_y))</code></pre> 
<p>运行结果：</p> 
<pre>SVM 准确率: 0.9867
 多项式朴素贝叶斯准确率: 0.8844
CART 决策树准确率: 0.8356</pre> 
<p>这里需要注意的是，<strong>我们在做多项式朴素贝叶斯分类的时候，传入的数据不能有负数。</strong></p> 
<p>因为 Z-Score 会将数值规范化为一个标准的正态分布，即均值为 0，方差为 1，数值会包含负数。因此我们需要采用 Min-Max 规范化，将数据规范化到 [0,1] 范围内。</p> 
<blockquote> 
 <p>数据预处理：无量纲化处理（线性：均值化与标准化;非线性），降维</p> 
 <p>当输入特征接近正态分布，使用Z评分归一化；</p> 
 <p>当输入特征呈现高度偏斜，而我们模型对输入特征的要求是正态分布时，选用Box-Cox变换；</p> 
 <p>Z评分归一化的特点：变换结果均值为0，方差为1；变换时对数值进行平移和缩放的同时，保留了密度图的总体形态</p> 
 <p>Box-Cox变换：变换时对数值进行平移和缩放的同时，改变了整体形态，产生了比原始图偏斜更少的密度图。</p> 
 <p>降维：特征缩减，比如PCA-主成分分析</p> 
 <p>特征工程：根据原有的特征，设计新的特征（实际应用需要反复验证）</p> 
</blockquote> 
<p><img alt="" class="has" src="https://images2.imgbox.com/06/b7/UKFj1fBh_o.png"></p> 
<p>倘若同样的数据集，改变k值，会得出如下结论：</p> 
<blockquote> 
 <p><br> knn默认k值为5 准确率:0.9756<br> knn的k值为200的准确率:0.8489<br> SVM分类准确率:0.9867<br> 高斯朴素贝叶斯准确率:0.8111<br> 多项式朴素贝叶斯分类器准确率:0.8844<br> CART决策树准确率:0.8400<br><br> K值的选取如果过大，正确率降低。 <br> 算法效率排行 SVM &gt; KNN(k值在合适范围内) &gt;多项式朴素贝叶斯 &gt; CART &gt; 高斯朴素贝叶斯</p> 
</blockquote> 
<p>分别用 KNN、SVM、朴素贝叶斯和决策树做分类器，并统计了四个分类器的准确率。在数据量不大的情况下，使用 sklearn 还是方便的。</p> 
<p>如果数据量很大，比如 MNIST 数据集中的 6 万个训练数据和 1 万个测试数据，那么采用深度学习 +GPU 运算的方式会更适合。</p> 
<p><strong>因为深度学习的特点就是需要大量并行的重复计算，GPU 最擅长的就是做大量的并行计算。</strong></p> 
<p> </p> 
<p>总结：</p> 
<p>1、各模型对输入特征是有分布要求的；比如多<strong>项式朴素贝叶斯分类要求数据非负；最小二乘法模型要求数据是正态分布，满足四大假设；神经网络则对数据分布无要求。</strong></p> 
<p><strong>2、</strong>特征变换是针对输入特征的；特征变化是数据处理的子集，决定模型的上限，而模型的好坏只是逼近这个上限</p> 
<p>3、skearn适合数据量较小的训练，若是数据量过大，可以采用深度学习框架+GPU运算。实际运用中，可以使用集成学习（机器学习+深度学习框架）完成。</p> 
<p> </p> 
<p>参考：</p> 
<p>数据分析实战45讲</p> 
<p>用商业案例学R语言数据挖掘--特征工程</p> 
<p>数据预处理</p> 
<p> </p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/35/a8/xW9fVzbS_o.png"></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a64b70067ec37bef11794a6b36c5da85/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">迭代器的原理及源码解析</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/72b7f38bf1f0e76d80e56b79f5f2963a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">DANet：Dual Attention Network for Scene Segmentation论文解读和源代码详解</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>