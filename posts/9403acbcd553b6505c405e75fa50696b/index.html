<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>tensorflow 1.15 gpu docker环境搭建；Nvidia Docker容器基于TensorFlow1.15测试GPU；——全流程应用指南 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="tensorflow 1.15 gpu docker环境搭建；Nvidia Docker容器基于TensorFlow1.15测试GPU；——全流程应用指南" />
<meta property="og:description" content="前言: TensorFlow简介 TensorFlow 在新款 NVIDIA Pascal GPU 上的运行速度可提升高达 50%，并且能够顺利跨 GPU 进行扩展。 如今，训练模型的时间可以从几天缩短到几小时
TensorFlow 使用优化的 C&#43;&#43; 和 NVIDIA® CUDA® 工具包编写，使模型能够在训练和推理时在 GPU 上运行，从而大幅提速
TensorFlow GPU 支持需要多个驱动和库。为简化安装并避免库冲突，建议利用 GPU 支持的 TensorFlow Docker 镜像。此设置仅需要 NVIDIA GPU 驱动并且安装 NVIDIA Docker。用户可以从预配置了预训练模型和 TensorFlow 库支持的 NGC (NVIDIA GPU Cloud) 中提取容器
CPU擅长逻辑控制、串行计算，而GPU擅长高强度计算、并行计算。CUDA是NVIDIA推出用于自家GPU的并行计算框架，cuDNN &amp; tensorflow是一系列机器学习，深度学习库，用于训练机器学习、深度学习模型
2. 依赖环境准备
选取centos7.3作为基础操作系统镜像，选取适配驱动：Nvidia
GPU部署预装机器
深度学习框架：cuda、cudnn、tensorflow
由于cuda、cudnn、tensorflow等机器学习、深度学习框架，依赖python3，需要在centos7.3操作系统中集成python3
一、 nvidia-docker的安装cpu架构：x86 受够了TensorRT&#43;cuda&#43;opencv&#43;ffmpeg&#43;x264运行环境的部署的繁琐，每次新服务器上部署环境都会花费很大的精力去部署环境，听说nvidia-docker可以省去部署的麻烦，好多人也推荐使用docker方便部署，咱也在网上搜索了下，学习了下，根据网上的资料，开始安装docker学习一下，把学习记录记在这儿，听说要想使用GPU，就要安装Docker-CE和NVIDIA Container Toolkit，好的，开始。
1. 安装Dokcer-CE
首先，我的机器上没有安装过docker,要先把docker安装上,执行以下脚本，开始安装。
curl https://get.docker.com | sh \ &gt; &amp;&amp; sudo systemctl --now enable docker 安装结束后，查看Docker版本：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/9403acbcd553b6505c405e75fa50696b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-14T19:49:55+08:00" />
<meta property="article:modified_time" content="2023-11-14T19:49:55+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">tensorflow 1.15 gpu docker环境搭建；Nvidia Docker容器基于TensorFlow1.15测试GPU；——全流程应用指南</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_TensorFlow_0"></a>前言: TensorFlow简介</h2> 
<p>TensorFlow 在新款 NVIDIA Pascal GPU 上的运行速度可提升高达 50%，并且能够顺利跨 GPU 进行扩展。 如今，训练模型的时间可以从几天缩短到几小时</p> 
<p>TensorFlow 使用优化的 C++ 和 NVIDIA® CUDA® 工具包编写，使模型能够在训练和推理时在 GPU 上运行，从而大幅提速</p> 
<p>TensorFlow GPU 支持需要多个驱动和库。为简化安装并避免库冲突，建议利用 GPU 支持的 TensorFlow Docker 镜像。此设置仅需要 NVIDIA GPU 驱动并且安装 NVIDIA Docker。用户可以从预配置了预训练模型和 TensorFlow 库支持的 NGC (NVIDIA GPU Cloud) 中提取容器</p> 
<p>CPU擅长逻辑控制、串行计算，而GPU擅长高强度计算、并行计算。CUDA是NVIDIA推出用于自家GPU的并行计算框架，cuDNN &amp; tensorflow是一系列机器学习，深度学习库，用于训练机器学习、深度学习模型<br> <img src="https://images2.imgbox.com/d5/18/300lSfoi_o.png" alt="在这里插入图片描述"><br> 2. 依赖环境准备<br> 选取centos7.3作为基础操作系统镜像，选取适配驱动：Nvidia</p> 
<p>GPU部署预装机器</p> 
<p>深度学习框架：cuda、cudnn、tensorflow</p> 
<p>由于cuda、cudnn、tensorflow等机器学习、深度学习框架，依赖python3，需要在centos7.3操作系统中集成python3<br> <img src="https://images2.imgbox.com/e2/c1/gHsjS6OZ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/f9/66/pMjEEY4D_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_nvidiadockercpux86_22"></a>一、 nvidia-docker的安装cpu架构：x86</h2> 
<p>受够了TensorRT+cuda+opencv+ffmpeg+x264运行环境的部署的繁琐，每次新服务器上部署环境都会花费很大的精力去部署环境，听说nvidia-docker可以省去部署的麻烦，好多人也推荐使用docker方便部署，咱也在网上搜索了下，学习了下，根据网上的资料，开始安装docker学习一下，把学习记录记在这儿，听说要想使用GPU，就要安装Docker-CE和NVIDIA Container Toolkit，好的，开始。</p> 
<p><strong>1. 安装Dokcer-CE</strong><br> 首先，我的机器上没有安装过docker,要先把docker安装上,执行以下脚本，开始安装。</p> 
<pre><code class="prism language-python"> curl https<span class="token punctuation">:</span><span class="token operator">//</span>get<span class="token punctuation">.</span>docker<span class="token punctuation">.</span>com <span class="token operator">|</span> sh \
<span class="token operator">&gt;</span>   <span class="token operator">&amp;</span><span class="token operator">&amp;</span> sudo systemctl <span class="token operator">-</span><span class="token operator">-</span>now enable docker

</code></pre> 
<p>安装结束后，查看Docker版本：</p> 
<pre><code class="prism language-python">docker <span class="token operator">-</span><span class="token operator">-</span>version
</code></pre> 
<p>结果如下：</p> 
<blockquote> 
 <p>Docker version 20.10.16, build aa7e414</p> 
</blockquote> 
<p><strong>CentOS7下安装docker详细教程</strong></p> 
<p>当基于nvidia gpu开发的docker镜像在实际部署时，需要先安装nvidia docker。安装nvidia docker前需要先安装原生docker compose</p> 
<p>安装docker</p> 
<ol><li>Docker 要求 CentOS 系统的内核版本高于 3.10 ，查看本页面的前提条件来验证你的CentOS 版本是否支持 Docker 。</li></ol> 
<p>通过 <code>uname -r</code> 命令查看你当前的内核版本</p> 
<pre><code class="prism language-bash"><span class="token function">uname</span> <span class="token parameter variable">-r</span>
</code></pre> 
<pre><code class="prism language-bash"><span class="token function">uname</span> <span class="token parameter variable">-a</span>
</code></pre> 
<p>Linux gputest 3.10.0-1160.90.1.el7.x86_64 #1 SMP Thu May 4 15:21:22 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux</p> 
<pre><code class="prism language-bash"><span class="token function">uname</span> <span class="token parameter variable">-r</span>
</code></pre> 
<p>3.10.0-1160.90.1.el7.x86_64</p> 
<ol start="2"><li>使用 root 权限登录 Centos 确保 yum 包更新到最新</li></ol> 
<pre><code class="prism language-bash"><span class="token function">sudo</span> yum update
</code></pre> 
<ol start="3"><li>卸载旧版本(如果安装过旧版本的话)</li></ol> 
<pre><code class="prism language-bash">yum remove <span class="token function">docker</span> 
docker-client 
docker-client-latest 
docker-common 
docker-latest 
docker-latest-logrotate 
docker-logrotate 
docker-selinux 
docker-engine-selinux 
docker-engine
</code></pre> 
<ol start="4"><li>安装需要的软件包， yum-util 提供yum-config-manager功能，另外两个是devicemapper驱动依赖的</li></ol> 
<pre><code class="prism language-bash">yum <span class="token function">install</span> <span class="token parameter variable">-y</span> yum-utils device-mapper-persistent-data lvm2
</code></pre> 
<ol start="5"><li>设置yum源</li></ol> 
<pre><code class="prism language-bash">yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
</code></pre> 
<p><img src="https://images2.imgbox.com/fc/f0/wdfKkK49_o.png" alt="在这里插入图片描述"></p> 
<ol start="6"><li>可以查看所有仓库中所有docker版本，并选择特定版本安装</li></ol> 
<pre><code class="prism language-bash">yum list docker-ce <span class="token parameter variable">--showduplicates</span> <span class="token operator">|</span> <span class="token function">sort</span> <span class="token parameter variable">-r</span>
</code></pre> 
<ol start="7"><li>安装docker，版本号自选</li></ol> 
<pre><code class="prism language-bash">yum <span class="token function">install</span> docker-ce-17.12.0.ce
</code></pre> 
<ol start="8"><li>启动并加入开机启动</li></ol> 
<pre><code class="prism language-bash">systemctl start <span class="token function">docker</span>
systemctl status <span class="token function">docker</span>
systemctl <span class="token builtin class-name">enable</span> <span class="token function">docker</span>
</code></pre> 
<ol start="9"><li>验证安装是否成功(有client和service两部分表示docker安装启动都成功了)</li></ol> 
<pre><code class="prism language-bash"><span class="token function">docker</span> version
</code></pre> 
<p><img src="https://images2.imgbox.com/94/aa/TqbH8fXo_o.png" alt="在这里插入图片描述"></p> 
<p><strong>2. 安装NVIDIA Container Toolkit</strong><br> <img src="https://images2.imgbox.com/37/eb/Bo3iYf10_o.png" alt="在这里插入图片描述"></p> 
<p>执行以下脚本：</p> 
<pre><code class="prism language-bash"><span class="token assign-left variable">distribution</span><span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span><span class="token builtin class-name">.</span> /etc/os-release<span class="token punctuation">;</span><span class="token builtin class-name">echo</span> $ID$VERSION_ID<span class="token variable">)</span></span> <span class="token punctuation">\</span>
   <span class="token operator">&amp;&amp;</span> <span class="token function">curl</span> <span class="token parameter variable">-s</span> <span class="token parameter variable">-L</span> https://nvidia.github.io/nvidia-docker/gpgkey <span class="token operator">|</span> <span class="token function">sudo</span> apt-key <span class="token function">add</span> - <span class="token punctuation">\</span>
   <span class="token operator">&amp;&amp;</span> <span class="token function">curl</span> <span class="token parameter variable">-s</span> <span class="token parameter variable">-L</span> https://nvidia.github.io/nvidia-docker/<span class="token variable">$distribution</span>/nvidia-docker.list <span class="token operator">|</span> <span class="token function">sudo</span> <span class="token function">tee</span> /etc/apt/sources.list.d/nvidia-docker.list
</code></pre> 
<p>控制台输出如下：</p> 
<blockquote> 
 <p>[sudo] dingxin 的密码： OK deb<br> https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/<span class="katex--inline">KaTeX parse error: Expected 'EOF', got '#' at position 10: (ARCH) / #̲deb https://nvi…</span>(ARCH)<br> / deb<br> https://nvidia.github.io/nvidia-container-runtime/stable/ubuntu18.04/<span class="katex--inline">KaTeX parse error: Expected 'EOF', got '#' at position 10: (ARCH) / #̲deb https://nvi…</span>(ARCH)<br> / deb https://nvidia.github.io/nvidia-docker/ubuntu18.04/$(ARCH) /</p> 
</blockquote> 
<p>安装nvidia-docker2包及其依赖</p> 
<pre><code class="prism language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> update
</code></pre> 
<p>接着执行安装nvidia-docker2：</p> 
<pre><code class="prism language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> <span class="token parameter variable">-y</span> nvidia-docker2
</code></pre> 
<p><strong>CentOS7下安装NVIDIA-Docker</strong><br> 依赖条件<br> 如果使用的 Tensorflow 版本大于 1.4.0，要求 CUDA 9.0 以上版本</p> 
<p>基于docker的测试环境的建立</p> 
<p>测试环境基于docker构建，需要Nvidia GPU驱动的支持（不需要安装CUDA），安装好GPU驱动和docker以后，下载最新的包含tensorflow，CUDA，cudnn等的image，然后就可以运行tf_cnn_benchmark了</p> 
<ol><li>下载nvidia-docker安装包</li></ol> 
<pre><code class="prism language-bash">$ <span class="token function">wget</span> https://github.com/NVIDIA/nvidia-docker/releases/download/v1.0.1/nvidia-docker-1.0.1-1.x86_64.rpm
</code></pre> 
<ol start="2"><li>安装nvidia-docker</li></ol> 
<pre><code class="prism language-bash">$ <span class="token function">rpm</span> <span class="token parameter variable">-ivh</span> nvidia-docker-1.0.1-1.x86_64.rpm
</code></pre> 
<ol start="3"><li>启动 nvidia-docker 服务</li></ol> 
<pre><code class="prism language-bash">$ <span class="token function">sudo</span> systemctl restart nvidia-docker
</code></pre> 
<ol start="4"><li>执行以下命令，若结果显示 active(running) 则说明启动成功</li></ol> 
<pre><code class="prism language-bash">$ systemctl status nvidia-docker.service
</code></pre> 
<p>Active: active (running) since Fri 2023-07-21 11:15:45 CST; 1min ago</p> 
<p><img src="https://images2.imgbox.com/74/e2/17ompF0O_o.png" alt="在这里插入图片描述"><br> 5. 使用 nvidia-docker查看 GPU 信息</p> 
<pre><code class="prism language-bash"> $ nvidia-docker run <span class="token parameter variable">--rm</span> nvidia/cuda nvidia-smi
</code></pre> 
<h2><a id="_216"></a>二、镜像安装</h2> 
<p><strong>1. cuda 11</strong>下的安装（可选）</p> 
<pre><code class="prism language-bash"><span class="token function">sudo</span> <span class="token function">docker</span> run <span class="token parameter variable">--rm</span> <span class="token parameter variable">--gpus</span> all nvidia/cuda:11.0-base nvidia-smi
</code></pre> 
<p><img src="https://images2.imgbox.com/34/ac/op4YXLq4_o.png" alt="在这里插入图片描述"><br> 查看已下载的镜像</p> 
<pre><code class="prism language-bash"><span class="token function">sudo</span> <span class="token function">docker</span> images <span class="token parameter variable">-a</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/3b/6b/fewaY6QD_o.png" alt="在这里插入图片描述"></p> 
<p><strong>2. 下载tensorflow v1.15.5版本的镜像</strong><br> 官网下载:<br> <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorflow/tags" rel="nofollow">https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorflow/tags</a><br> 大概1.5小时</p> 
<p>安装testflow1.0版本（向下兼容）</p> 
<pre><code class="prism language-bash"><span class="token function">docker</span> pull nvcr.io/nvidia/tensorflow:23.03-tf1-py3
</code></pre> 
<p><img src="https://images2.imgbox.com/b8/7e/kWxQyM5x_o.png" alt="在这里插入图片描述"></p> 
<p>再次查看下载的镜像</p> 
<pre><code class="prism language-bash"><span class="token function">docker</span> image <span class="token function">ls</span>
</code></pre> 
<p>image id = fc14c7fdf361为上述安装的tensorflow1.15版本容器</p> 
<p><img src="https://images2.imgbox.com/46/91/gMTxRQVR_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="tensorflow_252"></a>三、操作tensorflow容器</h2> 
<pre><code class="prism language-bash">nvidia-docker run <span class="token parameter variable">-it</span> nvcr.io/nvidia/tensorflow:23.03-tf1-py3
</code></pre> 
<p>格式：nvidia-docker run -it {REPOSITORY容器名称:TAG号}<br> <img src="https://images2.imgbox.com/de/34/iUFA2x7r_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-bash">pip list<span class="token operator">|</span><span class="token function">grep</span> tensor
</code></pre> 
<blockquote> 
 <p>jupyter-tensorboard 0.2.0<br> tensorboard 1.15.0<br> tensorflow 1.15.5+nv23.3<br> tensorflow-estimator 1.15.1<br> tensorrt 8.5.3.1</p> 
</blockquote> 
<p>测试脚本：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> os
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'TF_CPP_MIN_LOG_LEVEL'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'2'</span>
 
<span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
hello <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token string">'--------Hello, TensorFlow!----------'</span><span class="token punctuation">)</span>
sess <span class="token operator">=</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span>
sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>hello<span class="token punctuation">)</span>
</code></pre> 
<p>在container OS中使用命令cat /proc/driver/nvidia/version或nvcc --version可正常显示显卡驱动版本及CUDA版本</p> 
<p><img src="https://images2.imgbox.com/90/9d/DoNQJZRG_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="git_287"></a>四、配置git</h2> 
<ol><li>在本机生成公私钥<code>ssh-keygen -t rsa -b 4096 -C "xx@xx.com"</code> 默认生成的公私钥 ~/.ssh/</li></ol> 
<p>id_rsa.pub</p> 
<p>id_rsa</p> 
<p>-b 4096：b是bit的缩写</p> 
<p>-b 指定密钥长度。对于RSA密钥，最小要求768位，默认是2048位。命令中的4096指的是RSA密钥长度为4096位。</p> 
<p>DSA密钥必须恰好是1024位(FIPS 186-2 标准的要求)</p> 
<blockquote> 
 <p>Generating public/private rsa key pair. Enter file in which to save<br> the key (/Users/qa/.ssh/id_rsa): yes Enter passphrase (empty for no<br> passphrase): Enter same passphrase again: Your identification has been<br> saved in yes. Your public key has been saved in yes.pub. The key<br> fingerprint is: SHA256:MGbV/xx/xx lishan12@xx.com The key’s randomart<br> image is:<br> ±–[RSA 4096]----+ | …OBB=Eo| | . .O+oO=o=| | = .o*+B *o.| | o o o+B =… | | S.+o . | | . o |<br> | . . | | . . | | . |<br> ±—[SHA256]-----+</p> 
</blockquote> 
<ol start="2"><li>配置登录git的username email。为公司给你分配的用户名 密码</li></ol> 
<p><strong>第一步：</strong></p> 
<pre><code class="prism language-bash"><span class="token function">git</span> config <span class="token parameter variable">--global</span> user.name <span class="token string">'username'</span>
<span class="token function">git</span> config <span class="token parameter variable">--global</span> user.email <span class="token string">'username@xx.com'</span>
</code></pre> 
<p><strong>第二步：</strong> 设置永久保存</p> 
<pre><code class="prism language-bash"><span class="token function">git</span> config <span class="token parameter variable">--global</span> credential.helper store 
</code></pre> 
<p><strong>第三步</strong>：手动输入一次用户名和密码，GIT会自动保存密码，下次无须再次输入</p> 
<pre><code class="prism language-bash"><span class="token function">git</span> pull
</code></pre> 
<ol start="3"><li> <p>初始化仓库 <code>git init</code></p> </li><li> <p>拉取代码 <code>git clone git@gitlab.xx.com:xx/xx.git</code><br> Cloning into ‘xx-xx’…<br> git@gitlab.xx.com’s password:<br> Permission denied, please try again.<br> git@gitlab.xx.com’s password:</p> </li></ol> 
<p>遇到的问题：没有出username 和 password成对的输入项 ，而是出了password输入项</p> 
<p>都不知道密码是啥，跟登录git库的密码不一样。</p> 
<p>然后使用http的方式，报一个错误：</p> 
<p>use:~/ecox # <code>git clone https://vcs.in.ww-it.cn/ecox/ecox.git</code></p> 
<p>正克隆到 ‘ecox’…</p> 
<p>fatal: unable to access ‘https://vcs.in.ww-it.cn/ecox/ecox.git/’: SSL certificate problem: unable to get local issuer certificate</p> 
<p>提示SSL证书错误。发现说这个错误并不重要是系统证书的问题，系统判断到这个行为会造成不良影响，所以进行了阻止，只要设置跳过SSL证书验证就可以了，那么用命令 ：</p> 
<pre><code class="prism language-bash"><span class="token function">git</span> config <span class="token parameter variable">--global</span> http.sslVerify <span class="token boolean">false</span>
</code></pre> 
<h2><a id="Benchmarks_357"></a>五、下载Benchmarks源码并运行</h2> 
<p>从 TensorFlow 的 Github 仓库上下载 TensorFlow Benchmarks，可以通过以下命令来下载。<strong>非常重要的参考代码：</strong></p> 
<p><a href="https://github.com/tensorflow/benchmarks">https://github.com/tensorflow/benchmarks</a></p> 
<p>我的 - settings -SSH and GPG Keys 添加公钥id_rsa.pub</p> 
<p>拉取代码</p> 
<pre><code class="prism language-bash"><span class="token function">git</span> clone git@github.com:tensorflow/benchmarks.git
</code></pre> 
<p>git同步远程分支到本地，拉取tensorflow对应版本的分支</p> 
<p>git fetch origin 远程分支名xxx:本地分支名xxx<br> 使用这种方式会在本地仓库新建分支xxx，但是并不会自动切换到新建的分支xxx，需要手动checkout，当然了远程分支xxx的代码也拉取到了本地分支xxx中。采用这种方法建立的本地分支不会和远程分支建立映射关系</p> 
<p>root@818d19092cdc:/gpu/benchmarks# <code>git checkout -b tf1.15 origin/cnn_tf_v1.15_compatible</code></p> 
<p><img src="https://images2.imgbox.com/42/2b/uGJH5wjZ_o.png" alt="在这里插入图片描述"><br> <strong>运行不同模型</strong><br> root@818d19092cdc:/gpu/benchmarks/scripts/tf_cnn_benchmarks# <code>pwd</code><br> /gpu/benchmarks/scripts/tf_cnn_benchmarks<br> root@818d19092cdc:/gpu/benchmarks/scripts/tf_cnn_benchmarks#</p> 
<pre><code class="prism language-bash">python3 tf_cnn_benchmarks.py
</code></pre> 
<p>真实操作：</p> 
<p>[root@gputest ~]# <code>docker ps</code></p> 
<p>进入CONTAINER ID containerid</p> 
<p>[root@gputest ~]# <code>nvidia-docker exec -it 818d19092cdc /bin/bash</code></p> 
<p>新开窗口</p> 
<p>[root@gputest ~]# <code>nvidia-smi -l 3</code></p> 
<p>该命令将3秒钟输出一次GPU的状态和性能，可以通过查看输出结果来得出GPU的性能指标</p> 
<p><img src="https://images2.imgbox.com/b0/98/pR5na09b_o.png" alt="在这里插入图片描述"></p> 
<p><strong>一、resnet50模型</strong></p> 
<pre><code class="prism language-bash">python3 tf_cnn_benchmarks.py <span class="token parameter variable">--num_gpus</span><span class="token operator">=</span><span class="token number">1</span> <span class="token parameter variable">--batch_size</span><span class="token operator">=</span><span class="token number">2</span> <span class="token parameter variable">--model</span><span class="token operator">=</span>resnet50 <span class="token parameter variable">--variable_update</span><span class="token operator">=</span>parameter_server
</code></pre> 
<p><img src="https://images2.imgbox.com/e5/7c/uufD8qJk_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>Running warm up<br> 2023-07-21 09:50:55.398126: I tensorflow/stream_executor/platform/default/dso_loader.cc:50] Successfully opened dynamic library libcublas.so.12<br> 2023-07-21 09:50:55.533068: I tensorflow/stream_executor/platform/default/dso_loader.cc:50] Successfully opened dynamic library libcudnn.so.8<br> Done warm up<br> Step Img/sec total_loss<br> 1 images/sec: 10.1 +/- 0.0 (jitter = 0.0) 7.695<br> 10 images/sec: 10.7 +/- 0.1 (jitter = 0.1) 8.022<br> 20 images/sec: 10.7 +/- 0.1 (jitter = 0.2) 7.269<br> 30 images/sec: 10.7 +/- 0.1 (jitter = 0.2) 7.889<br> 40 images/sec: 10.7 +/- 0.1 (jitter = 0.2) 8.842<br> 50 images/sec: 10.6 +/- 0.1 (jitter = 0.2) 6.973<br> 60 images/sec: 10.6 +/- 0.1 (jitter = 0.2) 8.124<br> 70 images/sec: 10.6 +/- 0.0 (jitter = 0.2) 7.644<br> 80 images/sec: 10.6 +/- 0.0 (jitter = 0.2) 7.866<br> 90 images/sec: 10.6 +/- 0.0 (jitter = 0.3) 7.687<br> 100 images/sec: 10.6 +/- 0.0 (jitter = 0.3) 8.779<br> ----------------------------------------------------------------total images/sec: 10.63</p> 
</blockquote> 
<p><strong>二、vgg16模型</strong></p> 
<pre><code class="prism language-bash">python3 tf_cnn_benchmarks.py <span class="token parameter variable">--num_gpus</span><span class="token operator">=</span><span class="token number">1</span> <span class="token parameter variable">--batch_size</span><span class="token operator">=</span><span class="token number">2</span> <span class="token parameter variable">--model</span><span class="token operator">=</span>vgg16 <span class="token parameter variable">--variable_update</span><span class="token operator">=</span>parameter_server
</code></pre> 
<p><img src="https://images2.imgbox.com/da/ca/YGvJxm6R_o.png" alt="在这里插入图片描述"></p> 
<p>由于阿里云服务器申请的是2个G显存，所以只能跑size=1 2 和 4 ，超出会吐核</p> 
<p>已放弃(吐核)–linux 已放弃(吐核) (core dumped) 问题分析</p> 
<p>出现这种问题一般是下面这几种情况：</p> 
<ul><li> <p>1.内存越界</p> <p>2.使用了非线程安全的函数</p> <p>3.全局数据未加锁保护</p> <p>4.非法指针</p> <p>5.堆栈溢出</p> </li></ul> 
<p>也就是需要检查访问的内存、资源。</p> 
<p>可以使用 strace 命令来进行分析</p> 
<p>在程序的运行命令前加上 strace，在程序出现：已放弃（吐核），终止运行后，就可以通过 strace 打印在控制台的跟踪信息进行分析和定位问题</p> 
<p>方法2：docker启动普通镜像的Tensorflow</p> 
<pre><code class="prism language-bash">$ <span class="token function">docker</span> pull tensorflow/tensorflow:1.8.0-gpu-py3
$ <span class="token function">docker</span> tag tensorflow/tensorflow:1.8.0-gpu-py3 tensorflow:1.8.0-gpu
</code></pre> 
<p>nvidia-docker run -it -p 8888:8888 tensorflow:1.8.0-gpu</p> 
<pre><code class="prism language-bash">$ nvidia-docker run <span class="token parameter variable">-it</span> <span class="token parameter variable">-p</span> <span class="token number">8033</span>:8033 tensorflow:1.8.0-gpu
</code></pre> 
<p>浏览器进入指定 URL(见启动终端回显) 就可以利用 IPython Notebook 使用 tensorflow</p> 
<p><img src="https://images2.imgbox.com/04/04/49t63luW_o.png" alt="在这里插入图片描述"></p> 
<p>评测指标</p> 
<ul><li> <p>训练时间：在指定数据集上训练模型达到指定精度目标所需的时间</p> </li><li> <p>吞吐：单位时间内训练的样本数</p> </li><li> <p>加速效率：加速比/设备数*100%。其中，加速比定义为多设备吞吐数较单设备的倍数</p> </li><li> <p>成本：在指定数据集上训练模型达到指定精度目标所需的价格</p> </li><li> <p>功耗：在指定数据集上训练模型达到指定精度目标所需的功耗</p> </li></ul> 
<p>在初版评测指标设计中，我们重点关注训练时间、吞吐和加速效率三项</p> 
<h2><a id="_494"></a>六、保存镜像的修改</h2> 
<p>执行以下命令，保存TensorFlow镜像的修改</p> 
<pre><code class="prism language-bash"><span class="token function">docker</span> commit   <span class="token parameter variable">-m</span> <span class="token string">"commit docker"</span> CONTAINER_ID  nvcr.io/nvidia/tensorflow:18.03-py3
<span class="token comment"># CONTAINER_ID可通过docker ps命令查看。</span>
</code></pre> 
<blockquote> 
 <p>[root@gputest ~]# docker commit -m “commit docker” 818d19092cdc nvcr.io/nvidia/tensorflow:23.03-tf1-py3<br> sha256:fc14c7fdf361308817161d5d0cc018832575e7f2def99fe49876d2a41391c52c</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/c5/d0/R1KvfDzH_o.png" alt="在这里插入图片描述"></p> 
<p>查看docker进程</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>root@gputest ~<span class="token punctuation">]</span><span class="token comment"># docker ps</span>
</code></pre> 
<p>重新进入CONTAINER ID containerid</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>root@gputest ~<span class="token punctuation">]</span><span class="token comment"># nvidia-docker exec -it 818d19092cdc /bin/bash</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/04/bf/KsIU2xnf_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="benchmarks__519"></a>七、benchmarks 支持的所有参数</h2> 
<table><tbody><tr><td colspan="1" rowspan="1"> <p>参数名称</p> </td><td colspan="1" rowspan="1"> <p>描述</p> </td><td colspan="1" rowspan="1"> <p>备注</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--help</p> </td><td colspan="1" rowspan="1"> <p>查看帮助信息</p> </td><td colspan="1" rowspan="1"> <p></p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--backend</p> </td><td colspan="1" rowspan="1"> <p>使用的框架名称，如TensorFlow，PyTorch等，必须指定</p> </td><td colspan="1" rowspan="1"> <p>当前只支持TensorFlow，后续会增加对PyTorch的支持</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--model</p> </td><td colspan="1" rowspan="1"> <p>使用的模型名称，如alexnet、resnet50等，必须指定</p> </td><td colspan="1" rowspan="1"> <p>请查阅所有支持的模型</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--batch_size</p> </td><td colspan="1" rowspan="1"> <p>batch size大小</p> </td><td colspan="1" rowspan="1"> <p>默认值为32</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--num_epochs</p> </td><td colspan="1" rowspan="1"> <p>epoch的数量</p> </td><td colspan="1" rowspan="1"> <p>默认值为1</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--num_gpus</p> </td><td colspan="1" rowspan="1"> <p>使用的GPU数量。设置为0时，仅使用CPU。</p> 
    <ul><li> <p>在单机多卡模式下，指定每台机器使用的GPU数量；</p> </li><li> <p>在multi-worker模式下，指定每个worker使用的GPU数量</p> </li></ul></td><td colspan="1" rowspan="1"> <p></p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--data_dir</p> </td><td colspan="1" rowspan="1"> <p>输入数据的目录，对于CV任务，当前仅支持ImageNet数据集；如果没有指定，表明使用合成数据</p> </td><td colspan="1" rowspan="1"> <p></p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--do_train</p> </td><td colspan="1" rowspan="1"> <p>执行训练过程</p> </td><td colspan="1" rowspan="3"> <p>这三个选项必须指定其中的至少一个，可以同时指定多个选项。</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--do_eval</p> </td><td colspan="1" rowspan="1"> <p>执行evaluation过程</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--do_predict</p> </td><td colspan="1" rowspan="1"> <p>执行预测过程</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--data_format</p> </td><td colspan="1" rowspan="1"> <p>使用的数据格式，NCHW或NHWC，默认为NCHW。</p> 
    <ul><li> <p>对于CPU设备，建议使用NHWC格式</p> </li><li> <p>对于GPU设备，建议使用NCHW格式</p> </li></ul></td><td colspan="1" rowspan="1"> <p></p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--optimizer</p> </td><td colspan="1" rowspan="1"> <p>所使用的优化器，当前支持SGD、Adam和Momentum，默认为SGD</p> </td><td colspan="1" rowspan="1"> <p></p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--init_learning_rate</p> </td><td colspan="1" rowspan="1"> <p>使用的初始learning rate的值</p> </td><td colspan="1" rowspan="1"> <p></p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--num_epochs_per_decay</p> </td><td colspan="1" rowspan="1"> <p>learning rate decay的epoch间隔</p> </td><td colspan="1" rowspan="2"> <p>如果设置，这两项必须同时指定</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--learning_rate_decay_factor</p> </td><td colspan="1" rowspan="1"> <p>每次learning rate执行decay的因子</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--minimum_learning_rate</p> </td><td colspan="1" rowspan="1"> <p>最小的learning rate值</p> </td><td colspan="1" rowspan="1"> <p>如果设置，需要同时指定面的两项</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--momentum</p> </td><td colspan="1" rowspan="1"> <p>momentum参数的值</p> </td><td colspan="1" rowspan="1"> <p>用于设置momentum optimizer</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--adam_beta1</p> </td><td colspan="1" rowspan="1"> <p>adam_beta1参数的值</p> </td><td colspan="1" rowspan="3"> <p>用于设置Adam</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--adam_beta2</p> </td><td colspan="1" rowspan="1"> <p>adam_beta2参数的值</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--adam_epsilon</p> </td><td colspan="1" rowspan="1"> <p>adam_epsilon参数的值</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--use_fp16</p> </td><td colspan="1" rowspan="1"> <p>是否设置tensor的数据类型为float16</p> </td><td colspan="1" rowspan="1"> <p></p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--fp16_vars</p> </td><td colspan="1" rowspan="1"> <p>是否将变量的数据类型设置为float16。如果没有设置，变量存储为float32类型，并在使用时转换为fp16格式。</p> <p>建议：不要设置</p> </td><td colspan="1" rowspan="1"> <p>必须同时设置--use_fp16</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--all_reduce_spec</p> </td><td colspan="1" rowspan="1"> <p>使用的AllReduce方式</p> </td><td colspan="1" rowspan="1"> <p></p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--save_checkpoints_steps</p> </td><td colspan="1" rowspan="1"> <p>间隔多少step存储一次checkpoint</p> </td><td colspan="1" rowspan="1"> <p></p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--max_chkpts_to_keep</p> </td><td colspan="1" rowspan="1"> <p>保存的checkpoint的最大数量</p> </td><td colspan="1" rowspan="1"> <p></p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--ip_list</p> </td><td colspan="1" rowspan="1"> <p>集群中所有机器的IP地址，以逗号分隔</p> </td><td colspan="1" rowspan="3"> <p>用于多机分布式训练</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--job_name</p> </td><td colspan="1" rowspan="1"> <p>任务名称，如‘ps'、’worker‘</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--job_index</p> </td><td colspan="1" rowspan="1"> <p>任务的索引，如0，1等</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--model_dir</p> </td><td colspan="1" rowspan="1"> <p>checkpoint的存储目录</p> </td><td colspan="1" rowspan="1"> <p></p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--init_checkpoint</p> </td><td colspan="1" rowspan="1"> <p>初始模型checkpoint的路径，用于在训练前加载该checkpoint，进行finetune等</p> </td><td colspan="1" rowspan="1"> <p></p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--vocab_file</p> </td><td colspan="1" rowspan="1"> <p>vocabulary文件</p> </td><td colspan="1" rowspan="1"> <p>用于NLP</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--max_seq_length</p> </td><td colspan="1" rowspan="1"> <p>输入训练的最大长度</p> </td><td colspan="1" rowspan="1"> <p>用于NLP</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--param_set</p> </td><td colspan="1" rowspan="1"> <p>创建和训练模型时使用的参数集。</p> </td><td colspan="1" rowspan="3"> <p>用于Transformer</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--blue_source</p> </td><td colspan="1" rowspan="1"> <p>包含text translate的源文件，用于计算BLEU分数</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--blue_ref</p> </td><td colspan="1" rowspan="1"> <p>包含text translate的源文件，用于计算BLEU分数</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--task_name</p> </td><td colspan="1" rowspan="1"> <p>任务的名称，如MRPC，CoLA等</p> </td><td colspan="1" rowspan="2"> <p>用于Bert</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--do_lower_case</p> </td><td colspan="1" rowspan="1"> <p>是否为输入文本使用小写</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--train_file</p> </td><td colspan="1" rowspan="1"> <p>训练使用的SQuAD文件，如train-v1.1.json</p> </td><td colspan="1" rowspan="8"> <p>用于Bert模型，运行SQuAD， --run_squad必须指定</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--predict_file</p> </td><td colspan="1" rowspan="1"> <p>预测所使用的SQuAD文件，如dev-v1.1.json或test-v1.1.json</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--doc_stride</p> </td><td colspan="1" rowspan="1"> <p>当将长文档切分为块时，块之间取的间距大小</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--max_query_length</p> </td><td colspan="1" rowspan="1"> <p>问题包含的最大token数。当问题长度超过该值时，问题将被截断到这一长度。</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--n_best_size</p> </td><td colspan="1" rowspan="1"> <p>nbest_predictions.json输出文件中生成的n-best预测的总数</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--max_answer_length</p> </td><td colspan="1" rowspan="1"> <p>生成的回答的最大长度</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--version_2_with_negative</p> </td><td colspan="1" rowspan="1"> <p>如果为True，表明SQuAD样本中含有没有答案（answer）的问题</p> </td></tr><tr><td colspan="1" rowspan="1"> <p>--run_squad</p> </td><td colspan="1" rowspan="1"> <p>如果为True，运行SQUAD任务，否则，运行sequence （sequence-pair）分类任务</p> </td></tr></tbody></table> 
<h2><a id="GPU_525"></a>八、GPU使用注意事项</h2> 
<p><strong>1. 如何在tensorflow中指定使用GPU资源</strong></p> 
<p>在配置好GPU环境的TensorFlow中 ，如果操作没有明确地指定运行设备，那么TensorFlow会优先选择GPU。在默认情况下，TensorFlow只会将运算优先放到/gpu:0上。如果需要将某些运算放到不同的GPU或者CPU上，就需要通过tf.device来手工指定</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
 
<span class="token comment"># 通过tf.device将运算指定到特定的设备上。</span>
<span class="token keyword">with</span> tf<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'/cpu:0'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
   a <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">,</span> <span class="token number">3.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'a'</span><span class="token punctuation">)</span>
   b <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">,</span> <span class="token number">3.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'b'</span><span class="token punctuation">)</span>
<span class="token keyword">with</span> tf<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'/gpu:1'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    c <span class="token operator">=</span> a <span class="token operator">+</span> b
 
sess <span class="token operator">=</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span>config<span class="token operator">=</span>tf<span class="token punctuation">.</span>ConfigProto<span class="token punctuation">(</span>log_device_placement<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span> sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>c<span class="token punctuation">)</span>
</code></pre> 
<p><strong>2. 虚拟化使用GPU的方案</strong></p> 
<p><img src="https://images2.imgbox.com/66/d2/ETmULYIw_o.png" alt="在这里插入图片描述"><br> 通过KVM虚拟化实例使用CPU和内存等资源，GPU不参与虚拟化。不同容器共享使用物理GPU资源</p> 
<p><strong>3. 分布式TensorFlow</strong></p> 
<pre><code class="prism language-python"><span class="token comment">#coding=utf-8  </span>
<span class="token comment">#多台机器，每台机器有一个显卡、或者多个显卡，这种训练叫做分布式训练  </span>
<span class="token keyword">import</span>  tensorflow <span class="token keyword">as</span> tf  
<span class="token comment">#现在假设我们有A、B、C、D四台机器，首先需要在各台机器上写一份代码，并跑起来，各机器上的代码内容大部分相同  </span>
<span class="token comment"># ，除了开始定义的时候，需要各自指定该台机器的task之外。以机器A为例子，A机器上的代码如下：  </span>
cluster<span class="token operator">=</span>tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>ClusterSpec<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span>  
    <span class="token string">"worker"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>  
        <span class="token string">"A_IP:2222"</span><span class="token punctuation">,</span><span class="token comment">#格式 IP地址：端口号，第一台机器A的IP地址 ,在代码中需要用这台机器计算的时候，就要定义：/job:worker/task:0  </span>
        <span class="token string">"B_IP:1234"</span><span class="token comment">#第二台机器的IP地址 /job:worker/task:1  </span>
        <span class="token string">"C_IP:2222"</span><span class="token comment">#第三台机器的IP地址 /job:worker/task:2  </span>
    <span class="token punctuation">]</span><span class="token punctuation">,</span>  
    <span class="token string">"ps"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>  
        <span class="token string">"D_IP:2222"</span><span class="token punctuation">,</span><span class="token comment">#第四台机器的IP地址 对应到代码块：/job:ps/task:0  </span>
    <span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> 
<p>使用分布式的TensorFlow比较容易。只需在集群服务器中为 worker 节点分配带名字的IP。 然后 就可以手动或者自动为 worker 节点分配操作任务</p> 
<p></p> 
<p><strong>. GPU 显存资源监控</strong><br> 一个Server端的外挂模块，提供任务特征到资源特征的映射数据集，方便后续预测模型构建以及对芯片资源能力的定义</p> 
<p>利用 <code>with tf.device("{device-name}")</code> 这种写法，可以将with statement代码块中的变量或者op指定分配到该设备上。 在上面例子中，变量 W 和 b 就被分配到 /cpu:0 这个设备上。注意，如果一个变量被分配到一个设备上，读取这个变量也就要从这个设备读取，写入这个变量也将会写入到这个设备。 而 output （也就是一个 tf.matmul 矩阵乘法的计算操作，跟着一个tensor的加法的计算操作），以及后面的 loss 的计算（即对 output 调用了 f 这个函数，该函数中可能还有很多逻辑，涉及很多tensor运算的op），分配给了 <code>/gpu:0</code> 这个设备。</p> 
<p>基本原则：变量放到CPU，计算放到GPU。</p> 
<p>这时，TensorFlow实际上会将代码中定义的Graph（计算图）分割，根据指定的device placement将图的不同部分分配到不同的设备上，并且在设备间建立通信（如DMA，Direct Memory Access）。这些都不需要在应用代码层面操作。</p> 
<p>单机多卡<br> 当我们在一台机器上有多个GPU可用时，要利用多个GPU，代码编写方式的示意如下：</p> 
<pre><code class="prism language-python"><span class="token comment"># Calculate the gradients for each model tower.</span>
tower_grads <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>get_variable_scope<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">xrange</span><span class="token punctuation">(</span>FLAGS<span class="token punctuation">.</span>num_gpus<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'/gpu:%d'</span> <span class="token operator">%</span> i<span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token keyword">with</span> tf<span class="token punctuation">.</span>name_scope<span class="token punctuation">(</span><span class="token string">'%s_%d'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>cifar10<span class="token punctuation">.</span>TOWER_NAME<span class="token punctuation">,</span> i<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">as</span> scope<span class="token punctuation">:</span>
        <span class="token comment"># Dequeues one batch for the GPU</span>
        image_batch<span class="token punctuation">,</span> label_batch <span class="token operator">=</span> batch_queue<span class="token punctuation">.</span>dequeue<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># Calculate the loss for one tower of the CIFAR model. This function</span>
        <span class="token comment"># constructs the entire CIFAR model but shares the variables across</span>
        <span class="token comment"># all towers.</span>
        loss <span class="token operator">=</span> tower_loss<span class="token punctuation">(</span>scope<span class="token punctuation">,</span> image_batch<span class="token punctuation">,</span> label_batch<span class="token punctuation">)</span>
 
        <span class="token comment"># Reuse variables for the next tower.</span>
        tf<span class="token punctuation">.</span>get_variable_scope<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reuse_variables<span class="token punctuation">(</span><span class="token punctuation">)</span>
 
        <span class="token comment"># Retain the summaries from the final tower.</span>
        summaries <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_collection<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>GraphKeys<span class="token punctuation">.</span>SUMMARIES<span class="token punctuation">,</span> scope<span class="token punctuation">)</span>
 
        <span class="token comment"># Calculate the gradients for the batch of data on this CIFAR tower.</span>
        grads <span class="token operator">=</span> opt<span class="token punctuation">.</span>compute_gradients<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
 
        <span class="token comment"># Keep track of the gradients across all towers.</span>
        tower_grads<span class="token punctuation">.</span>append<span class="token punctuation">(</span>grads<span class="token punctuation">)</span>
 
<span class="token comment"># We must calculate the mean of each gradient. Note that this is the</span>
<span class="token comment"># synchronization point across all towers.</span>
grads <span class="token operator">=</span> average_gradients<span class="token punctuation">(</span>tower_grads<span class="token punctuation">)</span>
</code></pre> 
<p>本质上分配设备的方式和单机单卡的情况是一样的，使用同样的语法。 在上例中，假设我们有2个GPU，则代码会按照相同的逻辑定义两套操作，先后分配给名为 /gpu:0 和 /gpu:1 的两个设备。</p> 
<p><strong>注意：</strong></p> 
<ul><li> <p>tensorflow的代码中cpu和gpu的设备编号默认从0开始</p> </li><li> <p>比如我们在机器上看到有两块GPU，通过CUDA_VISIBLE_DEVICES环境变量进行控制，起了一个进程，只让0号GPU对其可见，再起一个进程，只让1号GPU对其可见，在两个进程的tensorflow代码中，都是通过/gpu:0来分别指代它们可用的GPU。</p> </li><li> <p>上例属于in-graph，从tensorboard绘制的计算图中可以明显看出来（下文会有对比展示）</p> </li><li> <p>上例属于数据并行</p> </li><li> <p>上例属于同步更新</p> </li></ul> 
<p>下面展示一些示例，运行的代码是以TensorFlow官网指南（<a href="https://www.tensorflow.org/guide/using_gpu" rel="nofollow">https://www.tensorflow.org/guide/using_gpu</a> ）为基础的，在单机2GPU的环境以multi-tower方式运行。运行过程中记录了Tensorboard使用的summary<br> <img src="https://images2.imgbox.com/da/73/0tTNckAt_o.png" alt="在这里插入图片描述"><br> 可以看到，CPU, GPU:0, GPU:1分别用三种颜色进行了标记。</p> 
<h2><a id="_639"></a>重要参考资料</h2> 
<p>本文大部分内容都是看了自以下几个资料再进行试验总结出来的：</p> 
<p>Distributed Tensorflow (TensorFlow官网): https://www.tensorflow.org/deploy/distributed</p> 
<p>Distributed TensorFlow (TensorFlow Dev Summit 2017): https://www.youtube.com/watch?v=la_M6bCV91M&amp;index=11&amp;list=PLOU2XLYxmsIKGc_NBoIhTn2Qhraji53cv</p> 
<p>Distributed TensorFlow (TensorFlow Dev Summit 2018): https://www.youtube.com/watch?v=-h0cWBiQ8s8 (本文没有包括Dev Summit 2018这个talk的内容，这里面除了基本原理之外，只讲了TensorFlow如何支持All Reduce，但是只适用于单机多卡，并且是High Level API。多机多卡的方面演讲者也只推荐了Horovod这种方式。)</p> 
<p>另外还有官网关于使用GPU的指南: <a href="https://www.tensorflow.org/guide/using_gpu" rel="nofollow">https://www.tensorflow.org/guide/using_gpu</a></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/144bf981c678d792d986c3b3f40ba35e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">vue通过span-method合并列之后，合并列显示在中间位置，根据鼠标滑动跟随展示</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5bf401e0f40b8d38078bca54d3a3f498/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">学妹教我区块链【2】--区块链中的数据结构</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>