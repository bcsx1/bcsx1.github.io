<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>机器学习入门必知 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="机器学习入门必知" />
<meta property="og:description" content="机器学习入门必知 一、机器学习中的术语表达输入空间和输出空间训练集，验证集和测试集 二、分类1 基本分类监督学习无监督学习强化学习半监督学习主动学习 2 按模型分类概率模型和非概率模型线性模型和非线性模型参数化模型和非参数化模型 3 按算法分类在线学习批量学习 三、机器学习三要素1 模型2 策略结构风险最小化策略交叉验证 3 算法 这篇文章的东西比较杂且碎，但都是很重要的必知知识，磨刀不误砍柴工。 一、机器学习中的术语表达 输入空间和输出空间 每个输入都是一个实例x，每个实例x都由n个特征来描述，一般用向量描述，x=(x(1),x(2),…,x(n))T。
n个实例x1,x2,…,xn的集合称为输入空间（有时候也叫特征空间）。
输入空间中的每个输入对应的输出y1,y2,…,yn的集合称为输出空间。
举个例子，假如在预测房价的模型中，每个输入都是一个房子x，每个房子x由面积x(1)和楼层x(2)两个特征来描述，那么房子x1可以描述为这样的向量:x1=(x(1)=120m2,x(2)=3层)T，其房价y1=300w；房子x2可以描述为这样的向量:x2=(x(1)=89m2,x(2)=6层)T，其房价y2=200w。
训练集，验证集和测试集 训练集用来训练模型，相当于课本的作用；
验证集用来选择模型，相当于课后作业的作用；
测试集用来最终评估，相当于最终的考试。
机器学习范围太广，并没有统一的理论体系涵盖所有内容，只能从多个角度对机器学习进行分类。
二、分类 1 基本分类 监督学习 监督学习从有标注数据中学习预测模型，本质是学习输入输出的映射的统计规律。而数据的标注通常是人工给出的，所以称为监督学习。（下面会结合一个关于预测房价的例子来讲）
在监督学习中，每个输入都是一个实例x，由特征向量来表示这个x，描述x的特征个数决定了特征向量的维度（比如每个输入都是一个房子x，每个房子由楼层x(1)，面积x(2)两个特征来描述，所以每个x其实都是一个二维向量，第一个房子x1，第二个房子x2…第n个房子xn的集合就是特征空间）。
每个输出都是一个y（比如第一个房子x1的房价为y1=100w）。
重点来了，监督学习的训练数据集T中的每个元素都是一个输入输出对，T={（x1，y1）,（x2，y2）,…,（xn，yn）}，每个训练数据都是带有标注y的，也就是说训练数据集中的每个房子的房价都已经告诉你了，而我们的目的是预测一个没见过的房子的价格。那么下一步就是通过训练数据集T来学习一个模型，当遇到一个训练数据集中没有的x时，通过该模型预测一个输出y。
监督学习有三个主要应用
回归问题，此类问题的输入变量连续，输出变量也是连续的，例如预测房价问题；分类问题，此类问题的输入变量可以是离散的，也可以是连续的，但输出变量一定是离散的，例如预测肿瘤良性或恶性的问题，输出只有良和恶两个离散值；标注问题，此类问题的输入和输出都是变量序列，例如词性标注问题。 无监督学习 可以很容易的想到，无监督学习就是从=无标注=的数据中学习预测模型。其本质是学习数据中的统计规律和潜在结构。它的训练数据集T中的每个元素并不是输入输出对，只有输入x。当遇到缺乏先验知识，人工标注太难或成本太高时，就可以来考虑考虑无监督学习了。
无监督学习最常用在聚类、降维中。
假如给出一堆数据，要学习它的潜在规律，学习之后，电脑发现某一堆点有共同的特征，便把它们归成一类，这便是聚类，如下图。
聚类又可以分为硬聚类和软聚类，硬聚类中的每个输入x只会被归为某一类，软聚类中的输入x有可能会被归为多类。
在机器学习中，并不是每个特征都对最后的结果有贡献，把没用的特征去掉，就是对特征空间进行降维，还降低了计算量。无监督学习用在降维中最常见的应用就是图像压缩，如下图，从第一张高清的s经过降维后转换成第二张，并不妨碍我们从第二张认出来它是个s，还降低了存储空间。
关于无监督学习的常用算法有：
等距映射法，局部线性嵌入法，拉普拉斯特征映射法，黑塞局部线性嵌入法，局部切空间排列法等
强化学习 强化学习是指智能系统在与环境的连续互动中学习最优行为策略。比如一个机器人要学习如何走到大门，当它走了一步离大门更近时，给它一个奖励，当它走了一步离大门更远了，便不给它奖励，为了得到更多的奖励，机器人最终会得到一条走到大门的最佳路线。
在强化学习中，智能系统不断地试错，它的目标不是短期奖励的最大化，而是长期累积奖励的最大化。比如阿尔法狗，它走当下这一步或许得到的奖励不是最大，但它通过计算知道下一步它会得到超级大的奖励，那么为了追求长期累积奖励的最大化，阿尔法狗会选择在目前来看并不是奖励最大的行为。
强化学习经常用在不需要进行一次决策的情形中，决策的量变才会引起结果的质变，关键的地方在于如何定义什么是应该奖励的行为，什么是应该惩罚的行为。
半监督学习 半监督学习的数据集通常有少量有标注，大量无标注，利用无标注的数据来辅助有标注数据，进行学习，以较低的成本达到较好的学习效果。
主动学习 主动学习是指机器不断主动给出实例让教师进行标注。
例如我们在刷小视频时，如果看到一条不喜欢的视频，我们可以将视频选成“不感兴趣”，之后app就会减少推送此类视频给你，在这里你相当于教师，你主动标注了你不喜欢的视频。
监督学习中的标注数据往往是随机得到的，可以看作是“被动学习”。
2 按模型分类 概率模型和非概率模型 概率模型在监督学习中是条件概率分布形式，比如“女生各个年龄段的身高模型”，预测18岁的女生身高是多少？模型并不是直接给出一个答案，而是从“18岁女生身高为163cm的概率=0.4，为168的概率=0.2，为170cm的概率=0.1”，从类似这样的概率分布中得到概率最大的身高为163cm，并把它作为答案输出。
非概率模型在监督学习中是函数f(x)的形式，比如输入一个女生的年龄=20岁，直接输出一个y=166cm。
概率模型的例子：决策树、朴素贝叶斯、隐马尔可夫模型、条件随机场、概率潜在语义分析、潜在迪利克雷分配、高斯混合模型；
非概率模型的例子：感知机、支持向量机、k近邻、AdaBoost、k均值、潜在语义分析、神经网络；
既可以看作概率模型也可看作非概率模型的例子：逻辑斯谛回归。
线性模型和非线性模型 特别的，对于非概率模型来说，如果函数是线性的，就是线性模型，否则就是非线性模型。
线性模型的例子：感知机、线性支持向量机、k近邻、k均值、潜在语义分析；
非线性模型的例子：核函数支持向量机、AdaBoost、神经网络。现在大火的深度学习其实就是学习复杂的非线性模型。
参数化模型和非参数化模型 模型是有参数的，参数化模型的参数固定或有限。非参数化模型的参数不固定或无限。
参数化模型的例子：感知机、朴素贝叶斯、逻辑斯谛回归、k均值、高斯混合模型、潜在语义分析、概率潜在语义分析、潜在迪利克雷分配；
非参数化模型的例子：决策树、支持向量机、AdaBoost，k近邻。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/254dba7ba4ec407ccd3c00875f7ed3f8/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-09-21T23:50:00+08:00" />
<meta property="article:modified_time" content="2022-09-21T23:50:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习入门必知</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>机器学习入门必知</h4> 
 <ul><li><a href="#_2" rel="nofollow">一、机器学习中的术语表达</a></li><li><ul><li><a href="#_3" rel="nofollow">输入空间和输出空间</a></li><li><a href="#_9" rel="nofollow">训练集，验证集和测试集</a></li></ul> 
  </li><li><a href="#_15" rel="nofollow">二、分类</a></li><li><ul><li><a href="#1__16" rel="nofollow">1 基本分类</a></li><li><ul><li><a href="#_17" rel="nofollow">监督学习</a></li><li><a href="#_31" rel="nofollow">无监督学习</a></li><li><a href="#_43" rel="nofollow">强化学习</a></li><li><a href="#_50" rel="nofollow">半监督学习</a></li><li><a href="#_52" rel="nofollow">主动学习</a></li></ul> 
   </li><li><a href="#2__56" rel="nofollow">2 按模型分类</a></li><li><ul><li><a href="#_57" rel="nofollow">概率模型和非概率模型</a></li><li><a href="#_66" rel="nofollow">线性模型和非线性模型</a></li><li><a href="#_71" rel="nofollow">参数化模型和非参数化模型</a></li></ul> 
   </li><li><a href="#3__76" rel="nofollow">3 按算法分类</a></li><li><ul><li><a href="#_77" rel="nofollow">在线学习</a></li><li><a href="#_79" rel="nofollow">批量学习</a></li></ul> 
  </li></ul> 
  </li><li><a href="#_82" rel="nofollow">三、机器学习三要素</a></li><li><ul><li><a href="#1__83" rel="nofollow">1 模型</a></li><li><a href="#2__89" rel="nofollow">2 策略</a></li><li><ul><li><a href="#_92" rel="nofollow">结构风险最小化策略</a></li><li><a href="#_116" rel="nofollow">交叉验证</a></li></ul> 
   </li><li><a href="#3__134" rel="nofollow">3 算法</a></li></ul> 
 </li></ul> 
</div> 
<br> 这篇文章的东西比较杂且碎，但都是很重要的必知知识，磨刀不误砍柴工。 
<p></p> 
<h2><a id="_2"></a>一、机器学习中的术语表达</h2> 
<h3><a id="_3"></a>输入空间和输出空间</h3> 
<p>每个输入都是一个实例x，每个实例x都由n个特征来描述，一般用向量描述，x=(x<sup>(1)</sup>,x<sup>(2)</sup>,…,x<sup>(n)</sup>)<sup>T</sup>。<br> n个实例x<sub>1</sub>,x<sub>2</sub>,…,x<sub>n</sub>的集合称为输入空间（有时候也叫特征空间）。<br> 输入空间中的每个输入对应的输出y<sub>1</sub>,y<sub>2</sub>,…,y<sub>n</sub>的集合称为输出空间。</p> 
<p>举个例子，假如在预测房价的模型中，每个输入都是一个房子x，每个房子x由面积x<sup>(1)</sup>和楼层x<sup>(2)</sup>两个特征来描述，那么房子x<sub>1</sub>可以描述为这样的向量:x<sub>1</sub>=(x<sup>(1)</sup>=120m<sup>2</sup>,x<sup>(2)</sup>=3层)<sup>T</sup>，其房价y<sub>1</sub>=300w；房子x<sub>2</sub>可以描述为这样的向量:x<sub>2</sub>=(x<sup>(1)</sup>=89m<sup>2</sup>,x<sup>(2)</sup>=6层)<sup>T</sup>，其房价y<sub>2</sub>=200w。</p> 
<h3><a id="_9"></a>训练集，验证集和测试集</h3> 
<p>训练集用来训练模型，相当于课本的作用；<br> 验证集用来选择模型，相当于课后作业的作用；<br> 测试集用来最终评估，相当于最终的考试。</p> 
<p>机器学习范围太广，并没有统一的理论体系涵盖所有内容，只能从多个角度对机器学习进行分类。</p> 
<h2><a id="_15"></a>二、分类</h2> 
<h3><a id="1__16"></a>1 基本分类</h3> 
<h4><a id="_17"></a>监督学习</h4> 
<p>监督学习从<mark>有标注</mark>数据中学习预测模型，本质是学习输入输出的映射的统计规律。而数据的标注通常是人工给出的，所以称为监督学习。（下面会结合一个关于预测房价的例子来讲）</p> 
<p>在监督学习中，每个输入都是一个实例x，由特征向量来表示这个x，描述x的特征个数决定了特征向量的维度（比如每个输入都是一个房子x，每个房子由楼层x<sup>(1)</sup>，面积x<sup>(2)</sup>两个特征来描述，所以每个x其实都是一个二维向量，第一个房子x<sub>1</sub>，第二个房子x<sub>2</sub>…第n个房子x<sub>n</sub>的集合就是特征空间）。</p> 
<p>每个输出都是一个y（比如第一个房子x<sub>1</sub>的房价为y<sub>1</sub>=100w）。</p> 
<p>重点来了，<mark>监督学习的训练数据集T中的每个元素都是一个输入输出对，T={（x<sub>1</sub>，y<sub>1</sub>）,（x<sub>2</sub>，y<sub>2</sub>）,…,（x<sub>n</sub>，y<sub>n</sub>）}，每个训练数据都是带有标注y的</mark>，也就是说训练数据集中的每个房子的房价都已经告诉你了，而我们的目的是预测一个没见过的房子的价格。那么下一步就是通过训练数据集T来学习一个模型，当遇到一个训练数据集中没有的x时，通过该模型预测一个输出y。</p> 
<p>监督学习有三个主要应用</p> 
<ol><li>回归问题，此类问题的输入变量连续，输出变量也是连续的，例如预测房价问题；</li><li>分类问题，此类问题的输入变量可以是离散的，也可以是连续的，但输出变量一定是离散的，例如预测肿瘤良性或恶性的问题，输出只有良和恶两个离散值；</li><li>标注问题，此类问题的输入和输出都是变量序列，例如词性标注问题。</li></ol> 
<h4><a id="_31"></a>无监督学习</h4> 
<p>可以很容易的想到，无监督学习就是从=无标注=的数据中学习预测模型。其本质是学习数据中的统计规律和潜在结构。它的训练数据集T中的每个元素并不是输入输出对，只有输入x。当遇到缺乏先验知识，人工标注太难或成本太高时，就可以来考虑考虑无监督学习了。</p> 
<p>无监督学习最常用在<mark>聚类、降维</mark>中。<br> 假如给出一堆数据，要学习它的潜在规律，学习之后，电脑发现某一堆点有共同的特征，便把它们归成一类，这便是聚类，如下图。<br> <img src="https://images2.imgbox.com/5b/fc/LeSIScrl_o.png" alt="在这里插入图片描述"><br> 聚类又可以分为硬聚类和软聚类，硬聚类中的每个输入x只会被归为某一类，软聚类中的输入x有可能会被归为多类。</p> 
<p>在机器学习中，并不是每个特征都对最后的结果有贡献，把没用的特征去掉，就是对特征空间进行降维，还降低了计算量。无监督学习用在降维中最常见的应用就是图像压缩，如下图，从第一张高清的s经过降维后转换成第二张，并不妨碍我们从第二张认出来它是个s，还降低了存储空间。<br> <img src="https://images2.imgbox.com/80/05/nBSAgDdh_o.png" alt="在这里插入图片描述"><br> 关于无监督学习的常用算法有：<br> 等距映射法，局部线性嵌入法，拉普拉斯特征映射法，黑塞局部线性嵌入法，局部切空间排列法等</p> 
<h4><a id="_43"></a>强化学习</h4> 
<p>强化学习是指智能系统在与环境的连续互动中学习最优行为策略。比如一个机器人要学习如何走到大门，当它走了一步离大门更近时，给它一个奖励，当它走了一步离大门更远了，便不给它奖励，为了得到更多的奖励，机器人最终会得到一条走到大门的最佳路线。</p> 
<p><mark>在强化学习中，智能系统不断地试错，它的目标不是短期奖励的最大化，而是长期累积奖励的最大化</mark>。比如阿尔法狗，它走当下这一步或许得到的奖励不是最大，但它通过计算知道下一步它会得到超级大的奖励，那么为了追求长期累积奖励的最大化，阿尔法狗会选择在目前来看并不是奖励最大的行为。</p> 
<p>强化学习经常用在不需要进行一次决策的情形中，决策的量变才会引起结果的质变，关键的地方在于如何定义什么是应该奖励的行为，什么是应该惩罚的行为。</p> 
<h4><a id="_50"></a>半监督学习</h4> 
<p>半监督学习的数据集通常有<mark>少量有标注，大量无标注</mark>，利用无标注的数据来辅助有标注数据，进行学习，以较低的成本达到较好的学习效果。</p> 
<h4><a id="_52"></a>主动学习</h4> 
<p>主动学习是指<mark>机器不断主动给出实例让教师进行标注</mark>。<br> 例如我们在刷小视频时，如果看到一条不喜欢的视频，我们可以将视频选成“不感兴趣”，之后app就会减少推送此类视频给你，在这里你相当于教师，你主动标注了你不喜欢的视频。<br> 监督学习中的标注数据往往是随机得到的，可以看作是“被动学习”。</p> 
<h3><a id="2__56"></a>2 按模型分类</h3> 
<h4><a id="_57"></a>概率模型和非概率模型</h4> 
<p>概率模型在监督学习中是条件概率分布形式，比如“女生各个年龄段的身高模型”，预测18岁的女生身高是多少？模型并不是直接给出一个答案，而是从“18岁女生身高为163cm的概率=0.4，为168的概率=0.2，为170cm的概率=0.1”，从类似这样的概率分布中得到概率最大的身高为163cm，并把它作为答案输出。</p> 
<p>非概率模型在监督学习中是函数f(x)的形式，比如输入一个女生的年龄=20岁，直接输出一个y=166cm。</p> 
<p><strong>概率模型的例子</strong>：决策树、朴素贝叶斯、隐马尔可夫模型、条件随机场、概率潜在语义分析、潜在迪利克雷分配、高斯混合模型；<br> <strong>非概率模型的例子</strong>：感知机、支持向量机、k近邻、AdaBoost、k均值、潜在语义分析、神经网络；<br> 既可以看作概率模型也可看作非概率模型的例子：逻辑斯谛回归。</p> 
<h4><a id="_66"></a>线性模型和非线性模型</h4> 
<p>特别的，对于非概率模型来说，如果函数是线性的，就是线性模型，否则就是非线性模型。<br> <strong>线性模型的例子</strong>：感知机、线性支持向量机、k近邻、k均值、潜在语义分析；<br> <strong>非线性模型的例子</strong>：核函数支持向量机、AdaBoost、神经网络。现在大火的深度学习其实就是学习复杂的非线性模型。</p> 
<h4><a id="_71"></a>参数化模型和非参数化模型</h4> 
<p>模型是有参数的，参数化模型的参数固定或有限。非参数化模型的参数不固定或无限。<br> <strong>参数化模型的例子</strong>：感知机、朴素贝叶斯、逻辑斯谛回归、k均值、高斯混合模型、潜在语义分析、概率潜在语义分析、潜在迪利克雷分配；<br> <strong>非参数化模型的例子</strong>：决策树、支持向量机、AdaBoost，k近邻。</p> 
<h3><a id="3__76"></a>3 按算法分类</h3> 
<h4><a id="_77"></a>在线学习</h4> 
<p>一次只接受一个样本，进行预测，然后学习模型，不断重复。当遇到数据无法存储、数据规模太大无法一次性处理、数据模式不断动态变化等情况时，最好用在线学习。比如采用随机梯度下降的感知机学习算法就是在线学习。</p> 
<h4><a id="_79"></a>批量学习</h4> 
<p>一次接受所有样本，学习模型，之后进行预测。<br> 深度学习常用的一种方法是一次接受部分样本。</p> 
<h2><a id="_82"></a>三、机器学习三要素</h2> 
<h3><a id="1__83"></a>1 模型</h3> 
<p><mark>机器学习第一个要考虑的问题就是要学习什么样的模型</mark>。</p> 
<p>此处引入假设空间的重要概念，模型的假设空间包含所有可能的条件概率分布或函数。例如，假设函数是f(x)=ax时，假设空间就是a的所有可能取值对应的所有f(x)的函数集合。假设函数是f(x)=ax<sup>2</sup>+bx时，假设空间就是a和b的所有可能取值对应的所有f(x)的函数集合。</p> 
<p>所以我们首要考虑的问题就是，对于每一个具体问题，应该选择哪种假设函数？</p> 
<h3><a id="2__89"></a>2 策略</h3> 
<p>假设函数选出来后，假设空间随之确定，<mark>下一个要考虑的问题就是按照什么样的策略来挑一个最优的模型</mark></p> 
<h4><a id="_92"></a>结构风险最小化策略</h4> 
<p>最常用的是<mark>结构风险最小化策略</mark>，下面来一步步得出这个策略。</p> 
<ol><li> <p>损失函数和风险函数<br> 损失函数度量模型一次预测的好坏；“好坏”说白了就是预测值和真实值的偏差。损失函数值越小，模型就越好，预测就越准确。<br> 常用的损失函数有：0-1损失函数，平方损失函数，绝对损失函数，对数损失函数。<br> 但一次预测并不能说明什么，我们要的是数据无穷大时的平均意义下的损失（联想微积分的思想）。<br> 而风险函数度量的是模型平均意义下预测的好坏。这个平均意义下的损失也就是损失函数的期望，所以也称为期望损失。<br> <strong>此时期望损失最小的模型就是最优的模型。</strong><br> 但是在计算期望损失的过程中，需要知道联合分布P(X,Y)，而该联合分布无法直接计算，此时便引出了经验风险的概念。</p> </li><li> <p>经验风险<br> <strong>经验风险</strong>是当前训练数据集的<strong>平均损失</strong>。<br> 由于大数定理（不知道的自行百度），当样本量趋于无穷大时，平均损失（经验损失）=平均意义下的损失（期望损失），所以可以用经验风险来估计期望风险，<strong>此时经验损失最小的模型就是最优的模型。</strong></p> </li><li> <p>经验风险最小化策略<br> 该策略认为经验风险最小的模型就是最优的模型，当样本量足够大时，该策略有很好的效果，且当模型是条件概率分布模型，损失函数是对数损失函数时，经验风险最小化等价于极大似然估计。<br> 然而当样本量很小时，用经验风险来估计期望风险并不理想，会出现过拟合现象。<mark>过拟合是一味追求对训练数据的预测能力，最终得到的模型参数过多，过于复杂，导致对训练数据拟合的很完美，对未知数据拟合的却很差</mark>。而我们追求的是对未知数据的拟合能力（称为泛化能力），所以引入了可以防止过拟合的结构风险最小化策略。</p> </li><li> <p>结构风险最小化策略<br> <strong>该策略认为结构损失最小的模型就是最优的模型</strong>。<br> 结构损失就是在计算经验损失的基础上加了一个正则化项（也称罚项），该正则化项表示的是模型的复杂度，模型越复杂，该项越大，从而得到的结构损失越大，以此来平衡模型复杂度和拟合训练数据。<br> 正则化符合奥卡姆剃刀原理，在所有可能选择的模型中，可以很好地拟合数据且十分简单的模型才是最好的模型。（从数学角度解释是KKT和拉格朗日）</p> </li></ol> 
<h4><a id="_116"></a>交叉验证</h4> 
<ol><li> <p>简单交叉验证<br> 该验证方法将数据分成两部分，大部分作为训练集，少部分作为测试集。<br> 用训练集训练模型，用测试集评价各个模型的测试误差，最后选出测试误差最小的模型。<br> 但此法只适用于数据足够的情况。</p> </li><li> <p>S折交叉验证<br> 该验证方法将数据分成S份子集，每份有多个样本，然后用S-1个子集训练模型，用剩下的1个子集测试模型，每个模型如此循环重复测试S次，最后选出S次的平均测试误差最小的模型。<br> 一表看懂：</p> </li></ol> 
<table><thead><tr><th align="center">分为3个子集</th><th align="right">子集1</th><th align="right">子集2</th><th align="right">子集3</th></tr></thead><tbody><tr><td align="center">第一次验证</td><td align="right">用来训练</td><td align="right">用来训练</td><td align="right"><strong>用来测试</strong></td></tr><tr><td align="center">第二次验证</td><td align="right">用来训练</td><td align="right"><strong>用来测试</strong></td><td align="right">用来训练</td></tr><tr><td align="center">第三次验证</td><td align="right"><strong>用来测试</strong></td><td align="right">用来训练</td><td align="right">用来训练</td></tr></tbody></table> 
<ol start="3"><li>留一交叉验证<br> 该验证方法将数据分成S份子集，每份只有一个样本，相当于S折交叉的特殊情况。当数据非常缺乏时可以选择此验证方法。</li></ol> 
<h3><a id="3__134"></a>3 算法</h3> 
<p><mark>最后要考虑的问题就是用什么样的计算方法求解最优模型</mark>。<br> 这时，机器学习问题转化为最优化问题，机器学习的算法转化为求解最优化问题的算法。一个好的求解算法既要保证找到全局最优解，也要保证求解过程十分高效。</p> 
<blockquote> 
 <p>参考李航老师的统计学习方法</p> 
</blockquote>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/967510b356c57c68964c8a7589ab7e3e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">JVM -XMX与XMS是什么</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0726f15dde1429181dd27caea6eeb5b7/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Java 中的阻塞队列</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>