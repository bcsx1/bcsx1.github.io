<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>（2021|CoRR，AugCLIP，优化）FuseDream：通过改进的 CLIP&#43;GAN 空间优化实现免训练文本到图像生成 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="（2021|CoRR，AugCLIP，优化）FuseDream：通过改进的 CLIP&#43;GAN 空间优化实现免训练文本到图像生成" />
<meta property="og:description" content="FuseDream: Training-Free Text-to-Image Generation with Improved CLIP&#43;GAN Space Optimization
公众：EDPJ（添加 VX：CV_EDPJ 或直接进 Q 交流群：922230617 获取资料）
目录
0. 摘要
1. 简介
2. CLIP&#43;GAN 文本到图像生成
2.1 CLIP容易受到攻击并陷入困境
3. 我们的方法 - FuseDream
3.1 AugCLIP：避免对抗生成
3.2 优化改进
3.3 组合生成
4. 相关工作 5. 实验
6. 结论
S. 总结
S.1 主要贡献
S.2 方法
0. 摘要 生成图像从自然语言指令是一个引人入胜但极具挑战性的任务。我们采用了一种 CLIP&#43;GAN 的方法来进行文本到图像的生成，该方法在现成的 GAN 的潜在空间中进行优化，以找到在 CLIP 模型度量下，与给定输入文本具有最大语义相关性分数的图像。与从头开始训练将文本映射到图像的传统方法相比，CLIP&#43;GAN 方法是无需训练的、zero-shot 的，并且可以轻松定制以适应不同的生成器。
然而，在 GAN 空间中优化 CLIP 分数是一个极具挑战性的优化问题，像 Adam 这样的现成优化器未能产生令人满意的结果。在这项工作中，我们提出了一种 FuseDream 流水线，通过三种关键技术改进了 CLIP&#43;GAN 方法： 1）通过在图像上引入随机增强来使标准的 CLIP 分数更加稳健的AugCLIP 分数。2）一种新颖的初始化和超参数化策略，允许我们有效地在 GAN 空间中的非凸区域进行优化。3）一种组合生成技术，通过利用一种新颖的双层优化公式，可以组合多个图像以扩展 GAN 空间并克服数据偏见。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/82120563b43443454fc4518d76f4eba2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-24T22:08:33+08:00" />
<meta property="article:modified_time" content="2023-12-24T22:08:33+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">（2021|CoRR，AugCLIP，优化）FuseDream：通过改进的 CLIP&#43;GAN 空间优化实现免训练文本到图像生成</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p style="text-align:center;"><strong>FuseDream: Training-Free Text-to-Image Generation  with Improved CLIP+GAN Space Optimization</strong></p> 
<p style="text-align:center;"><strong>公众：EDPJ（添加 VX：CV_EDPJ 或直接进 Q 交流群：922230617 获取资料）</strong></p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="0.%20%E6%91%98%E8%A6%81-toc" style="margin-left:0px;"><a href="#0.%20%E6%91%98%E8%A6%81" rel="nofollow">0. 摘要</a></p> 
<p id="1.%20%E7%AE%80%E4%BB%8B-toc" style="margin-left:0px;"><a href="#1.%20%E7%AE%80%E4%BB%8B" rel="nofollow">1. 简介</a></p> 
<p id="2.%20CLIP%2BGAN%20%E6%96%87%E6%9C%AC%E5%88%B0%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90-toc" style="margin-left:0px;"><a href="#2.%20CLIP%2BGAN%20%E6%96%87%E6%9C%AC%E5%88%B0%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90" rel="nofollow">2. CLIP+GAN 文本到图像生成</a></p> 
<p id="2.1%20CLIP%E5%AE%B9%E6%98%93%E5%8F%97%E5%88%B0%E6%94%BB%E5%87%BB%E5%B9%B6%E9%99%B7%E5%85%A5%E5%9B%B0%E5%A2%83-toc" style="margin-left:40px;"><a href="#2.1%20CLIP%E5%AE%B9%E6%98%93%E5%8F%97%E5%88%B0%E6%94%BB%E5%87%BB%E5%B9%B6%E9%99%B7%E5%85%A5%E5%9B%B0%E5%A2%83" rel="nofollow">2.1 CLIP容易受到攻击并陷入困境</a></p> 
<p id="3.%20%E6%88%91%E4%BB%AC%E7%9A%84%E6%96%B9%E6%B3%95%20-%20FuseDream-toc" style="margin-left:0px;"><a href="#3.%20%E6%88%91%E4%BB%AC%E7%9A%84%E6%96%B9%E6%B3%95%20-%20FuseDream" rel="nofollow">3. 我们的方法 - FuseDream</a></p> 
<p id="3.1%C2%A0AugCLIP%EF%BC%9A%E9%81%BF%E5%85%8D%E5%AF%B9%E6%8A%97%E7%94%9F%E6%88%90-toc" style="margin-left:40px;"><a href="#3.1%C2%A0AugCLIP%EF%BC%9A%E9%81%BF%E5%85%8D%E5%AF%B9%E6%8A%97%E7%94%9F%E6%88%90" rel="nofollow">3.1 AugCLIP：避免对抗生成</a></p> 
<p id="3.2%20%E4%BC%98%E5%8C%96%E6%94%B9%E8%BF%9B-toc" style="margin-left:40px;"><a href="#3.2%20%E4%BC%98%E5%8C%96%E6%94%B9%E8%BF%9B" rel="nofollow">3.2 优化改进</a></p> 
<p id="3.3%20%E7%BB%84%E5%90%88%E7%94%9F%E6%88%90-toc" style="margin-left:40px;"><a href="#3.3%20%E7%BB%84%E5%90%88%E7%94%9F%E6%88%90" rel="nofollow">3.3 组合生成</a></p> 
<p id="4.%20%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%C2%A0-toc" style="margin-left:0px;"><a href="#4.%20%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%C2%A0" rel="nofollow">4. 相关工作 </a></p> 
<p id="5.%20%E5%AE%9E%E9%AA%8C-toc" style="margin-left:0px;"><a href="#5.%20%E5%AE%9E%E9%AA%8C" rel="nofollow">5. 实验</a></p> 
<p id="6.%20%E7%BB%93%E8%AE%BA-toc" style="margin-left:0px;"><a href="#6.%20%E7%BB%93%E8%AE%BA" rel="nofollow">6. 结论</a></p> 
<p id="S.%20%E6%80%BB%E7%BB%93-toc" style="margin-left:0px;"><a href="#S.%20%E6%80%BB%E7%BB%93" rel="nofollow">S. 总结</a></p> 
<p id="S.1%20%E4%B8%BB%E8%A6%81%E8%B4%A1%E7%8C%AE-toc" style="margin-left:40px;"><a href="#S.1%20%E4%B8%BB%E8%A6%81%E8%B4%A1%E7%8C%AE" rel="nofollow">S.1 主要贡献</a></p> 
<p id="S.2%20%E6%96%B9%E6%B3%95-toc" style="margin-left:40px;"><a href="#S.2%20%E6%96%B9%E6%B3%95" rel="nofollow">S.2 方法</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="0.%20%E6%91%98%E8%A6%81" style="background-color:transparent;">0. 摘要</h2> 
<p>生成图像从自然语言指令是一个引人入胜但极具挑战性的任务。我们采用了一种 CLIP+GAN 的方法来进行文本到图像的生成，该方法在现成的 GAN 的潜在空间中进行优化，以找到在 CLIP 模型度量下，与给定输入文本具有最大语义相关性分数的图像。与从头开始训练将文本映射到图像的传统方法相比，CLIP+GAN 方法是无需训练的、zero-shot 的，并且可以轻松定制以适应不同的生成器。</p> 
<p>然而，在 GAN 空间中优化 CLIP 分数是一个极具挑战性的优化问题，像 Adam 这样的现成优化器未能产生令人满意的结果。在这项工作中，我们提出了一种 FuseDream 流水线，通过三种关键技术改进了 CLIP+GAN 方法： 1）通过在图像上引入随机增强来使标准的 CLIP 分数更加稳健的AugCLIP 分数。2）一种新颖的初始化和超参数化策略，允许我们有效地在 GAN 空间中的非凸区域进行优化。3）一种组合生成技术，通过利用一种新颖的双层优化公式，可以组合多个图像以扩展 GAN 空间并克服数据偏见。</p> 
<p>在受到不同输入文本的推动时，FuseDream 能够生成具有不同对象、背景、艺术风格和新颖的反事实概念的高质量图像，这些概念在我们使用的 GAN 的训练数据中并未出现。在定量方面，FuseDream 生成的图像在 MS COCO 数据集上获得了顶级的 Inception 分数和 FID 分数，而无需额外的架构设计或培训。我们的代码可在 https://github.com/gnobitab/FuseDream 上公开获取。</p> 
<h2 id="1.%20%E7%AE%80%E4%BB%8B">1. 简介</h2> 
<p>在多模态机器学习中的一个重要任务是文本到图像生成，即生成与给定文本输入在语义上相关的逼真图像 [9,18,25,26,31,35]。这是一项极具挑战性的任务，因为生成模型需要理解文本、图像以及它们在语义上应该如何关联。最近，[9, 25] 取得了显著的进展，它们使用在大规模数据集上进行自监督损失训练的模型生成了高质量且语义相关的图像。</p> 
<p>传统的文本到图像生成方法是使用（文本，图像）对的数据集从头开始训练有条件的生成模型 [18, 22, 25, 26, 31, 35]。然而，这个过程需要收集大量的训练数据，带来高昂的培训成本，并且不容易定制。最近，随着强大的联合文本-图像编码器的出现（特别是 CLIP 模型 [24]），它们提供了文本-图像对的忠实语义相关性得分。结合强大的预训练 GAN（如 [1, 4, 18, 40]），通过在 GAN 的潜在空间中进行优化，创建与输入文本具有高语义相关性的图像已经成为可能。值得注意的例子包括 BigSleep [20] 和 VQGAN+CLIP [6]，它们分别通过在 BigGAN 和 VQGAN 的潜在空间中最大化CLIP 分数来从文本生成引人入胜和艺术性的图像。与传统基准相比，结合 GAN 和 CLIP 的方法是无需训练且 zero-shot 的，不需要专门的训练数据集和训练成本。它还更加灵活和模块化：用户可以轻松替换生成器（GAN）或编码器模型（CLIP）为更强大或定制的模型，以最适合其问题和计算预算。</p> 
<p class="img-center"><img alt="" height="432" src="https://images2.imgbox.com/07/80/NiHKIxNW_o.png" width="1136"></p> 
<p>另一方面，现有的 CLIP+GAN 方法的结果 [6, 10, 20] 在许多情况下仍然令人不满意。例如，虽然BigSleep 能够以不同的风格生成图像并创造有趣的视觉艺术，但在生成清晰和逼真的图像方面存在困难，生成的图像可能只与 query 文本弱相关。如图 1 所示（每个面板的右上角），BigSleep无法为 “蓝色狗” 这个简单概念生成一个清晰可辨认的图像。对于像 “火焰狗” 这样的反事实概念，BigSleep 生成的图像往往以一种不自然的方式纠缠了火焰和狗的概念。在图 1 中（每个面板的左上角），我们实施了另一个基准，使用现成的Adam [17] 优化器在 BigGAN [4] 的输入空间中最大化 CLIP 分数，结果甚至比 BigSleep 更糟糕。</p> 
<p>在这项工作中，我们分析了现有 CLIP+GAN 过程中的问题。我们确定了该方法存在的三个关键瓶颈，并通过一系列技术来显著改进这个流程。</p> 
<ul><li><strong>稳健分数</strong>：我们观察到原始的 CLIP 分数在 GAN 空间中并不适用作为优化的良好目标函数，因为 它倾向于生成语义无关的图像，这些图像 “对抗性” 地最大化了 CLIP 分数。我们提出了 AugCLIP分数，通过对输入图像的多个扰动或增强进行平均来稳健 CLIP 分数。</li><li><strong>改进的优化策略</strong>：在 GAN 空间中最大化 CLIP 分数会产生一个高度非凸的多模态优化问题，现成的优化方法往往会陷入次优的局部最大值。我们通过一种新颖的初始化和过参数化策略来解决这个问题，允许我们更有效地在非凸损失中遍历。</li><li><strong>组合生成</strong>：CLIP+GAN 方法的图像空间受到我们使用的预训练 GAN 的限制。这使得难以生成在GAN 的训练数据中没有出现的对象的新组合的图像。我们通过提出一种组合生成技术来解决这个问题，该技术共同优化两个图像，使它们可以无缝地组合在一起，产生自然且在语义上相关的图像。我们将组合生成制定为一种新颖的双层优化问题，最大化 AugCLIP 分数，同时将感知一致性分数作为次要目标，并利用一种最近的动态障碍梯度下降算法 [11] 有效地解决它。</li></ul> 
<p>我们的流程，我们称之为 <strong>FuseDream</strong>（名称中的 “fuse” 指的是： 1. 融合 GAN 和 CLIP 以及 2. 我们的组合生成技术），不仅可以从复杂的文本描述中生成清晰的对象，还可以生成类似 MS COCO [19] 中的复杂场景。由于 CLIP 的表征能力，FuseDream 可以创建具有不同背景、纹理、位置、艺术风格甚至反事实对象的图像。借助组合生成技术，FuseDream 可以创建具有在我们使用的 GAN 的原始训练数据中没有出现的对象的新组合的图像。与直接训练大规模文本到图像生成模型相比，我们的方法在保持可比甚至更好的结果的同时更加计算友好。</p> 
<h2 id="2.%20CLIP%2BGAN%20%E6%96%87%E6%9C%AC%E5%88%B0%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90">2. CLIP+GAN 文本到图像生成</h2> 
<p>首先，我们介绍通过结合预训练图像生成器（特别是 GAN）和联合图像+文本编码器（特别是CLIP）进行文本到图像生成的一般思路。然后，我们分析这种方法的朴素实现的一个关键限制。</p> 
<p><strong>GAN</strong>：图像生成器 g：R^D → R^(H×W×3) 是一个神经网络，接受一个 D 维潜在编码 ξ，并输出尺寸为 H × W 的彩色图像 I。形式上，I = g(ξ)。</p> 
<p>可以通过控制输入的 ξ 生成和操纵不同的图像。在这项工作中，除非另有说明，我们使用 BigGAN [4]，这是一个类别条件的 GAN，其潜在向量 ξ = {z, y} 包括一个高斯噪声向量 z ∈ R^Z 和一个类别嵌入向量 y ∈ R^Y。它是在大规模的 ImageNet数据集 [27] 上训练的，包含来自 1,000 个不同类别的对象。</p> 
<p><strong>CLIP</strong>：一个联合图像-文本编码器，尤其是对比语言-图像预训练（Contrastive Language-Image Pretraining, CLIP）[24]，由语言编码器 f_text 和图像编码器 f_image 组成，它们将文本 T 和图像 I 映射到一个共同的潜在空间，通过余弦相似性可以评估它们的相关性。</p> 
<p class="img-center"><img alt="" height="58" src="https://images2.imgbox.com/58/99/edHOfQrV_o.png" width="457"></p> 
<p>CLIP 模型经过训练，使得语义相关的文本-图像对 T 和 I 具有高相似性分数。</p> 
<p><strong>CLIP+GAN</strong>：通过结合预训练的 GAN g 和 CLIP {f_text, f_image}，可以合成一个文本到图像的生成器。给定一个输入文本 T，我们可以通过优化潜在编码 ξ 生成一个与 T 在语义上相关的逼真图像 I，使得生成的图像 I = g(ξ) 具有最大的 CLIP 分数 s_CLIP(T, I)。形式上，</p> 
<p class="img-center"><img alt="" height="39" src="https://images2.imgbox.com/4b/3a/ELzON8FQ_o.png" width="367"></p> 
<p>这将使输出图像限制在自然图像的空间内，同时最大化与输入文本的语义相关性。在 [20, 39] 中，使用 Adam [17] 解决优化问题。在使用 BigGAN [4, 16] 时，通常的做法是将 z 截断为 [-2, 2]。 </p> 
<h3 id="2.1%20CLIP%E5%AE%B9%E6%98%93%E5%8F%97%E5%88%B0%E6%94%BB%E5%87%BB%E5%B9%B6%E9%99%B7%E5%85%A5%E5%9B%B0%E5%A2%83" style="background-color:transparent;">2.1 CLIP容易受到攻击并陷入困境</h3> 
<p>简单地解决（2）不会产生令人满意的图像，如图 1 左上角的图像所示。我们观察到不令人满意的结果可以归因于两个相互关联的原因：</p> 
<ul><li>CLIP 分数很容易受到 “攻击”，即在任何图像的小邻域内很容易最大化 CLIP，表明存在具有高CLIP 分数但与输入文本关联性低的 “对抗性” 图像。</li><li>在（2）中的优化实际上可以有效地作为对 s_CLIP 的对抗性优化，产生与初始化相似但虚假高 CLIP 分数的图像。</li></ul> 
<p class="img-center"><img alt="" height="392" src="https://images2.imgbox.com/10/30/mJJI5MW7_o.png" width="723"></p> 
<p><strong>案例研究 1：攻击 CLIP</strong>。如图 2 所示，我们对自然图像 I 应用对抗攻击器，使用 Fast Gradient Sign Method（FGSM）[13] 在 sCLIP 上进行操作，即解决 max s_CLIP(I + δ) s.t. ||δ|| ≤ ε，其中小扰动的幅度 ε &gt; 0。我们发现 FGSM 可以轻松找到一个几乎与原始图像相同但具有更高 CLIP 分数的图像。这表明在直接最大化 CLIP 分数时存在 “过拟合” 的危险。</p> 
<p class="img-center"><img alt="" height="613" src="https://images2.imgbox.com/6c/5a/bosXieNE_o.png" width="726"></p> 
<p><strong>案例研究 2：Dog → Cat</strong>。在图 3 中，我们展示了使用输入文本 T = ‘一张猫的照片’ 优化（2）的一个示例，从一个初始化的 ξ^0 开始，其图像 I = g(ξ^0) 是一只狗。我们可以看到尽管成功地最大化了 s_CLIP，图像仍然与初始化相似，并且没有按预期从狗转变为猫。在这种情况下，（2）中的优化表现出对抗性攻击的行为：它在初始化附近被困住，同时虚假地增加了 CLIP 分数。</p> 
<p>在上述两种情况中，可以通过使用我们在接下来介绍的 AugCLIP 分数来解决问题。</p> 
<h2 id="3.%20%E6%88%91%E4%BB%AC%E7%9A%84%E6%96%B9%E6%B3%95%20-%20FuseDream">3. 我们的方法 - FuseDream</h2> 
<p>我们现在介绍改进 CLIP+GAN 管道的主要技术。第 3.1 节介绍了 <strong>AugCLIP</strong> 分数，该分数使 CLIP分数更加健壮，以避免对抗性攻击现象。第 3.2 节介绍了一种<strong>初始化和过参数化（over-parameterization）</strong>技术，以更好地解决非凸优化问题。第 3.3 节介绍了一种<strong>组合生成</strong>方法，用于生成具有新颖对象和背景组合的分布外图像。</p> 
<h3 id="3.1%C2%A0AugCLIP%EF%BC%9A%E9%81%BF%E5%85%8D%E5%AF%B9%E6%8A%97%E7%94%9F%E6%88%90" style="background-color:transparent;">3.1 AugCLIP：避免对抗生成</h3> 
<p>为了解决 CLIP 分数的对抗性攻击问题，我们提出以下 AugCLIP 分数。 </p> 
<p class="img-center"><img alt="" height="28" src="https://images2.imgbox.com/08/94/wc6Xcnq9_o.png" width="476"></p> 
<p>其中 I' 是来自候选数据增强分布 π(· | I) 的输入图像 I 的随机扰动。在我们的工作中，我们采用了 DiffAugment [38] 中考虑的各种数据增强技术，包括随机着色、随机平移、随机调整大小和随机剪裁。</p> 
<p>由于 AugCLIP 必须同时攻击大多数随机增强图像上的  s_CLIP，这比攻击单个图像要困难得多，因此 AugCLIP 对抗对攻击更加稳健。对随机增强的平均化还使得攻击更加困难，正如理论和经验证明的那样 [5, 28]。同时，增加增强并不损害由 CLIP 编码的语义关系，因为 CLIP 模型最初是在具有不同着色、视图和平移的图像上训练的，因此与我们的增强策略兼容。</p> 
<p><img alt="" height="610" src="https://images2.imgbox.com/50/af/SvPax4ES_o.png" width="1200"> </p> 
<p><strong>案例研究 1 和 2</strong>。如图 3 所示，AugCLIP 分数对抗对攻击更加稳健。图 4 显示，简单地用 s_AugCLIP 替换 s_CLIP 允许我们摆脱对抗生成并产生更具语义相关性的图像。</p> 
<h3 id="3.2%20%E4%BC%98%E5%8C%96%E6%94%B9%E8%BF%9B" style="background-color:transparent;">3.2 优化改进</h3> 
<p>由于损失的高非凸性，s_AugCLIP 的优化仍然可能受到次优局部极大值的影响。我们引入了一种初始化和过参数化策略来改善优化。 与从单一初始化开始的传统方法不同，我们首先采样大量（ M 个）初始化副本</p> 
<p class="img-center"><img alt="" height="30" src="https://images2.imgbox.com/89/5c/v7Ye3Atb_o.png" width="75"></p> 
<p>然后我们选择具有最高 AugCLIP 分数的前 k 个初始化，即</p> 
<p class="img-center"><img alt="" height="29" src="https://images2.imgbox.com/7d/8c/5vlr14ZI_o.png" width="82"></p> 
<p>并将它们用作后续优化的初始基向量。换句话说，我们将解（solution）重新参数化为</p> 
<p class="img-center"><img alt="" height="29" src="https://images2.imgbox.com/a3/4d/omv40l8y_o.png" width="187"></p> 
<p>并联合优化基向量和系数，其中 w(i) ∈ R。</p> 
<p class="img-center"><img alt="" height="30" src="https://images2.imgbox.com/f7/85/9ubkBJHc_o.png" width="90"></p> 
<p class="img-center"><img alt="" height="33" src="https://images2.imgbox.com/45/5e/0REblOo7_o.png" width="91"></p> 
<p class="img-center"><img alt="" height="69" src="https://images2.imgbox.com/f8/22/vcmCqkYI_o.png" width="500"></p> 
<p>其中，{ ξ_(i) } 初始化为选定的 k 个</p> 
<p class="img-center"><img alt="" height="32" src="https://images2.imgbox.com/83/a5/2iMD94vc_o.png" width="56"></p> 
<p>而 w_(i) 初始化为 1/k。我们设置 M = 10,000（可以并行评估）和相对较小的 k（例如，k ≤ 15）。</p> 
<p>尽管等式（4）中的优化等效于等式（2）中的优化，但它配备过参数化，以及更自然的坐标和更好的初始化，因此在使用基于梯度的优化方法时往往会产生更好的结果。特别是，组合权重 {w_(i)} 的更新对应于在基向量</p> 
<p class="img-center"><img alt="" height="29" src="https://images2.imgbox.com/1d/cb/KkpbQUJq_o.png" width="86"></p> 
<p>的线性空间中的快速全局移动，使其更容易逃离局部最优点。</p> 
<p>在实践中，由于我们使用 BigGAN，潜在编码 ξ = (z, y) 被初始化为 z ~ N(0, I)，而 y 则从 ImageNet 的 1,000 类别的潜在表示中随机选择（这比我们在附录中展示的 y ~ N(0, I) 更好）。</p> 
<p><strong>基于梯度或无梯度的优化器？</strong>在这项工作中，我们采用了广泛使用的 Adam [17] 优化器。一些最近的工作推荐在 GAN 空间中进行优化时使用无梯度优化器，如 BasinCMA [3, 16, 32]，因为存在很高的非凸性。然而，我们的研究表明，与 Adam 相比，BasinCMA 往往会带来更高的计算成本，因为 BasinCMA 在每次迭代时需要在目标上进行大量前向传递，而 Adam 只需要单次前向和后向传递。经验上，我们发现 Adam 比 BasinCMA 快大约 20 倍。尽管基于梯度的方法比无梯度的方法更容易陷入局部最优点，但在我们的 AugCLIP 损失和提出的初始化和过参数化技术下，这不再是一个问题。我们在附录中包含了更多与 BasinCMA 的讨论和比较。</p> 
<h3 id="3.3%20%E7%BB%84%E5%90%88%E7%94%9F%E6%88%90">3.3 组合生成</h3> 
<p>CLIP+GAN 方法的图像空间受我们使用的 GAN 的表示能力限制。这使得该方法难以生成超出分布范围的图像，并且容易继承来自 GAN 的原始训练集的数据偏差，例如中心、空间和颜色偏差 [2, 16]。我们提出了组合生成，通过将 GAN 生成的两幅图像组合在一起，以增加图像空间并减少数据偏差，获得更高的灵活性。我们的方法共同优化前景图像 I_fg = g( ~ξ_fg) 和背景图像 I_bg = g( ~ξ_bg)，其中 ~ξ_fg 和 ~ξ_bg 是两个过参数化的潜在编码，如公式 4 所示。这两幅图像用于生成融合图像</p> 
<p class="img-center"><img alt="" height="28" src="https://images2.imgbox.com/96/b7/zJ2WTCHf_o.png" width="301"></p> 
<p>首先，设置 I_fg 的大小缩放因子为 α ∈ (0,1)，然后将其粘贴在 I_bg 上的九个位置（t ∈ {left, center, right}^2）之一。我们希望选择 ~ξ := {~ξ_fg, ~ξ_bg}，以及 ~α := {α , t}，以最大化 I 的 AugCLIP 分数：</p> 
<p class="img-center"><img alt="" height="28" src="https://images2.imgbox.com/67/99/j4cAYLhD_o.png" width="322"></p> 
<p>另一方面，由于两个图像 I_fg 和 I_bg 是独立生成的，合成图像可能在边界上具有不自然和人工的不连续性。为了解决这个问题，我们引入了一个额外的损失，强制在 I_fg 和 I_bg 之间保持感知一致性， </p> 
<p class="img-center"><img alt="" height="28" src="https://images2.imgbox.com/dd/e4/Qbx1jUi8_o.png" width="415"></p> 
<p>其中，𝑙_per 代表 LPIPS 度量[37]，这是一种近似人类对图像相似性感知的度量。</p> 
<p>因此，我们希望既最大化 AugCLIP 分数，又最小化感知损失 𝑙_Fuse。一个朴素的方法是优化它们的线性组合。然而，这将需要在生成每个图像时仔细而逐个案例地调整组合系数。</p> 
<p>双层优化：我们提出了一种无调优方法，通过一个简单的双层（或词典）优化问题来组合这两个损失（参见例如 [8, 12]）</p> 
<p class="img-center"><img alt="" height="43" src="https://images2.imgbox.com/0b/0a/azMfgL9e_o.png" width="505"></p> 
<p>在这里，<code>arg max sFuse</code> 表示 s_Fuse 的（局部）极大值的集合。这个公式在 s_Fuse 的最优集合中寻找那些最小化 𝑙_<code>Fuse</code> 的点。它优先考虑优化 s_Fuse，同时将 𝑙_<code>Fuse</code> 作为次要损失进行考虑。</p> 
<p>我们通过在离散集合 α ∈ {0.65, 0.5} 和 t ∈ {left, center, right}^2 上进行蛮力搜索来优化 ~α = {α, t}。对于每个固定的 ~α ，我们使用来自 [12] 的动态障碍梯度下降算法来优化连续向量 ~ξ，这产生了以下简单的迭代规则。</p> 
<p class="img-center"><img alt="" height="98" src="https://images2.imgbox.com/cb/92/9VxNPdnB_o.png" width="479"></p> 
<p class="img-center"><img alt="" height="34" src="https://images2.imgbox.com/61/ba/4rvqrqn3_o.png" width="230"></p> 
<p class="img-center"><img alt="" height="38" src="https://images2.imgbox.com/51/60/hWBfstQS_o.png" width="215"></p> 
<p>在这里，ε^t &gt; 0 是步长；β 是一个超参数（默认为 1）。直观地说，可以将这个算法视为迭代地最小化线性组合的损失 𝑙_<code>Fuse - λ_t·s_Fuse</code>，其中系数 λ_t 动态地由梯度 ▽𝑙_<code>Fuse 和 ▽s_Fuse </code>之间的角度决定，移除 -▽𝑙_Fuse 中与 <code>▽</code>s_Fuse 冲突的成分，以确保 s_Fuse 在作为主要损失时单调递减。有关更多详细信息，请参阅附录和 [11]。</p> 
<p class="img-center"><img alt="" height="500" src="https://images2.imgbox.com/22/d9/U5LcY0Wk_o.png" width="549"></p> 
<p>在实践中，我们通过将 v^t 视为梯度方向，将等式（6）与 Adam 结合起来。此外，我们通过在 I_fg 和 I_bg 上应用 Poisson blending 来获得最终的合成图像，从而产生更平滑的图像 I，遵循[16]。我们的算法总结在 Alg. 2 中。</p> 
<h2 id="4.%20%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%C2%A0">4. 相关工作 </h2> 
<p>在生成对抗网络（GAN）的潜在空间中进行优化的一般思想已被广泛应用作为一个强大的框架，用于生成、编辑和恢复图像；例如，参见 [1, 14, 16, 39] 等。例如，[39] 提出将真实图像投影到GAN 的潜在空间以编辑图像。[14] 把主成分分析（principal component analysis，PCA）应用到 GAN 空间，为图像合成创建可解释的控制。[16] 通过无梯度优化器 BasinCMA [3, 32] 优化潜在编码，将给定图像嵌入到 BigGAN [4] 中，以在 GAN 空间中实现灵活的图像编辑。最近的一项工作[7] 使用逐层优化来提高解决逆问题（inverse problems，例如超分辨率和修复）在 GAN 空间中的性能。大多数这些方法仅专注于图像域上的单一任务，而我们的方法旨在通过利用 CLIP 的能力将图像与文本连接起来。</p> 
<p>在另一个方向上，使用 CLIP 分数 [24] 的思想已经在各个方向上得到了探索，包括视频检索 [21]、视觉问答 [30]和语言引导的图像操作/生成 [6, 10, 20, 23] 等。特别地，[23] 采用了 CLIP 和StyleGAN 来引导简单图像（通常是脸部、宠物或汽车的照片）的风格。[6, 10, 20] 是实现了基本GAN+CLIP 过程的开源存储库，我们通过新技术在这个基础上有了显著的改进。</p> 
<h2 id="5.%20%E5%AE%9E%E9%AA%8C"><strong>5. 实验</strong></h2> 
<p>我们将配备 BigGAN-256 的 FuseDream 与一些基线方法进行比较，包括 DM-GAN [40]、Obj-GAN [18]、CogView [9] 等。我们在流行的 MS COCO 数据集 [19] 上测试这些方法，并发现尽管BigGAN 在 ImageNet 上进行了预训练，但 FuseDream 明显优于基线方法。由于 CLIP 带来的丰富表示能力，FuseDream 可以生成具有不同方面的图像，包括艺术风格、天气、背景、纹理等，能够创造不存在的、反事实但合理的对象。此外，通过组合生成技术，我们可以生成具有多个对象的更好图像。请参见附录以查看论文中显示的图像的高分辨率副本。</p> 
<p><strong>在 MS COCO 测试集上的定量评估</strong>。为了与其他文本到图像生成方法进行比较，我们在从 COCO数据集中采样的 30,000 个标题的子集上评估我们的方法。我们遵循 [18, 31, 35, 40] 中提供的相同标准评估协议，使用 [18] 提供的官方代码。我们使用 Fréchet Inception Distance（FID）、Inception Score（IS）和 R-precision 来评估性能。对于 R-precision，按照 [18, 31, 35, 40] 的方法，我们计算全局图像向量与通过预训练的 CNN-RNN 检索模型 [35] 提取的 100 个候选句向量之间的余弦相似度。候选文本描述包括一个真实标题和 99 个随机选择的不相关句子。 R-precision 计算为所有 30,000 个生成图像的检索精度。我们随机重复该过程 3 次，并报告 R-precision 的均值和标准差。注意，基线 GAN 通常是针对最大化该分数进行训练的。为了公平比较，我们将 [35]中使用的检索模型替换为 CLIP 文本和图像编码器，并报告额外的 CLIP R-precision 分数。</p> 
<p class="img-center"><img alt="" height="521" src="https://images2.imgbox.com/36/93/FGfaBzRu_o.png" width="544"></p> 
<p>结果如表 1 所示。FuseDream 在 IS 分数上达到了与真实图像相当的水平（34.26 对比 34.88）。与在数十亿互联网图像上进行训练并具有巨大计算成本的 DALL-E [25] 和 CogView [9] 相比，我们显著提高了 IS 分数，从约 18 提高到 34，FID从 27 降低到 21（例如，对于使用 BigGAN-256、k = 5 的 FuseDream，FID 为 21.16）。请注意，我们使用的 BigGAN 是在 ImageNet 上进行训练的，尽管评估是在 COCO 图像上进行的；通过使用在 COCO 数据集上训练的更强大的生成模型，我们可以期望获得更好的结果。</p> 
<p class="img-center"><img alt="" height="756" src="https://images2.imgbox.com/ec/55/KlPPpw8y_o.png" width="729"></p> 
<p><strong>从 COCO 标题生成的图像</strong>。我们在图 7 中展示了 FuseDream 根据 COCO 数据集中的输入标题生成的一些图像。FuseDream 生成具有更多细节和目标的图像。例如，对于给定的 “夜晚商业街角上的交通和人群”，FuseDream 可以生成人群、汽车和繁华的街道，灯光璀璨。</p> 
<p class="img-center"><img alt="" height="644" src="https://images2.imgbox.com/8f/09/SWcQdwsz_o.png" width="1200"></p> 
<p><strong>变化的艺术风格</strong>。尽管 BigGAN 是在 ImageNet 上训练的，其中的图像大多是现实的，但通过CLIP，FuseDream 能够以不同的艺术风格生成有意义的图像，如图 6 中的第一行所示。这些图像具有六种不同的风格，例如照片、单色、版画、绘画、抽象绘画和水墨画。即使输入句子很复杂，我们也可以生成具有许多粒度的有意义的虚假图像。给定来自 Percy Shelley 的《致西风颂》的句子（“古老的宫殿和塔楼在波涛汹涌的日光中颤动”），FuseDream 成功生成了宫殿、塔楼、波涛和日光。</p> 
<p><strong>变化的纹理、背景和更多内容：</strong> 如 [14, 29] 所示，在标准 GAN 中很难控制纹理和背景。然而，FuseDream 可以通过输入句子很好地控制图像的纹理和背景。如图 6 的第二行和第三行所示，FuseDream 可以轻松将汽车放在不同的背景中（例如水下、夜晚、天空），并具有不同的纹理（例如丰富多彩的光辉、星空、幽灵）。将对象更改为机器人，我们还可以在不同的天气条件下（例如雨天、阳光明媚、雪天）生成有意义但虚假的机器人。此外，通过生成明显不同的餐点，FuseDream 似乎表现出对文化差异的理解：美国的餐点包含玉米、土豆泥和炸鸡；俄罗斯的餐点包含黑面包和俄式红菜汤；中国的餐点包含蛋饺和春卷。</p> 
<p class="img-center"><img alt="" height="307" src="https://images2.imgbox.com/c7/a3/0l4CiIni_o.png" width="542"></p> 
<p><strong>生成反事实内容：</strong> 在先前的例子中，我们展示了一些反事实的例子，例如图 1 中的燃烧的狗，图 6 中的天空中的汽车。在这里，我们使用 FuseDream 生成更多具有不同对象、背景和风格的高质量反事实图像。图 8 展示了我们可以生成 “发光发亮的狗”、“天空中的城堡”、“立方体蝴蝶” 和 “水下森林” 等图像。这些图像具有不同的对象、背景和风格，并且在现实世界中不存在，也不在BigGAN 的 ImageNet 训练数据中。令人惊讶的是，尽管我们从未更改 BigGAN 的参数，FuseDream 成功地生成了这些高质量的跨域图像。</p> 
<p><strong>FuseDream-Composition 中的多个概念：</strong> 通过生成包含两个对象的图像，我们验证了组合生成技术的性能。这两个对象通常不会在正常图像中同时出现，例如猫和蝴蝶，狗和教堂等。如图 9 所示，FuseDream（使用单个图像生成）可能将两个对象缠绕在一起，或者遗漏其中一个对象。例如，“一只狗靠近一艘船” 生成了一艘带有狗状帆的船。“一只蝴蝶靠近一艘船” 只生成了一只蝴蝶，而忽略了船。然而，通过使用组合生成，我们可以生成包含两个对象的图像。即使对于更复杂的句子，我们也可以生成有意义且高质量的图像（见图 9 中的第二行）。</p> 
<p><img alt="" height="807" src="https://images2.imgbox.com/fe/a7/3pyupXSd_o.png" width="1137"></p> 
<p>为验证我们的方法对随机种子的鲁棒性，我们在图 10 中为 “一只色彩斑斓的机器人在月光下走在冰冻的湖面上” 生成了更多的图像；我们获得了与句子相关的多样化的图像集。</p> 
<h2 id="6.%20%E7%BB%93%E8%AE%BA" style="background-color:transparent;">6. 结论</h2> 
<p>我们提出了 FuseDream，利用 CLIP 引导的 GAN 实现高质量、最先进的文本到图像生成。与传统的基于训练的方法相比，我们的方法无需训练，零样本，易于定制，因此易于访问计算资源有限或有特殊需求的用户。我们的新颖技术，如 AugCLIP 分数、过参数化优化（over-parameterized optimization）和组合生成（composed generation），具有独立的兴趣，并在其他潜在空间优化问题中具有用处。</p> 
<p></p> 
<h2 id="S.%20%E6%80%BB%E7%BB%93">S. 总结</h2> 
<h3 id="S.1%20%E4%B8%BB%E8%A6%81%E8%B4%A1%E7%8C%AE">S.1 主要贡献</h3> 
<p>相比于传统的从头训练的文本到图像生成，CLIP+GAN 是无需训练的、zero-shot 的，并且可以轻松定制以适应不同的生成器。</p> 
<p>本文提出了一种 FuseDream，通过三种关键技术改进了 CLIP+GAN 方法： 1）提出 AugCLIP 分数：通过在图像上引入随机增强，来避免 CLIP 分数的对抗攻击现象。2）利用新颖的初始化和过参数化（over-parameterization）策略，从而可在 GAN 空间中的非凸区域进行优化。3）利用双层优化，组合多个图像以扩展 GAN 空间。</p> 
<h3 id="S.2%20%E6%96%B9%E6%B3%95" style="background-color:transparent;">S.2 方法</h3> 
<p><strong>AugCLIP。</strong>CLIP 分数存在对抗性攻击问题：一张图像与原始图像相同，但具有更高 CLIP 分数。为解决这个问题，提出了 AugCLIP 分数：</p> 
<p class="img-center"><img alt="" height="28" src="https://images2.imgbox.com/7f/c4/X1OTPa8e_o.png" width="476"></p> 
<p>其中 I' 是来自候选数据增强分布 π(· | I) 的输入图像 I 的随机扰动。本文采用了 DiffAugment 中考虑的各种数据增强技术，包括随机着色、随机平移、随机调整大小和随机剪裁。 </p> 
<p><strong>初始化和过参数化。</strong></p> 
<ul><li>由于损失的高非凸性，s_AugCLIP 的优化仍然可能受到次优局部极大值的影响。本文引入了一种初始化和过参数化策略来改善优化。 </li><li>与从单一初始化开始的传统方法不同，本文首先采样大量（ M 个）初始化副本，然后选择具有最高 AugCLIP 分数的前 k 个初始化，并将它们用作后续优化的初始基向量。</li><li>因为它有过参数化，以及更自然的坐标和更好的初始化，因此在使用基于梯度的优化方法时往往会产生更好的结果。</li></ul> 
<p><strong>组合生成和双层优化</strong>。</p> 
<ul><li>通过将 GAN 生成的两幅图像组合在一起，以生成超出分布范围的图像。</li><li>该方法共同优化前景图像和背景图像，然后用这两幅图像生成融合图像。</li><li>为避免合成图像可能在边界上具有不自然和人工的不连续性，引入了一个额外的感知损失，强制前景和背景之间保持感知一致性。</li><li>最终目标是最大化 AugCLIP 分数，并最小化感知损失。一个朴素的方法是优化它们的线性组合。然而，这将需要在生成每个图像时仔细而逐个案例地调整组合系数。本文提出了一种无调优方法，通过一个简单的双层（或词典）优化问题来组合这两个损失。</li></ul>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1cdddf2a1dbb01c7801b51029cef5f38/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Skywalking 中 Agent 自动同步配置源码解析</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4a9288df7ea65b06a39e0a68ae72db54/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">k8s的探针</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>