<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>机器学习分类，损失函数中为什么要用Log，机器学习的应用 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="机器学习分类，损失函数中为什么要用Log，机器学习的应用" />
<meta property="og:description" content="目录
损失函数中为什么要用Log
为什么对数可以将乘法转化为加法？
机器学习（Machine Learning）
机器学习的分类
监督学习
无监督学习
强化学习
机器学习的应用
应用举例：猫狗分类
1. 现实问题抽象为数学问题
2. 数据准备
3. 选择模型
4. 模型训练及评估
5.预测结果
推荐阅读
损失函数中为什么要用Log ​Loss 在使用似然函数最大化时，其形式是进行连乘，但是为了便于处理，一般会套上log，这样便可以将连乘转化为求和，求和形式更容易求偏导，应用到梯度下降中求最优解；
由于log函数是单调递增函数，因此不会改变优化结果。
极大似然估计中取对数的原因：取对数后，连乘可以转化为相加，方便求导，这是因为对数函数的求导更加简单，对数函数的导数比原函数更容易计算和优化；除此之外对数函数 ln为单调递增函数，不会改变似然函数极值点。
为什么对数可以将乘法转化为加法？ log2(x*y) = log2(y) &#43; log2(y)
1， 2 ，3 ，4，5， 6······
和指数序列
2^(1)， 2^(2) ，2^(3) ，2^(4)，2^(5)， 2^(6)······
，可以看出上一序列是下一序列的指数部分。那么我们如果想计算2*8 = (2^(1))*(2^(3))就可以将指数部分先加起来，即1&#43;3=4，然后找第二个序列进行对应，就得到了2^(4)=16。这就是对数里的思想啦。
机器学习（Machine Learning） 基本思路是模仿人类学习的过程，例如人们一般通过经验归纳，总结规律，从而预测未来。
机器学习本质上就是让计算机自己在数据中学习规律，并根据所得到的规律对未来数据进行预测。
比如，不需要通过编程来识别猫或狗，机器学习可以通过使用图片来进行训练，从而归纳和识别特定的目标。
机器学习算法包括如聚类、分类、决策树、贝叶斯、神经网络、深度学习（Deep Learning）等。
机器学习的分类 机器学习经过几十年的发展，衍生出了很多种分类方法，这里按学习模式的不同，可分为
监督学习半监督学习无监督学习强化学习。 为了便于理解，用灰色圆点代表没有标签的数据，其他颜色的圆点代表不同的类别有标签数据。监督学习、无监督学习、强化学习的示意图如下所示：
监督学习 监督学习（Supervised Learning）是从有标签的训练数据中学习模型，然后对某个给定的新数据利用模型预测它的标签。如果分类标签精确度越高，则学习模型准确度越高，预测结果越精确。
监督学习主要用于回归和分类问题。
常见的监督学习的回归算法有：线性回归、回归树、K邻近、Adaboost、神经网络等。
常见的监督学习的分类算法有：朴素贝叶斯、决策树、SVM、逻辑回归、K邻近、Adaboost、神经网络等。
无监督学习 无监督学习（Unsupervised Learning）是从未标注数据中寻找隐含结构的过程。其中，
自监督学习(Self-Supervised Learning)方法在最近的学术界和工业界几年备受关注。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/f5a4461115f6ffcdc75a819159338335/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-21T17:07:12+08:00" />
<meta property="article:modified_time" content="2023-08-21T17:07:12+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习分类，损失函数中为什么要用Log，机器学习的应用</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%AD%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8Log-toc" style="margin-left:0px;"><a href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%AD%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8Log" rel="nofollow">损失函数中为什么要用Log</a></p> 
<p id="%E4%B8%BA%E4%BB%80%E4%B9%88%E5%AF%B9%E6%95%B0%E5%8F%AF%E4%BB%A5%E5%B0%86%E4%B9%98%E6%B3%95%E8%BD%AC%E5%8C%96%E4%B8%BA%E5%8A%A0%E6%B3%95%EF%BC%9F-toc" style="margin-left:0px;"><a href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E5%AF%B9%E6%95%B0%E5%8F%AF%E4%BB%A5%E5%B0%86%E4%B9%98%E6%B3%95%E8%BD%AC%E5%8C%96%E4%B8%BA%E5%8A%A0%E6%B3%95%EF%BC%9F" rel="nofollow">为什么对数可以将乘法转化为加法？</a></p> 
<p id="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88Machine%20Learning%EF%BC%89-toc" style="margin-left:0px;"><a href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88Machine%20Learning%EF%BC%89" rel="nofollow">机器学习（Machine Learning）</a></p> 
<p id="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%88%86%E7%B1%BB-toc" style="margin-left:0px;"><a href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%88%86%E7%B1%BB" rel="nofollow">机器学习的分类</a></p> 
<p id="%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-toc" style="margin-left:40px;"><a href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0" rel="nofollow">监督学习</a></p> 
<p id="%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-toc" style="margin-left:40px;"><a href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0" rel="nofollow">无监督学习</a></p> 
<p id="%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-toc" style="margin-left:40px;"><a href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0" rel="nofollow">强化学习</a></p> 
<p id="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BA%94%E7%94%A8-toc" style="margin-left:0px;"><a href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BA%94%E7%94%A8" rel="nofollow">机器学习的应用</a></p> 
<p id="%E5%BA%94%E7%94%A8%E4%B8%BE%E4%BE%8B%EF%BC%9A%E7%8C%AB%E7%8B%97%E5%88%86%E7%B1%BB-toc" style="margin-left:0px;"><a href="#%E5%BA%94%E7%94%A8%E4%B8%BE%E4%BE%8B%EF%BC%9A%E7%8C%AB%E7%8B%97%E5%88%86%E7%B1%BB" rel="nofollow">应用举例：猫狗分类</a></p> 
<p id="1.%20%E7%8E%B0%E5%AE%9E%E9%97%AE%E9%A2%98%E6%8A%BD%E8%B1%A1%E4%B8%BA%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98-toc" style="margin-left:40px;"><a href="#1.%20%E7%8E%B0%E5%AE%9E%E9%97%AE%E9%A2%98%E6%8A%BD%E8%B1%A1%E4%B8%BA%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98" rel="nofollow">1. 现实问题抽象为数学问题</a></p> 
<p id="2.%20%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87-toc" style="margin-left:40px;"><a href="#2.%20%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87" rel="nofollow">2. 数据准备</a></p> 
<p id="3.%20%E9%80%89%E6%8B%A9%E6%A8%A1%E5%9E%8B-toc" style="margin-left:40px;"><a href="#3.%20%E9%80%89%E6%8B%A9%E6%A8%A1%E5%9E%8B" rel="nofollow">3. 选择模型</a></p> 
<p id="4.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%8F%8A%E8%AF%84%E4%BC%B0-toc" style="margin-left:40px;"><a href="#4.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%8F%8A%E8%AF%84%E4%BC%B0" rel="nofollow">4. 模型训练及评估</a></p> 
<p id="5.%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C-toc" style="margin-left:40px;"><a href="#5.%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C" rel="nofollow">5.预测结果</a></p> 
<p id="%E6%8E%A8%E8%8D%90%E9%98%85%E8%AF%BB-toc" style="margin-left:40px;"><a href="#%E6%8E%A8%E8%8D%90%E9%98%85%E8%AF%BB" rel="nofollow">推荐阅读</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%AD%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8Log">损失函数中为什么要用Log</h2> 
<p><br> ​Loss 在使用<span style="color:#fe2c24;"><strong>似然函数最大化</strong></span>时，其形式是进<span style="color:#fe2c24;"><strong>行连乘</strong></span>，但是为了便于处理，一般会套上log，这样便可以将连乘<span style="color:#fe2c24;"><strong>转化为求和</strong></span>，<span style="color:#fe2c24;">求和形式更容易求偏导，应用到梯度下降中求最优解；</span></p> 
<p>由于log函数是单调递增函数，因此不会改变优化结果。</p> 
<p>极大似然估计中取对数的原因：取对数后，<span style="color:#fe2c24;">连乘可以转化为相加，方便求导，这是因为对数函数的求导更加简单</span>，对数函数的导数比原函数更容易<span style="color:#fe2c24;">计算和优化</span>；除此之外对数函数<span style="color:#fe2c24;"> ln为单调递增函数，不会改变似然函数极值点。</span></p> 
<h3></h3> 
<h2 id="%E4%B8%BA%E4%BB%80%E4%B9%88%E5%AF%B9%E6%95%B0%E5%8F%AF%E4%BB%A5%E5%B0%86%E4%B9%98%E6%B3%95%E8%BD%AC%E5%8C%96%E4%B8%BA%E5%8A%A0%E6%B3%95%EF%BC%9F">为什么对数可以将乘法转化为加法？</h2> 
<p></p> 
<p>log2(x*y) = log2(y) + log2(y)</p> 
<p>1， 2 ，3 ，4，5， 6······</p> 
<p>和指数序列</p> 
<p>2^(1)， 2^(2) ，2^(3) ，2^(4)，2^(5)， 2^(6)······</p> 
<p>，可以看出上一序列是下一序列的指数部分。那么我们如果想计算2*8 = (2^(1))*(2^(3))就可以将指数部分先加起来，即1+3=4，然后找第二个序列进行对应，就得到了2^(4)=16。这就是对数里的思想啦。</p> 
<p></p> 
<h2 id="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88Machine%20Learning%EF%BC%89">机器学习（Machine Learning）</h2> 
<p>基本思路是模仿人类学习的过程，例如人们一般通过经验归纳，总结规律，从而预测未来。</p> 
<p>机器学习本质上就是让计算机自己在数据中学习规律，并根据所得到的规律对未来数据进行预测。</p> 
<p>比如，不需要通过编程来识别猫或狗，机器学习可以通过使用图片来进行训练，从而归纳和识别特定的目标。</p> 
<p>机器学习算法包括如<span style="color:#fe2c24;"><strong>聚类、分类、决策树、贝叶斯、神经网络、深度学习（Deep Learning）</strong></span>等。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="279" src="https://images2.imgbox.com/46/13/KOLPpK3R_o.jpg" width="640"></p> 
<h2 id="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%88%86%E7%B1%BB">机器学习的分类</h2> 
<p>机器学习经过几十年的发展，衍生出了很多种分类方法，这里按学习模式的不同，可分为</p> 
<ul><li>监督学习</li><li>半监督学习</li><li>无监督学习</li><li>强化学习。</li></ul> 
<p>为了便于理解，用灰色圆点代表没有标签的数据，其他颜色的圆点代表不同的类别有标签数据。监督学习、无监督学习、强化学习的示意图如下所示：</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="361" src="https://images2.imgbox.com/50/b8/HCVmczxs_o.png" width="1080"></p> 
<h3 id="%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0">监督学习</h3> 
<p>监督学习（Supervised Learning）是从<span style="color:#fe2c24;"><strong>有标签的训练数据中学习模</strong></span>型，然后对某个给定的新数据利用模型预测它的标签。如果分类标签精确度越高，则学习模型准确度越高，预测结果越精确。</p> 
<p>监督学习主要用于<span style="color:#fe2c24;">回归和分类问</span>题。</p> 
<p>常见的监督学习的回归算法有：<span style="color:#fe2c24;">线性回归、回归树、K邻近、Adaboost、神经网络等</span>。</p> 
<p>常见的监督学习的分类算法有：<span style="color:#fe2c24;"><strong>朴素贝叶斯、决策树、SVM、逻辑回归、K邻近、Adaboost、神经网络</strong></span>等。</p> 
<h3 id="%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0">无监督学习</h3> 
<p>无监督学习（Unsupervised Learning）是从<span style="color:#fe2c24;"><strong>未标注数据中寻找隐含结构</strong></span>的过程。其中，</p> 
<p>自监督学习(Self-Supervised Learning)方法在最近的学术界和工业界几年备受关注。</p> 
<p>无监督学习主要用于<span style="color:#fe2c24;"><strong>关联分析、聚类和降维</strong></span>。</p> 
<p>常见的无监督学习算法有：稀疏自编码（Sparse Auto-Encoder）、<span style="color:#fe2c24;"><strong>主成分分析（Principal Component Analysis, PCA）、K-Means算法（K均值算法）</strong></span>、DBSCAN算法（Density-Based Spatial Clustering of Applications with Noise）、最大期望算法（Expectation-Maximization algorithm, EM）等。</p> 
<blockquote>
  “ 
 <p>如果人工智能是一块蛋糕，<span style="color:#fe2c24;"><strong>强化学习好比蛋糕上的樱桃，监督学习好比蛋糕上的糖衣，而蛋糕本身是非监督学习</strong></span>。—— Yann Lecun</p> ” 
</blockquote> 
<p>LeCun 的蛋糕强调了无监督的重要性，他认为这可以突破当前 AI 技术的局限性。今天的 AI 可以轻松对图像进行分类并识别声音，但不能执行诸如<strong>推理不同对象之间的关系或预测人类运动等任务。这是无监督学习可以填补空白的地方。</strong></p> 
<p></p> 
<p class="img-center"><img alt="图片" height="608" src="https://images2.imgbox.com/86/62/9kpkAfCK_o.png" width="1080"></p> 
<h3 id="%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0">强化学习</h3> 
<p>强化学习（Reinforcement Learning）类似于监督学习，但未使用样本数据进行训练，而是<span style="color:#fe2c24;"><strong>通过智能体（Agnet）与环境（Environment）</strong></span>的交互，在不断试错中进行学习的模式。</p> 
<p>在监督学习和非监督学习中，<span style="color:#fe2c24;"><strong>数据是静态的、不需要与环境进行交互，</strong></span>比如猫狗识别，只要给出足够的差异样本，将数据输入神经网络中进行训练即可。</p> 
<p>然而，强化学习的学习过程是<span style="color:#fe2c24;"><strong>动态的、不断交互的过程，所需要的数据也是通过与环境不断交互所产生的。</strong></span></p> 
<p>所以，与监督学习和非监督学习相比，强化学习涉及的对象更多，比如动作、环境、状态转移概率和回报函数等。</p> 
<p>强化学习常用于<span style="color:#fe2c24;"><strong>机器人避障、棋牌类游戏（AlphaGo）、广告和推荐等应用场景中，解决的是决策问题。</strong></span></p> 
<h2 id="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BA%94%E7%94%A8">机器学习的应用</h2> 
<p>机器学习是将现实中的<span style="color:#fe2c24;"><strong>问题抽象为数学模型，利用历史数据对数据模型进行训练，然后基于数据模型对新数据进行求解，并将结果再转为现实问题的答案的过程。</strong></span></p> 
<p>机器学习一般的应用实现步骤如下：</p> 
<ul><li> <p>将现实问题抽象为数学问题；</p> </li><li> <p>数据准备；</p> </li><li> <p>选择或创建模型；</p> </li><li> <p>模型训练及评估；</p> </li><li> <p>预测结果。</p> </li></ul> 
<p></p> 
<p class="img-center"><img alt="图片" height="453" src="https://images2.imgbox.com/60/74/kxzhrKbS_o.png" width="1080"></p> 
<h2 id="%E5%BA%94%E7%94%A8%E4%B8%BE%E4%BE%8B%EF%BC%9A%E7%8C%AB%E7%8B%97%E5%88%86%E7%B1%BB">应用举例：猫狗分类</h2> 
<p>这里我们以Kaggle上的一个竞赛Cats vs. Dogs（猫狗大战）来举例，感兴趣的同学可亲自动手实验。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="278" src="https://images2.imgbox.com/c2/3c/PQul2SJh_o.png" width="450"></p> 
<h3 id="1.%20%E7%8E%B0%E5%AE%9E%E9%97%AE%E9%A2%98%E6%8A%BD%E8%B1%A1%E4%B8%BA%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98">1. 现实问题抽象为数学问题</h3> 
<p>现实问题：给定一张图片，让计算机判断是猫还是狗？</p> 
<p>数学问题：二分类问题，1表示分类结果是狗，0表示分类结果是猫。</p> 
<h3 id="2.%20%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87">2. 数据准备</h3> 
<p>数据下载地址：<br> https://www.kaggle.com/c/dogs-vs-cats。</p> 
<p> </p> 
<p>下载 kaggle 猫狗数据集解压后分为 3 个文件 train.zip、 test.zip 和 sample_submission.csv。</p> 
<p>训练集 train.zip，包含25000张已标记的图片文件，文件名格式为“类别.图片id.jpg”，类别为cat或dog，图片id为数字，如cat.0.jpg、dog.12247.jpg。训练集数据中标记为猫、狗的图片分别有12500张，比例1:1。</p> 
<p>测试集 test.zip，包含12500张未标记的图片文件，文件名格式为“图片id.jpg”，图片id为数字，如1.jpg、11605.jpg。</p> 
<p>数据集中图片尺寸大小不一，但在训练和测试时需要统一尺寸。数据中图像不一定完整包含完整猫或狗的身体，有的主体在图片中很小，图片背景复杂，图片里会出现人或其他物体，如左图1。另外，训练集中包含少量非猫或狗的图片，如右图2，这些异常数据大约占训练集的5.6 ‱，需要被清理掉。</p> 
<p>这些异常图片文件名如下：cat.4688.jpg，cat.5418.jpg，cat.7377.jpg，cat.7564.jpg，cat.8100.jpg，cat.8456.jpg，cat.10029.jpg，cat.12272.jpg，dog.1259.jpg，dog.1895.jpg，dog.4367.jpg，dog.8736.jpg，dog.9517.jpg，dog.10190.jpg，dog.11299.jpg。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="182" src="https://images2.imgbox.com/16/fd/e4qAIQKf_o.png" width="240"></p> 
<p>复杂背景</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="182" src="https://images2.imgbox.com/b1/ef/1KEHo5TR_o.png" width="242"></p> 
<p>异常数据</p> 
<ul><li> <p>sample_submission.csv 需要将最终测试集的测试结果写入.csv 文件中。</p> </li></ul> 
<p>后续的实验中，我们将数据分成3个部分：训练集（60%）、验证集（20%）、测试集（20%），用于后面的验证和评估工作。一般三者切分的比例是：6：2：2，不过验证集并不是必须的，没有也是可以的。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="293" src="https://images2.imgbox.com/79/b8/Ci0mzRjX_o.png" width="1080"></p> 
<p>训练集、验证集、测试集作用这里说明一下：</p> 
<ul><li> <p>训练集用来调试神经网络</p> </li><li> <p>验证集用来查看训练效果</p> </li><li> <p>测试集用来测试网络的实际学习能力</p> </li></ul> 
<p>训练集(train)毋庸置疑，是用于模型拟合的数据样本，用来调试网络中的参数。我们容易混淆的是验证集和测试集：验证集没有参与网络参数更新的工作，按理说也能用来测试网络的实际学习能力；测试集本来也能就是用来测试效果的，按理来说也能查看训练效果。</p> 
<p>我们换个说法或者详细一些可能就会明白了：</p> 
<p>验证集(validation): 查看模型训练的效果是否朝着坏的方向进行。验证集的作用是体现在训练的过程。举个栗子：通过查看训练集和验证集的损失值随着epoch的变化关系可以看出模型是否过拟合，如果是可以及时停止训练，然后根据情况调整模型结构和超参数，大大节省时间。</p> 
<p>测试集(test): 用来评估模最终模型的泛化能力。但不能作为调参、选择特征等算法相关的选择的依据。测试集的作用是体现在测试的过程。</p> 
<p>一个形象的比喻：</p> 
<ul><li> <p><strong>训练集：学生的课本</strong>；学生根据课本里的内容来掌握知识。训练集直接参与了模型调参的过程，显然不能用来反映模型真实的能力(防止课本死记硬背的学生拥有最好的成绩，即防止过拟合)。</p> </li><li> <p><strong>验证集：作业</strong>；通过作业可以知道不同学生学习情况、进步的速度快慢。验证集参与了人工调<strong>参(超参数)的过程，也不能用来最终评判一个模型(刷题库的学生不能算是学习好的学生)。</strong></p> </li><li> <p><strong>测试集：考试</strong>；考的题是平常都没有见过，考察学生举一反三的能力。所以要通过最终的考试(测试集)来考察一个学(模)生(型)真正的能力(期末考试)。</p> </li></ul> 
<p>对原始数据进行三个数据集的划分，也是为了防止模型过拟合。当使用了所有的原始数据去训练模型，得到的结果很可能是该模型最大程度地拟合了原始训练数据。当新的样本出现，再使用该模型进行预测，效果可能还不如只使用一部分数据训练的模型。</p> 
<pre><code>import cv2
import os
import numpy as np

import random
import time

import pickle

data_dir = './data'  # 解压后数据

start_time = time.time()

print("正在制作数据....")

# 图片统一大小100*100
# 训练集 20000张
# 测试集 剩下的所有，测试集从训练集中进行切分，因为测试集没有标签

all_data_files = os.listdir(os.path.join(data_dir, "train/"))

random.shuffle(all_data_files)  # 打乱文件顺序

all_train_files = all_data_files[:20000]  # 前20000个图片用来训练
all_test_files = all_data_files[20000:]  # 后5000个图片用来测试

train_images = []  # 存储图片对应的narry数组的
train_labels = []  # 存储图片对应标签
train_files = []  # 存储对应图片名

test_images = []
test_labels = []
test_files = []

for each in all_train_files:
    img = cv2.imread(os.path.join(data_dir, "train", each), 1)
    # print(img.shape)  # 每张图片的大小不一致，需要转换成统一大小
    resized_img = cv2.resize(img, (100, 100))

    img_data = np.array(resized_img)  # 统一转换成narray数组类型，因为tensorflow支持narray
    train_images.append(img_data)
    if 'cat' in each:
        train_labels.append(0)  # 0表示猫
    elif 'dog' in each:
        train_labels.append(1)  # 1表示狗
    else:
        raise Exception("\n%s is a wrong train file" % (each))
    train_files.append(each)

for each in all_test_files:
    img = cv2.imread(os.path.join(data_dir, "train", each), 1)
    # print(img.shape)  # 每张图片的大小不一致，需要转换成统一大小
    resized_img = cv2.resize(img, (100, 100))

    img_data = np.array(resized_img)  # 统一转换成narray数组类型，因为tensorflow支持narray
    test_images.append(img_data)
    if 'cat' in each:
        test_labels.append(0)  # 0表示猫
    elif 'dog' in each:
        test_labels.append(1)  # 1表示狗
    else:
        raise Exception("\n%s is a wrong test file" % (each))
    test_files.append(each)

# print(len(train_images), len(test_images))

train_data = {
    'images': train_images,
    'labels': train_labels,
    'files': train_files
}

test_data = {
    'images': test_images,
    'labels': test_labels,
    'files': test_files
}

with open(os.path.join(data_dir,"train-data"),'wb') as f:
    pickle.dump(train_data,f)

with open(os.path.join(data_dir,'test-data'),'wb') as f:
    pickle.dump(test_data,f)

end_time = time.time()

print('制作结束，用时{}秒.'.format(end_time-start_time))
</code></pre> 
<h3 id="3.%20%E9%80%89%E6%8B%A9%E6%A8%A1%E5%9E%8B">3. 选择模型</h3> 
<p>机器学习有很多模型，需要选择哪种模型，需要根据数据类型，样本数量，问题本身综合考虑。</p> 
<p>如本问题主要是处理图像数据，可以考虑使用卷积神经网络(Convolutional Neural Network, CNN)模型来实现二分类，因为选择CNN的优点之一在于避<span style="color:#fe2c24;"><strong>免了对图像前期预处理过程（提取特征等）。</strong></span></p> 
<p>猫狗识别的卷积神经网络结构如下图所示：</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="360" src="https://images2.imgbox.com/29/70/7mu7UKse_o.gif" width="640"></p> 
<p>最下层是网络的输入层（Input Layer），用于读入图像作为网络的数据输入；最上层是网络的输出层（Output Layer），其作用是预测并输出读入图像的类别，由于只需要区分猫和狗，因此输出层只有2个神经计算单元；位于输入和输出层之间的，都称之为隐含层（Hidden Layer），也叫卷积层（Convolutional Layer），图示中包含3个隐含层。</p> 
<h3 id="4.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%8F%8A%E8%AF%84%E4%BC%B0">4. 模型训练及评估</h3> 
<p></p> 
<h3></h3> 
<p></p> 
<p>我们需要预先设定损失函数Loss计算得到的损失值，这里选择对数损失函数（Log Loss）作为模型评价指标。</p> 
<p>对数损失函数（Log Loss）亦被称为<strong>逻辑回归损失（Logistic regression loss）或交叉熵损失（Cross-entropy loss）</strong>，刻画的是<strong>两个概率分布之间的距离</strong>，是分类问题中使用广泛的一种损失函数。<strong><span style="color:#fe2c24;">交叉熵损失越小，代表模型的性能越好</span></strong>。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="92" src="https://images2.imgbox.com/00/e4/Ul9S3cEF_o.png" width="716"></p> 
<ul><li> <p>n是测试集中图片数量；</p> </li><li> <p>y尖 是图片预测为狗的概率；</p> </li><li> <p><img alt="y_{i}" class="mathcode" src="https://images2.imgbox.com/bf/4c/nuDqx2bo_o.png">如果图像是狗，则为1，如果是猫，则为0；</p> </li><li> <p>loge 是自然常数  为底的自然对数。</p> </li></ul> 
<p>我们用准确率(Accuracy)来衡量算法预测结果的准确程度：</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="96" src="https://images2.imgbox.com/d1/c0/ObwfoTzb_o.png" width="495"></p> 
<ul><li> <p>TP(True Positive)是将正类预测为正类的结果数目；</p> </li><li> <p>FP(False Positive)是将负类预测为正类的结果数目；</p> </li><li> <p>TN(True Negative)是将负类预测为负类的结果数目；</p> </li><li> <p>FN(False Negative)是将正类预测为负类的结果数目。</p> </li></ul> 
<pre><code>from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout, Convolution2D, MaxPool2D, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
import pickle
import numpy as np


def load_data(filename):
    with open(filename, 'rb') as f:
        data = pickle.load(f, encoding='utf-8')
    return np.array(data['images']), to_categorical(np.array(data['labels']), num_classes=2), np.array(data['files'])


TRAIN_DIR = "data/train-data"

train_images, train_labels, train_files = load_data(TRAIN_DIR)

model = Sequential([
    Convolution2D(16, kernel_size=(3, 3), strides=(1, 1), padding="same", input_shape=(100, 100, 3), activation='relu'),
    # 100*100*96
    MaxPool2D((2, 2), strides=(2, 2), padding='same'),  # 50*50*96
    Convolution2D(32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu'),  # 50*50*192
    MaxPool2D((2, 2), strides=(2, 2), padding='same'),  # 25*25*192
    Convolution2D(64, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu'),  # 25*25*384
    MaxPool2D((2, 2), strides=(2, 2), padding='same'),  #
    Convolution2D(128, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu'),
    MaxPool2D((2, 2), strides=(2, 2), padding='same'),
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.3),
    Dense(256, activation='relu'),
    Dropout(0.3),
    Dense(2)
])
# 模型编译
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
# 模型训练
model.fit(train_images, train_labels, batch_size=200, epochs=10)
# 训练完保存模型
model.save("cat_and_dog.h5") # hdf5文件 pip intall h5py
</code></pre> 
<p></p> 
<p class="img-center"><img alt="图片" height="472" src="https://images2.imgbox.com/02/a1/CDYntR3t_o.png" width="1080"></p> 
<p>训练过中的 loss 和 accuracy，使用GPU训练速度会更快，i5 CPU也是可以跑的，增加训练轮次，准确率会更高</p> 
<h3 id="5.%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C">5.预测结果</h3> 
<p>训练好的模型，我们可以看看模型的识别效果：</p> 
<pre><code>from tensorflow.keras.models import load_model
from tensorflow.keras.utils import to_categorical
import pickle
import numpy as np


def load_data(filename):
    with open(filename, 'rb') as f:
        data = pickle.load(f, encoding='utf-8')
    return np.array(data['images']), to_categorical(np.array(data['labels']), num_classes=2), np.array(data['files'])

TEST_DIR = "data/test-data"              

test_image, test_labels, test_files = load_data(TEST_DIR)

model = load_model("cat_and_dog.h5") # 同时加载结构和参数

# 模型评估

loss, accuracy = model.evaluate(test_image, test_labels)
print("test loss", loss)
print("test accuracy", accuracy)
</code></pre> 
<p></p> 
<p class="img-center"><img alt="图片" height="396" src="https://images2.imgbox.com/60/17/z2KVKqRm_o.png" width="1080"></p> 
<p>至此，我们就完成了一个简单的机器学习二分类任务。重在明白流程，细节我们都会在日后的文章中慢慢说清楚。</p> 
<h3 id="%E6%8E%A8%E8%8D%90%E9%98%85%E8%AF%BB">推荐阅读</h3> 
<ul><li> <p><a href="http://mp.weixin.qq.com/s?__biz=MzU5NzkyMTYzNw==&amp;mid=2247506389&amp;idx=1&amp;sn=b4379c7c9e2ef1789a9c9e968c562b70&amp;chksm=fe4e83dfc9390ac95b577c8b948f5378ba5cbf04c96ca0fb5fd2701dcfebe6c42d561704ea01&amp;scene=21#wechat_redirect" rel="nofollow" title="一份最有效的小白学AI路线图">一份最有效的小白学AI路线图</a></p> </li><li> <p><a href="http://mp.weixin.qq.com/s?__biz=MzU5NzkyMTYzNw==&amp;mid=2247503768&amp;idx=1&amp;sn=be2a9c3ad8343ea6e4a96a65f8cd9836&amp;chksm=fe4e9592c9391c84520496a3bfed663114c8e5fb82e82bda666460044969ffc242c212de2966&amp;scene=21#wechat_redirect" rel="nofollow" title="AI常用编程工具介绍与安装">AI常用编程工具介绍与安装</a></p> </li><li> <p><a href="http://mp.weixin.qq.com/s?__biz=MzU5NzkyMTYzNw==&amp;mid=2247496216&amp;idx=1&amp;sn=60d8086207642e23026f7019c1065884&amp;chksm=fe4ea812c93921046ca134ec2302948c56cc71e6d28c41c3ac31bd8bc86f88cff105acf3dac2&amp;scene=21#wechat_redirect" rel="nofollow" title="XGBoost详解">XGBoost详解</a></p> </li><li> <p><a href="http://mp.weixin.qq.com/s?__biz=MzU5NzkyMTYzNw==&amp;mid=2247505256&amp;idx=1&amp;sn=6fb47c5f1977038ff9f3ef50a7f78535&amp;chksm=fe4e8f62c9390674dae5f8538175a326e9f0ccd0a395d64ac80f8937c6fa4c66a34225bafa1f&amp;scene=21#wechat_redirect" rel="nofollow" title="通俗易懂详解注意力机制">通俗易懂详解注意力机制</a></p> </li><li> <p><a href="http://mp.weixin.qq.com/s?__biz=MzU5NzkyMTYzNw==&amp;mid=2247486667&amp;idx=1&amp;sn=837f88f9f11dab43448a60defdc55424&amp;chksm=fe4d56c1c93adfd76557cd6fbc81f013589ce130e84a668c9153eccc6161e3906ae34dff0e55&amp;scene=21#wechat_redirect" rel="nofollow" title="关于Attention的总结">关于Attention的总结</a></p> </li></ul> 
<h3></h3> 
<p></p> 
<p></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/51b1591b82da61b6e27c3990ac3e2203/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">机器视觉系列（四）——相机部分</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7caafe46ca28ce9adb1345563916e1f2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">mybatis级联查询</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>