<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>电商搜索场景结构化匹配 使用命名实体识别（NER）&#43;类目预测（意图识别）&#43;bert4keras实现k-bert - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="电商搜索场景结构化匹配 使用命名实体识别（NER）&#43;类目预测（意图识别）&#43;bert4keras实现k-bert" />
<meta property="og:description" content="上一篇的文章中电商搜索使用BM25算法召回&#43;其他匹配特征主要讲了BM25算法的召回以及一些特征的融入，本篇继续进行剩余特征如核心词匹配，同义词匹配 ，上下位词，query类目与商品title类目匹配以及商品的业态等特征
整体结构图如下：
示例： 乐事薯片黄瓜 分词： 乐事 薯片 黄瓜 词性：品牌，商品核心词，口位词 这里面 补充一下：
一.分词的话你可以参照 知乎上面的 分词或者命名实体识别融合词典
二.词性匹配要注意 词性消歧义，因为不同的词在不同的商品中可能会有不同的词性，例如山东新鲜黄瓜， 黄瓜味薯片 这里面的黄瓜分别为商品核心词，口位词
具体的做法 当然不局限这些，你都可以尝试：
譬如：
黄瓜薯片 黄瓜为口位词属性
黄瓜茄子组合 黄瓜为商品核心词
基于规则分词处理方法：
1.根据类目信息，假设有分词性中有两个商品核心词a,b ，c为类目信息，要进行过滤
p(a|b,c)=p(a,b,c)/p(b,c)
p(b|a,c)=p(a,b,c)/p(a,c)
比较两个核心词谁是谁的典型
p(a|b,c)&lt;p(b|a,c) 则选取a为核心词否则选取b
2.根据term权重 tfidff,deepct或者其他的
3.相似度模型，可以根据类目预测的模型，或者向量召回的模型 来进行相似度的权重衡量，分别计算不同的term 占原query或者原商品名称的cos值，占比越大可以作为最核心的词
基于模型处理的方法：
1.预训练增加相应的知识 融入类目信息，cls .....sep....sep....sep 分别为 query 商品 类目信息 loss=mlm_loss&#43;quer是否匹配商品 （可以参照美团技术预训练搜索）
2.融入额外知识，k-bert,k-adapter, gcn网络 组合 编造品牌，商品核心词，口位词的图谱关系（spellgcn https://github.com/ACL2020SpellGCN/SpellGCN ）
这里使用k-bert 来实现https://arxiv.org/pdf/1909.07606.pdf
罗列的知识库体系如下：
乐事	类别	品牌 薯片	类别	商品 黄瓜 类别 商品 黄瓜 类别 口位 乐事 售卖 薯片 薯片 属性 黄瓜 可口 竞品 百事 可口 类别 品牌 百事 类别 品牌 可乐 类别 商品 雪碧 类别 商品 巧克力 属性 牛奶味 牛奶味 类别 口味 牛奶 类别 口味 巧克力 类别 商品 法国 售卖 香槟 #本文使用的bert4keras 实现k-bert 将下面的三个函数 粘贴到models." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/55b1ff628f5c409d51fcbbb77e5020ee/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-03-14T22:08:02+08:00" />
<meta property="article:modified_time" content="2022-03-14T22:08:02+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">电商搜索场景结构化匹配 使用命名实体识别（NER）&#43;类目预测（意图识别）&#43;bert4keras实现k-bert</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>上一篇的文章中<a class="link-info" href="https://blog.csdn.net/bingheshidai_1234/article/details/122358157" title="电商搜索使用BM25算法召回+其他匹配特征">电商搜索使用BM25算法召回+其他匹配特征</a>主要讲了BM25算法的召回以及一些特征的融入，本篇继续进行剩余特征如核心词匹配，同义词匹配  ，上下位词，query类目与商品title类目匹配以及商品的业态等特征</p> 
<p>整体结构图如下：</p> 
<p><img alt="" height="821" src="https://images2.imgbox.com/68/67/mwpceCda_o.png" width="1200"></p> 
<p></p> 
<p>示例：  乐事薯片黄瓜   </p> 
<p>        分词： 乐事   薯片  黄瓜   </p> 
<p>        词性：品牌，商品核心词，口位词 </p> 
<p>这里面 补充一下：</p> 
<p><strong>一.分词的话你可以参照  知乎上面的 分词或者命名实体识别融合词典</strong></p> 
<p><strong>二.词性匹配要注意 词性消歧义，因为不同的词在不同的商品中可能会有不同的词性，例如山东新鲜黄瓜， 黄瓜味薯片  这里面的黄瓜分别为商品核心词，口位词</strong></p> 
<p>具体的做法  当然不局限这些，你都可以尝试：</p> 
<p><br><br>         譬如：<br><br>         黄瓜薯片  黄瓜为口位词属性<br><br>         黄瓜茄子组合  黄瓜为商品核心词<br><br><br> 基于规则分词处理方法：<br><br>         1.根据类目信息，假设有分词性中有两个商品核心词a,b ，c为类目信息，要进行过滤<br> p(a|b,c)=p(a,b,c)/p(b,c)<br> p(b|a,c)=p(a,b,c)/p(a,c)<br> 比较两个核心词谁是谁的典型<br><br> p(a|b,c)&lt;p(b|a,c) 则选取a为核心词否则选取b<br><br>         2.根据term权重  tfidff,deepct或者其他的<br><br>         3.相似度模型，可以根据类目预测的模型，或者向量召回的模型 来进行相似度的权重衡量，分别计算不同的term 占原query或者原商品名称的cos值，占比越大可以作为最核心的词<br><br><br><br> 基于模型处理的方法：<br>         1.预训练增加相应的知识   融入类目信息，cls .....sep....sep....sep   分别为 query  商品 类目信息  <br> loss=mlm_loss+quer是否匹配商品   （可以参照美团技术预训练搜索）<br><br>         2.融入额外知识，k-bert,k-adapter, gcn网络 组合 编造品牌，商品核心词，口位词的图谱关系（spellgcn <a href="https://github.com/ACL2020SpellGCN/SpellGCN" title="https://github.com/ACL2020SpellGCN/SpellGCN">https://github.com/ACL2020SpellGCN/SpellGCN</a> ）</p> 
<p>这里使用k-bert 来实现<a href="https://arxiv.org/pdf/1909.07606.pdf" rel="nofollow" title="https://arxiv.org/pdf/1909.07606.pdf">https://arxiv.org/pdf/1909.07606.pdf</a></p> 
<p> 罗列的知识库体系如下：</p> 
<pre><code>乐事	类别	品牌
薯片	类别	商品
黄瓜 类别 商品
黄瓜 类别 口位
乐事 售卖 薯片
薯片 属性 黄瓜
可口 竞品 百事
可口 类别 品牌
百事  类别 品牌
可乐 类别 商品
雪碧 类别 商品
巧克力 属性 牛奶味
牛奶味 类别 口味
牛奶 类别 口味
巧克力 类别 商品
法国 售卖 香槟</code></pre> 
<p><img alt="" height="526" src="https://images2.imgbox.com/94/91/5AIGv9U9_o.png" width="476"></p> 
<pre><code>#本文使用的bert4keras 实现k-bert 将下面的三个函数 粘贴到models.py(bert4keras的源码)
def extend_with_vi_model(BaseModel):
    """添加下三角的Attention Mask（语言模型用）
    """
    class VI_Model(VI_Mask, BaseModel):
        """带下三角Attention Mask的派生模型
        """
        def __init__(self, *args, **kwargs):
            super(VI_Model, self).__init__(*args, **kwargs)
            self.with_mlm = self.with_mlm or True

    return VI_Model


class VI_Mask(object):
    """定义下三角Attention Mask（语言模型用）
    """
    def compute_attention_bias(self, inputs=None):
        """通过idxs序列的比较来得到对应的mask
        """
        if self.attention_bias is None:
            def vi_mask(s):
                mask = K.cast(s, K.floatx())
                mask=-(1 - mask) * K.infinity()
                mask = K.expand_dims(mask, axis=1)
                mask = K.tile(mask, [1, 12,1, 1])
                return mask
            self.attention_bias = self.apply(
                inputs=self.inputs[-1],
                layer=Lambda,
                function=vi_mask,
                name='Attention-VI-Mask'
            )

        return self.attention_bias

class Kbert(BERT):
    """构建GPT模型
    链接：https://github.com/openai/finetune-transformer-lm
    """
    @insert_arguments(final_activation='softmax')
    @delete_arguments('with_pool', 'with_mlm')
    def __init__(self, **kwargs):
        super(Kbert, self).__init__(**kwargs)
        self.custom_position_ids = True


    def get_inputs(self):
        """BERT的输入是token_ids和segment_ids
        （但允许自行传入位置id，以实现一些特殊需求）
        """
        x_in = self.apply(
            layer=Input, shape=(self.sequence_length,), name='Input-Token'
        )
        inputs = [x_in]

        if self.segment_vocab_size &gt; 0:
            s_in = self.apply(
                layer=Input,
                shape=(self.sequence_length,),
                name='Input-Segment'
            )
            inputs.append(s_in)

        if self.custom_position_ids:
            p_in = self.apply(
                layer=Input,
                shape=(self.sequence_length,),
                name='Input-Position'
            )
            inputs.append(p_in)

        vm_in = self.apply(
            layer=Input,
            shape=(self.sequence_length,self.sequence_length),
            name='Input-Vmask'
        )
        inputs.append(vm_in)

        return inputs

    def apply_embeddings(self, inputs):
        """GPT的embedding是token、position、segment三者embedding之和
        跟BERT的主要区别是三者相加之后没有加LayerNormalization层。
        """
        inputs = inputs[:]
        x = inputs.pop(0)
        s = inputs.pop(0)
        p = inputs.pop(0)
        x = self.apply(
            inputs=x,
            layer=Embedding,
            input_dim=self.vocab_size,
            output_dim=self.embedding_size,
            embeddings_initializer=self.initializer,
            mask_zero=True,
            name='Embedding-Token'
        )
        if 1==1:
            name = 'Embedding-Segment'
            s = self.apply(
                inputs=s,
                layer=Embedding,
                input_dim=2,
                output_dim=self.embedding_size,
                embeddings_initializer=self.initializer,
                name=name
            )
            x = self.apply(
                inputs=[x, s], layer=Add, name='Embedding-Token-Segment'
            )
        x = self.apply(
            inputs=self.simplify([x, p]),
            layer=PositionEmbedding,
            input_dim=self.max_position,
            output_dim=self.embedding_size,
            merge_mode='add',
            hierarchical=self.hierarchical_position,
            embeddings_initializer=self.initializer,
            custom_position_ids=self.custom_position_ids,
            name='Embedding-Position'
        )
        x = self.apply(
            inputs=x,
            layer=Dropout,
            rate=self.dropout_rate,
            name='Embedding-Dropout'
        )
        if self.embedding_size != self.hidden_size:
            x = self.apply(
                inputs=x,
                layer=Dense,
                units=self.hidden_size,
                kernel_initializer=self.initializer,
                name='Embedding-Mapping'
            )

        return x

    def apply_final_layers(self, inputs):
        """剩余部分
        """
        x = inputs

        # Language Model部分
        x = self.apply(
            inputs=x,
            layer=Embedding,
            arguments={'mode': 'dense'},
            name='Embedding-Token'
        )
        x = self.apply(
            inputs=x,
            layer=Activation,
            activation=self.final_activation,
            name='LM-Activation'
        )

        return x

    def load_variable(self, checkpoint, name):
        """加载单个变量的函数
        """
        variable = super(GPT, self).load_variable(checkpoint, name)
        if name == 'gpt/embeddings/word_embeddings':
            return self.load_embeddings(variable)
        else:
            return variable

    def variable_mapping(self):
        """映射到TF版GPT权重格式
        """
        mapping = super(GPT, self).variable_mapping()
        mapping = {
            k: [
                i.replace('bert/', 'gpt/').replace('encoder', 'transformer')
                for i in v
            ]
            for k, v in mapping.items()
        }
        return mapping




#######################
######自定义模型#########
######自定义模型##########
#######################


base = build_transformer_model(config_path, application='kbert', model='kbert')







</code></pre> 
<p>   </p> 
<p></p> 
<p><strong>三.命中es不同的列，里面会有多级商品核心词，多级商品品牌</strong></p> 
<p>        譬如 伊利特仑苏鲜奶      品牌 伊利,特仑苏   商品核心词：鲜奶 牛奶  奶制品</p> 
<p>多级品牌是维护的，多级商品核心词是实体识别出来 或者切词切出来，牛奶 ，奶制品是 上下位词处理的 </p> 
<p></p> 
<p><strong>四.命中es不同的列，加不同的分数</strong></p> 
<p><strong>      query中原词、同义词到指定的列做召回</strong></p> 
<p></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/60a7aa00f8692a9b016ba8e9f2c4e92f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">实时数仓：基于 Flink CDC 实现 Oracle 数据实时更新到 Kudu</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0e8875c6a4232c6d65811683d1862019/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Postgis使用工具raster2pgsql导入栅格数据</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>