<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Python语音增强 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Python语音增强" />
<meta property="og:description" content="简介 音频时域波形具有以下特征：音调，响度，质量。我们在进行数据增强时，最好只做一些小改动，使得增强数据和源数据存在较小差异即可，切记不能改变原有数据的结构，不然将产生“脏数据”，通过对音频数据进行数据增强，能有助于我们的模型避免过度拟合并变得更加通用。
经过实验发现对声波的以下改变是有用的：Noise addition(增加噪音)、Add reverb(增加混响)、Time shifting(时移)、Pitch shifting(改变音调)和Time stretching(时间拉伸)。
本文需要使用的python库：
matplotlib：绘制图像librosa：音频数据处理numpy：矩阵数据处理 常见的失真有：
加性声学噪声：加性噪声与期望信号不相干，平稳加性噪声(背景环境声音、嗡嗡声、功放噪音)，非平稳加性噪声(媒体干扰、非期望语音干扰和一些电子干扰)声学混响：多径反射引起的叠加效应(与期望信号相关)卷积信道效应：导致不均匀或带宽限制响应，为了去除信道脉冲响应，做信道均衡时对通信信道没有有效建模非线性失真：信号输入时不适当的增益，常出现与幅度限制、麦克风功放等加性宽带电子噪声电器干扰编码失真：比如压缩编码录音仪器引起的失真：麦克风频率响应不足 先画出原始语音数据的语谱图和波形图:
import librosa import numpy as np import matplotlib.pyplot as plt plt.rcParams[&#39;font.sans-serif&#39;] = [&#39;SimHei&#39;] # 用来正常显示中文标签 plt.rcParams[&#39;axes.unicode_minus&#39;] = False # 用来正常显示符号 fs = 16000 wav_data, _ = librosa.load(&#34;/home/gxli/lgx/Data/gather_crop/clean1/2148_farend.wav&#34;, sr=fs, mono=True) # ########### 画图 plt.subplot(2, 2, 1) plt.title(&#34;语谱图&#34;, fontsize=15) plt.specgram(wav_data, Fs=16000, scale_by_freq=True, sides=&#39;default&#39;, cmap=&#34;jet&#34;) plt.xlabel(&#39;秒/s&#39;, fontsize=15) plt.ylabel(&#39;频率/Hz&#39;, fontsize=15) plt.subplot(2, 2, 2) plt.title(&#34;波形图&#34;, fontsize=15) time = np.arange(0, len(wav_data)) * (1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/b73d6f6ed6907fc75017194965bd2c7b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-08T19:37:09+08:00" />
<meta property="article:modified_time" content="2023-05-08T19:37:09+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Python语音增强</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="cnblogs_post_body"> 
 <h2 id="简介">简介</h2> 
 <p>音频时域波形具有以下特征：音调，响度，质量。我们在进行数据增强时，最好只做一些小改动，使得增强数据和源数据存在较小差异即可，切记不能改变原有数据的结构，不然将产生“脏数据”，通过对音频数据进行数据增强，能有助于我们的模型避免过度拟合并变得更加通用。</p> 
 <p>经过实验发现对声波的以下改变是有用的：<strong>Noise addition</strong>(增加噪音)、<strong>Add reverb</strong>(增加混响)、<strong>Time shifting</strong>(时移)、<strong>Pitch shifting</strong>(改变音调)和<strong>Time stretching</strong>(时间拉伸)。</p> 
 <p>本文需要使用的python库：</p> 
 <ul><li>matplotlib：绘制图像</li><li>librosa：音频数据处理</li><li>numpy：矩阵数据处理</li></ul> 
 <p>常见的失真有：</p> 
 <ol><li>加性声学噪声：加性噪声与期望信号不相干，平稳加性噪声(背景环境声音、嗡嗡声、功放噪音)，非平稳加性噪声(媒体干扰、非期望语音干扰和一些电子干扰)</li><li>声学混响：多径反射引起的叠加效应(与期望信号相关)</li><li>卷积信道效应：导致不均匀或带宽限制响应，为了去除信道脉冲响应，做信道均衡时对通信信道没有有效建模</li><li>非线性失真：信号输入时不适当的增益，常出现与幅度限制、麦克风功放等加性宽带电子噪声电器干扰</li><li>编码失真：比如压缩编码</li><li>录音仪器引起的失真：麦克风频率响应不足</li></ol> 
 <p>先画出原始语音数据的语谱图和波形图:</p> 
 <pre class="has"><code class="language-python">import librosa
import numpy as np
import matplotlib.pyplot as plt

plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示符号
fs = 16000

wav_data, _ = librosa.load("/home/gxli/lgx/Data/gather_crop/clean1/2148_farend.wav", sr=fs, mono=True)

# ########### 画图
plt.subplot(2, 2, 1)
plt.title("语谱图", fontsize=15)
plt.specgram(wav_data, Fs=16000, scale_by_freq=True, sides='default', cmap="jet")
plt.xlabel('秒/s', fontsize=15)
plt.ylabel('频率/Hz', fontsize=15)

plt.subplot(2, 2, 2)
plt.title("波形图", fontsize=15)
time = np.arange(0, len(wav_data)) * (1.0 / fs)
plt.plot(time, wav_data)
plt.xlabel('秒/s', fontsize=15)
plt.ylabel('振幅', fontsize=15)

plt.tight_layout()
# plt.savefig("save.png")
plt.show()</code></pre> 
 <p><img alt="img" src="https://images2.imgbox.com/c5/7e/PDAglKYW_o.png"></p> 
 <h2 id="时域增强">时域增强</h2> 
 <h3 id="噪声增强">噪声增强</h3> 
 <h4 id="第一种控制噪声因子">第一种：控制噪声因子</h4> 
 <pre class="has"><code class="language-python">def add_noise1(clean, noise, gain=0.004):
    # gain：噪声增益因子
    noisy = clean + gain * noise
    return noisy</code></pre> 
 <p><img alt="img" src="https://images2.imgbox.com/d3/62/a4bd6vdv_o.png"></p> 
 <h4 id="第二种根据snr生成noisy">第二种：根据SNR生成noisy</h4> 
 <p>通过信噪比的公式推导出噪声的增益系数k。</p> 
 <div>
   \[SNR=10*log_{10}(\frac{S^2}{(kN)^2}) \Rightarrow k=\sqrt{\frac{S^2}{N^2*10^{\frac{SNR}{10}}}} \] 
 </div> 
 <pre class="has"><code class="language-python">def snr2noise(clean, noise, SNR):
    """
    :param clean: 纯净语音
    :param far_echo: 噪音
    :param SER: 指定的SNR
    :return: 根据指定的SNR求带噪语音(纯净语音+噪声)
    """
    p_clean = np.mean(clean ** 2)  # 纯净语音功率
    p_noise = np.mean(noise ** 2)  # 噪声功率

    scalar = np.sqrt(p_clean / (10 ** (SNR / 10)) / (p_noise + np.finfo(np.float32).eps))
    noisy = clean + scalar * noise

    return noisy</code></pre> 
 <h4 id="第三种制造鸡尾酒效应的带噪语音">第三种：制造鸡尾酒效应的带噪语音</h4> 
 <p>其实并没有那么玄乎，就是将纯净语音和多段带噪语音进行相加，然后控制一下信噪比。</p> 
 <h3 id="音量增强">音量增强</h3> 
 <p>语音音量的单位为dB，音量增益可以基于平均音量或者最大瞬时音量，下面公式是基于平均音量推得dB增益：</p> 
 <div>
   \[dB=10*log_{10}(kS)^2\Rightarrow k=\sqrt{\frac{10^{\frac{dB}{10}}}{S^2}} \] 
 </div> 
 <pre class="has"><code class="language-python">def volumeAument1(wav, dB):
    """
    :param wav: 语音
    :param dB: 音量
    :return:返回以指定dB增益后的语音
    """
    power = np.mean(wav ** 2)  # 平均功率
    scalar = np.sqrt(10 ** (dB / 10) / (power + np.finfo(np.float32).eps))
    wav *= scalar
    return wav, scalar</code></pre> 
 <div>
   \[dB=20*log_{10}kS\Rightarrow k=\frac{10^{\frac{db}{20}}}{A} \] 
 </div> 
 <pre class="has"><code class="language-python">def volumeAument2(wav, dB):
    """
    :param wav: 语音
    :param dB: 音量
    :return:返回以指定dB增益后的语音
    """
    rmswav = (wav ** 2).mean() ** 0.5
    scalar = 10 ** (dB / 20) / (rmswav + np.finfo(np.float32).eps)
    wav = wav * scalar
    return wav, scalar</code></pre> 
 <p>其实这两个函数都可以，都可以达到目的，本质上都一样。</p> 
 <pre class="has"><code class="language-python">"""
音量增强
"""
import numpy as np
import librosa

EPS = np.finfo(float).eps


def mean_dbfs(sample_data):
    rms = np.sqrt(np.mean(np.square(sample_data, dtype=np.float64)))
    dbfs = 20.0 * np.log10(max(1e-16, rms))
    return dbfs


def volumeAument1(wav, dB):
    """
    :param wav: 语音
    :param dB: 音量
    :return:返回以指定dB增益后的语音
    """
    power = np.mean(wav ** 2)  # 平均功率
    scalar = np.sqrt(10 ** (dB / 10) / (power + np.finfo(np.float32).eps))
    wav *= scalar
    return wav, scalar


def volumeAument2(wav, dB):
    """
    :param wav: 语音
    :param dB: 音量
    :return:返回以指定dB增益后的语音
    """
    rmswav = (wav ** 2).mean() ** 0.5
    scalar = 10 ** (dB / 20) / (rmswav + np.finfo(np.float32).eps)
    wav = wav * scalar
    return wav, scalar


sr = 16000
wav = librosa.load("./wavdata/TIMIT.WAV", sr=sr)[0]  # (46797,)
print(wav.shape)
wav, scalar = volumeAument1(wav, dB=15)
print(mean_dbfs(wav))  # 18.0103004778581

wav, scalar = volumeAument2(wav, 15)
print(mean_dbfs(wav))  # 18.010299731550788</code></pre> 
 <h3 id="混响增强">混响增强</h3> 
 <p>我这里使用的是Image Source Method(镜像源方法)来实现语音加混响，我想用两种方法来给大家实现，第一种是直接调用python库—— Pyroomacoustics来实现音频加混响，第二种就是按照公式推导一步一步来实现，两种效果一样，想看细节的可以参考第二种方法，只想开始实现效果的可以只看第一种方法：</p> 
 <h4 id="方法一-pyroomacoustics实现音频加混响">方法一： Pyroomacoustics实现音频加混响</h4> 
 <p>首先需要安装 Pyroomacoustics，这个库非常强大，感兴趣也可以多看看其他API接口</p> 
 <pre class="has"><code class="language-python">pip install  Pyroomacoustics</code></pre> 
 <p>步骤：</p> 
 <ol><li>创建房间(定义房间大小、所需的混响时间、墙面材料、允许的最大反射次数、)</li><li>在房间内创建信号源</li><li>在房间内放置麦克风</li><li>创建房间冲击响应</li><li>模拟声音传播</li></ol> 
 <pre class="has"><code class="language-python">import pyroomacoustics as pra
import numpy as np
import matplotlib.pyplot as plt
import librosa

# 1、创建房间
# 所需的混响时间和房间的尺寸
rt60_tgt = 0.5  # 所需的混响时间，秒
room_dim = [9, 7.5, 3.5]  # 我们定义了一个9m x 7.5m x 3.5m的房间，米

# 我们可以使用Sabine’s公式来计算壁面能量吸收和达到预期混响时间所需的ISM的最大阶数(RT60，即RIR衰减60分贝所需的时间)
e_absorption, max_order = pra.inverse_sabine(rt60_tgt, room_dim)    # 返回 墙壁吸收的能量 和 允许的反射次数
# 我们还可以自定义 墙壁材料 和 最大反射次数
# m = pra.Material(energy_absorption="hard_surface")    # 定义 墙的材料，我们还可以定义不同墙面的的材料
# max_order = 3

room = pra.ShoeBox(room_dim, fs=16000, materials=pra.Material(e_absorption), max_order=max_order)

# 在房间内创建一个位于[2.5,3.73,1.76]的源，从0.3秒开始向仿真中发出wav文件的内容
audio, _ = librosa.load("speech.wav",sr=16000)  # 导入一个单通道语音作为源信号 source signal
room.add_source([2.5, 3.73, 1.76], signal=audio, delay=0.3)

# 3、在房间放置麦克风
# 定义麦克风的位置：(ndim, nmics) 即每个列包含一个麦克风的坐标
# 在这里我们创建一个带有两个麦克风的数组，
# 分别位于[6.3,4.87,1.2]和[6.3,4.93,1.2]。
mic_locs = np.c_[
    [6.3, 4.87, 1.2],  # mic 1
    [6.3, 4.93, 1.2],  # mic 2
]

room.add_microphone_array(mic_locs)     # 最后将麦克风阵列放在房间里

# 4、创建房间冲击响应(Room Impulse Response)
room.compute_rir()

# 5、模拟声音传播，每个源的信号将与相应的房间脉冲响应进行卷积。卷积的输出将在麦克风上求和。
room.simulate()

# 保存所有的信号到wav文件
room.mic_array.to_wav("./guitar_16k_reverb_ISM.wav", norm=True, bitdepth=np.float32,)

# 测量混响时间
rt60 = room.measure_rt60()
print("The desired RT60 was {}".format(rt60_tgt))
print("The measured RT60 is {}".format(rt60[1, 0]))


plt.figure()
# 绘制其中一个RIR. both can also be plotted using room.plot_rir()
rir_1_0 = room.rir[1][0]    # 画出 mic 1和 source 0 之间的 RIR
plt.subplot(2, 1, 1)
plt.plot(np.arange(len(rir_1_0)) / room.fs, rir_1_0)
plt.title("The RIR from source 0 to mic 1")
plt.xlabel("Time [s]")

# 绘制 microphone 1 处接收到的信号
plt.subplot(2, 1, 2)
plt.plot(np.arange(len(room.mic_array.signals[1, :])) / room.fs, room.mic_array.signals[1, :])
plt.title("Microphone 1 signal")
plt.xlabel("Time [s]")

plt.tight_layout()
plt.show()</code></pre> 
 <p>混合ISM/射线跟踪房间模拟器</p> 
 <pre class="has"><code class="language-python">room = pra.ShoeBox(
    room_dim,
    fs=16000,
    materials=pra.Material(e_absorption),
    max_order=3,
    ray_tracing=True,
    air_absorption=True,
)

# 激活射线追踪
room.set_ray_tracing()</code></pre> 
 <p>控制信噪比</p> 
 <pre class="has"><code class="language-python">room.simulate(reference_mic=0, snr=10)      # 控制信噪比</code></pre> 
 <p><img alt="img" src="https://images2.imgbox.com/b6/ab/EdDqR7d5_o.png"><img alt="img" src="https://images2.imgbox.com/24/7b/nJz3HYOl_o.png"></p> 
 <h4 id="方法二image-source-method-算法讲解">方法二：Image Source Method 算法讲解</h4> 
 <p>从这里要讲算法和原理了，</p> 
 <p>代码参考：matlab版本：<a href="https://github.com/ehabets/RIR-Generator" title="RIR-Generator">RIR-Generator</a> python版本：<a href="https://github.com/audiolabs/rir-generator" title="rir-generator">rir-generator</a></p> 
 <p>镜像源法简介：<img alt="img" src="https://images2.imgbox.com/41/86/WgVCxNd2_o.png"></p> 
 <p>将反射面等效为一个虚像，或者说镜像。比如说，在一个开放空间里有一面平整墙面，那么一个声源可以等效为2两个声源；一个开放空间里有两面垂直的平整墙面，那么一个声源可以等效为4个；同理三面的话是8个。原理上就是这样，但是封闭的三维空间里情况有那么点复杂，一般来说，家里的空房间可以一定程度上近似为矩形盒子，假设房间尺寸为：</p> 
 <div>
   \[L=\left[x_{r}, y_{r}, z_{r}\right] \] 
 </div> 
 <p>元素大小分别代表长宽高，而声源的三维坐标为:</p> 
 <div>
   \[S=\left[x_{s}, y_{s}, z_{s}\right] \] 
 </div> 
 <p>麦克风的三维坐标为:</p> 
 <div>
   \[M=\left[x_{m}, y_{m}, z_{m}\right] \] 
 </div> 
 <p>镜像声源\((i,j,k)\)到麦克风距离在三个坐标轴上的位置为</p> 
 <div>
   \[x_{i}=(-1)^{i} x_{s}+\left[i+\left(1-(-1)^{i}\right) / 2\right] x_{r}-x_{m} \] 
 </div> 
 <div>
   \[y_{j}=(-1)^{j} y_{s}+\left[j+\left(1-(-1)^{j}\right) / 2\right] y_{r}-y_{m} \] 
 </div> 
 <div>
   \[z_{k}=(-1)^{k} z_{s}+\left[k+\left(1-(-1)^{k}\right) / 2\right] z_{r}-z_{m} \] 
 </div> 
 <p>那么声源\((i,j,k)\)距离麦克风的距离为:</p> 
 <div>
   \[d_{i j k}=\sqrt{\left(x_{i}^{2}+y_{j}^{2}+z_{k}^{2}\right)} \] 
 </div> 
 <p>相对于直达声的到达延迟时间为:</p> 
 <div>
   \[\tau_{i j k}=\left(d_{i j k}-r\right) / c \] 
 </div> 
 <p>其中\(c\)为声速，\(r\)为声源到麦克风的直线距离。那么，混响效果等效为不同延迟的信号的叠加，即混响效果可以表示为一个FIR滤波器与信号源卷积的形式，此滤波器可写为如下形式:</p> 
 <div>
   \[h(t)=\sum_{i} \sum_{j} \sum_{k}\left[A_{i j k} \delta\left(t-\tau_{i j k}\right)\right] \] 
 </div> 
 <p>滤波器的抽头系数与镜面的反射系数与距离相关，如果每个面的反射系数不同则形式略复杂。详细代码还是要看RIR-Generator，我这里只做抛转引玉，写一个最简单的。</p> 
 <p>模拟镜像源：<br> 房间尺寸(m)：4 X 4 X 3<br> 声源坐标(m)：2 X 2 X 0<br> 麦克风坐标(m)：2 X 2 X 1.5<br> 混响时间(s)：0.2<br> RIR长度：512</p> 
 <p><strong>Image Source方法</strong></p> 
 <pre class="has"><code>clc;clear;
c = 340;                    % 声速 (m/s)
fs = 16000;                 % Sample frequency (samples/s)
r = [2 2 1.5];              % 麦克风位置 [x y z] (m)
s = [2 2 0];              % 扬声器位置 [x y z] (m)
L = [4 4 3];                % 房间大小 [x y z] (m)
beta = 0.2;                 % 混响时间 (s)
n = 512;                   % RIR长度

h = rir_generator(c, fs, r, s, L, beta, n);
disp(size(h))   % (1,4096)

[speech, fs] = audioread("./test_wav/p225_001.wav");
disp(size(speech)); % (46797,1)

y = conv(speech', h);
disp(length(y))


% 开始画图
figure('color','w');    % 背景色设置成白色
subplot(3,1,1)
plot(h)
title("房间冲击响应 RIR","FontSize",14)

subplot(3,2,3)
plot(speech)
title("原语音波形","FontSize",14)

subplot(3,2,4)
plot(y)
title("加混响语音波形","FontSize",14)

subplot(3,2,5)
specgram(speech,512,fs,512,256);
title("原语音频谱","FontSize",14)

subplot(3,2,6)
specgram(y,512,fs,512,256);
title("加混响语音频谱","FontSize",14)

audiowrite("./test_wav/matlab_p225_001_reverber.wav",y,fs)</code></pre> 
 <p><img alt="img" src="https://images2.imgbox.com/09/6c/Ve9V8bsc_o.png"></p> 
 <h4 id="方法三利用rir生成混响">方法三：利用RIR生成混响</h4> 
 <pre class="has"><code class="language-python">def add_pyreverb(wav, rir):
    reverb_wav = signal.fftconvolve(wav, rir, mode="full")
    reverb_wav = reverb_wav[0: wav.shape[0]]  # 使 reverb_wav 和 wav 具有相同的长度
    return reverb_wav</code></pre> 
 <h3 id="指定ser生成远端语音">指定SER生成远端语音</h3> 
 <p>SER的公式为:</p> 
 <div>
   \[SER=10\log_{10}\frac{E\{s^2(n)\}}{E\{d^2(n)\}} \] 
 </div> 
 <p>其中E是统计 期望操作，\(s(n)\)是近端语音，\(d(n)\)是远端回声，由于我们需要根据指定的SER求混响信号，并且近端语音和远端混响都是已知的，我们只需要求得一个系数，来调整回声信号的能量大小，与远端混响相乘即可得我们想要的混响语音，即调整后的回声信号为\(kd(n)\)。</p> 
 <div>
   \[k=\sqrt{\frac{E\{s^2(n)\}}{E\{d^2(n)\}*10^{\frac{SER}{10}}}} \] 
 </div> 
 <p>根据以上公式，可以推导出\(k\)的值，最终\(kd(n)\)即我们所求的指定SER的混响。</p> 
 <pre class="has"><code class="language-python">def add_echo_ser(near_speech, far_echo, SER):
    """根据指定的SER求回声
    :param near_speech: 近端语音
    :param far_echo: 远端回声
    :param SER: 指定的SER
    :return: 指定SER的回声
    """
    p_near_speech = np.mean(near_speech ** 2)  # 近端语音功率
    p_far_echo = np.mean(far_echo ** 2)  # 远端回声功率

    k = np.sqrt(p_near_speech / (10 ** (SER / 10)) / p_far_echo)

    return k * far_echo</code></pre> 
 <p><img alt="img" src="https://images2.imgbox.com/52/7c/rVUF2wLG_o.png"></p> 
 <h3 id="波形位移">波形位移</h3> 
 <p>语音波形移动使用numpy.roll函数向右移动shift距离</p> 
 <pre class="has"><code>numpy.roll(a, shift, axis=None)</code></pre> 
 <p>参数：</p> 
 <ul><li>a：数组</li><li>shift：滚动的长度</li><li>axis：滚动的维度。0为垂直滚动，1为水平滚动，参数为None时，会先将数组扁平化，进行滚动操作后，恢复原始形状</li></ul> 
 <pre class="has"><code class="language-python">x = np.arange(10)
# array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

print(np.roll(x, 2))
# array([8, 9, 0, 1, 2, 3, 4, 5, 6, 7])</code></pre> 
 <p>波形位移函数：</p> 
 <pre class="has"><code class="language-python">def time_shift(x, shift):
    # shift：移动的长度
    return np.roll(x, int(shift))
Augmentation = time_shift(wav_data, shift=fs//2)</code></pre> 
 <p><img alt="img" src="https://images2.imgbox.com/44/b5/y7aZQ4ja_o.png"></p> 
 <h3 id="谐波失真">谐波失真</h3> 
 <p>参考自：soundpy</p> 
 <pre class="has"><code class="language-python">def harmonic_distortion(wav):
    wav = 2 * np.pi * wav
    count = 0
    while count &lt; 5:
        wav = np.sin(wav)
        count += 1
    return wav</code></pre> 
 <p><img alt="img" src="https://images2.imgbox.com/c2/e5/vtRlgubo_o.png"></p> 
 <h3 id="重采样数据增强">重采样数据增强</h3> 
 <p>重采样后语音数据会丢失 重采样采样率到源采样值之间的频谱信息。</p> 
 <pre class="has"><code class="language-python">def augment_resample(wav, sr):
    resample_sr = np.random.uniform(sr)     # 从一个均匀分布中随机采样
    print("target_sr", resample_sr)
    resample = librosa.resample(wav, orig_sr=sr, target_sr=resample_sr)
    resample = librosa.resample(resample, orig_sr=resample_sr, target_sr=sr)
    return resample</code></pre> 
 <p><img alt="img" src="https://images2.imgbox.com/da/ab/YeNNPlGy_o.png"></p> 
 <h2 id="频域增强">频域增强</h2> 
 <h3 id="音高增强pitch-shifting">音高增强(Pitch Shifting)</h3> 
 <p>在频率轴上缩放频谱图，从而改变音高。音高修正只改变音高而不影响音速，发现-5到5之间的步数更合适</p> 
 <pre class="has"><code class="language-python"># sr: 音频采样率
# n_steps: 要移动多少步
# bins_per_octave: 每个八度音阶(半音)多少步
    
# 上移大三度(如果bins_per_octave为12，则4步)
augment = librosa.effects.pitch_shift(wav_data, sr=sr, n_steps=4, bins_per_octave=12)
# 向下移动一个三全音(如果bins_per_octave是 12，则为六步)
augment = librosa.effects.pitch_shift(wav_data, sr=sr, n_steps=-6, bins_per_octave=12)
# 上移 3 个四分音符
augment = librosa.effects.pitch_shift(wav_data, sr=sr, n_steps=3, bins_per_octave=24)</code></pre> 
 <p><img alt="img" src="https://images2.imgbox.com/0c/da/4TOxLMDL_o.png"></p> 
 <h3 id="速度增强tempo">速度增强(Tempo)</h3> 
 <p>在时间轴上缩放频谱图，从而改变播放速度。</p> 
 <h4 id="变速不变调">变速不变调</h4> 
 <p>方法一：ffmpeg</p> 
 <p>在变速之前我们需要安装 <code>pip install ffmpeg </code></p> 
 <pre class="has"><code class="language-python">from ffmpeg import audio

# 加快2倍速度
audio.a_speed("./sample/p225_001.wav",speed=2,out_file="./sample/p225_001_2.wav")

# 放慢2倍速度
audio.a_speed("./sample/p225_001.wav",speed=0.5,out_file="./sample/p225_001_0.5.wav")</code></pre> 
 <p>ffmpeg是基于fmpeg开发的，Python的这个库不能加载太大的文件，但是原生的fmpeg。或者我们可以直接使用原生的ffmpeg工具包<img alt="img" src="https://images2.imgbox.com/16/70/htoG9POd_o.png"></p> 
 <p>我们可以看到变速前后的波形图和语谱图没变，但是他们的时间维度却减少了一半。</p> 
 <p>方法二：SoundTorch</p> 
 <p>SoundTouch 是一个开源音频处理库，用于更改音频流或音频文件的速度、音高和播放速率。该库还支持估计音轨的稳定每分钟节拍率。</p> 
 <p>命令实例见：<a href="https://www.surina.net/soundtouch/soundstretch.html" rel="nofollow" title="SoundStretch Audio Processing Utility">SoundStretch Audio Processing Utility</a></p> 
 <p>速度增加100%</p> 
 <pre class="has"><code>soundstretch input.wav output.wav -tempo=100</code></pre> 
 <p>速度降低50%</p> 
 <pre class="has"><code>soundstretch input.wav output.wav -tempo=-50</code></pre> 
 <p><img alt="img" src="https://images2.imgbox.com/57/60/xME93Ohr_o.png"></p> 
 <h4 id="变速变调">变速变调</h4> 
 <p>方法一：SOX</p> 
 <p>需要在linux上运行，具体参考https://github.com/rabitt/pysox</p> 
 <pre class="has"><code class="language-python">import soundfile
import sox

sr = 16000

tfm = sox.Transformer()     # create transformer
tfm.speed(2)                # 变速2倍

# 创建输出文件
# tfm.build_file("./sample/p225_001.wav", "./sample/pysox_2x.wav")

# 内存中以numpy数组的形式获取输出
array_out = tfm.build_array(input_filepath="./sample/p225_001.wav")
soundfile.write("./sample/pysox_2x.wav",data=array_out,samplerate=sr)</code></pre> 
 <p>或者我们直接使用原生的sox工具包</p> 
 <pre class="has"><code>$ sox input.wav output.wav speed 1.3 #速度变为原来的1.3倍
$ sox input.wav output.wav speed 0.8 #速度变为原来的0.8倍</code></pre> 
 <p><img alt="img" src="https://images2.imgbox.com/be/b4/kI2z7qiq_o.png"></p> 
 <p>方法二：librosa</p> 
 <p>按固定速率对音频系列进行时间拉伸。</p> 
 <pre class="has"><code>def time_stretch(x, rate):
    # rate：拉伸的尺寸，
    # rate &gt; 1 加快速度
    # rate &lt; 1 放慢速度
    return librosa.effects.time_stretch(x, rate)

Augmentation = time_stretch(wav_data, rate=2)</code></pre> 
 <p><img alt="img" src="https://images2.imgbox.com/9e/52/aDMpQOE1_o.png"></p> 
 <p>我们来观察语谱图和波形图，发现形状变了，并且变速后的语音波形振幅降低了，为什么呢？难道变速还会减少语音的音量？求解答</p> 
 <p>SpecAugment 通过在时间方向上通过在时间方向上扭曲来增强，并屏蔽(多个)连续时间步长(垂直掩模)和 mel 频率通道(水平掩模)的块</p> 
 <ol><li>帮助网络在时间方向上的变形、频率信息的部分丢失和输入的小段语音的部分丢失方面具有鲁棒性</li><li>防止网络过度拟合</li></ol> 
 <p>SpecAugment 中有三种增强策略：</p> 
 <ul><li>时间扭曲(Time Warping)：在时间轴上随机扭曲频谱图。与速度扰动不同，这种方法不会增加或减少持续时间，而是在局部压缩和拉伸频谱图。</li><li>频率掩蔽(Frequency Mask)：频谱图的 连续频率bin被随机掩蔽</li><li>时间掩蔽(Time Mask)：频谱图的 连续时间帧被掩蔽</li></ul> 
 <p><img alt="img" src="https://images2.imgbox.com/f5/a2/RD2kDnbK_o.png"></p> 
 <p>paperwithcode：<a href="https://paperswithcode.com/paper/specaugment-a-simple-data-augmentation-method" rel="nofollow" title="SpecAugment">SpecAugment</a>几乎所有的代码都列出来了</p> 
 <ul><li>tensorflow和pytorch实现：<a href="https://github.com/DemisEom/SpecAugment" title="SpecAugment">SpecAugment</a></li><li>numpy实现：<a href="https://github.com/KimJeongSun/SpecAugment_numpy_scipy" title="SpecAugment_numpy_scipy">SpecAugment_numpy_scipy</a></li><li>numpy实现：<a href="https://github.com/pyyush/SpecAugment" title="SpecAugment">SpecAugment</a> (很好理解)</li></ul> 
 <h3 id="扭曲增强warp">扭曲增强(Warp)</h3> 
 <p>将非线性图像扭曲应用于频谱图。这是通过沿时间和频率轴随机移动均匀分布的扭曲点网格来实现的。代码修改自：<a href="https://github.com/mozilla/DeepSpeech" title="DeepSpeech">DeepSpeech</a></p> 
 <pre class="has"><code class="language-python">def tf_pick_value_from_range(value, r, clock=None, double_precision=False):
    clock = (tf.random.stateless_uniform([], seed=(-1, 1), dtype=tf.float64) if clock is None
             else tf.maximum(tf.constant(0.0, dtype=tf.float64), tf.minimum(tf.constant(1.0, dtype=tf.float64), clock)))
    value = tf.random.stateless_uniform([],
                                        minval=value - r,
                                        maxval=value + r,
                                        seed=(clock * tf.int32.min, clock * tf.int32.max),
                                        dtype=tf.float64)
    if isinstance(value, int):
        return tf.cast(tf.math.round(value), tf.int64 if double_precision else tf.int32)
    return tf.cast(value, tf.float64 if double_precision else tf.float32)


def Warp(spectrogram, num_t=1, num_f=1, warp_t=0.1, warp_f=0.0, r=0, clock=0.0):
    """
    :param spectrogram: tensor (batch size,t,f)
    :param num_t: 
    :param num_f: 
    :param warp_t: 
    :param warp_f: 
    :param r: 波动范围
    :param clock: 
    :return: 
    """
    size_t, size_f = spectrogram.shape

    seed = (clock * tf.int32.min, clock * tf.int32.max)

    num_t = tf_pick_value_from_range(num_t, r, clock=clock)
    num_f = tf_pick_value_from_range(num_f, r, clock=clock)

    def get_flows(n, size, warp, r):
        warp = tf_pick_value_from_range(warp, range, clock=clock)
        warp = warp * tf.cast(size, dtype=tf.float32) / tf.cast(2 * (n + 1), dtype=tf.float32)
        f = tf.random.stateless_normal([num_t, num_f], seed, mean=0.0, stddev=warp, dtype=tf.float32)
        return tf.pad(f, tf.constant([[1, 1], [1, 1]]), 'CONSTANT')  # zero flow at all edges

    flows = tf.stack([get_flows(num_t, size_t, warp_t, r), get_flows(num_f, size_f, warp_f, r)], axis=2)
    flows = tf.image.resize_bicubic(tf.expand_dims(flows, 0), [size_t, size_f])
    spectrogram_aug = tf.contrib.image.dense_image_warp(tf.expand_dims(spectrogram, -1), flows)
    spectrogram_aug = tf.reshape(spectrogram_aug, shape=(1, -1, size_f))
    return spectrogram_aug</code></pre> 
 <h3 id="频率掩膜frequency-mask">频率掩膜(Frequency Mask)</h3> 
 <p>在幅度谱随机的将频点置零。有关更多详细信息，请参阅 SpecAugment 论文：<a href="https://arxiv.org/abs/1904.08779" rel="nofollow" title="SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition">SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition</a></p> 
 <pre class="has"><code class="language-python">def FreqMask(mag, num_mask=1, mask_percentage=0.01):
    """
    :param mag: (F,T)
    :param num_freq_mask: mask的数量
    :param mask_percentage: mask的百分比 0.001~0.015
    """
    F = mag.shape[0]  # 频点数
    mask_width = int(mask_percentage * F)  # mask的宽度
    for i in range(num_mask):
        mask_start = np.random.randint(low=0, high=F - mask_width)  # mask的index
        mag[mask_start: mask_start + mask_width:] = 0   # 掩码F维度
    return mag</code></pre> 
 <p><img alt="img" src="https://images2.imgbox.com/7d/e3/qcmJ0bnN_o.png"></p> 
 <h3 id="时间掩码time-mask">时间掩码(Time Mask)</h3> 
 <p>在随机位置将增强样本内的时间间隔设置为零(静音)。代码修改自：<a href="https://github.com/mozilla/DeepSpeech" title="DeepSpeech">DeepSpeech</a></p> 
 <pre class="has"><code class="language-python">def TimeMask(mag, num_mask=1, mask_percentage=0.01):
    """
    :param mag: (F,T)
    :param num_freq_mask: mask的数量
    :param mask_percentage: mask的百分比 0.001~0.015
    """
    T = mag.shape[1]  # 频点数
    mask_width = int(mask_percentage * T)  # mask的宽度
    for i in range(num_mask):
        mask_start = np.random.randint(low=0, high=T - mask_width)
        mag[:, mask_start:mask_start + mask_width] = 0  # 掩码T维度
    return mag</code></pre> 
 <p><img alt="img" src="https://images2.imgbox.com/e7/89/Zqg2Xm3t_o.png"></p> 
 <h2 id="多领域增强">多领域增强</h2> 
 <h3 id="drop增强">drop增强</h3> 
 <p>将目标数据表示的随机数据点归零。代码修改自：<a href="https://github.com/mozilla/DeepSpeech" title="DeepSpeech">DeepSpeech</a></p> 
 <pre class="has"><code class="language-python">def tf_pick_value_from_range(value, r, clock=None, double_precision=False):
    clock = (tf.random.stateless_uniform([], seed=(-1, 1), dtype=tf.float64) if clock is None
             else tf.maximum(tf.constant(0.0, dtype=tf.float64), tf.minimum(tf.constant(1.0, dtype=tf.float64), clock)))
    value = tf.random.stateless_uniform([],
                                        minval=value - r,
                                        maxval=value + r,
                                        seed=(clock * tf.int32.min, clock * tf.int32.max),
                                        dtype=tf.float64)
    if isinstance(value, int):
        return tf.cast(tf.math.round(value), tf.int64 if double_precision else tf.int32)
    return tf.cast(value, tf.float64 if double_precision else tf.float32)


def Dropout(tensor, rate=0.05, r=0, transcript=None, clock=0.0):
    rate = tf_pick_value_from_range(rate, r, clock=clock)
    rate = tf.math.maximum(0.0, rate)
    factors = tf.random.stateless_uniform(tf.shape(tensor),
                                          (clock * tf.int32.min, clock * tf.int32.max),
                                          minval=0.0,
                                          maxval=1.0,
                                          dtype=tf.float32)
    return tensor * tf.math.sign(tf.math.floor(factors + rate))</code></pre> 
 <h3 id="添加增强">添加增强</h3> 
 <p>将从正态分布(均值为 0.0)中选取的随机值添加到目标数据表示的所有数据点。代码修改自：<a href="https://github.com/mozilla/DeepSpeech" title="DeepSpeech">DeepSpeech</a></p> 
 <pre class="has"><code class="language-python">def tf_pick_value_from_range(value, r, clock=None, double_precision=False):
    clock = (tf.random.stateless_uniform([], seed=(-1, 1), dtype=tf.float64) if clock is None
             else tf.maximum(tf.constant(0.0, dtype=tf.float64), tf.minimum(tf.constant(1.0, dtype=tf.float64), clock)))
    value = tf.random.stateless_uniform([],
                                        minval=value - r,
                                        maxval=value + r,
                                        seed=(clock * tf.int32.min, clock * tf.int32.max),
                                        dtype=tf.float64)
    if isinstance(value, int):
        return tf.cast(tf.math.round(value), tf.int64 if double_precision else tf.int32)
    return tf.cast(value, tf.float64 if double_precision else tf.float32)


def Add(tensor, stddev=5, r=0, transcript=None, clock=0.0):
    stddev = tf_pick_value_from_range(stddev, r, clock=clock)
    seed = (clock * tf.int32.min, clock * tf.int32.max)
    return tensor + tf.random.stateless_normal(tf.shape(tensor), seed, mean=0.0, stddev=stddev)</code></pre> 
 <h3 id="乘法增强">乘法增强</h3> 
 <p>将目标数据表示的所有数据点与从正态分布(均值为 1.0)中选取的随机值相乘。代码修改自：<a href="https://github.com/mozilla/DeepSpeech" title="DeepSpeech">DeepSpeech</a></p> 
 <pre class="has"><code class="language-python">def tf_pick_value_from_range(value, r, clock=None, double_precision=False):
    clock = (tf.random.stateless_uniform([], seed=(-1, 1), dtype=tf.float64) if clock is None
             else tf.maximum(tf.constant(0.0, dtype=tf.float64), tf.minimum(tf.constant(1.0, dtype=tf.float64), clock)))
    value = tf.random.stateless_uniform([],
                                        minval=value - r,
                                        maxval=value + r,
                                        seed=(clock * tf.int32.min, clock * tf.int32.max),
                                        dtype=tf.float64)
    if isinstance(value, int):
        return tf.cast(tf.math.round(value), tf.int64 if double_precision else tf.int32)
    return tf.cast(value, tf.float64 if double_precision else tf.float32)


def Multiply(self, tensor, stddev=5, r=0, transcript=None, clock=0.0):
    stddev = tf_pick_value_from_range(stddev, r=0, clock=clock)
    seed = (clock * tf.int32.min, clock * tf.int32.max)
    return tensor * tf.random.stateless_normal(tf.shape(tensor), seed, mean=1.0, stddev=stddev)</code></pre> 
 <h2 id="参考">参考</h2> 
 <p>【python 音频处理库】</p> 
 <ul><li><a href="https://librosa.org/doc/latest/index.html" rel="nofollow" title="librosa">librosa</a></li><li><a href="https://pyroomacoustics.readthedocs.io/en/pypi-release/index.html" rel="nofollow" title="Pyroomacoustics">Pyroomacoustics</a></li><li><a href="https://github.com/nxbyte/PythonAudioEffects" title="PythonAudioEffects：专注音效">PythonAudioEffects：专注音效</a></li><li><a href="https://github.com/facebookresearch/WavAugment" title="WavAugment">WavAugment</a></li><li><a href="https://github.com/LorisNanni/Audiogmenter" title="Audiogmenter：一个用于音频数据增强的MATLAB工具箱">Audiogmenter：一个用于音频数据增强的MATLAB工具箱</a></li><li><a href="https://github.com/iver56/audiomentations" title="audiomentations：我愿称他为最强王者，专门的数据增强库，很全面，没事可以多看看">audiomentations：我愿称他为最强王者，专门的数据增强库，很全面，没事可以多看看</a></li></ul> 
 <p>【知乎文章】<a href="https://zhuanlan.zhihu.com/p/42388642" rel="nofollow" title="简单地为语音加混响">简单地为语音加混响</a></p> 
 <p>【国际音频实验室EmanuëlHabets提供的代码】<a href="https://www.audiolabs-erlangen.de/fau/professor/habets/software/rir-generator" rel="nofollow" title="International Audio Laboratories Erlangen">International Audio Laboratories Erlangen</a></p> 
 <p>【Image-source method】<a href="http://www.eric-lehmann.com/ism_bg.html" rel="nofollow" title="Image-source method for room acoustics">Image-source method for room acoustics</a></p> 
 <p>【Image-source 原理讲解】<a href="https://reuk.github.io/wayverb/image_source.html" rel="nofollow" title="Image-source Model">Image-source Model</a></p> 
 <p>【CSDN】<a href="https://www.cnblogs.com/welen/p/3782896.html" rel="nofollow" title="变速变调原理与方法总结">变速变调原理与方法总结</a></p> 
 <p>【CSDN】<a href="https://blog.csdn.net/abcSunl/article/details/77196788" title="音频倍速(变速不变调)的实现">音频倍速(变速不变调)的实现</a></p> 
 <p>【CSDN】<a href="https://blog.csdn.net/ImplFancy/article/details/50396444?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.no_search_link" title="音频变调算法小结">音频变调算法小结</a></p> 
 <p>【CSDN】<a href="https://blog.csdn.net/veritasalice/article/details/112234115?spm=1001.2101.3001.6650.7&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~OPENSEARCH~default-7.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~OPENSEARCH~default-7.no_search_link" title="python 音频变调不变速方法">python 音频变调不变速方法</a></p> 
 <p>【论文】<a href="https://www.danielpovey.com/files/2015_interspeech_augmentation.pdf" rel="nofollow" title="用于语音识别的音频增强">用于语音识别的音频增强</a></p> 
 <p>【论文】<a href="https://arxiv.org/abs/1904.08779" rel="nofollow" title="SpecAugment:一种简单的自动语音识别数据增强方法">SpecAugment:一种简单的自动语音识别数据增强方法</a></p> 
 <p>【CSDN】<a href="https://blog.csdn.net/qq_39516859/article/details/87980189" title="SoX 音频处理工具使用方法">SoX 音频处理工具使用方法</a></p> 
 <p><strong>本文转载自作者：凌逆战，地址：<a href="https://www.cnblogs.com/LXP-Never/p/13404523.html" rel="nofollow" title="https://www.cnblogs.com/LXP-Never/p/13404523.html">https://www.cnblogs.com/LXP-Never/p/13404523.html</a></strong></p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f52a05ccf2bb1ef2aebd7489b753e010/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">在Ubuntu安装软件时，显示“E:无法定位软件包</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b543d3d3f3e51d07627398ffcc17bdf4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Python指定路径式——自动删除文件夹</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>