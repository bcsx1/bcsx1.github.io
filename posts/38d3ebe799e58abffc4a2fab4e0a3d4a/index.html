<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>从ICCV 2021看域泛化与域自适应最新研究进展 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="从ICCV 2021看域泛化与域自适应最新研究进展" />
<meta property="og:description" content="©PaperWeekly 原创 · 作者 | 张一帆
学校 | 中科院自动化所博士生
研究方向 | 计算机视觉
Domain adaptation（DA: 域自适应），Domain generalization（DG: 域泛化）一直以来都是各大顶会的热门研究方向。DA 假设我们有有一个带标签的训练集（源域），这时候我们想让模型在另一个数据集上同样表现很好（目标域），利用目标域的无标签数据，提升模型在域间的适应能力是 DA 所强调的。
以此为基础，DG 进一步弱化了假设，我们只有多个源域的数据，根本不知道目标域是什么，这个时候如何提升模型泛化性呢？核心在于如何利用多个源域带来的丰富信息。本文挑选了四篇 ICCV 2021 域泛化与域自适应相关的文章来研究最新的进展。
CrossNorm-SelfNorm
论文标题：
CrossNorm and SelfNorm for Generalization under Distribution Shifts
论文链接：
https://arxiv.org/abs/2102.02811
代码链接：
https://github.com/amazon-research/crossnorm-selfnorm
简而言之，Batch Normalization 与 instance normlization 通常都假设训练数据与测试数据独立同分布。本文提出了两种新的 normalization 方法，CrossNorm 和 SelfNorm，二者可以很好的提升模型的泛化性。
1.1 Motivation and Insights
文章的两个方法分别对应两个目标：
1. 扩增训练数据分布。传统的 normalization 都是为了使得训练更加稳定和快速，那么作者是否能用 normalization 的方法进行数据的增强呢？答案是肯定的。一般来说，作者认为 RGB 图像的均值和方差代表了他的风格信息，交换他们的均值和方差不会对类别标签产生影响。CrossNorm 要做的事情就是在训练中交换特征图 channel-wise 的均值和方差，使得模型对风格变化更加鲁棒。
2. 减小域分布偏差。即使有了 1，模型依然会遇到完全没有见过的风格。一种可行的方式即减小训练和测试数据之间的分布。文章发现，作者可以通过调整 RGB 图像的均值和方差来缩小这种风格的差异。而 SelfNorm 的目标就是使用 attention 机制来调整 channel-wise 的均值和方差。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/38d3ebe799e58abffc4a2fab4e0a3d4a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-10-28T13:25:52+08:00" />
<meta property="article:modified_time" content="2021-10-28T13:25:52+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">从ICCV 2021看域泛化与域自适应最新研究进展</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/cf/68/PLHpyUvM_o.gif" alt="1e0842cd885a7b0a3284c935bbe5871b.gif"></p> 
 <p style="text-align:right;"><strong>©PaperWeekly 原创 · 作者 |</strong> 张一帆<br></p> 
 <p style="text-align:right;"><strong>学校 |</strong> 中科院自动化所博士生</p> 
 <p style="text-align:right;"><strong>研究方向 | </strong>计算机视觉</p> 
 <p>Domain adaptation（DA: 域自适应），Domain generalization（DG: 域泛化）一直以来都是各大顶会的热门研究方向。DA 假设我们有有一个带标签的训练集（源域），这时候我们想让模型在另一个数据集上同样表现很好（目标域），利用目标域的无标签数据，提升模型在域间的适应能力是 DA 所强调的。</p> 
 <p>以此为基础，DG 进一步弱化了假设，我们只有多个源域的数据，根本不知道目标域是什么，这个时候如何提升模型泛化性呢？核心在于如何利用多个源域带来的丰富信息。本文挑选了四篇 ICCV 2021 域泛化与域自适应相关的文章来研究最新的进展。</p> 
 <p style="text-align:left;"><img src="https://images2.imgbox.com/3f/a6/jBOeX3YH_o.png" alt="73975ec8f9cd63c0246b55472eb5499e.png"></p> 
 <p style="text-align:left;"><strong>CrossNorm-SelfNorm</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/12/d1/f3TGfqvp_o.png" alt="bdb0c0a5e34a53e29073f71f79335661.png"></p> 
 <p style="text-align:left;"><strong>论文标题：</strong></p> 
 <p style="text-align:left;">CrossNorm and SelfNorm for Generalization under Distribution Shifts</p> 
 <p style="text-align:left;"><strong>论文链接：</strong></p> 
 <p style="text-align:left;">https://arxiv.org/abs/2102.02811</p> 
 <p style="text-align:left;"><strong>代码链接：</strong></p> 
 <p style="text-align:left;">https://github.com/amazon-research/crossnorm-selfnorm</p> 
 <p>简而言之，Batch Normalization 与 instance normlization 通常都假设训练数据与测试数据独立同分布。本文提出了两种新的 normalization 方法，CrossNorm 和 SelfNorm，二者可以很好的提升模型的泛化性。</p> 
 <p style="text-align:left;"><strong>1.1 Motivation and Insights</strong></p> 
 <p>文章的两个方法分别对应两个目标：</p> 
 <p>1. 扩增训练数据分布。传统的 normalization 都是为了使得训练更加稳定和快速，那么作者是否能用 normalization 的方法进行数据的增强呢？答案是肯定的。一般来说，作者认为 RGB 图像的均值和方差代表了他的风格信息，交换他们的均值和方差不会对类别标签产生影响。CrossNorm 要做的事情就是在训练中交换特征图 channel-wise 的均值和方差，使得模型对风格变化更加鲁棒。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/03/4d/bvweFZmS_o.png" alt="cb5d8b64a3961ef107a67a6c5862666b.png"></p> 
 <p>2. 减小域分布偏差。即使有了 1，模型依然会遇到完全没有见过的风格。一种可行的方式即减小训练和测试数据之间的分布。文章发现，作者可以通过调整 RGB 图像的均值和方差来缩小这种风格的差异。而 SelfNorm 的目标就是使用 attention 机制来调整 channel-wise 的均值和方差。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/5f/f7/9IPaHLNP_o.png" alt="2a3215b279e1c1a8d027c0baa6927363.png"></p> 
 <p style="text-align:left;"><strong>1.2 Methodology</strong></p> 
 <p>先来简单复习一下 instance normalization。对于 2D 的 CNN，作者有  的特征图， 就是 IN 要做的事情。</p> 
 <p><strong>CrossNorm</strong> 要做的事情要不复杂，他将不同特征图的均值和方差进行互换：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/f1/6c/oHDHkg0u_o.png" alt="b29ba238867ae0a7197bd96cfc4f65fa.png"></p> 
 <p>更详细的说，对于单个样本，作者可以交换他每个 channel 之间的均值和方差，对多个样本作者可以交换 instance level 的均值和方差。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/c9/05/j8qARANc_o.png" alt="884389e1eda77e337b2040b5c9dc13dd.png"></p> 
 <p>作者还提出了一个小 trick，即通过裁剪来提供更多的风格信息。文中给出了三种裁剪策略，content only, style only 和 both。对于 content only 而言，当作者给  替换统计变量时才进行裁剪，当计算  的统计变量以提供给  时不裁剪，style 则相反。</p> 
 <p><strong>SelfNorm</strong> 这里的核心就是一个重新标准化的均值和方差 ，这里的  都是可学习的注意力方程（两个全连接层，输入是两个标量，输出是一个标量），是通过训练得到的，那么被校准后的特征图可以写作：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/f5/3c/SWYbmEal_o.png" alt="208fc2ef34cce455d38c779d336c367f.png"></p> 
 <p style="text-align:left;"><strong>1.3 Experiments</strong></p> 
 <p>在实验过程中，一个重要的问题是在哪里？以及放置多少 crossnorm 或者 selfnorm 层。这个问题本身是非常难搜索的，因为每个位置都可以放。为了简化问题，作者转向模块化设计，将它们嵌入到一个网络单元中。比如在 resnet 中，作者将它放在 residual module 里。</p> 
 <p>CN+SN 的方法在绝大多数 backbone上的效果好于现有的数据增强的方法，而且 CNSN 和他们是正交的，CNSN 不依赖于任何形式的图像操作，将 CNSN 与 augmix 结合之后效果来到了目前的 SOTA。文中还有更多数据集以及 numerical 的实验，这里不再赘述。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/5c/b7/ECL3O9pq_o.png" alt="e02c203986c43aafa70cda2595c96a7f.png"></p> 
 <p>我们来看看文章的可视化结果。随着 CN 在网络中的深入，风格的变化变得更加局部和微妙。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/e8/71/aQMV7cri_o.png" alt="c72ec73f59df0cc97197c06a8ea582f8.png"></p> 
 <p>对于 SN 而言，随着 SN 的深入，重新校准效应变得微妙，因为高级特征的统计数据并不直接与低级视觉线索联系在一起。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/95/3f/DVbFb5EK_o.png" alt="692089d904fa3fabdf2c66018bf51bd2.png"></p> 
 <p>SN 还可以抑制图像中的一些风格信息并保留目标纹理。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/8a/12/tdQEVe5y_o.png" alt="bd90386934da7507728d0c80aa2bc761.png"></p> 
 <p style="text-align:left;"><img src="https://images2.imgbox.com/75/84/HfJrI2mO_o.png" alt="90c40b79a23a2334e99c2b1f9d3a69ff.png"></p> 
 <p style="text-align:left;"><strong>STEAM</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/16/57/RILHz4Mg_o.png" alt="1769c2ca926ea3007e0fc8dc3525d655.png"></p> 
 <p style="text-align:left;"><strong>论文标题：</strong></p> 
 <p style="text-align:left;">A Style and Semantic Memory Mechanism for Domain Generalization</p> 
 <p style="text-align:left;"><strong>论文链接：</strong></p> 
 <p style="text-align:left;">https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_A_Style_and_Semantic_Memory_Mechanism_for_Domain_Generalization_ICCV_2021_paper.pdf</p> 
 <p style="text-align:left;"><strong>2.1 Motivation and Insights</strong></p> 
 <p>本文强调了，现有 DG 的工作都说我们要保证特征是 semantic invariance 的，但是忽略了每个 domain 内的 style 也是不变的。这篇文章尝试着去使用 intra-domain style invariance 来提升模型的泛化性能。文章提出了 STEAM, STEAM 融入了额外限制：同一个 domain 的样本共享同样的风格信息，加了这个额外限制之后，有助于有效地解耦风格特征，从而以较少的自由度简化对真正语义特征的搜索。</p> 
 <p style="text-align:left;"><strong>2.2 Methodology</strong></p> 
 <p>文章的框架很复杂，虽然 intuition 是好的，但是设计着实不敢恭维，很难 follow，因此这里简单讲一下思路。首先每个 image 提特征，然后分两个 encoder （style 和 semantic）分别提取风格信息和语义信息。</p> 
 <p>1. 语义信息提取出来后自然要做一个分类，除此之外文章设计上的一个亮点（看着实在过于复杂）的 memory bank，依靠当前语义信息和 memory bank 中的语义信息计算一个相似度。</p> 
 <p>2. 重点在于风格信息的处理。首先和语义分类计算一个正交的 loss，然后每个 domain 维护一个 memory bank，分别对每个 domain 的风格相似度计算一个 contrastive loss 来保证同一 domain 风格信息接近。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/1c/7e/8VSgqkcI_o.png" alt="f90ab57dfbf7fac3a0f3688bc3a883ad.png"></p> 
 <p>如下表所示，本文的方法明显优于现有的先进方法。而且该模型在比较有挑战的 DGsetting上（例如 MNIST-MandSVHN）尤其有效，因为与其他方向相比，它们似乎有很大的领域变化。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/ff/d2/7UwQ07FY_o.png" alt="290a5646de87ddcb1205bd7a8dd43079.png"></p> 
 <p>总的来说，本文提出了一个 DG 下比较合理并且验证有效的假设：域内风格不变性，并且通过 contrastive learning 那一套的方法进行了模型设计取得了不错的结果。但是结构设计过于复杂，虽然性能有所提升但是工作很难 follow。</p> 
 <p style="text-align:left;"><img src="https://images2.imgbox.com/bc/f1/LTdIy13A_o.png" alt="6a563e14ef5cac28cd27b4c1d6d7b75e.png"></p> 
 <p style="text-align:left;"><strong>A2Net</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/b5/64/8Op2BwSP_o.png" alt="f460ef0a0bcff1eba674516d30637bea.png"></p> 
 <p style="text-align:left;"><strong>论文标题：</strong></p> 
 <p style="text-align:left;">Adaptive Adversarial Network for Source-free Domain Adaptation</p> 
 <p style="text-align:left;"><strong>论文链接：</strong></p> 
 <p style="text-align:left;">https://openaccess.thecvf.com/content/ICCV2021/papers/Xia_Adaptive_Adversarial_Network_for_Source-Free_Domain_Adaptation_ICCV_2021_paper.pdf</p> 
 <p style="text-align:left;"><strong>3.1 Motivation and Insights</strong></p> 
 <p>本文关注 source free domain adaptation：即训练过程中我们拿不到源数据，只能拿到在源域训练好的模型。本文的 intuition 在于，如果源数据本身极度不平衡或者存在偏差，那么分类器本身就不太合理。由于受到冻结分类器的限制，这个时候他很难在距离 source distribution 远的分布上表现的好。从另一个角度，本文提出了一个问题：在模型优化过程中，我们能否寻找一种新的针对目标的分类器，并使其适应目标特征？</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/bd/ad/AdtUVg0b_o.png" alt="c7303d47325e766344a967f76fd758c5.png"></p> 
 <p>答案是肯定的。</p> 
 <p style="text-align:left;"><strong>3.2 Methodology</strong></p> 
 <p>本文核心的贡献之一在多 classifier 的设计，即保留原有的 source classifier，重新训练一个 target classifier，这样既可以利用目标域数据提升鉴别性，又保留了源域训练得到的宝贵经验，那么如何同时训练这多个分类器就是首当其冲的问题。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/7f/e9/sWaBtEkL_o.png" alt="5fb7066d47f51ce5a289de071506c989.png"></p> 
 <p>具体来说作者假设将对于每个类别，都存在一些与源域分布类似的易于识别的目标实例。因此，作者将样本分为两类：源相似特征（SS）和源不相似特征（SNS）。frozen 的模型，他的分类器  可以很好的分类 SS，但 SNS 就无能为力了。因此新加入的分类器  承担起这个重任。</p> 
 <p>那么我们的第一个问题，如何区分 SS 和 SNS，这好像变成了 OOD detection 的问题，所以看得出来 DG, DA,ODD, OOD detection 之间还是可以相互借鉴的。</p> 
 <p style="text-align:left;"><strong>3.2.1 Soft-Adversarial Inference</strong></p> 
 <p>这部分是收到 voting strategy 的启发，使用两个分类器分别分类得到 ，这里  是类别数目。我们将两个分类向量，concatenate 起来然后通过一个 softmax，。记  和 。当  时相应的特征就属于 source similar，那么损失函数定义为：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/dc/21/n5W4Gxee_o.png" alt="222c5e672eaea8b1ce5ba910e538e88a.png"></p> 
 <p> 在训练的时候 frozen 住。那么我们如何去减少两类样本之间的 discrepancy 呢。最优的结果就是在 SS 上  能做到与  一样。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/2c/97/aJXncmY9_o.png" alt="bece9270b9194918f7f566d77e5fcbd3.png"></p> 
 <p style="text-align:left;"><strong>3.2.2 Contrastive Category-wise Matching</strong></p> 
 <p>通过对抗学习学习到的领域不变特征不能代表类别级匹配。此外，如果没有对目标域的标注，则很难确定样本之间的类别关系。本文设计了一种新颖的双分类器鉴别方法来探索配对样本的关联，以实现无监督方式下的分类对齐。首先我们定义样本特征 ，样本相似度 。如果  是同一类的话，那么这个相似度应该比较高。</p> 
 <p>然而，当  在中间区间（不是很大也不是很小）时，我们不能自信地判断一对样本的关系。因此，通过将  与定义的上界 μ 和下界 μ 进行比较，将每个小批量的所有对划分为三个子集：正集、不确定集和负集。上下界都是训练 epoch  的函数。</p> 
 <p style="text-align:center;"><img height="122" width="723" src="https://images2.imgbox.com/86/f0/iiPGuR5H_o.png" alt="8835f26086d93e619cedca645b169cf6.png"></p> 
 <p>那么对我们已经确定的正负样本，就可以进行对比学习。</p> 
 <p style="text-align:center;"><img height="86" width="672" src="https://images2.imgbox.com/9e/8f/4ytcLSo1_o.png" alt="8f26fd18233c124353a6976806964981.png"></p> 
 <p style="text-align:left;"><strong>3.2.3 Self-Supervised Rotation</strong></p> 
 <p>到目前为止，本文主要考虑如何在训练有素的源模型的指导下将知识转移到目标领域。然而，纯分类模型严重依赖于给定的源数据，往往分布不均衡，进一步限制了目标分类器的泛化能力。为了解决这一问题，作者探索了目标域上的自监督旋转方式，以增强样本空间，增强了特征提取和目标分类器的学习。</p> 
 <p>换句话说，模型能够很容易地看到每个高置信度的预测目标样本的更多变体。作者设置四个旋转角度 θ  和相应的 4 类旋转标签 。在一个批次中，作者随机选择旋转标签，然后通过旋转  将原始图像进行修改。此外，作者还引入了的旋转分类器 ，以旋转图像为输入，预测旋转标签。最后，采用交叉熵损失来测量预测与真实旋转的差异。</p> 
 <p style="text-align:center;"><img height="90" width="595" src="https://images2.imgbox.com/d1/24/tKfPeEJn_o.png" alt="b6b9136324200cab0d93617d8e7af4ab.png"></p> 
 <p>通过识别旋转程度，模型能够有效地捕捉到原始图像中用于目标分类的重要视觉信号。</p> 
 <p style="text-align:left;"><strong>3.3 Experiments</strong></p> 
 <p>本文提出的方法在 office31，officehome 上都取得了不错的效果。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/83/55/3CCPp4wM_o.png" alt="bc188c929c06cb0fe9b12558e471e2ca.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/f2/b8/mbK9iLlb_o.png" alt="3e85307fbacaac8bc365ec40a19f20b9.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/62/5a/QsVf9m0q_o.png" alt="863b256e5c66bbe544066a63cda9c709.png"></p> 
 <p style="text-align:left;"><strong>FGDA</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/e2/00/pUWnHMzG_o.png" alt="db2b4efbba68aa3b6116d6f90b458b4f.png"></p> 
 <p style="text-align:left;"><strong>论文标题：</strong></p> 
 <p style="text-align:left;">Gradient Distribution Alignment Certificates Better Adversarial Domain Adaptation</p> 
 <p style="text-align:left;"><strong>论文链接：</strong></p> 
 <p style="text-align:left;">https://openaccess.thecvf.com/content/ICCV2021/papers/Gao_Gradient_Distribution_Alignment_Certificates_Better_Adversarial_Domain_Adaptation_ICCV_2021_paper.pdf</p> 
 <p style="text-align:left;"><strong>代码链接：</strong></p> 
 <p style="text-align:left;">https://github.com/gzqhappy/FGDA</p> 
 <p style="text-align:left;"><strong>4.1 Motivation and Insights</strong></p> 
 <p>传统的方法大多是基于对抗学习来减小域之间的 discrepancy，但是对抗学习所采用的 MIN-MAX game 并没有一个很好的纳什均衡的保证，即使域分类器完全无法鉴别两个 domain，他们的分布也不一定对齐了。</p> 
 <p>本文提出了 FGDA 方法，如下图所示，该算法通过特征提取器和鉴别器之间的对抗学习来减小<strong>特征梯度</strong>在两个域之间的分布差异。当达到平衡时，特征分布差异的值是最小的。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/93/7d/XP33viUF_o.png" alt="b573f236b410c3c036c6993f04f2428d.png"></p> 
 <p>从对抗学习的角度来看，样本的输入梯度可以看作是对输入扰动最小、对模型输出影响最大的敏感方向。直观上，一个样本的特征梯度方向可能倾向于指向其最近决策边界的区域。因此，对齐特征梯度有助于学习潜在表示，从而迫使两个域分布保持更近的距离。这样可以减小特征分布的差异。文章还有一部分理论，验证了 FGDA 可以进一步得到更小的 upper bound。</p> 
 <p style="text-align:left;"><strong>4.2 Methodology</strong></p> 
 <p>方法主要分成三个模块，一对 gradient 分类器，一个 Jacobian regularization 模块和 self-supervised pseudo-labeling 模块来进一步提升对抗学习的能力。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/06/9d/vmmBLrCD_o.png" alt="dbd3d1e44aee0d4c6c708c0dad2914e8.png"></p> 
 <p style="text-align:left;"><strong>4.2.1 Feature Gradient Distribution Alignment</strong></p> 
 <p>将特征提取器记作 ，因为要计算梯度，而 target domain 没有标签，因此本文使用模型预测的标签作为伪标签来计算交叉熵从而得到梯度向量 （直观来看这种方法很容易造成 collapse 啊）。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/80/56/KyweFAaf_o.png" alt="2cfea71ea2f1c5c0afd48a7f52d95e1b.png"></p> 
 <p>文中依然是使用了对抗学习来，具体来说，鉴别器预测源域和目标域特征梯度的域标签，而特征提取器学习混淆鉴别器。当达到均衡时，特征分布差异达到最小值。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/c8/d2/74Udysbz_o.png" alt="33dbbeedf4eb172ea2d1a7c347cae4ad.png"></p> 
 <p>上图绘制了两个域之间每个类别的平均梯度距离（因为应用了归一化，因此距离反映了梯度方向的方差）。从图中可以看出，在特征梯度对齐过程中，梯度分布差异逐渐减小。</p> 
 <p style="text-align:left;"><strong>4.2.2 Feature-level Jacobian Regularization</strong></p> 
 <p>本文也采取了梯度正则化的手段来提升模型的分类能力，具体而言，本文在特征层面采用雅可比矩阵正则化，使得特征提取器可以学习到更多远离决策边界的具有鉴别性的特征，同时分类器可以扩大分类 margin。其中输入输出的雅可比矩阵定义为：</p> 
 <p style="text-align:center;"><img height="90" width="568" src="https://images2.imgbox.com/9a/9e/LAkF1l46_o.png" alt="f00252665bdb1f092e147200297413c2.png"></p> 
 <p>整体的正则化过程定义为：</p> 
 <p style="text-align:center;"><img height="130" width="744" src="https://images2.imgbox.com/48/a4/cloVhDx4_o.png" alt="f301fa2b82128d879381329358d85779.png"></p> 
 <p style="text-align:left;"><strong>4.2.3 Self-supervised Pseudo-labeling</strong></p> 
 <p>虽然使用模型预测的伪标签进行梯度对齐是可行的，但错误的预测将产生一个次最优梯度分布，从而阻止梯度对齐达到最优性能。为了在 target domain 中获得高质量的伪标签，本文又又又集成了一种自监督方法，在本文的框架中捕获不同类的目标分布，称为自监督伪标签。</p> 
 <p>在初始训练阶段后，每次固定的迭代次数都执行此策略，生成一个离线伪标签 （使用特征质心预测当前样本的离线伪标签）。一旦离线伪标签被获得， 将取代在线伪标签（在线伪标签是用分类器对当前样本的预测）来计算每个目标样本的特征梯度。</p> 
 <p>每个类的特征分布质心也被认为是一个原型表示（来了，又融合了 prototype learning），它的分布应该出现在分类器能够自信预测大量样本的区域。质心的更新类似于 k-means 聚类，通过用分类器的置信度加权每个目标特征：</p> 
 <p style="text-align:center;"><img height="135" width="761" src="https://images2.imgbox.com/93/c1/mb62DsuE_o.png" alt="72e725f426bf4724f4829103785e94d6.png"></p> 
 <p>每个目标样本的离线伪标签用最接近质心的标签进行分配：</p> 
 <p style="text-align:center;"><img height="74" width="683" src="https://images2.imgbox.com/86/48/ePNFh2Hw_o.png" alt="7c30a88b0dd7dcd2f57330de6e2d919d.png"></p> 
 <p>这里的  是距离函数，再接着，类别质心将根据新的伪标签确定：</p> 
 <p style="text-align:center;"><img height="182" width="630" src="https://images2.imgbox.com/5d/66/umE4ROgJ_o.png" alt="1326e6c7e501887691fe2f2af4b8925c.png"></p> 
 <p> 是一个脉冲函数，只有当  的时候为 1。实践证明，即使这些参数更新一次，仍然可以提高伪标签的质量。</p> 
 <p style="text-align:left;"><strong>4.3 Experiments</strong></p> 
 <p>我们想从实验中得到的无非两点：模型是否有效以及梯度对齐为什么有效，作者回答了这两个问题。</p> 
 <p>首先 FGDA 与传统减少 domain discrepancy 的方法结合摸到了 sota。虽然 FGDA 单独拿出来效果并没有想象的那么理想，但是 FGDA 毕竟是和现有方法正交的。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/bd/b0/sR4Nhsbm_o.png" alt="43881551f8e928d072153af3234237f6.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/9e/ab/toZigsYp_o.png" alt="1a9691a88c577b301515db1728f121af.png"></p> 
 <p>从 t-sne 的结果来看，加了 FGDA 之后特征对齐确实更好的对齐了。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/87/d9/IaEc2iG0_o.png" alt="452e5d5fee3e689fd654f765d7af7e21.png"></p> 
 <p>总结一下，这几篇 ICCV 的文章都是有不错的 insight 和效果的，值得一提的是，他们很多都用到了 contrastive learning，self training，ood detection 类似的工具来提升性能，可以看到这几个领域的紧密联系。但是有几篇工作的架构实在过于复杂，很难 follow。</p> 
 <p style="text-align:center;"><strong>特别鸣谢</strong></p> 
 <p>感谢 TCCI 天桥脑科学研究院对于 PaperWeekly 的支持。TCCI 关注大脑探知、大脑功能和大脑健康。</p> 
 <p><strong>更多阅读</strong></p> 
 <p style="text-align:center;"><a href="http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247543295&amp;idx=1&amp;sn=795bd6c44a425912ba3519ed63c35d95&amp;chksm=96eaf67fa19d7f696c75f4ce1ededc7a7c54c4403bbb873b249f32ad8d8e9768dde908d0f13c&amp;scene=21#wechat_redirect" rel="nofollow"><img src="https://images2.imgbox.com/e3/dd/EcQKLCtP_o.png" alt="b0aec9a7b538011f7bc447dca4f1eb3e.png"></a></p> 
 <p style="text-align:center;"><a href="http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247542526&amp;idx=2&amp;sn=26542037a8b1ef273a9c07e276a0b6bb&amp;chksm=96eaf97ea19d7068dce48c20ffbe7a713b333228a261f5f69c9088efd3f252db6e6d9ac67b74&amp;scene=21#wechat_redirect" rel="nofollow"><img src="https://images2.imgbox.com/d6/94/f5SokT4Q_o.png" alt="e34cbadbc1f438293ae7fe3e05141599.png"></a></p> 
 <p style="text-align:center;"><a href="http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247538599&amp;idx=3&amp;sn=561f738207d35be01ae923574499cd8e&amp;chksm=96ea8827a19d0131a4eb81aff13f6ee81bcb784d1430d38163fb11fb75ebb394f08d84e84a8d&amp;scene=21#wechat_redirect" rel="nofollow"><img src="https://images2.imgbox.com/33/d8/mHCAOefA_o.png" alt="92763b0da221e17bcd16534a4321142e.png"></a></p> 
 <p><img src="https://images2.imgbox.com/19/e7/As8ikEA8_o.gif" alt="f9fbd2edc03f74a8c4403c1bb71ac68b.gif"></p> 
 <p><strong>#投 稿 通 道#</strong></p> 
 <p><strong> 让你的文字被更多人看到 </strong></p> 
 <p>如何才能让更多的优质内容以更短路径到达读者群体，缩短读者寻找优质内容的成本呢？<strong>答案就是：你不认识的人。</strong></p> 
 <p>总有一些你不认识的人，知道你想知道的东西。PaperWeekly 或许可以成为一座桥梁，促使不同背景、不同方向的学者和学术灵感相互碰撞，迸发出更多的可能性。 </p> 
 <p>PaperWeekly 鼓励高校实验室或个人，在我们的平台上分享各类优质内容，可以是<strong>最新论文解读</strong>，也可以是<strong>学术热点剖析</strong>、<strong>科研心得</strong>或<strong>竞赛经验讲解</strong>等。我们的目的只有一个，让知识真正流动起来。</p> 
 <p>📝 <strong>稿件基本要求：</strong></p> 
 <p>• 文章确系个人<strong>原创作品</strong>，未曾在公开渠道发表，如为其他平台已发表或待发表的文章，请明确标注 </p> 
 <p>• 稿件建议以 <strong>markdown</strong> 格式撰写，文中配图以附件形式发送，要求图片清晰，无版权问题</p> 
 <p>• PaperWeekly 尊重原作者署名权，并将为每篇被采纳的原创首发稿件，提供<strong>业内具有竞争力稿酬</strong>，具体依据文章阅读量和文章质量阶梯制结算</p> 
 <p>📬 <strong>投稿通道：</strong></p> 
 <p>• 投稿邮箱：hr@paperweekly.site </p> 
 <p>• 来稿请备注即时联系方式（微信），以便我们在稿件选用的第一时间联系作者</p> 
 <p>• 您也可以直接添加小编微信（<strong>pwbot02</strong>）快速投稿，备注：姓名-投稿</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/5b/3a/A23F7Dvg_o.png" alt="55119cb33c2d95fbc294b640cec051ae.png"></p> 
 <p style="text-align:center;"><strong>△长按添加PaperWeekly小编</strong></p> 
 <p style="text-align:center;">🔍</p> 
 <p style="text-align:center;">现在，在<strong>「知乎」</strong>也能找到我们了</p> 
 <p style="text-align:center;">进入知乎首页搜索<strong>「PaperWeekly」</strong></p> 
 <p style="text-align:center;">点击<strong>「关注」</strong>订阅我们的专栏吧</p> 
 <p>·</p> 
 <p style="text-align:right;"><img src="https://images2.imgbox.com/bc/55/Ug26UigA_o.png" alt="56e049f8f6eacf52e2eaf33c3e5b8c15.png"></p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1ab61a4fcea7606f8abc05f356fb514d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">编写一个程序，输入若干个整数，以-1标记为结束，输出其中的最大值和最小值。</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/34f8b5766848226a7367d5768046c8b8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">arm64 kaslr 开启配置就能用？</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>