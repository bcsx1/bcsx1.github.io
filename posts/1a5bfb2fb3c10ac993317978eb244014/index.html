<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>win10 编译tensorflow1.8和tensorflow1.9生成64位和32位的dll（都是cpu） - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="win10 编译tensorflow1.8和tensorflow1.9生成64位和32位的dll（都是cpu）" />
<meta property="og:description" content="开始我们的工作，为什么编译tensorflow,主要是使用tesnorflow训练的模型，供c&#43;&#43;使用。
1：无用功
第一次尝试，编译tensorflow1.12,网上说不能用cmake，要使用bazel。编译中。。。。。。。。。。。。（省略一千字）成功
什么鬼，为什么是64位的，因为我们的机器是32位的。开始查找怎么编译32位的，查了几天，竟然没找到。
2:开始cmake编译
准备工具
1:cmake-gui一个我的是3.150；
2：swigwin-3.0.12；
3:tensorflow1.80源码；
4：anaconda
5:xxx工具（你懂的，比如蓝灯呀什么，最好是收费的，收费的快呀）
6：Git(这很重要，编译过程中要下载文件的，就是通过Git);
开始干活，打开cmake-gui,选择编译的cmake文件，路径如下
2：然后选择你要编译的地方，随便那里都行，最好不要c盘，有点大。如下
3：
如果编译64位的，就按照上面的选择，如果编译32位的就选择win32。，点击编译，会报错
选中部分需要更改路径，就是你下载的swigwin-3.0.12的路径，具体操作如下
点击编译，成功，然后选选择
点击编译，成功以后点击生成。
我这里是用vs2017打开All_build.vcxproj这个工程，网上都是vs2015,不想在装，懒=====
打开你的xxx工具，因为编译的时候需要下载一些资源包，很多都在Google上。
开始生成，坐等，肯定会报错，如果没报错，赶紧买彩票去，肯定中。。。。。。。。。。
等生成以后，看是那里报错了，就去找这个文件，选择这个文件，例如
如果编译成功，按照这种方法重复编译其它的文件，一般都是资源没下载成功。最后编译tensorflow这个文件，生成dll文件
无用功
想着64位的都成功了，32位的改一下cmake就行了，其实这是不对的，按64位的走了一遍。报错
这是 什么鬼，开始百度，Google并没有找到有人遇见过，是不是版本问题，然后开始漫长的测试，vs2017不行，换vs2015,还是同样的错误，vs2019，还是同样的错误，是不是版本的问题，看是从tf1.4测试，慢慢试，然后到tf1.9成功.
二 tensorflow的c&#43;&#43;使用
编译成功了怎么用，不会呀，开始各种百度。发现一个问题，为毛还是官网的可以调，自己的又报错误。我是使用slim框架训练自己的模型。一直报keepout没有参数传入，istraing没有传入。参考代码如下
#include &lt;fstream&gt; #include &lt;utility&gt; #include &lt;vector&gt; #include &#34;tensorflow/cc/ops/const_op.h&#34; #include &#34;tensorflow/cc/ops/image_ops.h&#34; #include &#34;tensorflow/cc/ops/standard_ops.h&#34; #include &#34;tensorflow/core/framework/graph.pb.h&#34; #include &#34;tensorflow/core/framework/tensor.h&#34; #include &#34;tensorflow/core/graph/default_device.h&#34; #include &#34;tensorflow/core/graph/graph_def_builder.h&#34; #include &#34;tensorflow/core/lib/core/errors.h&#34; #include &#34;tensorflow/core/lib/core/stringpiece.h&#34; #include &#34;tensorflow/core/lib/core/threadpool.h&#34; #include &#34;tensorflow/core/lib/io/path.h&#34; #include &#34;tensorflow/core/lib/strings/str_util.h&#34; #include &#34;tensorflow/core/lib/strings/stringprintf.h&#34; #include &#34;tensorflow/core/platform/env.h&#34; #include &#34;tensorflow/core/platform/init_main.h&#34; #include &#34;tensorflow/core/platform/logging.h&#34; #include &#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/1a5bfb2fb3c10ac993317978eb244014/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-10-16T10:22:59+08:00" />
<meta property="article:modified_time" content="2019-10-16T10:22:59+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">win10 编译tensorflow1.8和tensorflow1.9生成64位和32位的dll（都是cpu）</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>开始我们的工作，为什么编译tensorflow,主要是使用tesnorflow训练的模型，供c++使用。</p> 
<p><strong>1：无用功</strong></p> 
<p>第一次尝试，编译tensorflow1.12,网上说不能用cmake，要使用bazel。编译中。。。。。。。。。。。。（省略一千字）成功</p> 
<p>什么鬼，为什么是64位的，因为我们的机器是32位的。开始查找怎么编译32位的，查了几天，竟然没找到。</p> 
<p>2:<strong>开始cmake编译</strong></p> 
<p><strong>准备工具</strong></p> 
<p><strong>1:cmake-gui一个我的是3.150；</strong></p> 
<p><strong>2：</strong><a href="http://swig.org/download.html" rel="nofollow">swigwin-3.0.12</a>；</p> 
<p>3:tensorflow1.80<a href="https://github.com/tensorflow/tensorflow">源码</a>；</p> 
<p>4：anaconda</p> 
<p>5:xxx工具（你懂的，比如蓝灯呀什么，最好是收费的，收费的快呀）</p> 
<p>6：Git(这很重要，编译过程中要下载文件的，就是通过Git);</p> 
<p>开始干活，打开cmake-gui,选择编译的cmake文件，路径如下</p> 
<p><img alt="" class="has" height="74" src="https://images2.imgbox.com/cd/1c/4aSpTkka_o.png" width="829"></p> 
<p>2：然后选择你要编译的地方，随便那里都行，最好不要c盘，有点大。如下</p> 
<p><img alt="" class="has" height="40" src="https://images2.imgbox.com/ea/2b/wchbUmY3_o.png" width="829"></p> 
<p> 3：<img alt="" class="has" height="380" src="https://images2.imgbox.com/1e/f0/CMvAy27Q_o.png" width="512"></p> 
<p>如果编译64位的，就按照上面的选择，如果编译32位的就选择win32。，点击编译，会报错</p> 
<p><img alt="" class="has" height="111" src="https://images2.imgbox.com/b5/e6/aoCGOfgL_o.png" width="834"></p> 
<p> 选中部分需要更改路径，就是你下载的<a href="http://swig.org/download.html" rel="nofollow">swigwin-3.0.12</a>的路径，具体操作如下</p> 
<p><img alt="" class="has" height="47" src="https://images2.imgbox.com/f9/66/8mmyQ2W2_o.png" width="814"></p> 
<p>点击编译，成功，然后选选择</p> 
<p><img alt="" class="has" height="479" src="https://images2.imgbox.com/ae/63/vA3zzogc_o.png" width="840"></p> 
<p>点击编译，成功以后点击生成。</p> 
<p><img alt="" class="has" height="575" src="https://images2.imgbox.com/0a/f6/xk1zKH2I_o.png" width="687"></p> 
<p> </p> 
<p>我这里是用vs2017打开All_build.vcxproj这个工程，网上都是vs2015,不想在装，懒=====</p> 
<p>打开你的xxx工具，因为编译的时候需要下载一些资源包，很多都在Google上。</p> 
<p><img alt="" class="has" height="470" src="https://images2.imgbox.com/79/b2/pFZtkzal_o.png" width="735"></p> 
<p>开始生成，坐等，肯定会报错，如果没报错，赶紧买彩票去，肯定中。。。。。。。。。。</p> 
<p>等生成以后，看是那里报错了，就去找这个文件，选择这个文件，例如</p> 
<p><img alt="" class="has" height="583" src="https://images2.imgbox.com/96/1d/OLyo37nb_o.png" width="689"></p> 
<p>如果编译成功，按照这种方法重复编译其它的文件，一般都是资源没下载成功。最后编译tensorflow这个文件，生成dll文件</p> 
<p><strong>无用功</strong></p> 
<p><strong>想着64位的都成功了，32位的改一下cmake就行了，其实这是不对的，按64位的走了一遍。报错</strong></p> 
<p><img alt="" class="has" height="295" src="https://images2.imgbox.com/05/96/01cFDxE6_o.png" width="1200"></p> 
<p> </p> 
<p>这是 什么鬼，开始百度，Google并没有找到有人遇见过，是不是版本问题，然后开始漫长的测试，vs2017不行，换vs2015,还是同样的错误，vs2019，还是同样的错误，是不是版本的问题，看是从tf1.4测试，慢慢试，然后到tf1.9成功.</p> 
<p><strong>二   tensorflow的c++使用</strong></p> 
<p><strong>      编译成功了怎么用，不会呀，开始各种百度。发现一个问题，为毛还是官网的可以调，自己的又报错误。我是使用slim框架训练自己的模型。一直报keepout没有参数传入，istraing没有传入。参考代码如下</strong></p> 
<pre class="has"><code class="language-cpp">#include &lt;fstream&gt;
#include &lt;utility&gt;
#include &lt;vector&gt;
 
#include "tensorflow/cc/ops/const_op.h"
#include "tensorflow/cc/ops/image_ops.h"
#include "tensorflow/cc/ops/standard_ops.h"
#include "tensorflow/core/framework/graph.pb.h"
#include "tensorflow/core/framework/tensor.h"
#include "tensorflow/core/graph/default_device.h"
#include "tensorflow/core/graph/graph_def_builder.h"
#include "tensorflow/core/lib/core/errors.h"
#include "tensorflow/core/lib/core/stringpiece.h"
#include "tensorflow/core/lib/core/threadpool.h"
#include "tensorflow/core/lib/io/path.h"
#include "tensorflow/core/lib/strings/str_util.h"
#include "tensorflow/core/lib/strings/stringprintf.h"
#include "tensorflow/core/platform/env.h"
#include "tensorflow/core/platform/init_main.h"
#include "tensorflow/core/platform/logging.h"
#include "tensorflow/core/platform/types.h"
#include "tensorflow/core/public/session.h"
#include "tensorflow/core/util/command_line_flags.h"
 
// These are all common classes it's handy to reference with no namespace.
using tensorflow::Flag;
using tensorflow::Tensor;
using tensorflow::Status;
using tensorflow::string;
using tensorflow::int32;
 
// Takes a file name, and loads a list of labels from it, one per line, and
// returns a vector of the strings. It pads with empty strings so the length
// of the result is a multiple of 16, because our model expects that.
Status ReadLabelsFile(const string&amp; file_name, std::vector&lt;string&gt;* result,
                      size_t* found_label_count) {
    std::ifstream file(file_name);
    if (!file) {
        return tensorflow::errors::NotFound("Labels file ", file_name,
                                            " not found.");
    }
    result-&gt;clear();
    string line;
    while (std::getline(file, line)) {
        result-&gt;push_back(line);
    }
    *found_label_count = result-&gt;size();
    const int padding = 16;
    while (result-&gt;size() % padding) {
        result-&gt;emplace_back();
    }
    return Status::OK();
}
 
static Status ReadEntireFile(tensorflow::Env* env, const string&amp; filename,
                             Tensor* output) {
    tensorflow::uint64 file_size = 0;
    TF_RETURN_IF_ERROR(env-&gt;GetFileSize(filename, &amp;file_size));
 
    string contents;
    contents.resize(file_size);
 
    std::unique_ptr&lt;tensorflow::RandomAccessFile&gt; file;
    TF_RETURN_IF_ERROR(env-&gt;NewRandomAccessFile(filename, &amp;file));
 
    tensorflow::StringPiece data;
    TF_RETURN_IF_ERROR(file-&gt;Read(0, file_size, &amp;data, &amp;(contents)[0]));
    if (data.size() != file_size) {
        return tensorflow::errors::DataLoss("Truncated read of '", filename,
                                            "' expected ", file_size, " got ",
                                            data.size());
    }
    output-&gt;scalar&lt;string&gt;()() = data.ToString();
    return Status::OK();
}
 
// Given an image file name, read in the data, try to decode it as an image,
// resize it to the requested size, and then scale the values as desired.
Status ReadTensorFromImageFile(const string&amp; file_name, const int input_height,
                               const int input_width, const float input_mean,
                               const float input_std,
                               std::vector&lt;Tensor&gt;* out_tensors) {
    auto root = tensorflow::Scope::NewRootScope();
    using namespace ::tensorflow::ops;  // NOLINT(build/namespaces)
 
    string input_name = "file_reader";
    string output_name = "normalized";
 
    // read file_name into a tensor named input
    Tensor input(tensorflow::DT_STRING, tensorflow::TensorShape());
    TF_RETURN_IF_ERROR(
            ReadEntireFile(tensorflow::Env::Default(), file_name, &amp;input));
 
    // use a placeholder to read input data
    auto file_reader =
            Placeholder(root.WithOpName("input"), tensorflow::DataType::DT_STRING);
 
    std::vector&lt;std::pair&lt;string, tensorflow::Tensor&gt;&gt; inputs = {
            {"input", input},
    };
 
    // Now try to figure out what kind of file it is and decode it.
    const int wanted_channels = 3;
    tensorflow::Output image_reader;
    if (tensorflow::str_util::EndsWith(file_name, ".png")) {
        image_reader = DecodePng(root.WithOpName("png_reader"), file_reader,
                                 DecodePng::Channels(wanted_channels));
    } else if (tensorflow::str_util::EndsWith(file_name, ".gif")) {
        // gif decoder returns 4-D tensor, remove the first dim
        image_reader =
                Squeeze(root.WithOpName("squeeze_first_dim"),
                        DecodeGif(root.WithOpName("gif_reader"), file_reader));
    } else if (tensorflow::str_util::EndsWith(file_name, ".bmp")) {
        image_reader = DecodeBmp(root.WithOpName("bmp_reader"), file_reader);
    } else {
        // Assume if it's neither a PNG nor a GIF then it must be a JPEG.
        image_reader = DecodeJpeg(root.WithOpName("jpeg_reader"), file_reader,
                                  DecodeJpeg::Channels(wanted_channels));
    }
    // Now cast the image data to float so we can do normal math on it.
    auto float_caster =
            Cast(root.WithOpName("float_caster"), image_reader, tensorflow::DT_FLOAT);
    // The convention for image ops in TensorFlow is that all images are expected
    // to be in batches, so that they're four-dimensional arrays with indices of
    // [batch, height, width, channel]. Because we only have a single image, we
    // have to add a batch dimension of 1 to the start with ExpandDims().
    auto dims_expander = ExpandDims(root, float_caster, 0);
    // Bilinearly resize the image to fit the required dimensions.
    auto resized = ResizeBilinear(
            root, dims_expander,
            Const(root.WithOpName("size"), {input_height, input_width}));
    // Subtract the mean and divide by the scale.
    Div(root.WithOpName(output_name), Sub(root, resized, {input_mean}),
        {input_std});
 
    // This runs the GraphDef network definition that we've just constructed, and
    // returns the results in the output tensor.
    tensorflow::GraphDef graph;
    TF_RETURN_IF_ERROR(root.ToGraphDef(&amp;graph));
 
    std::unique_ptr&lt;tensorflow::Session&gt; session(
            tensorflow::NewSession(tensorflow::SessionOptions()));
    TF_RETURN_IF_ERROR(session-&gt;Create(graph));
    TF_RETURN_IF_ERROR(session-&gt;Run({inputs}, {output_name}, {}, out_tensors));
    return Status::OK();
}
 
// Reads a model graph definition from disk, and creates a session object you
// can use to run it.
Status LoadGraph(const string&amp; graph_file_name,
                 std::unique_ptr&lt;tensorflow::Session&gt;* session) {
    tensorflow::GraphDef graph_def;
    Status load_graph_status =
            ReadBinaryProto(tensorflow::Env::Default(), graph_file_name, &amp;graph_def);
    if (!load_graph_status.ok()) {
        return tensorflow::errors::NotFound("Failed to load compute graph at '",
                                            graph_file_name, "'");
    }
    session-&gt;reset(tensorflow::NewSession(tensorflow::SessionOptions()));
    Status session_create_status = (*session)-&gt;Create(graph_def);
    if (!session_create_status.ok()) {
        return session_create_status;
    }
    return Status::OK();
}
 
// Analyzes the output of the Inception graph to retrieve the highest scores and
// their positions in the tensor, which correspond to categories.
Status GetTopLabels(const std::vector&lt;Tensor&gt;&amp; outputs, int how_many_labels,
                    Tensor* indices, Tensor* scores) {
    auto root = tensorflow::Scope::NewRootScope();
    using namespace ::tensorflow::ops;  // NOLINT(build/namespaces)
 
    string output_name = "top_k";
    TopK(root.WithOpName(output_name), outputs[0], how_many_labels);
    // This runs the GraphDef network definition that we've just constructed, and
    // returns the results in the output tensors.
    tensorflow::GraphDef graph;
    TF_RETURN_IF_ERROR(root.ToGraphDef(&amp;graph));
 
    std::unique_ptr&lt;tensorflow::Session&gt; session(
            tensorflow::NewSession(tensorflow::SessionOptions()));
    TF_RETURN_IF_ERROR(session-&gt;Create(graph));
    // The TopK node returns two outputs, the scores and their original indices,
    // so we have to append :0 and :1 to specify them both.
    std::vector&lt;Tensor&gt; out_tensors;
    TF_RETURN_IF_ERROR(session-&gt;Run({}, {output_name + ":0", output_name + ":1"},
                                    {}, &amp;out_tensors));
    *scores = out_tensors[0];
    *indices = out_tensors[1];
    return Status::OK();
}
 
// Given the output of a model run, and the name of a file containing the labels
// this prints out the top five highest-scoring values.
Status PrintTopLabels(const std::vector&lt;Tensor&gt;&amp; outputs,
                      const string&amp; labels_file_name) {
    std::vector&lt;string&gt; labels;
    size_t label_count;
    Status read_labels_status =
            ReadLabelsFile(labels_file_name, &amp;labels, &amp;label_count);
    if (!read_labels_status.ok()) {
        //LOG(ERROR) &lt;&lt; read_labels_status;
        return read_labels_status;
    }
    const int how_many_labels = std::min(5, static_cast&lt;int&gt;(label_count));
    Tensor indices;
    Tensor scores;
    TF_RETURN_IF_ERROR(GetTopLabels(outputs, how_many_labels, &amp;indices, &amp;scores));
    tensorflow::TTypes&lt;float&gt;::Flat scores_flat = scores.flat&lt;float&gt;();
    tensorflow::TTypes&lt;int32&gt;::Flat indices_flat = indices.flat&lt;int32&gt;();
    for (int pos = 0; pos &lt; how_many_labels; ++pos) {
        const int label_index = indices_flat(pos);
        const float score = scores_flat(pos);
        LOG(INFO) &lt;&lt; labels[label_index] &lt;&lt; " (" &lt;&lt; label_index &lt;&lt; "): " &lt;&lt; score;
    }
    return Status::OK();
}
 
// This is a testing function that returns whether the top label index is the
// one that's expected.
Status CheckTopLabel(const std::vector&lt;Tensor&gt;&amp; outputs, int expected,
                     bool* is_expected) {
    *is_expected = false;
    Tensor indices;
    Tensor scores;
    const int how_many_labels = 1;
    TF_RETURN_IF_ERROR(GetTopLabels(outputs, how_many_labels, &amp;indices, &amp;scores));
    tensorflow::TTypes&lt;int32&gt;::Flat indices_flat = indices.flat&lt;int32&gt;();
    if (indices_flat(0) != expected) {
        LOG(ERROR) &lt;&lt; "Expected label #" &lt;&lt; expected &lt;&lt; " but got #"
                   &lt;&lt; indices_flat(0);
        *is_expected = false;
    } else {
        *is_expected = true;
    }
    return Status::OK();
}
 
int main(int argc, char* argv[]) {
    // These are the command-line flags the program can understand.
    // They define where the graph and input data is located, and what kind of
    // input the model expects. If you train your own model, or use something
    // other than inception_v3, then you'll need to update these.
    string image = "/Users/xxx/Downloads/inception_v3_model/grace_hopper.jpg";
    string graph =
            "/Users/xxx/Downloads/inception_v3_model/inception_v3_2016_08_28_frozen.pb";
    string labels =
            "/Users/xxx/Downloads/inception_v3_model/imagenet_slim_labels.txt";
    int32 input_width = 299;
    int32 input_height = 299;
    float input_mean = 0;
    float input_std = 255;
    string input_layer = "input";
    string output_layer = "InceptionV3/Predictions/Reshape_1";
    bool self_test = false;
    string root_dir = "";
    std::vector&lt;Flag&gt; flag_list = {
            Flag("image", &amp;image, "image to be processed"),
            Flag("graph", &amp;graph, "graph to be executed"),
            Flag("labels", &amp;labels, "name of file containing labels"),
            Flag("input_width", &amp;input_width, "resize image to this width in pixels"),
            Flag("input_height", &amp;input_height,
                 "resize image to this height in pixels"),
            Flag("input_mean", &amp;input_mean, "scale pixel values to this mean"),
            Flag("input_std", &amp;input_std, "scale pixel values to this std deviation"),
            Flag("input_layer", &amp;input_layer, "name of input layer"),
            Flag("output_layer", &amp;output_layer, "name of output layer"),
            Flag("self_test", &amp;self_test, "run a self test"),
            Flag("root_dir", &amp;root_dir,
                 "interpret image and graph file names relative to this directory"),
    };
    string usage = tensorflow::Flags::Usage(argv[0], flag_list);
    const bool parse_result = tensorflow::Flags::Parse(&amp;argc, argv, flag_list);
    if (!parse_result) {
        LOG(ERROR) &lt;&lt; usage;
        return -1;
    }
 
    // We need to call this to set up global state for TensorFlow.
    tensorflow::port::InitMain(argv[0], &amp;argc, &amp;argv);
    if (argc &gt; 1) {
        LOG(ERROR) &lt;&lt; "Unknown argument " &lt;&lt; argv[1] &lt;&lt; "\n" &lt;&lt; usage;
        return -1;
    }
 
    // First we load and initialize the model.
    std::unique_ptr&lt;tensorflow::Session&gt; session;
    string graph_path = tensorflow::io::JoinPath(root_dir, graph);
    Status load_graph_status = LoadGraph(graph_path, &amp;session);
    if (!load_graph_status.ok()) {
        //LOG(ERROR) &lt;&lt; load_graph_status;
        return -1;
    }
 
    // Get the image from disk as a float array of numbers, resized and normalized
    // to the specifications the main graph expects.
    std::vector&lt;Tensor&gt; resized_tensors;
    string image_path = tensorflow::io::JoinPath(root_dir, image);
    Status read_tensor_status =
            ReadTensorFromImageFile(image_path, input_height, input_width, input_mean,
                                    input_std, &amp;resized_tensors);
    if (!read_tensor_status.ok()) {
       // LOG(ERROR) &lt;&lt; read_tensor_status;
        return -1;
    }
    const Tensor&amp; resized_tensor = resized_tensors[0];
 
    // Actually run the image through the model.
    std::vector&lt;Tensor&gt; outputs;
    Status run_status = session-&gt;Run({<!-- -->{input_layer, resized_tensor}},
                                     {output_layer}, {}, &amp;outputs);
    if (!run_status.ok()) {
        LOG(ERROR) &lt;&lt; "Running model failed: " &lt;&lt; run_status;
        return -1;
    }
 
    // This is for automated testing to make sure we get the expected result with
    // the default settings. We know that label 653 (military uniform) should be
    // the top label for the Admiral Hopper image.
    if (self_test) {
        bool expected_matches;
        Status check_status = CheckTopLabel(outputs, 653, &amp;expected_matches);
        if (!check_status.ok()) {
            LOG(ERROR) &lt;&lt; "Running check failed: " &lt;&lt; check_status;
            return -1;
        }
        if (!expected_matches) {
            LOG(ERROR) &lt;&lt; "Self-test failed!";
            return -1;
        }
    }
 
    // Do something interesting with the results we've generated.
    Status print_status = PrintTopLabels(outputs, labels);
    if (!print_status.ok()) {
        LOG(ERROR) &lt;&lt; "Running print failed: " &lt;&lt; print_status;
        return -1;
    }
 
    return 0;
}
</code></pre> 
<p>这里面的模型是官网的下载链接 <a href="https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz" rel="nofollow">https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz</a>，测试一下可以运行。（具体的怎么加载tensorflow.dll,自己百度去）</p> 
<p>打算修改代码，讲道理了说，python可以运行，编译好的c++也应该运行。其实就是keepout没有传，修改代码如下。</p> 
<pre class="has"><code class="language-cpp">
#include &lt;fstream&gt;
#include &lt;utility&gt;
#include &lt;vector&gt;

#include "tensorflow/cc/ops/const_op.h"
#include "tensorflow/cc/ops/image_ops.h"
#include "tensorflow/cc/ops/standard_ops.h"
#include "tensorflow/core/framework/graph.pb.h"
#include "tensorflow/core/framework/tensor.h"
#include "tensorflow/core/graph/default_device.h"
#include "tensorflow/core/graph/graph_def_builder.h"
#include "tensorflow/core/lib/core/errors.h"
#include "tensorflow/core/lib/core/stringpiece.h"
#include "tensorflow/core/lib/core/threadpool.h"
#include "tensorflow/core/lib/io/path.h"
#include "tensorflow/core/lib/strings/str_util.h"
#include "tensorflow/core/lib/strings/stringprintf.h"
#include "tensorflow/core/platform/env.h"
#include "tensorflow/core/platform/init_main.h"
#include "tensorflow/core/platform/logging.h"
#include "tensorflow/core/platform/types.h"
#include "tensorflow/core/public/session.h"
#include "tensorflow/core/util/command_line_flags.h"


#include &lt;opencv2\\opencv.hpp&gt;

#include "tensorflow/core/protobuf/meta_graph.pb.h"
#include "tensorflow/cc/client/client_session.h"

using namespace std;
using namespace tensorflow;
using namespace cv;


// These are all common classes it's handy to reference with no namespace.
using tensorflow::Flag;
using tensorflow::Tensor;
using tensorflow::Status;
using tensorflow::string;
using tensorflow::int32;

// Takes a file name, and loads a list of labels from it, one per line, and
// returns a vector of the strings. It pads with empty strings so the length
// of the result is a multiple of 16, because our model expects that.
Status ReadLabelsFile(const string&amp; file_name, std::vector&lt;string&gt;* result,
 size_t* found_label_count) {
 std::ifstream file(file_name);
 if (!file) {
  return tensorflow::errors::NotFound("Labels file ", file_name,
   " not found.");
 }
 result-&gt;clear();
 string line;
 while (std::getline(file, line)) {
  result-&gt;push_back(line);
 }
 *found_label_count = result-&gt;size();
 const int padding = 16;
 while (result-&gt;size() % padding) {
  result-&gt;emplace_back();
 }
 ret</code></pre> 
<p>修改成功开始测试32位（或者64位是否成功）</p> 
<p><img alt="" class="has" height="113" src="https://images2.imgbox.com/79/1b/lrC03EsA_o.png" width="664"></p> 
<p>测试成功，不要急，同样的模型和图像用python测试一下，如果返回值相差很小，证明成功。 我看了一下，很多人都是编译gpu的，但是gpu的一般不能直接 用，他会检测你的电脑是否配置了相同型号的cudnn,如果没有不会自动变cpu，而是报错，编译的cpu版本的就没有这个问题。</p> 
<p>如果c++想使用opencv读数据，代码如下</p> 
<pre class="has"><code class="language-cpp">
#include"stdafx.h"

//***********  tensorflow C++接口调用图像分类pb模型代码  inception_v3_2016_08_28_frozen.pb ******************************
//#include &lt;fstream&gt;
//#include &lt;utility&gt;
//#include &lt;Eigen/Core&gt;
//#include &lt;Eigen/Dense&gt;
//#include &lt;iostream&gt;
//
//#include "tensorflow/cc/ops/const_op.h"
//#include "tensorflow/cc/ops/image_ops.h"
//#include "tensorflow/cc/ops/standard_ops.h"
//
//#include "tensorflow/core/framework/graph.pb.h"
//#include "tensorflow/core/framework/tensor.h"
//
//#include "tensorflow/core/graph/default_device.h"
//#include "tensorflow/core/graph/graph_def_builder.h"
//
//#include "tensorflow/core/lib/core/errors.h"
//#include "tensorflow/core/lib/core/stringpiece.h"
//#include "tensorflow/core/lib/core/threadpool.h"
//#include "tensorflow/core/lib/io/path.h"
//#include "tensorflow/core/lib/strings/stringprintf.h"
//
//#include "tensorflow/core/public/session.h"
//#include "tensorflow/core/util/command_line_flags.h"
//
//#include "tensorflow/core/platform/env.h"
//#include "tensorflow/core/platform/init_main.h"
//#include "tensorflow/core/platform/logging.h"
//#include "tensorflow/core/platform/types.h"
//
//#include "opencv2/opencv.hpp"
//
//using namespace tensorflow::ops;
//using namespace tensorflow;
//using namespace std;
//using namespace cv;
//using tensorflow::Flag;
//using tensorflow::Tensor;
//using tensorflow::Status;
//using tensorflow::string;
//using tensorflow::int32;
//
 定义一个函数讲OpenCV的Mat数据转化为tensor，python里面只要对cv2.read读进来的矩阵进行np.reshape之后，
 数据类型就成了一个tensor，即tensor与矩阵一样，然后就可以输入到网络的入口了，但是C++版本，我们网络开放的入口
 也需要将输入图片转化成一个tensor，所以如果用OpenCV读取图片的话，就是一个Mat，然后就要考虑怎么将Mat转化为
 Tensor了
//void CVMat_to_Tensor(Mat img, Tensor* output_tensor, int input_rows, int input_cols)
//{
//	//imshow("input image",img);
//	//图像进行resize处理
//	resize(img, img, cv::Size(input_cols, input_rows));
//	//imshow("resized image",img);
//
//	//归一化
//	img.convertTo(img, CV_32FC3);
//	//img = 1 - img / 255;
//	img = img/255.0; 
//
//
//	//创建一个指向tensor的内容的指针
//	float *p = output_tensor-&gt;flat&lt;float&gt;().data();
//
//	//创建一个Mat，与tensor的指针绑定,改变这个Mat的值，就相当于改变tensor的值
//	cv::Mat tempMat(input_rows, input_cols, CV_32FC3, p);
//	img.convertTo(tempMat, CV_32FC3);
//
//	//    waitKey(0);
//
//}
//
//int main(int argc, char** argv)
//{
//	/*--------------------------------配置关键信息------------------------------*/
//	string model_path = "inception_v3_2016_08_28_frozen.pb";
//	string image_path = "grace_hopper.png";
//	int input_height = 299;
//	int input_width = 299;
//	string input_tensor_name = "input";
//	string output_tensor_name = "InceptionV3/Predictions/Reshape_1";
//
//	/*--------------------------------创建session------------------------------*/
//	Session* session;
//	Status status = NewSession(SessionOptions(), &amp;session);//创建新会话Session
//
//														   /*--------------------------------从pb文件中读取模型--------------------------------*/
//	GraphDef graphdef; //Graph Definition for current model
//
//	Status status_load = ReadBinaryProto(Env::Default(), model_path, &amp;graphdef); //从pb文件中读取图模型;
//	if (!status_load.ok()) {
//		cout &lt;&lt; "ERROR: Loading model failed..." &lt;&lt; model_path &lt;&lt; std::endl;
//		cout &lt;&lt; status_load.ToString() &lt;&lt; "\n";
//		return -1;
//	}
//	Status status_create = session-&gt;Create(graphdef); //将模型导入会话Session中;
//	if (!status_create.ok()) {
//		cout &lt;&lt; "ERROR: Creating graph in session failed..." &lt;&lt; status_create.ToString() &lt;&lt; std::endl;
//		return -1;
//	}
//	cout &lt;&lt; "&lt;----Successfully created session and load graph.-------&gt;" &lt;&lt; endl;
//
//	/*---------------------------------载入测试图片-------------------------------------*/
//	cout &lt;&lt; endl &lt;&lt; "&lt;------------loading test_image--------------&gt;" &lt;&lt; endl;
//	//Mat img = imread(image_path, 0);
//	Mat img = imread(image_path);
//	if (img.empty())
//	{
//		cout &lt;&lt; "can't open the image!!!!!!!" &lt;&lt; endl;
//		return -1;
//	}
//
//	//创建一个tensor作为输入网络的接口
//	Tensor resized_tensor(DT_FLOAT, TensorShape({ 1,input_height,input_width,3 }));
//
//	//将Opencv的Mat格式的图片存入tensor
//	CVMat_to_Tensor(img, &amp;resized_tensor, input_height, input_width);
//
//	cout &lt;&lt; resized_tensor.DebugString() &lt;&lt; endl;
//
//	/*-----------------------------------用网络进行测试-----------------------------------------*/
//	cout &lt;&lt; endl &lt;&lt; "&lt;-------------Running the model with test_image---------------&gt;" &lt;&lt; endl;
//	//前向运行，输出结果一定是一个tensor的vector
//	vector&lt;tensorflow::Tensor&gt; outputs;
//	string output_node = output_tensor_name;
//	Status status_run = session-&gt;Run({ { input_tensor_name, resized_tensor } }, { output_node }, {}, &amp;outputs);
//
//	if (!status_run.ok()) {
//		cout &lt;&lt; "ERROR: RUN failed..." &lt;&lt; std::endl;
//		cout &lt;&lt; status_run.ToString() &lt;&lt; "\n";
//		return -1;
//	}
//	//把输出值给提取出来
//	cout &lt;&lt; "Output tensor size:" &lt;&lt; outputs.size() &lt;&lt; std::endl;
//	for (std::size_t i = 0; i &lt; outputs.size(); i++) {
//		cout &lt;&lt; outputs[i].DebugString() &lt;&lt; endl;
//	}
//
//	Tensor t = outputs[0];                   // Fetch the first tensor
//	auto tmap = t.tensor&lt;float, 2&gt;();        // Tensor Shape: [batch_size, target_class_num]
//	int output_dim = t.shape().dim_size(1);  // Get the target_class_num from 1st dimension
//
//											 // Argmax: Get Final Prediction Label and Probability
//	int output_class_id = -1;
//	double output_prob = 0.0;
//	for (int j = 0; j &lt; output_dim; j++)
//	{
//		cout &lt;&lt; "Class " &lt;&lt; j &lt;&lt; " prob:" &lt;&lt; tmap(0, j) &lt;&lt; "," &lt;&lt; std::endl;
//		if (tmap(0, j) &gt;= output_prob) {
//			output_class_id = j;
//			output_prob = tmap(0, j);
//		}
//	}
//
//	// 输出结果
//	cout &lt;&lt; "Final class id: " &lt;&lt; output_class_id &lt;&lt; std::endl;
//	cout &lt;&lt; "Final class prob: " &lt;&lt; output_prob &lt;&lt; std::endl;
//
//	return 0;
//}

//**************************************   test mobilenet model    *******************************************************

#include &lt;fstream&gt;
#include &lt;utility&gt;
#include &lt;Eigen/Core&gt;
#include &lt;Eigen/Dense&gt;
#include &lt;iostream&gt;

#include "tensorflow/cc/ops/const_op.h"
#include "tensorflow/cc/ops/image_ops.h"
#include "tensorflow/cc/ops/standard_ops.h"

#include "tensorflow/core/framework/graph.pb.h"
#include "tensorflow/core/framework/tensor.h"

#include "tensorflow/core/graph/default_device.h"
#include "tensorflow/core/graph/graph_def_builder.h"

#include "tensorflow/core/lib/core/errors.h"
#include "tensorflow/core/lib/core/stringpiece.h"
#include "tensorflow/core/lib/core/threadpool.h"
#include "tensorflow/core/lib/io/path.h"
#include "tensorflow/core/lib/strings/stringprintf.h"

#include "tensorflow/core/public/session.h"
#include "tensorflow/core/util/command_line_flags.h"

#include "tensorflow/core/platform/env.h"
#include "tensorflow/core/platform/init_main.h"
#include "tensorflow/core/platform/logging.h"
#include "tensorflow/core/platform/types.h"

#include "opencv2/opencv.hpp"

using namespace tensorflow::ops;
using namespace tensorflow;
using namespace std;
using namespace cv;
using tensorflow::Flag;
using tensorflow::Tensor;
using tensorflow::Status;
using tensorflow::string;
using tensorflow::int32;

// 定义一个函数讲OpenCV的Mat数据转化为tensor，python里面只要对cv2.read读进来的矩阵进行np.reshape之后，
// 数据类型就成了一个tensor，即tensor与矩阵一样，然后就可以输入到网络的入口了，但是C++版本，我们网络开放的入口
// 也需要将输入图片转化成一个tensor，所以如果用OpenCV读取图片的话，就是一个Mat，然后就要考虑怎么将Mat转化为
// Tensor了
void CVMat_to_Tensor(Mat img, Tensor* output_tensor, int input_rows, int input_cols)
{
	//imshow("input image",img);
	//图像进行resize处理
	resize(img, img, cv::Size(input_cols, input_rows));
	//imshow("resized image",img);

	//归一化
	img.convertTo(img, CV_32FC3);
	//img = 1 - img / 255;
	img = img/255.0;


	//创建一个指向tensor的内容的指针
	float *p = output_tensor-&gt;flat&lt;float&gt;().data();

	//创建一个Mat，与tensor的指针绑定,改变这个Mat的值，就相当于改变tensor的值
	cv::Mat tempMat(input_rows, input_cols, CV_32FC3, p);
	img.convertTo(tempMat, CV_32FC3);

	//    waitKey(0);

}


int main(int argc, char** argv)

{
	/*--------------------------------配置关键信息------------------------------*/
	string model_path = "D:/testmoooodel/tooth_mobilenet.pb";
	string image_path = "valclose_12.jpg";
	int input_height = 224;
	int input_width = 224;
	string input_tensor_name = "input";
	string output_tensor_name = "MobilenetV1/Logits/SpatialSqueeze";

	/*--------------------------------创建session------------------------------*/
	Session* session;
	Status status = NewSession(SessionOptions(), &amp;session);//创建新会话Session

														   /*--------------------------------从pb文件中读取模型--------------------------------*/
	GraphDef graphdef; //Graph Definition for current model

	Status status_load = ReadBinaryProto(Env::Default(), model_path, &amp;graphdef); //从pb文件中读取图模型;
	if (!status_load.ok()) {
		cout &lt;&lt; "ERROR: Loading model failed..." &lt;&lt; model_path &lt;&lt; std::endl;
		cout &lt;&lt; status_load.ToString() &lt;&lt; "\n";
		return -1;
	}
	Status status_create = session-&gt;Create(graphdef); //将模型导入会话Session中;
	if (!status_create.ok()) {
		cout &lt;&lt; "ERROR: Creating graph in session failed..." &lt;&lt; status_create.ToString() &lt;&lt; std::endl;
		return -1;
	}
	cout &lt;&lt; "&lt;----Successfully created session and load graph.-------&gt;" &lt;&lt; endl;

	/*---------------------------------载入测试图片-------------------------------------*/
	cout &lt;&lt; endl &lt;&lt; "&lt;------------loading test_image--------------&gt;" &lt;&lt; endl;
	//Mat img = imread(image_path, 0);
	Mat img = imread(image_path);
	Mat RGBImage;
	cvtColor(img, RGBImage, COLOR_BGR2RGB);


	if (img.empty())
	{
		cout &lt;&lt; "can't open the image!!!!!!!" &lt;&lt; endl;
		return -1;
	}

	//创建一个tensor作为输入网络的接口
	Tensor resized_tensor(DT_FLOAT, TensorShape({ 1,input_height,input_width,3 }));

	//将Opencv的Mat格式的图片存入tensor
	CVMat_to_Tensor(RGBImage, &amp;resized_tensor, input_height, input_width);

	cout &lt;&lt; resized_tensor.DebugString() &lt;&lt; endl;

	/*-----------------------------------用网络进行测试-----------------------------------------*/

	// 输入inputs，“ x_input”是我在模型中定义的输入数据名称，此外模型用到了dropout，所以这里有个“keep_prob”
	Tensor keep_prob(DT_FLOAT, TensorShape());
	Tensor is_training(DT_BOOL, TensorShape());


	keep_prob.scalar&lt;float&gt;()() = 1.0;
	is_training.scalar&lt;bool&gt;()() = false;

	std::vector&lt;std::pair&lt;std::string, tensorflow::Tensor&gt;&gt; inputs = {
		{ "input", resized_tensor },
		{ "keep_prob", keep_prob },
		{ "is_training", is_training },

	};


	cout &lt;&lt; endl &lt;&lt; "&lt;-------------Running the model with test_image---------------&gt;" &lt;&lt; endl;
	//前向运行，输出结果一定是一个tensor的vector
	vector&lt;tensorflow::Tensor&gt; outputs;
	string output_node = output_tensor_name;
	//Status status_run = session-&gt;Run({ { input_tensor_name, resized_tensor } }, { output_node }, {}, &amp;outputs);
	Status status_run = session-&gt;Run({ inputs }, { output_node }, {}, &amp;outputs);

	if (!status_run.ok()) {
		cout &lt;&lt; "ERROR: RUN failed..." &lt;&lt; std::endl;
		cout &lt;&lt; status_run.ToString() &lt;&lt; "\n";
		return -1;
	}
	//把输出值给提取出来
	cout &lt;&lt; "Output tensor size:" &lt;&lt; outputs.size() &lt;&lt; std::endl;
	for (std::size_t i = 0; i &lt; outputs.size(); i++) {
		cout &lt;&lt; "outputs[i].DebugString()  :"&lt;&lt;outputs[i].DebugString() &lt;&lt; endl;




		Tensor t = outputs[0];                   // Fetch the first tensor
		auto tmap = t.tensor&lt;float, 2&gt;();        // Tensor Shape: [batch_size, target_class_num]

		cout &lt;&lt; "tmap :"&lt;&lt; tmap &lt;&lt; endl;
		int output_dim = t.shape().dim_size(1);  // Get the target_class_num from 1st dimension
		cout &lt;&lt; "output_dim :" &lt;&lt; output_dim &lt;&lt; endl;
												 // Argmax: Get Final Prediction Label and Probability
		int output_class_id = -1;
		double output_prob = 0.0;
		for (int j = 0; j &lt; output_dim; j++)
		{
			cout &lt;&lt; "Class " &lt;&lt; j &lt;&lt; " prob:" &lt;&lt; tmap(0, j) &lt;&lt; "," &lt;&lt; std::endl;
			if (tmap(0, j) &gt;= output_prob) {
				output_class_id = j;
				output_prob = tmap(0, j);
			}
		}

		// 输出结果
		cout &lt;&lt; "Final class id: " &lt;&lt; output_class_id &lt;&lt; std::endl;
		cout &lt;&lt; "Final class prob: " &lt;&lt; output_prob &lt;&lt; std::endl;

		return 0;
	}
}</code></pre> 
<pre class="has"><code class="language-cpp">#define COMPILER_MSVC
#define NOMINMAX

#include &lt;fstream&gt;
#include &lt;utility&gt;
#include &lt;vector&gt;

#include "tensorflow/cc/ops/const_op.h"
#include "tensorflow/cc/ops/image_ops.h"
#include "tensorflow/cc/ops/standard_ops.h"
#include "tensorflow/core/framework/graph.pb.h"
#include "tensorflow/core/framework/tensor.h"
#include "tensorflow/core/graph/default_device.h"
#include "tensorflow/core/graph/graph_def_builder.h"
#include "tensorflow/core/lib/core/errors.h"
#include "tensorflow/core/lib/core/stringpiece.h"
#include "tensorflow/core/lib/core/threadpool.h"
#include "tensorflow/core/lib/io/path.h"
#include "tensorflow/core/lib/strings/str_util.h"
#include "tensorflow/core/lib/strings/stringprintf.h"
#include "tensorflow/core/platform/env.h"
#include "tensorflow/core/platform/init_main.h"
#include "tensorflow/core/platform/logging.h"
#include "tensorflow/core/platform/types.h"
#include "tensorflow/core/public/session.h"
#include "tensorflow/core/util/command_line_flags.h"


#include &lt;opencv2\\opencv.hpp&gt;

#include "tensorflow/core/protobuf/meta_graph.pb.h"
#include "tensorflow/cc/client/client_session.h"

using namespace std;
using namespace tensorflow;
using namespace cv;


// These are all common classes it's handy to reference with no namespace.
using tensorflow::Flag;
using tensorflow::Tensor;
using tensorflow::Status;
using tensorflow::string;
using tensorflow::int32;</code></pre> 
<p>                                   <u><em>如果想要64位的或者32位的dll，可以留言呀============</em></u></p> 
<p><strong> </strong></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8408d703de96ce03f96d1b28653e657a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">笔记——OpenCV无边框显示图片</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e649a96af33978f6c58aa0104f886216/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">window10下labelme的安装与使用（图像分割中数据标注）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>