<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>万字长文解析CV中的注意力机制（通道/空间/时域/分支注意力） - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="万字长文解析CV中的注意力机制（通道/空间/时域/分支注意力）" />
<meta property="og:description" content="点击下方卡片，关注“自动驾驶之心”公众号
ADAS巨卷干货，即可获取
点击进入→自动驾驶之心技术交流群
后台回复【transformer综述】获取2022最新ViT综述论文！
注意力机制是机器学习中嵌入的一个网络结构，主要用来学习输入数据对输出数据贡献；注意力机制在NLP和CV中均有使用，本文从注意力机制的起源和演进开始，并主要介绍注意力机制以及在cv中的各类注意力机制。
前言
transformer从2020年开始在cv领域通过vision transformer大放异彩过后，让cv和nlp走入大一统成为可能，而后swin transformer,DETR等在目标检测等cv任务上的transformer系列迅速霸占榜首。
图源：https://paperswithcode.com
瞅瞅这个仗势，只要不考虑参数量，transformer可以在cv领域野蛮生长，今天给大家分享transformer中的核心部件：注意力机制，因大刀从事cv领域，因此在学习和整理注意力相关知识时，主要看注意力机制的前世今生，着力点在cv领域中，看看注意力机制怎么在cv领域散开花。
1
注意力机制来源
假设下班后的你，躺窝在沙发里，听着广播里的音乐，被旋律深深打动，你会持续的关注音乐旋律、歌词，而忽略掉楼下马路上的汽车鸣笛，这种自上而下有意识的聚焦称为聚焦式注意力；而这时你的猫跳到你身上，对你喵喵叫，你会从旋律中跳出来，转过身摸它，这种自下而上无意识、由外界刺激引发的注意力称为显著式注意力。
01 注意力机制的来源
注意力机制来源于上个世纪90年代，认知领域的学者发现，人类在处理信息时，天然会过滤掉不太关注的信息，着重于感兴趣信息，于是将这种处理信息的机制称为注意力机制。
在传统机器学习中，通过建立特征工程，将输入数据转换成数值向量，帮助模型选择有效、适当规模的特征，进而让模型可以有效且高效的提取特征，让下游任务更聚焦与和任务关系更密切的信号，这即是一种简单有效的注意力体现。
到2014年，Volodymyr的《Recurrent Models of Visual Attention》一文中将其应用在视觉领域，后来伴随着2017年Ashish Vaswani的《Attention is all you need》中Transformer结构的提出，注意力机制在NLP,CV相关问题的网络设计上被广泛应用。
02 注意力机制的分类
注意力机制主要分成软注意力机制（全局注意）、硬注意力机制（局部注意）、和自注意力机制（内注意）。
软注意力机制：根据每个区域被关注程度的高低，用0~1之间的概率值来表示；与硬注意力相比，软注意力是一个可微的过程，可以通过训练过程的前向和后向反馈学习得到；因为对每部分信息都有考虑，所以相对于硬注意机制，计算量比较大。
硬注意力机制：又称强注意力机制，即哪些区域是被关注的，哪些区域是不被关注的，是一个是或不是的问题，会直接舍弃掉一些不相关项，如在图像领域的图像裁剪，裁剪后留下的部分即被关注的区域；优势在于会节省一定的时间和计算成本，但是有可能会丢失一部分信息。值得注意的是，因其是一个不可微的过程，所以在cv领域，一般用在强化学习中；如在视频领域中，因为有时序性关系，每张图片即为某个时间点下的采样，强注意力机制则可以看成是否对该时间点的采样关注，可以通过强化学习来训练。
自注意力机制：自注意力是对每个输入赋予的权重取决于输入数据之间的关系，即通过输入项内部之间的相互博弈决定每个输入项的权重。与前两项机制相比，自注意力在计算时，具有并行计算的优势。
同时在计算机视觉领域，按照注意力关注的域，可以将其分成空间域注意力、通道域注意力、时间域注意力、混合域注意力和自注意力等。
引进一张cv领域一看就懂的综述性的图：
2
cv中的注意力机制
在cv领域，有各式各样的注意力机制，通过赋予空间中的不同通道或者区域以不同的权重，确定关注的权重，而不像之前如池化操作时，将局部空间中某块位置视为相同的权重，本文就去年注意力机制的一篇综述[1]，介绍cv中常见的带有注意力机制的网络。下图总结了目前常用的cv中的注意力机制，以及相互关系（建议收藏）。
本章就各种不同域的注意力机制选择其中一个重点讲解，其他的注意力机制会配上论文和对应代码，并简要解释。
通道注意力机制
通道注意力机制在计算机视觉中，更关注特征图中channel之间的关系，而普通的卷积会对通道做通道融合，这个开山鼻祖是SENet,后面有GSoP-Net，FcaNet 对SENet中的squeeze部分改进，EACNet对SENet中的excitation部分改进，SRM,GCT等对SENet中的scale部分改进。
SENet
paper：https://arxiv.org/abs/1709.01507 github: pytorch:https://github.com/moskomule/senet.pytorch
SENet《Squeeze-and-Excitation Networks》是CVPR17年的一篇文章，提出SE module。在卷积神经网络中，卷积操作更多的是关注感受野，在通道上默认为是所有通道的融合（深度可分离卷积不对通道进行融合，但是没有学习通道之间的关系，其主要目的是为了减少计算量），SENet提出SE模块，将注意力放到通道之间，希望模型可以学习到不同通道之间的权重：
SE模块由三个部分组成：squeeze、excitation和scale。首先对h*w*c2大小的feature map 做squeeze操作将其变成1*1*c2大小，这一步原文中使用简单粗暴的GAP(全局平均池化)；第二步excitation操作，对1*1*c2的特征图经过两次全连接，特征图大小1*1*c2→1*1*c3→1*1*c2，再对1*1*c2的特征图做sigmoid将值限制到[0,1]范围内；第三步scale,将1*1*c2的特征权重图与输入的h*w*c2的feature map在通道上相乘；完成SE模块的通道注意力机制。
值得注意的是，在squeeze部分，作者使用的是最简单的全局平均池化的方式，主要原因是作者基于通道的整体信息，关注通道之间的相关性，而不是空间分布的相关性，也尽可能的屏蔽掉空间分布信息；
在excitation部分，作者通过两个全连接实现1*1*c2→1*1*c3→1*1*c2，第一个全连接层将c2压缩到c3，用于计算量减少，在原文中作者做实验后发现c3=c2/16时，能实现性能和计算量的平衡。
SE模型同Inception，可以即插即用，理论上可以安插在任意一个卷积后面，简单方便，只会增加一点参数量和计算量。
GSoP-Net
paper：https://arxiv.org/abs/1811.12006
github:https://github.com/ZilinGao/Global-Second-order-Pooling-Convolutional-Networks
SE模块中squeeze部分用的是全局平均池化，GSoP-Net的主要创新点是将一阶全局平均池化替换成二阶池化，主要操作为对输入为h*w*c‘的特征图先通过卷积降维到h*w*c,然后通道之间两两计算相关性，得到c*c的协方差矩阵，第i行元素表明第i i个通道和其他通道的统计层面的依赖。由于二次运算涉及到改变数据的顺序，因此对协方差矩阵执行逐行归一化，保留固有的结构信息。然后通过激励模块对协方差特征图做非线性逐行卷积得到1*1*4c的特征信息，后面同SENet。相对于SNet,增加了全局的统计建模。
FcaNet
paper：http://arxiv.org/abs/2012.11879
github: https://github.com/cfzd/FcaNet
FCANet主要也是更新SE模块中squeeze部分，作者设计了一种新的高效多谱通道注意力框架。该框架在GAP是DCT的一种特殊形式的基础上，在频域上推广了GAP通道注意力机制，提出使用有限制的多个频率分量代替只有最低频的GAP。通过集成更多频率分量，不同的信息被提取从而形成一个多谱描述。此外，为了更好进行分量选择，作者设计了一种二阶段特征选择准则，在该准则的帮助下，提出的多谱通道注意力框架达到了SOTA效果。
ECA-Net
paper: https://arxiv." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/8696add08783ca81bb668494f8329f20/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-10-24T09:00:30+08:00" />
<meta property="article:modified_time" content="2022-10-24T09:00:30+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">万字长文解析CV中的注意力机制（通道/空间/时域/分支注意力）</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p style="text-align:center;">点击下方<strong>卡片</strong>，关注“<strong>自动驾驶之心</strong>”公众号</p> 
 <p style="text-align:center;">ADAS巨卷干货，即可获取</p> 
 <p style="text-align:justify;"><strong>点击进入→</strong><a href="" rel="nofollow"><strong>自动驾驶之心技术交流群</strong></a></p> 
 <p style="text-align:justify;">后台回复【transformer综述】获取2022最新ViT综述论文！</p> 
 <p>注意力机制是机器学习中嵌入的一个网络结构，主要用来学习输入数据对输出数据贡献；注意力机制在NLP和CV中均有使用，本文从注意力机制的起源和演进开始，并主要介绍注意力机制以及在cv中的各类注意力机制。</p> 
 <blockquote> 
  <p><strong>前言</strong></p> 
 </blockquote> 
 <p>transformer从2020年开始在cv领域通过vision transformer大放异彩过后，让cv和nlp走入大一统成为可能，而后swin transformer,DETR等在目标检测等cv任务上的transformer系列迅速霸占榜首。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/1b/c2/zDA235bQ_o.png" alt="21b479ad1afaf802799a3736c612f8c2.png"></p> 
 <p style="text-align:center;">图源：https://paperswithcode.com<br></p> 
 <p>瞅瞅这个仗势，只要不考虑参数量，transformer可以在cv领域野蛮生长，今天给大家分享transformer中的核心部件：注意力机制，因大刀从事cv领域，因此在学习和整理注意力相关知识时，主要看注意力机制的前世今生，着力点在cv领域中，看看注意力机制怎么在cv领域散开花。</p> 
 <p>1</p> 
 <p><strong>注意力机制来源</strong></p> 
 <p>假设下班后的你，躺窝在沙发里，听着广播里的音乐，被旋律深深打动，你会持续的关注音乐旋律、歌词，而忽略掉楼下马路上的汽车鸣笛，这种自上而下有意识的聚焦称为<strong>聚焦式注意力</strong>；而这时你的猫跳到你身上，对你喵喵叫，你会从旋律中跳出来，转过身摸它，这种自下而上无意识、由外界刺激引发的注意力称为<strong>显著式注意力</strong>。</p> 
 <p><strong>01 注意力机制的来源<br></strong></p> 
 <p>注意力机制来源于上个世纪90年代，认知领域的学者发现，人类在处理信息时，天然会过滤掉不太关注的信息，着重于感兴趣信息，于是将这种处理信息的机制称为注意力机制。</p> 
 <p>在传统机器学习中，通过建立特征工程，将输入数据转换成数值向量，帮助模型选择有效、适当规模的特征，进而让模型可以有效且高效的提取特征，让下游任务更聚焦与和任务关系更密切的信号，这即是一种简单有效的注意力体现。</p> 
 <p>到2014年，Volodymyr的《Recurrent Models of Visual Attention》一文中将其应用在视觉领域，后来伴随着2017年Ashish Vaswani的《Attention is all you need》中Transformer结构的提出，注意力机制在NLP,CV相关问题的网络设计上被广泛应用。</p> 
 <p><strong>02 注意力机制的分类</strong></p> 
 <p>注意力机制主要分成软注意力机制（全局注意）、硬注意力机制（局部注意）、和自注意力机制（内注意）。</p> 
 <p><strong>软注意力机制</strong>：根据每个区域被关注程度的高低，用0~1之间的概率值来表示；与硬注意力相比，软注意力是一个可微的过程，可以通过训练过程的前向和后向反馈学习得到；因为对每部分信息都有考虑，所以相对于硬注意机制，计算量比较大。<br></p> 
 <p><strong>硬注意力机制</strong>：又称强注意力机制，即哪些区域是被关注的，哪些区域是不被关注的，是一个是或不是的问题，会直接舍弃掉一些不相关项，如在图像领域的图像裁剪，裁剪后留下的部分即被关注的区域；优势在于会节省一定的时间和计算成本，但是有可能会丢失一部分信息。值得注意的是，因其是一个不可微的过程，所以在cv领域，一般用在强化学习中；如在视频领域中，因为有时序性关系，每张图片即为某个时间点下的采样，强注意力机制则可以看成是否对该时间点的采样关注，可以通过强化学习来训练。</p> 
 <p><strong>自注意力机制</strong>：自注意力是对每个输入赋予的权重取决于输入数据之间的关系，即通过输入项内部之间的相互博弈决定每个输入项的权重。与前两项机制相比，自注意力在计算时，具有并行计算的优势。</p> 
 <p>同时在计算机视觉领域，按照注意力关注的域，可以将其分成空间域注意力、通道域注意力、时间域注意力、混合域注意力和自注意力等。<br></p> 
 <p>引进一张cv领域一看就懂的综述性的图：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/a8/af/w4avSU4N_o.png" alt="e8c1fba31fb8b6cd6ee8974de29ebf9f.png"></p> 
 <p>2</p> 
 <p><strong>cv中的注意力机制</strong></p> 
 <p>在cv领域，有各式各样的注意力机制，通过赋予空间中的不同通道或者区域以不同的权重，确定关注的权重，而不像之前如池化操作时，将局部空间中某块位置视为相同的权重，本文就去年注意力机制的一篇综述[1]，介绍cv中常见的带有注意力机制的网络。下图总结了目前常用的cv中的注意力机制，以及相互关系（建议收藏）。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/75/d9/C1S8cHZ6_o.png" alt="outside_default.png"></p> 
 <p>本章就各种不同域的<strong>注意力机制</strong>选择其中一个重点讲解，其他的注意力机制会配上论文和对应代码，并简要解释。<br></p> 
 <p><strong>通道注意力机制</strong></p> 
 <p>通道注意力机制在计算机视觉中，更关注特征图中channel之间的关系，而普通的卷积会对通道做通道融合，这个开山鼻祖是<strong>SENe</strong>t,后面有GSoP-Net，FcaNet 对SENet中的squeeze部分改进，EACNet对SENet中的excitation部分改进，SRM,GCT等对SENet中的scale部分改进。<br></p> 
 <p>SENet</p> 
 <p><em>paper：https://arxiv.org/abs/1709.01507 <br></em></p> 
 <p><em>github:  </em><em>pytorch:https://github.com/moskomule/senet.pytorch</em></p> 
 <p>SENet《Squeeze-and-Excitation Networks》是CVPR17年的一篇文章，提出SE module。在卷积神经网络中，卷积操作更多的是关注感受野，在<strong>通道</strong>上默认为是所有通道的融合（深度可分离卷积不对通道进行融合，但是没有学习通道之间的关系，其主要目的是为了减少计算量），<strong>SENet提出SE模块</strong>，将注意力放到通道之间，希望模型可以学习到不同通道之间的权重：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/eb/39/r9cKvEYW_o.png" alt="870d61a6b34dfe132d6cbefe17b36ba1.png"></p> 
 <p>SE模块由三个部分组成：<strong>squeeze</strong>、<strong>excitation</strong>和<strong>scale</strong>。首先对h*w*c2大小的feature map 做squeeze操作将其变成1*1*c2大小，这一步原文中使用简单粗暴的<a href="" rel="nofollow">GAP</a>(全局平均池化)；第二步excitation操作，对1*1*c2的特征图经过两次<a href="" rel="nofollow">全连接</a>，特征图大小1*1*c2→1*1*c3→1*1*c2，再对1*1*c2的特征图做sigmoid将值限制到[0,1]范围内；第三步scale,将1*1*c2的特征权重图与输入的h*w*c2的feature map在通道上相乘；完成SE模块的通道注意力机制。</p> 
 <p>值得注意的是，在<strong>squeeze</strong>部分，作者使用的是最简单的全局平均池化的方式，主要原因是作者基于通道的整体信息，关注通道之间的相关性，而不是空间分布的相关性，也尽可能的屏蔽掉空间分布信息；<br></p> 
 <p>在<strong>excitation</strong>部分，作者通过两个全连接实现1*1*c2→1*1*c3→1*1*c2，第一个全连接层将c2压缩到c3，用于计算量减少，在原文中作者做实验后发现c3=c2/16时，能实现性能和计算量的平衡。<br></p> 
 <p>SE模型同Inception，可以即插即用，理论上可以安插在任意一个卷积后面，简单方便，只会增加一点参数量和计算量。</p> 
 <p>GSoP-Net</p> 
 <p><em>paper：https://arxiv.org/abs/1811.12006</em></p> 
 <p><em>github:</em><em>https://github.com/ZilinGao/Global-Second-order-Pooling-Convolutional-Networks</em></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/eb/0d/9REiHPEH_o.png" alt="cbc2307ab001869e94cb88c3c9a10a96.png"></p> 
 <p>SE模块中<strong>squeeze</strong>部分用的是全局平均池化，GSoP-Net的主要创新点是将一阶全局平均池化替换成<strong>二阶池化</strong>，主要操作为对输入为h*w*c‘的特征图先通过卷积降维到h*w*c,然后通道之间两两计算相关性，得到c*c的<strong>协方差矩阵</strong>，第i行元素表明第i i个通道和其他通道的统计层面的依赖。由于二次运算涉及到改变数据的顺序，因此对协方差矩阵执行逐行归一化，保留固有的结构信息。然后通过<strong>激励模块</strong>对协方差特征图做非线性逐行卷积得到1*1*4c的特征信息，后面同SENet。相对于SNet,增加了全局的统计建模。</p> 
 <p>FcaNet</p> 
 <p><em>paper：http://arxiv.org/abs/2012.11879<br></em></p> 
 <p><em>github:  </em><em>https://github.com/cfzd/FcaNet</em></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/d9/34/VpQffGzf_o.png" alt="c0eb4bdd06ec1a58dcd9f15ec9bd5e26.png"></p> 
 <p>FCANet主要也是更新SE模块中squeeze部分，作者设计了一种新的高效<strong>多谱通道注意力框架</strong>。该框架在GAP是DCT的一种特殊形式的基础上，在频域上推广了GAP通道注意力机制，提出使用有限制的多个频率分量代替只有最低频的GAP。通过集成更多频率分量，不同的信息被提取从而形成一个多谱描述。此外，为了更好进行分量选择，作者设计了一种二阶段特征选择准则，在该准则的帮助下，提出的多谱通道注意力框架达到了SOTA效果。</p> 
 <p>ECA-Net</p> 
 <p><em>paper:  https://arxiv.org/pdf/1910.03151</em></p> 
 <p><em>github: https://github.com/BangguWu/ECANet</em></p> 
 <p>ECANet 改进SENet中的excitation部分，SENet采用的降维操作会对通道注意力的预测产生负面影响，且获取依赖关系效率低且不必要 ;基于此，提出了一种针对CNN的高效通道注意力(ECA)模块，避免了降维，有效地实现了跨通道交互 ；</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/bb/d5/xNUZkjp4_o.png" alt="outside_default.png"></p> 
 <p>主要提出了一个用CNN的极轻量话的<strong>通道注意力模型</strong>，通过大小为 k 的快速一维卷积实现，其中核大小k表示局部跨通道交互的覆盖范围，即有多少领域参与了一个通道的注意预测 ；同时为了避免通过交叉验证手动调整 k，开发了一种自适应方法确定<strong> k</strong>，其中跨通道交互的覆盖范围 (即核大小k) 与通道维度成比例 。</p> 
 <p>SRM</p> 
 <p><em>paper:   https://arxiv.org/pdf/1903.10829</em></p> 
 <p><em>github:https://github.com/hyunjaelee410/style-based-recalibration-module</em></p> 
 <p>SRM受风格迁移的启发，对SENet中的squeeze和excitation部分进行更新：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/a5/18/bbwt6Q26_o.png" alt="cacf8e1b9e778f29777ced15c4da3c60.png"></p> 
 <p>提出了一种基于style的重新校准模块（SRM)，可以通过利用其style自适应地重新校准中间特征图。SRM首先通过样式池从特征图的每个通道中提取样式信息，然后通过与通道无关的style集成来估计每个通道的重新校准权重。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/37/3b/psrA2hYz_o.png" alt="1eeb8b149c572409152168794e74aa5d.png"></p> 
 <p>给定一个输入特征特征图大小为C × H × W  ，SRM首先通过结合<strong>全局平均池</strong>和<strong>全局标准差池化</strong>的风格式池化收集全局信息。然后使用通道全连接层（即每个通道全连接）、批量归一化BN和sigmoid函数σ来提供注意力向量。最后，与SE块一样，输入特征乘以注意力向量。<strong>它利用输入特征的均值和标准差</strong>来提高捕获全局信息的能力。为了降低计算层（CFC）的完全连接要求，在全连接的地方也采用了CFC。通过将各个style的相对重要性纳入特征图，SRM有效地增强了CNN的表示能力。重点是轻量级，引入的参数非常少，同时效果还优于SENet.。</p> 
 <p>GCT</p> 
 <p><em>paper:   https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Gated_Channel_Transformation_for_Visual_Recognition_CVPR_2020_paper.pdf</em></p> 
 <p><em>github:  https://github.com/z-x-yang/GCT</em></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/e5/e4/g4JYl5Di_o.png" alt="outside_default.png"></p> 
 <p>GCT是由百度提出，一种通用且轻量型变化单元，结合了归一化和注意力机制，分析通道之间的相互关系（竞争or协作），对SENet中的squeeze、excitation和scale部分进行更新：在squeeze部分中使用<strong>L2 norm</strong>进行了<strong>global context embedi</strong><strong>ng</strong>，同时在excitation中，仍然使用<strong>L2 norm</strong> 对通道归一化，在scale中通过设置权重γ和偏置β来控制通道特征是否<strong>激活</strong>。当一个通道的特征权重γc被正激活，GCT将促进这个通道的特征和其它通道的特征“竞争”。当一个通道的特征 γc 被负激活，GCT将促进这个通道的特征和其它通道的特征“合作”。</p> 
 <p><strong>空间注意力机制</strong></p> 
 <p>空间注意力机制主要包括RNN为基础的注意力机制、预测相关区域的注意力机制、预测潜在mask的注意力机制以及自注意力机制。其中<strong>RAM</strong>为用于cv领域，backbone为RNN的注意力机制，<strong>DRAW,Glimps Net </strong>是基于此的拓展和延申；STN为最早关注相关区域的注意力机制，<strong>DCN,DCN V2</strong> 也是此后研究关注相关区域的注意力；<strong>GENet</strong>为预测潜在mask的注意力机制；在自注意力机制上，一开始提出是<strong>Non-local</strong> ,后续有提高效率的自注意力：<strong>CCNet,EMANet</strong>; 有关注局部的自注意力：<strong>SASA,SAN</strong>;从transformer 进入cv后，又有基于transformer改进的自注意力：<strong>ViT,DETR</strong>等，下面一一介绍。</p> 
 <p>RAM</p> 
 <p><em>paper：<em> https://arxiv.org/abs/1406.6247</em></em></p> 
 <p><em>github:  <em>https://github.com/jlindsey15/RAM</em></em></p> 
 <p>这是谷歌在14年基于RNN提出的注意力机制，用于图像分类，在14年卷积网络刚提出时，受限于硬件资源等因素，对计算量敏感，而当时减少计算量的方法主要有两个：一是减少窗口数量、二是引入注意力。</p> 
 <p>于是作者将注意力问题看作目标驱动的序列决策问题，使用了一个代理（agent）实现与环境的可视交互。代理每次只会用有限波长宽度的sensor来检测图像中的环境，但不会是全部图像。由于环境也都只是被部分检测到，因此后面还需要agent整合所有信息。在每一步，代理都会得到一个奖励，目的是要最大化代理得到奖励，一种强化学习的机制，因此该模型是典型的<strong>硬性注意力模型</strong>。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/2f/20/iopvIiSW_o.png" alt="048fc4ccacb4614fe7125a65b0dcec39.png"></p> 
 <p>该模型利用RNN结构，并结合LSTM，按照<strong>序列决策（sequential decision）</strong>的思路，从图像或者视频中以自适应方式不断挑选重要区域并对其进行<strong>高分辨率处理</strong>，而这些所谓的“重要区域”即为那些能够为视觉任务带来决定性影响的注意力投射区域。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/f5/e2/X1wZ9mi1_o.jpg" alt="8eebf698afe99c0304450f9d2204a717.jpeg"></p> 
 <p style="text-align:center;">源自网图，侵删</p> 
 <p>RAM模型对图像的分析过程可以用上图所示“瓢虫看图”的例子做以类比：一只瓢虫趴在一幅图像上看图，它每次只能看到身体下方的一小块图像（即注意力投射区域，如图中白色实线边框所示）。在某一时刻t，瓢虫所在位置为<em>l_t-1</em>（在开始的时候瓢虫趴在一个随机位置<em>l_0</em>），它收集当前所看见图像的信息（即G传感器抽取的类视网膜特征）并将其与位置信息进行综合（即G网络进行的图像-位置特征融合）得到一个信息表示<em>g_t</em>。然后瓢虫将该信息和其脑子中对图像已有的认知信息<em>h_t-1</em>进行再次整合（即RNN单元完成的工作），得到一个对图像新的认知信息<em>h_t</em>。然后，瓢虫按照当前认知信息<em>h_t</em>做出对图像状态的判断<em>a_t</em>，并决定下一个看图位置<em>l_t</em>（如图中黄色实线边框所示，白色虚线边框表示历史看图位置），如果瓢虫判断对了，就给它一个<strong>奖励</strong>。瓢虫以获得最多奖励为目标在图像上不断前行，在奖励的“诱惑”下，不断在图像上选择重点区域并对图像状态做出更加恰当的判断。</p> 
 <p>这种网络的优势在于：（1）参数量和计算量都能够通过输入图像的尺寸进行控制（2）RAM能够忽略杂乱图像中的噪声部分（3）能够方便的进行扩展（4）网络对输入图像采样的尺度能够控制，使它可用于不同尺寸的图像。</p> 
 <p>DRAW</p> 
 <p><em>paper：https://arxiv.org/pdf/1502.04623.pdf<br></em></p> 
 <p><em>github:  https://github.com/ericjang/draw</em></p> 
 <p>DRAW是15年Google Deepmind 发表在ICML上的一篇文章，用于图像生成领域，该网络用于模仿人眼视觉偏好性的空间注意力机制，基于变分自动编码器（Variational AutoEncoder VAEs）,对图像自动生成。<br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/fe/db/WsLFUEV9_o.png" alt="b8c79a2754a77f4b830ac77e75a88d80.png"></p> 
 <p>相对于上图左侧一般的变分编码器，右侧会基于前一次的解码器的输出来选取重点区域关注，参与这次的编码过程，解码器的结果也是之前结果的累加。</p> 
 <p>Glimpse Net</p> 
 <p><em>paper：https://arxiv.org/pdf/1412.7755.pdf<br></em></p> 
 <p><em>github:  https://github.com/sai19/Multiple-object-recognition-with-visual-attention</em></p> 
 <p>Glimpse Net是15年Google Deepmind 发表在ICRL上《Multiple Object Recognition With Visual Attention》文章中提到的一个网络，</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/ec/75/JRTbmur1_o.png" alt="68871c9b931b386806fd28448e50bebc.png"></p> 
 <p>STN-Net</p> 
 <p><em>paper：https://arxiv.org/pdf/1506.02025.pdf<br></em></p> 
 <p><em>github: tf: https://github.com/kevinzakka/spatial-transformer-network</em><br></p> 
 <p><em>            pytorch: https://github.com/qassemoquab/stnbhwd</em></p> 
 <p>STN-Net《Spatial Transformer Networks》是15年NIPS上的文章，提出了空间变换器的概念，STN-Net认为传统的卷积网络如果没有pooling层会要求输入数据空间固定，而pooling的方法过于暴力，可能会导致关键信息无法识别，所以提出了一个空间转换器（Spatial Transformer）的模块，主要作用是找到图片中需要被关注的区域，并对其旋转、缩放，提取出固定大小的区域。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/b4/5b/c5m66e5K_o.png" alt="016e12eb6d3b581c0ac87760beac0d9d.png"></p> 
 <p>空间采样器的实现主要分成三个部分:1）局部网络（Localisation Network）;2）参数化网格采样( Parameterised Sampling Grid)；3）差分图像采样（Differentiable Image Sampling）。</p> 
 <p>在理解采样器前需要大家对图像的仿射变化、旋转、平移等图像处理有先验知识，会发现对图像做旋转等变换时，实际上即是通过矩阵则可达到：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/15/15/ME3lb1Dc_o.png" alt="859b483a6e399582fbe1309a65e79755.png"></p> 
 <p><strong>局部网络</strong>是将图像提出特征的feature map作为输入，通过本地网络（卷积，FC等）训练出来的参数θ，在图像类一般是个[2,3]大小的6维变换参数；</p> 
 <p><strong>参数化网格采样</strong>则是将训练的参数将输入的特征图进行变换，这决定了变换前后图像之间的坐标映射关系。</p> 
 <p><strong>差分图像采样</strong>是根据第二步产生的坐标映射关系将输入图像U像素点变换成输出图片V的像素点。这里需要注意的是，提取到输出特征图对应到输入特征的位置可能不一定是整数点，非整数坐标的像素点需要通过双向线性插值提取。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/97/16/qIlCpPNe_o.png" alt="810b88218358de846a00125a2b2b5a8e.png"></p> 
 <p>因以上三个网络均为可微的，因此它可以插入到正常的网络中，通过反向传播更新参数，且无需额外的监督信息。</p> 
 <p>DCN v1/v2<br></p> 
 <p><em>v1: paper：https://arxiv.org/pdf/1703.06211.pdf</em></p> 
 <p><em>      github:  https://github.com/ msracver/Deformable-ConvNets</em></p> 
 <p><em>v2: paper: https://arxiv.org/pdf/1811.11168.pdf</em></p> 
 <p><em>     github:  https://github.com/CharlesShang/DCNv2</em></p> 
 <p>可变形卷积因为空间域中，会关注感兴趣点的位置，有空间注意力的思想在，具体DCN实现如下：</p> 
 <p>其核心思想在于，它不认为卷积核应该是规规整整的矩形，其可以是任意形状的</p> 
 <p>如下图：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/e7/00/9kUeiLQx_o.png" alt="bf0e7f506c2c765b696cc9a5fb0ba73f.png"></p> 
 <p style="text-align:center;">图a标准3x3卷积，b,c,d是给普通卷积加上偏移量的卷积核，蓝色是新的卷积点，箭头是位移方向。</p> 
 <p>首先普通卷积的操作可以参见<a href="" rel="nofollow">这里</a>，对于每个输出y(p0)，都要从x上采样9个位置，（-1，-1）代表x(p0)的左上角，（1,1）代表x(p0)的右下角：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/c8/ec/9sq1pDZ1_o.png" alt="8ed60aa183587582eacc2d0856e4e463.png"></p> 
 <p>对于传统卷积，公式如下：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/f8/c3/TcTe7QW8_o.png" alt="e0b58a2c1f36c3a49d1fe09a9b581191.png"></p> 
 <p>对于可变形卷积，卷积的映射是由下式表示的：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/70/52/DTXki6ew_o.png" alt="ace82b2d74cb20b766de6ad83275d097.png"></p> 
 <p>在传统卷积的基础上，增加一个偏移量，为啥增加这个呢，看下图：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/a4/a0/oj0r2AcN_o.png" alt="8bf92b872029378984974e4be819eccb.png"></p> 
 <p>对于左图来说，传统的卷积并不能完整的表示羊的特征，右图采用可变形卷积，充分获取羊的特征。这个偏移量是位置的偏移量，比如卷积核某个点一开始位于（-1，-1）这个位置，有了偏移量后，可能在（-1,2）的位置，另外偏移量可能是个浮点型，所以上式中的x这个特征值需要通过<strong>双线性插值</strong>的方法来计算。如何获得这个偏移量呢？通过训练学习得到，如下图。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/d0/0f/LKu2dtO8_o.png" alt="5374efee4eed087c9caec21fef670973.png"></p> 
 <p>对于输入一张5x5x3的feature map，假设原来的卷积是3x3,inchanel 和outchannel均是3，输出维度仍是5x5x3，现在我们为了学习偏移量，需要重新定义一个3x3的卷积，如上图, 这里inchannel仍是3，但是输出的outchannel需要是2倍，即6，因为他需要输出x和y两个方向的偏移量，获得每个点偏移量后，即获得△Pn, 因为△Pn不一定是整数，所以需要基于输入的特征值<strong>双线性插值</strong>获得x(p0+pn+△Pn)值，后面和普通的卷积一样，权值相乘后相加。以上可变形卷积可以自适应的学习感受野，学习与目标区域感兴趣的区域。</p> 
 <p>DCN v2 作者可视化了DCNv1的结果：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/d1/cd/LlWfIVeb_o.png" alt="e56032b64f2ea33f694c416e573767da.png"></p> 
 <p>图中发现可变形卷积会引入无用的上下文信息，其感受野容易超出目标范围，干扰特征提取，为此作者提出了DCNv2解决此问题：<br></p> 
 <p>1. 使用更多的可变形卷积；<br></p> 
 <p>2. 在DCNv1的基础上添加每个采样点的权重；</p> 
 <p>3. 针对可变形卷积的训练，为了让其更好的学习，通过RCNN指导Faster RCNN做知识蒸馏。</p> 
 <p>DCN v1 给普通卷积的采样点增加偏移，v2在此基础上还允许调节每个采样位置或者bin的特征的权值，就是给这个点的特征乘以个系数，位于（0,1）之间，如果系数为0，就表示这部分区域的特征对输出没有影响，这个系数也是通过训练学习得到，这样在可变形卷积的输出通道由之前的2N增加到3N:<br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/f5/65/dM0Um7iw_o.png" alt="a0b2b527bd2ba5fc75927298e7caefed.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/39/bd/dn3Sfi4b_o.jpg" alt="e121491a4c725480a5aa773aa4a8caa5.jpeg"></p> 
 <p>GENet</p> 
 <p><em>paper：https://arxiv.org/abs/1810.12348<br></em></p> 
 <p><em>github:  https://github.com/hujie-frank/GENet.</em></p> 
 <p>受 SENet 启发，Hu 等人设计 GENet 通过在空间域中提供重新校准功能来捕获远程空间上下文信息。GENet 结合了部分收集和激发操作。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/38/ec/s5IgFPbW_o.png" alt="c65e15449a2bd3ef2fba701747dfc475.png"></p> 
 <p>第一步，它聚合大邻域的输入特征，并对不同空间位置之间的关系进行建模。在第二步中，它首先使用插值生成与输入特征图大小相同的注意力图。然后通过乘以注意力图中的相应元素来缩放输入特征图中的每个位置。这个过程可以描述为：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/aa/c6/qa69ayFP_o.png" alt="b8be754c667da548a69cc9b4abed7038.png"></p> 
 <p>在这里，fgather(X)可以采用任何捕捉空间相关性的形式，例如全局平均池化或一系列深度卷积；Interp ( ⋅ ) (\cdot)(⋅)表示插值。</p> 
 <p>Gather-excite 模块是轻量级的，可以像 SE 块一样插入到每个残差单元中。它在抑制噪音的同时强调重要特征。</p> 
 <p>Non-local Neural Networks</p> 
 <p><em>paper: https://arxiv.org/abs/1711.07971<br></em></p> 
 <p><em>github: https://github.com/facebookresearch/video-nonlocal-net</em></p> 
 <p>Non-local Neural networks 是CVPR2018提出的一个自注意力模型，普通的卷积即局部注意力，是3*3的卷积核，然后在整个图片上移动，non-local是综合一个比较大的搜索范围，并进行加权。作者基于图片滤波中的non-local means 的思想，提出的一个非局部操作算子，非局部操作可以直接计算图片空间中两个位置之间的关系，忽略其空间位置的影响，具体计算公式如下：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/e6/73/PLVPMngF_o.png" alt="0354a38006bca0544dc1dd9939510fe8.png"></p> 
 <p>其中x为输入信号，在cv中为feature map; i为输出位置，输出位置响应等于所有枚举j对i的响应计算得到；f计算i与j之间的相似度；g计算feature map在j位置上的表示；y通过c(x)进行标准化处理后得到；</p> 
 <p>具体在cv上的计算示例图如下：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/42/b6/Mcw1ZICg_o.png" alt="8b804373338314b793b7104001db0ff5.png"></p> 
 <p style="text-align:center;">源自网图，侵删<br></p> 
 <p>简化版如下：<br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/ca/d5/kJWMnmYW_o.png" alt="c54e626e0c681d852c1955da83c5616c.png"></p> 
 <p>文中谈到有多种实现方式，本文中介绍其中一种简单的：</p> 
 <p>1. 首先在输入的特征图进行线性映射（通过1*1的卷积，压缩通道数），得到θ，φ，g 特征；</p> 
 <p>2. 通过reshape操作，强行合并上述三个特征除通道外了其他维度，对θ和φ进行举证点乘操作，用于计算特征中的自相关性，得到每个像素对其他像素的关系；<br></p> 
 <p>3. 通过对自相关特征进行softmax操作，得到[0,1]的权重，即自注意力系数；<br></p> 
 <p>4. 将自注意力系数乘回矩阵g中，并通过1*1卷积，拓展通道数，与输入的特征图相加，得到non-local 模块的输出。</p> 
 <p>non-local 模块保证了输入和输出尺寸不变，容易嵌入网络架构中，通过直接融合全局信息，计算量和参数量均有一定的增加。non-local 因为时间原因也有一定的局限性：1. 没有考虑到通道注意力；2. 如果特征图很大，则会很耗内存和计算量。</p> 
 <p>CCNet</p> 
 <p><em>paper: https://arxiv.org/pdf/1811.11721.pdf<br></em></p> 
 <p><em>github: https://github.com/speedinghzl/CCNet</em></p> 
 <p>Non-local可以看成是注意力机制下的密集连接的GNN,虽然能够捕获全局的上下文信息，但是其计算复杂度为O(N^2),为了解决复杂度问题，CCNet提出一个更高效的注意力机制——交叉注意力模块，用于语义分割领域，其与Non-local的对比见图1：<br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/f6/71/Edtm13mk_o.png" alt="03af5ff78e88c0dcfa087f3d767cfb52.png"></p> 
 <p>如上图可知，与Non-local相比，criss-cross attention module生成attention map只利用了十字交叉路径上的特征，这种操作大大降低了计算复杂度。交叉注意力模块的结构如下图，但是单次的criss-cross attention module只能捕获十字交叉路径上的特征，为了同时用上其他位置特征，作者<strong>用了两次criss-cross attention module</strong>，第一次相关性计算会将当前像素的信息传递到同行同列的所有位置，第二次计算时之前接受到信息的每个像素又会将再次将信息传递到自己的同行同列。如下二图所示，称为RCCA模块：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/36/59/TEC7Q6mt_o.png" alt="c6652bcebfa1340bc6d10a8e581d761a.png"></p> 
 <p>这样通过RCCA模块，即可获得全局信息，与Non-local达到相同的目的，但是其复杂度从<em>O(N^2)</em>变成<em>O(N^(3/2))</em>.</p> 
 <p>EMANet<br></p> 
 <p><em>paper: https://arxiv.org/pdf/1907.13426.pdf</em></p> 
 <p><em>github: <em>https://github.com/XiaLiPKU/EMANet</em></em></p> 
 <p>EMANet的提出也是为了解决Non local带来的计算量过于庞大的问题，该论文将注意力机制表述为一种<strong>期望最大化</strong>方式，并自动估计出一组更紧凑的基，在此基础上计算注意力图。通过对这些基的加权求和，得到的表示是低秩的，并且有效消除来自输入的噪声信息。所提出的期望最大化注意力（EMA）模块对输入方差具有鲁棒性，并且在内存和计算方面也很友好。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/74/da/eLYBlRTc_o.png" alt="8e46b9d1a31f253d0fb8578d99a0110a.png"></p> 
 <p>SASA</p> 
 <p><em>paper: https://arxiv.org/pdf/1906.05909.pdf</em></p> 
 <p><em>github: https://github.com/leaderj1001/Stand-Alone-Self-Attention</em></p> 
 <p>主要的贡献是提出使用 “<strong>自注意力操作</strong>”代替一般的空间卷积操作，以弥补空间卷积无法有效捕捉长距离信息间的关系的不足，同时使用的计算量和参数量更少.目前的self attention平等对待中心像素邻近的其他像素点，没有利用位置信息，因此文中进一步通过用嵌入向量来表示相对位置，把位置信息也添加到了自注意力操作中。</p> 
 <p>这里作者通过实验也说明了 <strong>相对位置信息</strong>&gt; <strong>绝对位置信息 </strong>&gt; <strong>没有位置信息.</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/61/5a/OyHtTzfx_o.png" alt="a096e12621b469d33bc92ff89fedade0.png"></p> 
 <p>SAN</p> 
 <p><em>paper: https://arxiv.org/abs/2004.13621</em></p> 
 <p><em>github: https://github.com/hszhao/SAN</em></p> 
 <p>这篇文章发表在CVPR2020。作者提出一种将self-attention机制应用到图像识别领域的方法。作者认为，使用卷积网络进行图像识别任务实际上在实现两个函数:</p> 
 <p>1.<strong> 特征聚集(feature aggregation)</strong>: 即通过卷积核在特征图上进行卷积来融合特征的过程。</p> 
 <p>2. <strong>特征变换(feature transformation)</strong>: 在卷积完成后进行的一系列线性和非线性变换（比如全连接和激活函数。这一部分通过感知机就能很好地完成。</p> 
 <p>在以上观点的基础上，作者提出使用self-attention机制来替代卷积作为特征聚集方法。为此，作者考虑两种self-attention形式：pairwise self-attention和patchwise self-attention。用这两种形式的self-attention机制作为网络的basic block提出SAN网络结构。与经典卷积网络ResNet进行对比，SAN网络具有更少参数和运算量，同时在ImageNet数据集上的分类精确度有较大提升。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/a4/05/jiiz3cC7_o.png" alt="73b9a3c1fb580c07c7da441697373dd4.png"></p> 
 <p>ViT/DETR</p> 
 <p><em>VIT: paper: https://arxiv.org/pdf/2010.11929.pdf</em></p> 
 <p><em>       github: </em></p> 
 <p><em>https://github.com/rwightman/pytorch-image-models/blob/main/timm/models/vision_transformer.py</em></p> 
 <p><em>DETR:  p</em><em>aper: <em>https://arxiv.org/pdf/2005.12872.pdf</em></em></p> 
 <p><em>github: <em>https://github.com/facebookresearch/detr</em></em></p> 
 <p>vision transformer 中的注意力机制为transformer在encode时的多头注意力机制，因为vision transformer 是将transformer从nlp中用于cv中，尽量少的变动transformer ，所以文中提出（key,query, value）三元组捕捉长距离依赖的建模方式同nlp中的self-attention，如下图所示，key和query通过点乘的方式获得相应的注意力权重，最后把得到的权重和value做点乘得到最终的输出。公式如下：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/d6/8b/hA3cdE62_o.png" alt="f0bc6d5001d7698b93ad514bec5ed86c.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/c7/d9/Uz764ppz_o.png" alt="c9d0fa43ab3947c2c2bfa20933a01920.png"></p> 
 <p>整体的vision transformer 网络架构如下图：<br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/a4/71/9ttkyGTI_o.png" alt="27ed681381e6038c2fafcc2c0910a2fd.png"></p> 
 <p>DETR是Facebook在2020年提出，是transformer在目标检测任务上的应用，利用Transformer中attention机制能够有效建模图像中的长程关系（long range dependency），简化目标检测的pipeline，构建端到端的目标检测器。 <br></p> 
 <p><strong>时域注意力机制</strong></p> 
 <p>时域注意力机制在cv领域主要考虑有时序信息的领域，如视频领域中的动作识别方向，其注意力机制主要是在时序列中，关注某一时序即某一帧的信息。</p> 
 <p>TAM</p> 
 <p><em>paper: <em><em>https://arxiv.org/abs/2005.06803v1</em></em><br></em></p> 
 <p><em>github: <em>https://github.com/liu-zhy/temporal-adaptive-module</em></em></p> 
 <p>由于存在拍摄视角变化和摄像机运动等多个因素，视频数据通常表现出较为复杂的时序动态特性，不同视频在时序维度上呈出不同的运动模式。为了解决这个问题，时序自适应模块（TAM）为每个视频生成特定的时序建模核。该算法针对不同视频片段，灵活高效地生成动态时序核，自适应地进行时序信息聚合。整体结构入下图所示：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/27/ef/cuiMoedT_o.png" alt="4104011ebee081c0895b7d14b6ebb68e.png"></p> 
 <p>TAM将时序自适应核的学习过程分解为局部分支和全局分支。全局分支（ G ）基于全局时序信息生成视频自适应的动态卷积核以聚合时序信息，这种方式的特点是对时序位置不敏感，忽略了局部间的差异性。而局部分支（L ）使用带有局部时序视野的 1D 卷积学习视频的局部结构信息，生成对时序位置敏感的重要性权重，以弥补全局分支存在的不足。</p> 
 <p>GLTR</p> 
 <p><em>paper: <em><em>https://arxiv.org/abs/1908.10049</em></em><br></em></p> 
 <p><em>github: <em>https://github.com/ljn114514/GLTR</em></em></p> 
 <p>这是一篇用于行人ReID领域的一篇论文，作者提出在短期建模，基于当前帧的相邻几帧，能加强当前帧人物在该时间段的外观和运动情况，当任务发生遮挡时，则需要使用长期建模，增加时间跨度。所以论文在融合帧的特征时，短期建模和长期建模一起用上：<br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/4a/a6/cuXlRAvB_o.png" alt="68cf59384ea05f9bcfbd49e1e2766828.png"></p> 
 <p>在短期建模时，使用了空洞卷积，增加感受野，在这里则是增加对当前帧的相邻几帧一起进行卷积处理，也就是综合相邻几帧的信息来增强当前帧的信息。在长期建模中，则使用的是transformer中的self-attention 机制。transformer的attention计算是通过所有信息与当前信息的关系计算的，也就是相当于基于当前帧与全部帧的关系，将全部帧的信息选择性的给予到当前帧，是一个长期建模的过程。也是变相的将注意力机制用在建模中。</p> 
 <p><strong>Branch注意力机制</strong></p> 
 <p>branch注意力机制主要是关注哪个图片的意思，如一个branch中对不同图片以不同的权重，如CondConv,Dynamic Conv 等；或者在多个branch中，对不同的branch不同的权重，如Highway Network，SKNet, ResNeSt等。</p> 
 <p>Highway Network</p> 
 <p><em>paper: <em>https://arxiv.org/abs/1507.06228</em><br></em></p> 
 <p><em>github: <em>https://github.com/jzilly/RecurrentHighwayNetworks</em></em></p> 
 <p>Highway Network基于门机制引入了transform gate T  和carry gate C ，输出output是由tranform input和carry input组成，和resnet的思想有点相似。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/d3/9b/xVnm8ehH_o.png" alt="e8a5afa6049f53e07b131407a5c251d5.png"></p> 
 <p>SKNet</p> 
 <p><em>paper: <em>https://arxiv.org/pdf/1903.06586.pdf</em><br></em></p> 
 <p><em>github: <em>https://github.com/implus/SKNet</em></em></p> 
 <p>SKNet 对不同输入使用的卷积核感受野不同,参数权重也不同,可以自适应的对输出进行处理，与SENet有相同的地位：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/67/5e/6N4tsQAq_o.png" alt="5c60a5487f532c4b45ecf2e54848aa0d.png"></p> 
 <p>sknet模块主要由 Split、Fuse、Select 三部分组成。</p> 
 <p>这里的<strong>Split</strong>是指对输入特征进行不同卷积核大小的完整卷积操作(包括efficient grouped/depthwise convolutions，Batch Normalization，ReLU function)。如结构图所示，对特征图进行Kernel3×3和Kernel5×5的卷积操作，得到两个输出，这里为了进一步减少计算量，会将5x5的卷积由两个3x3的卷积实现。在得到两个特征图后，第二步为<strong>Fuse</strong>部分，和SE模块相似，先将两个特征图逐像素相加后，使用全局平均池化（GAP），压缩成1*1*c的特征图后，先降维再升维经过两次全连接，输出两个矩阵a和b,a和b各位置逐值相加和为1，即a=1-b。第三步为<strong>select</strong>部分，区别SENet,这里使用a和b的权重矩阵分别对第一步输出的两个特征图加权，最后求和得到最后的输出。</p> 
 <p>SKNet也是可直接嵌入网络的轻量级模块，SKNet使用时涉及到了卷积核数量和大小的选择问题。直观来说SKNet相当于给网络融入了soft attention机制，使网络可以获取不同感受野的信息，这或许可以成为一种泛化能力更好的网络结构。至于为何将SKNet放在branch attention 下面，可能是因为在第一步时使用了分组卷积吧。</p> 
 <p>ResNeSt</p> 
 <p><em>paper: <em>https://hangzhang.org/files/resnest.pdf</em><br></em></p> 
 <p><em>github: <em>https://github.com/zhanghang1989/ResNeSt</em></em></p> 
 <p>ResNeSt是基于SENet,SKNet和ResNext ，把attention 做到group level。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/7e/c5/uItOTGU7_o.png" alt="96bf456a5202926b06b1cb1c9371e4ac.png"></p> 
 <p>CondConv</p> 
 <p><em>paper: </em><em>https://arxiv.org/abs/1904.04971</em></p> 
 <p><em>github: <em>https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/condconv</em></em></p> 
 <p>作者提出一种条件参数卷积，它可以为每个样例学习一个特定的卷积核参数，通过替换标准卷积，CondConv可以提升模型的尺寸与容量，同时保持高效推理。<em><br></em></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/a4/65/BQuKI7nh_o.png" alt="3e8912ce0ffdd08a7fef1e859d7312ea.png"></p> 
 <p>CondConv提出的方法与<strong>混合专家方法（Mixture of Experts）</strong>类似，需要在执行卷积计算之前通过多个专家对输入样本计算加权卷积核。主要需要计算几个较为昂贵的依赖样本的routing函数，Routing函数对应的模块与注意力模块类似，包括平均池化，全连接层和Sigmoid激活层。关键的是，每个卷积核只需计算一次并作用于不同位置即可。这意味着：通过提升专家数据量可达到提升网络容量的目的，而代码仅仅是很小的推理耗时：每个额外参数仅需一次乘加。如上图所示。</p> 
 <p>Dynamic Conv</p> 
 <p><em>paper: <em>https://arxiv.org/pdf/1912.03458.pdf</em><br></em></p> 
 <p><em>github: <em>https://github.com/kaijieshi7/Dynamic-convolution-Pytorch</em></em></p> 
 <p>文章提出的动态卷积能够根据输入，动态地集成多个并行的卷据核为一个动态核，可以提升模型表达能力而无需提升网络深度与宽度。通过简单替换成动态卷积。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/e1/65/Y23tfnSB_o.png" alt="d4b055035cb808eeb6196d93b4d75481.png"></p> 
 <p>动态卷积有K个kernel，共享相同的kernel size和输入输出维度，通过attention weight结合起来，与SENet对卷积的通道加权不同，动态卷积对卷积核加权。</p> 
 <p><strong>通道和空间注意力机制</strong></p> 
 <p>通道和空间注意力是基于通道注意力和空间注意力机制，将两者有效的结合在一起，让注意力能关注到两者，又称混合注意力机制，如CBAM,BAM,scSE等，同时基于混合注意力机制的一些关注点，如<strong>Triplet Attention </strong>关注各种跨维度的相互作用；<strong>Coordinate Attention, DANet</strong>关注长距离的依赖；<strong>RGA </strong>关注关系感知注意力。还有一种混合注意力机制，为3D的attention :<strong>Residual attention,SimAM, Strip Pooling, SCNet</strong>等。</p> 
 <p>CBAM</p> 
 <p><em>paper: <em>https://arxiv.org/abs/1807.06521</em><br></em></p> 
 <p><em>github: <em>https://github.com/luuuyi/CBAM.PyTorch </em></em></p> 
 <p>CBAM (Convolutional Block Attention Module)是SENet的一种拓展，SENet主要基于通道注意力，CBAM是通道注意力和空间注意力融合的注意力机制。<br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/6c/69/qYA3djz2_o.png" alt="36c3354d7c3a83f96bddf01a46ebbd8a.png"></p> 
 <p>如上图所示，输入一个h*w*c的特征图，通过<strong>channel Attention Module</strong> 生成通道注意力权重对输入特征图在通道层添加权重，再通过<strong>spatial Attention Module</strong> 生成空间注意力权重，对特征图在空间层添加权重，输出特征图。<br></p> 
 <p>具体通道注意力实现图如下：<br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/40/7d/qAMcM8I1_o.png" alt="856c3049cb3f18e0c9b3453773ca25fb.png"></p> 
 <p>输入特征图h*w*c，先分别通过一个全局最大池化和全局平均池化，分别得到一个1*1*c的特征图，将两个特征图分别送进两个全连接层，得到两个1*1*c的特征图，讲两个特征图相加，通过sigmoid激活到[0,1]范围内的权重，与输入的特征图在通道层相乘，得到输出特征图。这里的思路基本与SENet相同，除了增加一个池化，增加通道注意力的语义丰富性。作者也实验证明了同时使用两个池化比单独使用其中一种效果要好，可以理解最大池化关注图片的显著信息，平均池化关注图片的全局信息，都是有可用性，同理后面的空间注意力机制也使用了两种池化方式。</p> 
 <p>空间注意力实现如下：<br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/ca/7c/Ycnu82Vd_o.png" alt="c63cc97e1c7675f8fe3d84393de4ad4e.png"></p> 
 <p>输入特征图为 H x W x C，分别通过一个通道维度的最大池化和平均池化得到两个H x W x 1的特征图，然后将这两个特征图在通道维度拼接起来，得到一个H x W x 2的特征图，然后再经过一个卷积层，降为1个通道，保持H W 不变，输出feature map为H x W x 1，然后再通过Sigmoid函数生成空间权重系数，与输入feature map相乘得到最终feature map。同时作者实验证明通道注意力放在空间注意力前面效果更好。</p> 
 <p>BAM</p> 
 <p><em>paper: <em><em>https://arxiv.org/abs/1807.06514v2</em></em><br></em></p> 
 <p><em>github: https://github.com/Jongchan/attention-module</em><br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/f7/e6/iSwEaXfS_o.png" alt="2a84572f3424d40f608c0877bdc07922.png"></p> 
 <p>BAM和CBAM几乎相同时间提出，称为<strong>瓶颈注意力机制</strong>，主要有两个分支，上面那个分支和SENet基本上相同，相当于通道注意力机制；下面的分支为空间注意力，先将特征层通过1*1的卷积将通道压缩到C/r，再经过两个3*3的空洞卷积，做特征融合，通道数不变，最后通过一个1*1的卷积将通道数变成1。再将通道注意力机制与空间注意力机制生成的特征图融合成和输入相同大小的特征图，再与输入做跳层连接。<br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/c1/24/o9q4eUby_o.png" alt="b8bfd5a8f1a6c0d0aae141e2952d54b8.png"></p> 
 <p>scSE</p> 
 <p><em>paper: <em><em>https://arxiv.org/abs/1808.08127</em></em><br></em></p> 
 <p><em>github: https://github.com/pprp/SimpleCVReproduction/tree/master/Plug-and-play%20module/attention/scSE（unofficial）</em></p> 
 <p>这篇文章主要用于语义分割上，对SE模块进行了改进，提出了SE模块的三个变体cSE、sSE、scSE，并通过实验证明了了这样的模块可以增强有意义的特征，抑制无用特征。一开始作者提出了两个模块：<strong>sSE模块</strong>和<strong>cSE模块</strong>，分别为通道注意力和空间注意力：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/a2/76/KkxkjIHn_o.png" alt="76564f768ed3043ff9862328d7636f9a.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/a6/e6/FUcWUCTf_o.png" alt="ac387f618e176a3ca9ea7058abac5a23.png"></p> 
 <p>scSE则是将两者并联起来，有点类似于BAM结构：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/66/4f/eKKofQaj_o.png" alt="98b6a08daeb4812148e16ff80161e2bf.png"></p> 
 <p style="text-align:justify;">Triplet Attention</p> 
 <p><em>paper: <em><em>https://arxiv.org/abs/2010.03045</em></em><br></em></p> 
 <p><em>github: https://github.com/landskape-ai/triplet-attention</em></p> 
 <p>triplet Attention 也是在CNN中加入进spatial attention和channel attention的一个即插即用模块，文章认为CBAM建立Channel Attention和Spatial Attention的操作是分别进行的，而二者之间可能有联系，应该同时建模Channel Attention和Spatial Attention。</p> 
 <p>于是文章中的Triplet其实就是从三个角度去建模attention关系：以输入的feature map大小为(b, c, h, w)为例，围绕channel，height，width可以建模三组关系</p> 
 <ul><li><p>channel to height（c，h）</p></li><li><p>channel to width（c，w）</p></li><li><p>height to width（h，w）</p></li></ul> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/ce/d5/653IfUOB_o.png" alt="81fb2c4b384b60009fdbd0423b3a9834.png"></p> 
 <p>相对其他Attention Module而言，Triplet Attention近乎无参，效果提升不是很明显。</p> 
 <p style="text-align:justify;">Coordinate Attention</p> 
 <p><em>paper: <em><em>https://arxiv.org/abs/2103.02907</em></em><br></em></p> 
 <p><em>github: https://github.com/Andrew-Qibin/CoordAttention</em></p> 
 <p>作者设计了一种新的同时考虑通道关系和位置信息的注意力模块，通过精确的位置信息对通道关系和长程依赖进行编码，类似SE模块，也分为两个步骤：<strong>坐标信息嵌入（coordinate information embedding）</strong>和<strong>坐标注意力生成（coordinate attention generation）</strong>。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/4f/1e/AgdMgwbi_o.png" alt="65219c350edf1443633a2b1029be5cf8.png"></p> 
 <p style="text-align:justify;">DANet</p> 
 <p><em>paper: <em><em><em>https://arxiv.org/abs/1809.02983</em></em></em><br></em></p> 
 <p><em>github: <em>https://github.com/junfu1115/DANet/</em></em></p> 
 <p>作者提出了双重注意网络（<strong>DANet</strong>），它引入了一种自注意力机制来模拟空间注意力和通道注意力，分别捕获空间维度和通道维度中的特征依赖关系，可用于语义分割领域。<br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/37/8a/nyk8WImF_o.png" alt="2a6b7d7020ae2132a511110571d9cb51.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/0b/d3/SoCSHRYB_o.png" alt="516fd990cc1d83a3da4dc8644f0d1de4.png"></p> 
 <p style="text-align:justify;">RGA<br></p> 
 <p><em>paper: <em><em><em>https://arxiv.org/abs/1904.02998</em></em></em><br></em></p> 
 <p><em>github: <em>https://github.com/microsoft/Relation-Aware-Global-Attention-Networks</em></em></p> 
 <p>这是一篇cvpr2020由微软提出的关系感知全局注意力机制，用于捕获全局信息。区别于常用的获取注意力的方法，作者将特征图中的每个位置上的feature看作一个node，通过强调node间的对称关系来挖掘全局范围的相关性和语义信息，提出了即插即用在残差块后的注意力模块<strong>Relation-aware global attention module(RGA)</strong>，该模块将注意力同时应用与spatial及channel上。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/75/13/MapgM2tE_o.png" alt="ebb10d07888695d99f2bc8b516320efc.png"></p> 
 <p>Residual attention</p> 
 <p><em>paper: <em><em><em>https://arxiv.org/abs/1704.06904</em></em></em><br></em></p> 
 <p><em>github: <em>https://github.com/fwang91/residual-attention-network</em></em></p> 
 <p>作者提出了Residual Attention Network，主要通过堆叠多个Attention Modules来构造Residual Attention Network。堆叠结构是混合注意力机制的基本应用。因此，不同类型的注意力能够被不同的注意力模块捕获，而直接叠加注意力模块会导致明显的学习性能下降。因此，提出了attention residual learning mechanism来优化几百层的非常深层的Residual Attention Network。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/83/d9/tcweDmUF_o.png" alt="a40e2f80d32c112f6cbec9b8e0fdf4d6.png"></p> 
 <p>SimAM</p> 
 <p><em>paper: <em><em><em>http://proceedings.mlr.press/v139/yang21o/yang21o.pdf</em></em></em><br></em></p> 
 <p><em>github: <em>https://github.com/ZjjConan/SimAM</em></em></p> 
 <p>作者提出一种概念简单且非常有效的注意力模块。不同于现有的通道/空域注意力模块，该模块无需额外参数为特征图推导出3D注意力权值。同时推导出了能量函数的解析解加速了注意力权值的计算并得到了一种轻量型注意力模块；</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/b0/9f/7vqYfDvf_o.png" alt="67d1347e940897d338da08b71c0ec7ba.png"></p> 
 <p>Strip Pooling</p> 
 <p><em>paper: <em><em><em>https://arxiv.org/pdf/2003.13328.pdf</em></em></em><br></em></p> 
 <p><em>github: </em><em>https://github.com/Andrew-Qibin/SPNet</em><em><em><br></em></em></p> 
 <p>作者介绍一种有效的方法，通过利用条形池化来帮助骨干网络捕获远程上下文。特别是，文中提出了一种新颖的条形池化模块（SPM），该模块利用水平和垂直条形池化操作来收集来自不同空间维度的远程上下文。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/68/54/U6HFBvlv_o.png" alt="18600758b3d6b55b1f2a02e1e71356a1.png"></p> 
 <p>和金字塔池化（MPM）：<br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/e0/0d/1pWwz77w_o.png" alt="f91aeb70c63f68d81cb07908966b1988.png"></p> 
 <p>SCNet</p> 
 <p><em>paper: <em><em><em>http://mftp.mmcheng.net/Papers/20cvprSCNet.pdf</em></em></em><br></em></p> 
 <p><em>github: </em><em>https://github.com/MCG-NKU/SCNet</em></p> 
 <p>在本文中，没有设计复杂的网络体系结构来增强特征表示，而是引入了自校正卷积作为一种有效的方法来帮助卷积网络通过增加每层的基本卷积变换来学习判别表示。类似于分组卷积，它将特定层的卷积核分为多个部分，但不均匀地每个部分中的卷积核以异构方式被利用。具体而言，自校正卷积不是通过均匀地对原始空间中的输入执行所有卷积，而是首先通过下采样将输入转换为低维嵌入, 采用由一个卷积核变换的低维嵌入来校准另一部分中卷积核的卷积变换。得益于这种异构卷积和卷积核间通信，可以有效地扩大每个空间位置的感受野.</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/12/7d/DX22CtpW_o.png" alt="15a6f86c793423804e795e6720fa953c.png"></p> 
 <p><strong>时域和空间注意力机制</strong></p> 
 <p>时域和空间注意力是基于时域注意力和空间注意力机制，综合考虑两者一起提出的注意力机制，也称混合注意力机制，主要用于视频领域，如动作识别或者行人ReID中，主要包括区分时域和空间的注意力：<strong>RSTAN</strong>；联合产生的时域和空间注意力机制：<strong>STA</strong>；基于成对关系的注意力：<strong>STGCN</strong>。</p> 
 <p>RSTAN</p> 
 <p><em>paper: https://ieeexplore.ieee.org/document/8123939</em></p> 
 <p><em>github: </em><em>https://github.com/PancakeAwesome/ran_two_stream_to_recognize_drives（unofficial）</em></p> 
 <p>对视频中的动作识别增加软注意力机制，使用多层循环神经网络 (RNN) 和长短期记忆 (LSTM) 单元。<br></p> 
 <p>STA</p> 
 <p><em>paper: https://ojs.aaai.org//index.php/AAAI/article/view/4841</em></p> 
 <p><em>github: </em><em>https://github.com/justchenhao/STANet（unofficial）</em></p> 
 <p>STA在空间和时间维度上充分利用了一个目标的那些判别部分，从而通过帧间正则化生成一个二维注意力得分矩阵，以测量不同帧中空间部分的重要性。因此，可以根据挖掘的二维注意力得分矩阵引导的加权求和运算生成更稳健的剪辑级特征表示。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/49/ff/jPGeJHbZ_o.png" alt="b92073ac9fec16a3bef4da25457a08cb.png"></p> 
 <p>STGCN</p> 
 <p><em>paper: https://ieeexplore.ieee.org/document/9156954/</em></p> 
 <p><em>github: </em><em>https://github.com/justchenhao/STANet（unofficial）</em></p> 
 <p>该论文将GCN运用到Person ReID上面来，将图像信息分块作为节点信息；作者提出一个新的时空图卷积网络(STGCN)。STGCN包括两个GCN分支：<strong>空间分支、时间分支</strong>。空间分支提取人体的结构信息，时间分支从相邻帧中挖掘判别线索。通过联合优化这些分支，模型提取了与外观信息互补的鲁棒时空信息。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/32/b3/9t4cOM5m_o.png" alt="63c188457cee4065dacfbe56d9f39b2b.png"></p> 
 <blockquote> 
  <p><strong>结语</strong></p> 
 </blockquote> 
 <p>以上为在cv领域中的注意力机制，总体来说，cv中有很多有注意力思想的机制，遍地开花，弱水三千，只取一瓢，如果有一个注意力机制对项目或者任务有所提升，不枉大佬学者们的深深耕耘。</p> 
 <p>希望对大家有帮助。</p> 
 <p>参考：</p> 
 <p>[1] https://arxiv.org/pdf/2111.07624.pdf</p> 
 <p>[2] https://www.zhihu.com/people/ai-hardcore</p> 
 <p>[3] https://blog.csdn.net/z704630835/article/details/96827430</p> 
 <p>[4] https://mp.weixin.qq.com/s/0iOZ45NTK9qSWJQlcI3_kQ</p> 
 <p>[5] https://github.com/MenghaoGuo/Awesome-Vision-Attentions</p> 
 <p>[6] https://arxiv.org/abs/1709.01507</p> 
 <p>[7] Squeeze-and-Excitation Networks</p> 
 <p>[8] Global Second-order Pooling Convolutional Networks</p> 
 <p>[9] FcaNet: Frequency Channel Attention Networks</p> 
 <p>[10] ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks<br></p> 
 <p>[11] SRM : A Style-based Recalibration Module for Convolutional Neural Networks</p> 
 <p>[12] Gated Channel Transformation for Visual Recognition</p> 
 <p>[13] Recurrent Models of Visual Attention</p> 
 <p>[14] DRAW: A Recurrent Neural Network For Image Generation</p> 
 <p>[15] MULTIPLE OBJECT RECOGNITION WITH VISUAL ATTENTION</p> 
 <p>[16] Spatial Transformer Networks</p> 
 <p>[17] Deformable Convolutional Networks</p> 
 <p>[18] Deformable ConvNets v2: More Deformable, Better Results</p> 
 <p>[19] Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks</p> 
 <p>[20] Non-local Neural Networks</p> 
 <p>[21] CCNet: Criss-Cross Attention for Semantic Segmentation</p> 
 <p>[22] Expectation-Maximization Attention Networks for Semantic Segmentation</p> 
 <p>[23] Stand-Alone Self-Attention in Vision Models</p> 
 <p>[24] Exploring Self-attention for Image Recognition</p> 
 <p>[25] AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</p> 
 <p>[26] End-to-End Object Detection with Transformers</p> 
 <p>[27] TAM: Temporal Adaptive Module for Video Recognition</p> 
 <p>[28] Global-Local Temporal Representations For Video Person Re-Identification</p> 
 <p>[29] Training Very Deep Networks</p> 
 <p>[30] Selective Kernel Network</p> 
 <p>[31] ResNeSt: Split-Attention Networks</p> 
 <p>[32] CondConv: Conditionally Parameterized Convolutions for Efficient Inference</p> 
 <p>[33] Dynamic Convolution: Attention over Convolution Kernels</p> 
 <p>[34] CBAM: Convolutional Block Attention Module</p> 
 <p>[35] BAM: Bottleneck Attention Module</p> 
 <p>[36] Recalibrating Fully Convolutional Networks with Spatial and Channel 'Squeeze &amp; Excitation' Blocks</p> 
 <p>[37] Rotate to Attend: Convolutional Triplet Attention Module</p> 
 <p>[38] Coordinate Attention for Efficient Mobile Network Design</p> 
 <p>[39] Dual Attention Network for Scene Segmentation</p> 
 <p>[40] Relation-Aware Global Attention for Person Re-identification</p> 
 <p>[41] Residual Attention Network for Image Classification</p> 
 <p>[42] SimAM: A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</p> 
 <p>[43] Strip Pooling: Rethinking Spatial Pooling for Scene Parsing</p> 
 <p>[44] Improving Convolutional Networks with Self-Calibrated Convolutions</p> 
 <p>[45] Recurrent Spatial-Temporal Attention Network for Action Recognition in Videos</p> 
 <p>[46] STA: Spatial-Temporal Attention for Large-Scale Video-Based Person Re-Identification</p> 
 <p>[47] Spatial-Temporal Graph Convolutional Network for Video-Based Person Re-Identification</p> 
 <p style="text-align:center;">【<strong>自动驾驶之心</strong>】全栈技术交流群</p> 
 <p><strong>自动驾驶之心是首个自动驾驶开发者社区，聚焦目标检测、语义分割、全景分割、实例分割、关键点检测、车道线、目标跟踪、3D目标检测、BEV感知、多传感器融合、SLAM、光流估计、深度估计、轨迹预测、高精地图、规划控制、模型部署落地、自动驾驶仿真测试、硬件配置、AI求职交流等方向；</strong></p> 
 <p><strong>加入我们：</strong><a href="" rel="nofollow">自动驾驶之心技术交流群汇总！</a></p> 
 <p style="text-align:center;">自动驾驶之心【知识星球】</p> 
 <p>想要了解更多自动驾驶感知（分类、检测、分割、关键点、车道线、3D目标检测、多传感器融合、目标跟踪、光流估计、轨迹预测）、自动驾驶定位建图（SLAM、高精地图）、自动驾驶规划控制、领域技术方案、AI模型部署落地实战、行业动态、岗位发布，欢迎扫描下方二维码，加入自动驾驶之心知识星球（三天内无条件退款），日常分享论文+代码，这里汇聚行业和学术界大佬，前沿技术方向尽在掌握中，期待交流！</p> 
 <p style="text-align:left;"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/d9/cc/HUjIS6Nt_o.png" alt="8ba00964a2b02c37530b190ae1c61949.jpeg"></p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/bf24628edf1f2cd2306468407a8d6125/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【python数据处理】针对列表元素进行计数的多种方法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/36458292215df088efb68f0319d768f1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">unity — 将海康SDK嵌入unity，发布后不能播放视频</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>