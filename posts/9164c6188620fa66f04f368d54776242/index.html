<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Hadoop笔记 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Hadoop笔记" />
<meta property="og:description" content="Hadoop概念 Hadoop是一个软件，这个软件包含三个模块 HDFS: Hadoop分布式文件系统MapReduce:分布式计算系统Yarn:分布式资源调度系统 资源网址 apache的资源包下载 https://archive.apache.org/dist/ Hadoop-3.1.4官网 https://hadoop.apache.org/docs/r3.1.4/ HDFS文件系统 概述 HDFS的全称：Hadoop分布式文件系统HDFS合适存储大容量数据，合适存储大文件HDFS可以将一些廉价的计算机进行整合，形成一个完整的存储系统，并且对外提供统一的访问路径 特性 HDFS在存储数据时将文件进行切分，切分成多个block（128=134217728字节），每一个block会有多个副本（默认3个副本）HDFS在读取文件的时候，不能保证实时性，HDFS在存储大体量数据时，速度慢，时间长，一般适合一次写入多次读取,所以不适合做网盘HDFS只适合存大文件，不适合存小文件，因为每在HDFS上存储一个文件，namenode的内存就会记录一条元数据，每条元数据大概150字节，小文件过多，元数据过多，则会大量占用namenode内存HDFS不支持文件的随机修改，只支持文件的追加写入HDFS的存储可以近乎无限扩展 HDFS的架构 HDFS是主从架构，主节点是namenode，从节点是datanode
![image.png](https://img-blog.csdnimg.cn/img_convert/48c4ea7411ad75879d6233821e3d84dd.png#averageHue=#fbfbfb&amp;clientId=u7f919bba-9188-4&amp;from=paste&amp;height=212&amp;id=uacd450c9&amp;originHeight=212&amp;originWidth=465&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=29806&amp;status=done&amp;style=none&amp;taskId=ud652cce0-3c84-4faf-b259-277086da4fb&amp;title=&amp;width=465) HDFS角色的功能
NameNode 保存整个HDFS集群的元数据 NameNode需要知道每一个DataNode上block的信息客户端在上传或者下载文件时，需要从NameNode设置或者获取元数据信息NameNode的元数据信息是保存在内存中，但是会定时保存到硬盘（Secondary NameNode） DataNode 保存具体文件数据要定时与NameNode之间发送心跳包要定时向NameNode汇报Block信息客户端要下载或者上传文件时，具体的文件操作是和DataNode进行交互 Seconday NameNode 辅助NameNode进行元数据管理(元数据持久化存储，保存到硬盘) Client Client负责上传文件和下载文件的发起工作Client在上传文件时会对文件进行切片 HDFS的切片机制 HDFS的BLOCK只是一个逻辑单位
假如BLOCK的大小设置为128M，意思是这个BLOCK最大是128M
BLOCK的大小可以通过：hdfs-site.xml中的dfs.blocksize参数来进行设置
HDFS的副本机制 HDFS的每个BLOCK都会有多个副本，默认是3个
HDFS的副本数可以通过hdfs-site.xml中的dfs.replication参数来进行设置
HDFS的NameSpace HDFS会给每一个存储的文件提供一个统一的访问路径 #格式 hdfs://namenode:port/dir-a/dir-b/dir-c/file.data。 #使用1-使用绝对前缀方式 hdfs://node1:8020/dir/a.txt hadoop fs -put a.txt hdfs://node1:8020/dir #使用1-使用相对前缀方式 /dir/a.txt hadoop fs -put a.txt /dir HDFS的元数据 在HDFS中，Namenode管理的元数据具有两种类型： 1、文件自身属性信息 文件名称、权限，修改时间，文件大小，副本数，数据块大小。 2、文件块位置映射信息 记录文件块和DataNode之间的映射信息，即哪个块位于哪个节点上。 HDFS的机架感知 第一个BLOCK副本会存储在离客户端最近的一台主机上，如果客户端就是集群中的主机，则直接存在客户端所在主机，如果Client不在集群范围内或者不在同一个子网，则会在集群中随机选一个机架，在该机架中随机选一个健康（心跳正常，硬盘容量正常）的主机，将这个BLOCK存入第二个BLOCK副本会存入另外一个机架（随机选择），会在该机架上随机选一台健康的主机，将数据存入第三个BLOCK副本会在第二个BLOCK副本的机架上随机选择另外一台健康主机，将BLOCK数据存入 HDFS目录规划 目录说明/source用于存储原始采集数据/common用于存储公共数据集，例如：IP库、省份信息、经纬度等/workspace工作空间，存储各团队计算出来的结果数据/tmp存储临时数据，每周清理一次/warehouse/warehouse存储hive数据仓库中的数据 HDFS集群命令 # 一键操作命令 stop-dfs.sh start-dfs.sh HDFS的shell命令 #格式 hadoop fs -命令 参数 #该命令可以操作任何文件系统 hdfs dfs -命令 #该命令只能操作HDFS文件系统 #文件的上传 hadoop fs -put a." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/9164c6188620fa66f04f368d54776242/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-21T13:51:09+08:00" />
<meta property="article:modified_time" content="2023-07-21T13:51:09+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Hadoop笔记</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h4><a id="Hadoop_0"></a>Hadoop概念</h4> 
<ul><li>Hadoop是一个软件，这个软件包含三个模块 
  <ul><li><strong>HDFS: Hadoop分布式文件系统</strong></li><li>MapReduce:分布式计算系统</li><li>Yarn:分布式资源调度系统</li></ul> </li></ul> 
<h3><a id="_7"></a>资源网址</h3> 
<ul><li>apache的资源包下载</li></ul> 
<pre><code class="prism language-shell">https://archive.apache.org/dist/
</code></pre> 
<ul><li>Hadoop-3.1.4官网</li></ul> 
<pre><code class="prism language-shell">https://hadoop.apache.org/docs/r3.1.4/
</code></pre> 
<h3><a id="HDFS_19"></a>HDFS文件系统</h3> 
<h4><a id="_20"></a>概述</h4> 
<ul><li>HDFS的全称：Hadoop分布式文件系统</li><li>HDFS合适存储大容量数据，合适存储大文件</li><li>HDFS可以将一些廉价的计算机进行整合，形成一个完整的存储系统，并且对外提供统一的访问路径</li></ul> 
<h4><a id="_25"></a>特性</h4> 
<ul><li>HDFS在存储数据时将文件进行切分，切分成多个block（128=134217728字节），每一个block会有多个副本（默认3个副本）</li><li>HDFS在读取文件的时候，不能保证实时性，HDFS在存储大体量数据时，速度慢，时间长，一般适合一次写入多次读取,所以不适合做网盘</li><li>HDFS只适合存大文件，不适合存小文件，因为每在HDFS上存储一个文件，namenode的内存就会记录一条元数据，每条元数据大概150字节，小文件过多，元数据过多，则会大量占用namenode内存</li><li>HDFS不支持文件的随机修改，只支持文件的追加写入</li><li>HDFS的存储可以近乎无限扩展</li></ul> 
<h4><a id="HDFS_33"></a>HDFS的架构</h4> 
<ul><li> <p>HDFS是主从架构，主节点是namenode，从节点是datanode</p> <pre><code> ![image.png](https://img-blog.csdnimg.cn/img_convert/48c4ea7411ad75879d6233821e3d84dd.png#averageHue=#fbfbfb&amp;clientId=u7f919bba-9188-4&amp;from=paste&amp;height=212&amp;id=uacd450c9&amp;originHeight=212&amp;originWidth=465&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=29806&amp;status=done&amp;style=none&amp;taskId=ud652cce0-3c84-4faf-b259-277086da4fb&amp;title=&amp;width=465)
</code></pre> </li><li> <p>HDFS角色的功能</p> 
  <ul><li>NameNode 
    <ul><li>保存整个HDFS集群的元数据 
      <ul><li>NameNode需要知道每一个DataNode上block的信息</li><li>客户端在上传或者下载文件时，需要从NameNode设置或者获取元数据信息</li><li>NameNode的元数据信息是保存在内存中，但是会定时保存到硬盘（Secondary NameNode）</li></ul> </li></ul> </li><li>DataNode 
    <ul><li>保存具体文件数据</li><li>要定时与NameNode之间发送心跳包</li><li>要定时向NameNode汇报Block信息</li><li>客户端要下载或者上传文件时，具体的文件操作是和DataNode进行交互</li></ul> </li><li>Seconday NameNode 
    <ul><li>辅助NameNode进行元数据管理(元数据持久化存储，保存到硬盘)</li></ul> </li><li>Client 
    <ul><li>Client负责上传文件和下载文件的发起工作</li><li>Client在上传文件时会对文件进行切片</li></ul> </li></ul> </li></ul> 
<h4><a id="HDFS_55"></a>HDFS的切片机制</h4> 
<ul><li> <p>HDFS的BLOCK只是一个逻辑单位</p> </li><li> <p>假如BLOCK的大小设置为128M，意思是这个BLOCK最大是128M</p> </li><li> <p>BLOCK的大小可以通过：hdfs-site.xml中的dfs.blocksize参数来进行设置</p> <p><img src="https://images2.imgbox.com/f7/20/CaCp0G64_o.png" alt="image.png"></p> </li></ul> 
<h4><a id="HDFS_62"></a>HDFS的副本机制</h4> 
<ul><li> <p>HDFS的每个BLOCK都会有多个副本，默认是3个</p> </li><li> <p>HDFS的副本数可以通过hdfs-site.xml中的dfs.replication参数来进行设置</p> <p><img src="https://images2.imgbox.com/bf/07/zoSov2Wt_o.png" alt="image.png"></p> </li></ul> 
<h4><a id="HDFSNameSpace_68"></a>HDFS的NameSpace</h4> 
<ul><li>HDFS会给每一个存储的文件提供一个统一的访问路径</li></ul> 
<pre><code class="prism language-shell"><span class="token comment">#格式</span>
hdfs://namenode:port/dir-a/dir-b/dir-c/file.data。
<span class="token comment">#使用1-使用绝对前缀方式</span>
hdfs://node1:8020/dir/a.txt
hadoop  fs <span class="token parameter variable">-put</span> a.txt hdfs://node1:8020/dir
<span class="token comment">#使用1-使用相对前缀方式</span>
/dir/a.txt
hadoop  fs <span class="token parameter variable">-put</span> a.txt /dir
</code></pre> 
<h4><a id="HDFS_81"></a>HDFS的元数据</h4> 
<pre><code class="prism language-shell">在HDFS中，Namenode管理的元数据具有两种类型：
<span class="token number">1</span>、文件自身属性信息
文件名称、权限，修改时间，文件大小，副本数，数据块大小。
<span class="token number">2</span>、文件块位置映射信息
记录文件块和DataNode之间的映射信息，即哪个块位于哪个节点上。
</code></pre> 
<h4><a id="HDFS_89"></a>HDFS的机架感知</h4> 
<ul><li>第一个BLOCK副本会存储在离客户端最近的一台主机上，如果客户端就是集群中的主机，则直接存在客户端所在主机，如果Client不在集群范围内或者不在同一个子网，则会在集群中随机选一个机架，在该机架中随机选一个健康（心跳正常，硬盘容量正常）的主机，将这个BLOCK存入</li><li>第二个BLOCK副本会存入另外一个机架（随机选择），会在该机架上随机选一台健康的主机，将数据存入</li><li>第三个BLOCK副本会在第二个BLOCK副本的机架上随机选择另外一台健康主机，将BLOCK数据存入</li></ul> 
<h4><a id="HDFS_94"></a>HDFS目录规划</h4> 
<table><thead><tr><th>目录</th><th>说明</th></tr></thead><tbody><tr><td>/source</td><td>用于存储原始采集数据</td></tr><tr><td>/common</td><td>用于存储公共数据集，例如：IP库、省份信息、经纬度等</td></tr><tr><td>/workspace</td><td>工作空间，存储各团队计算出来的结果数据</td></tr><tr><td>/tmp</td><td>存储临时数据，每周清理一次</td></tr><tr><td>/warehouse</td><td>/warehouse存储hive数据仓库中的数据</td></tr></tbody></table> 
<h4><a id="HDFS_104"></a>HDFS集群命令</h4> 
<pre><code class="prism language-shell"><span class="token comment"># 一键操作命令</span>
stop-dfs.sh
start-dfs.sh
</code></pre> 
<h4><a id="HDFSshell_110"></a>HDFS的shell命令</h4> 
<pre><code class="prism language-shell"><span class="token comment">#格式</span>
hadoop fs -命令  参数    <span class="token comment">#该命令可以操作任何文件系统</span>
hdfs dfs  -命令         <span class="token comment">#该命令只能操作HDFS文件系统</span>

<span class="token comment">#文件的上传</span>
hadoop fs <span class="token parameter variable">-put</span> a.txt /dir
hdfs  dfs <span class="token parameter variable">-put</span>  a.txt /dir
<span class="token comment"># 上传后并删除本地文件</span>
hadoop fs <span class="token parameter variable">-moveFromLocal</span> a.txt /dir
hdfs  dfs <span class="token parameter variable">-moveFromLocal</span>  a.txt /dir

<span class="token comment">#文件的下载 -get 目标路径  下载路径</span>
hadoop fs <span class="token parameter variable">-get</span> /dir/a.txt /root  
hdfs dfs  <span class="token parameter variable">-get</span> /dir/a.txt /root

<span class="token comment"># 查看命令</span>
hadoop fs <span class="token parameter variable">-cat</span> /dir/a.txt  
hdfs dfs  <span class="token parameter variable">-cat</span> /dir/a.txt 
<span class="token comment"># 查看文件开头1kb数据</span>
hadoop fs <span class="token parameter variable">-head</span> /dir/a.txt  
hdfs dfs  <span class="token parameter variable">-head</span> /dir/a.txt 
<span class="token comment"># 查看文件结尾1kb数据</span>
hadoop fs <span class="token parameter variable">-tail</span> /dir/a.txt  
hdfs dfs  <span class="token parameter variable">-tail</span> /dir/a.txt 

<span class="token comment">#创建文件夹 -单级</span>
hadoop fs <span class="token parameter variable">-mkdir</span> /dir2
hdfs dfs  <span class="token parameter variable">-mkdir</span> /dir2

<span class="token comment">#创建文件夹-多级</span>
hadoop fs <span class="token parameter variable">-mkdir</span> <span class="token parameter variable">-p</span>   /aaa/bbb/ccc
hdfs dfs <span class="token parameter variable">-mkdir</span> <span class="token parameter variable">-p</span> /aaa/bbb/ccc

<span class="token comment">#删除文件</span>
hadoop fs <span class="token parameter variable">-rm</span> /a.txt
hdfs dfs <span class="token parameter variable">-rm</span> /a.txt

<span class="token comment">#删除文件夹</span>
hadoop fs <span class="token parameter variable">-rm</span> <span class="token parameter variable">-r</span> /a.txt
hdfs dfs  <span class="token parameter variable">-rm</span> <span class="token parameter variable">-r</span> /a.txt


<span class="token comment">#在HDFS上进行文件的复制</span>
hadoop fs <span class="token parameter variable">-cp</span> /dir/1.txt /  <span class="token comment">#这里的两个路径都是HDFS路径</span>
hdfs dfs <span class="token parameter variable">-cp</span> /dir/1.txt /  <span class="token comment">#这里的两个路径都是HDFS路径</span>

<span class="token comment">#在HDFS上进行文件的移动（剪切）</span>
hadoop fs <span class="token parameter variable">-mv</span> /dir/1.txt /    <span class="token comment">#这里的两个路径都是HDFS路径</span>
hadoop fs <span class="token parameter variable">-mv</span> /dir/1.txt /    <span class="token comment">#这里的两个路径都是HDFS路径</span>

<span class="token comment"># 追加文件</span>
hadoop fs <span class="token parameter variable">-appendToFile</span> /file  /dir    <span class="token comment">#追加文件路径， 追加路径</span>
hadoop fs <span class="token parameter variable">-appendToFile</span> /file  /dir   

<span class="token comment">#调整HDFS文件副本的数量</span>
hdfs dfs <span class="token parameter variable">-setrep</span> <span class="token parameter variable">-w</span> <span class="token number">2</span> /1.txt

<span class="token comment"># 查看edits_log 和 dfsimage路径, 进入里面的current</span>
hdfs getconf <span class="token parameter variable">-confKey</span> dfs.namenode.name.dir

</code></pre> 
<h4><a id="HDFS_174"></a>HDFS的安全模式</h4> 
<h5><a id="_175"></a>概述</h5> 
<p>安全模式是hadoop的一种<strong>保护机制</strong>，用于保证集群中的数据块的安全性。当集群启动的时候，会首先进入安全模式。当系统处于安全模式时会检查数据块的完整性。<br> 在安全模式下，HDFS主要做两件事情</p> 
<ul><li>DataNode会将自己的Block信息汇报给NameNode</li><li>NameNode会检查副本率（实际的副本数/理论的副本数）是否达到 0.9990 ,如果没有达到，则会进行副本的动态调整，如果副本率满足了需求之后，则默认在20多秒之后自动关闭安全模式</li></ul> 
<h5><a id="_181"></a>特点</h5> 
<p>在安全模式状态下，文件系统只接受读数据请求，而不接受删除、修改等变更请求。在当整个系统达到安全标准时，HDFS自动离开安全模式。</p> 
<h6><a id="_183"></a>操作命令</h6> 
<pre><code class="prism language-shell">hdfs  dfsadmin <span class="token parameter variable">-safemode</span>  get <span class="token comment">#查看安全模式状态</span>
hdfs  dfsadmin <span class="token parameter variable">-safemode</span>  enter <span class="token comment">#进入安全模式</span>
hdfs  dfsadmin <span class="token parameter variable">-safemode</span>  leave <span class="token comment">#离开安全模式</span>
</code></pre> 
<h4><a id="HDFS_189"></a>HDFS的读写流程</h4> 
<h5><a id="_190"></a>写流程</h5> 
<p><img src="https://images2.imgbox.com/c6/fd/PWhSxeyW_o.png" alt=""></p> 
<h5><a id="_192"></a>读流程</h5> 
<p><img src="https://images2.imgbox.com/b7/ee/qmfNhYXF_o.png" alt=""></p> 
<h4><a id="HDFS_194"></a>HDFS元数据的辅助管理</h4> 
<h5><a id="_195"></a>概述</h5> 
<ul><li>NameNode在工作的时候元数据在存放在内存中，保证访问速度最优</li><li>NamNode内存中的元数据容易发生掉电丢失，所以必须持久化存储到硬盘上</li><li>由于NameNode要接收客户端的各种请求，所以为了减轻NameNode压力，持久化元数据的任务就交给了SecondaryNameNode</li></ul> 
<h5><a id="_200"></a>持久化文件</h5> 
<ul><li>fsimage镜像文件 
  <ul><li>镜像文件，保存了所有过去NameNode的元数据</li></ul> </li><li>edits日志文件 
  <ul><li>保存了最近一段时间的元数据操作日志</li></ul> </li></ul> 
<p>NameNode完整的元数据 = fsimage文件 + edits日志文件</p> 
<h5><a id="SecondaryNameNode_208"></a>SecondaryNameNode</h5> 
<h6><a id="_209"></a>概述</h6> 
<p>SecondaryNameNode主要是用来合并NameNode的edits logs到fsimage文件中</p> 
<h6><a id="SNN_checkpoint_212"></a>SNN checkpoint机制</h6> 
<h6><a id="_213"></a>概述：</h6> 
<p>checkpoint就是将fsimage和edits log 合并生成新的fsimage，解决以下问题</p> 
<ul><li>edits logs会变的很大，fsimage将会变得很旧；</li><li>namenode重启会花费很长时间，因为有很多改动要合并到fsimage文件上；</li><li>如果频繁进行fsimage持久化，又会影响NameNode正常服务，毕竟IO操作是一种内存到磁盘的耗精力操作</li></ul> 
<h6><a id="_219"></a>流程</h6> 
<p><img src="https://images2.imgbox.com/7a/aa/aMBPN5XK_o.png" alt=""><br> 1、触发checkpoint操作时， SNN会发请求给NN，NN收到后，会生成一个edits new，用于记录后续操作记录<br> 2、SNN会将edits 和 fsimage复制到本地，进行加载到内存，并执行edits文件，执行结束后，会将内存中的数据dump成一个新的fsimage文件，并将它传给NN, 替换原来的fsimages ,edits new也会改回edits</p> 
<h6><a id="_223"></a>触发机制</h6> 
<p>通过core-site.xml配置这两个触发参数</p> 
<pre><code class="prism language-shell"><span class="token assign-left variable">dfs.namenode.checkpoint.period</span><span class="token operator">=</span><span class="token number">3600</span>  //两次连续的checkpoint之间的时间间隔。默认1小时
<span class="token assign-left variable">dfs.namenode.checkpoint.txns</span><span class="token operator">=</span><span class="token number">1000000</span> //最大没有执行checkpoint事务的数量，满足将强制执行紧急checkpoint，即使尚未达到检查点周期。默认100万事务数量。
</code></pre> 
<h5><a id="NameNode_230"></a>NameNode元数据恢复</h5> 
<p>主要有两种方式：</p> 
<ol><li>通过dfs.namenode.name.dir属性配置多个目录，相当于做一个备份，可以恢复到最新的</li><li>通过SecondaryNameNode中的fsimage恢复，无法完全恢复到最新</li></ol> 
<h4><a id="HDFS_235"></a>HDFS的远程拷贝命令</h4> 
<h5><a id="_236"></a>集群内主机之间拷贝</h5> 
<pre><code class="prism language-python"><span class="token comment">#方式1 指定用户名，命令执行后，需要输入密码 -r表示拷贝是的目录</span>
scp <span class="token operator">-</span>r local_folder remote_username@remote_ip<span class="token punctuation">:</span>remote_folder 
<span class="token comment"># 简化版写法，默认以root用户身份访问</span>
scp <span class="token operator">-</span>r local_folder remote_ip<span class="token punctuation">:</span>remote_folder 

<span class="token comment">#推送</span>
scp <span class="token operator">-</span>r <span class="token operator">/</span>root<span class="token operator">/</span><span class="token builtin">dir</span>  root@node2<span class="token punctuation">:</span><span class="token operator">/</span>root  <span class="token comment">#指定远程访问的用户身份</span>
scp <span class="token operator">-</span>r <span class="token operator">/</span>root<span class="token operator">/</span><span class="token builtin">dir</span>   node2<span class="token punctuation">:</span><span class="token operator">/</span>root      <span class="token comment">#简化写法：默认以root用户身份访问</span>

<span class="token comment">#拉取</span>
scp <span class="token operator">-</span>r root@node1<span class="token punctuation">:</span><span class="token operator">/</span>root<span class="token operator">/</span><span class="token builtin">dir</span> <span class="token operator">/</span>root  <span class="token comment">#把远端的文件拉取到本地</span>
</code></pre> 
<h5><a id="_250"></a>跨集群的数据拷贝</h5> 
<pre><code class="prism language-shell">hadoop distcp hdfs://node1:8020/jdk-8u241-linux-x64.tar.gz  hdfs://node10:8020/
</code></pre> 
<pre><code class="prism language-shell">$ hadoop distcp
usage: distcp OPTIONS <span class="token punctuation">[</span>source_path<span class="token punctuation">..</span>.<span class="token punctuation">]</span> <span class="token operator">&lt;</span>target_path<span class="token operator">&gt;</span>
             
 <span class="token parameter variable">-append</span>                //拷贝文件时支持对现有文件进行追加写操作
 <span class="token parameter variable">-async</span>                	//异步执行distcp拷贝任务
 <span class="token parameter variable">-bandwidth</span> <span class="token operator">&lt;</span>arg<span class="token operator">&gt;</span>      	//对每个Map任务的带宽限速
 <span class="token parameter variable">-delete</span>               	//删除相对于源端,目标端多出来的文件
 <span class="token parameter variable">-diff</span> <span class="token operator">&lt;</span>arg<span class="token operator">&gt;</span>           	//通过快照diff信息进行数据的同步                  
 <span class="token parameter variable">-overwrite</span>            	//以覆盖的方式进行拷贝，如果目标端文件已经存在，则直接覆盖
 <span class="token parameter variable">-p</span> <span class="token operator">&lt;</span>arg<span class="token operator">&gt;</span>              	//拷贝数据时,扩展属性信息的保留，包括权限信息、块大小信息等等
 <span class="token parameter variable">-skipcrccheck</span>          //拷贝数据时是否跳过cheacksum的校验
 <span class="token parameter variable">-update</span>               	//拷贝数据时,只拷贝相对于源端 ，目标端不存在的文件数据
</code></pre> 
<h4><a id="HDFS__268"></a>HDFS 回收机制</h4> 
<p>Trash机制，类似回收站和垃圾桶，默认情况是不开启的，保留时间可以自行配置，目录：/user/<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          u 
         
        
          s 
         
        
          e 
         
        
          r 
         
        
          n 
         
        
          a 
         
        
          m 
         
        
          e 
         
        
       
         / 
        
       
         . 
        
       
         T 
        
       
         r 
        
       
         a 
        
       
         s 
        
       
         h 
        
       
         / 
        
       
         c 
        
       
         u 
        
       
         r 
        
       
         r 
        
       
         e 
        
       
         n 
        
       
         t 
        
       
         在可配置的时间间隔内， 
        
       
         H 
        
       
         D 
        
       
         F 
        
       
         S 
        
       
         会为 
        
       
         ∗ 
        
       
         ∗ 
        
       
         C 
        
       
         u 
        
       
         r 
        
       
         r 
        
       
         e 
        
       
         n 
        
       
         t 
        
       
         ∗ 
        
       
         ∗ 
        
       
         目录下的文件创建一个以日期命名的检查点目录，即 
        
       
         ∗ 
        
       
         ∗ 
        
       
         / 
        
       
         u 
        
       
         s 
        
       
         e 
        
       
         r 
        
       
         / 
        
       
      
        {username}/.Trash/current 在可配置的时间间隔内，HDFS会为**Current**目录下的文件创建一个以日期命名的检查点目录，即**/user/ 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathnormal">u</span><span class="mord mathnormal" style="margin-right: 0.0278em;">ser</span><span class="mord mathnormal">nam</span><span class="mord mathnormal">e</span></span><span class="mord">/.</span><span class="mord mathnormal" style="margin-right: 0.1389em;">T</span><span class="mord mathnormal" style="margin-right: 0.0278em;">r</span><span class="mord mathnormal">a</span><span class="mord mathnormal">s</span><span class="mord mathnormal">h</span><span class="mord">/</span><span class="mord mathnormal">c</span><span class="mord mathnormal">u</span><span class="mord mathnormal">rre</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord cjk_fallback">在可配置的时间间隔内，</span><span class="mord mathnormal" style="margin-right: 0.0278em;">HD</span><span class="mord mathnormal" style="margin-right: 0.0576em;">FS</span><span class="mord cjk_fallback">会为</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord">∗</span><span class="mord mathnormal" style="margin-right: 0.0715em;">C</span><span class="mord mathnormal">u</span><span class="mord mathnormal">rre</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord">∗</span><span class="mord cjk_fallback">目录下的文件创建一个以日期命名的检查点目录，即</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">∗</span><span class="mord">/</span><span class="mord mathnormal">u</span><span class="mord mathnormal" style="margin-right: 0.0278em;">ser</span><span class="mord">/</span></span></span></span></span>{username}/.Trash/**。过期的检查点会被删除。<br> <strong>相关配置属性，在core-site.xml文件:</strong></p> 
<pre><code class="prism language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>  
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>fs.trash.interval<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>  
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>1440<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>  
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>  
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>  
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>fs.trash.checkpoint.interval<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>  
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>  
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> 
<p>fs.trash.interval：分钟数，当超过这个分钟数后检查点会被删除。如果为零，Trash回收站功能将被禁用。fs.trash.checkpoint.interval：检查点创建的时间间隔(单位为分钟)。其值应该小于或等于fs.trash.interval。如果为零，则将该值设置为fs.trash.interval的值。每次运行检查点时，它都会从当前版本中创建一个新的检查点，并删除在数分钟之前创建的检查点。</p> 
<h5><a id="Trash_283"></a>Trash相关命令</h5> 
<pre><code class="prism language-shell"><span class="token comment"># 删除文件跳过Trash， 只需要加个 -skipTrash参数</span>
hadoop fs <span class="token parameter variable">-rm</span> <span class="token parameter variable">-skipTrash</span> /root/a.txt
<span class="token comment"># 恢复</span>
hadoop fs <span class="token parameter variable">-mv</span> /user/root/.Trash/Current/xx/* /xx/
<span class="token comment"># 清空</span>
hadoop fs <span class="token parameter variable">-expunge</span>

</code></pre> 
<h4><a id="HDFS_293"></a>HDFS小文件解决方案</h4> 
<pre><code class="prism language-shell"><span class="token comment">#---------在上传到HDFS之前，将小文件合并---------------------</span>
<span class="token comment">#方式1</span>
hadoop fs <span class="token parameter variable">-appendToFile</span>  *.txt  /big.txt

<span class="token comment">#方式2</span>
    FileInputStream inputStream <span class="token operator">=</span> new FileInputStream<span class="token punctuation">(</span>file1<span class="token punctuation">)</span><span class="token punctuation">;</span> //根据File对象获取输入流
    IOUtils.copy<span class="token punctuation">(</span>inputStream,outputStream<span class="token punctuation">)</span><span class="token punctuation">;</span>
    
<span class="token comment">#---------在上传到HDFS之后，将小文件合并---------------------</span>
<span class="token comment">#方式3</span>
 Archive档案,将小文件打包形成一个归档文件
</code></pre> 
<h4><a id="HDFSArchive_307"></a>HDFS的Archive(档案）</h4> 
<h5><a id="_308"></a>概念</h5> 
<p>HDFS并不擅长存储小文件，因为每个文件最少一个block，每个block的元数据都会在NameNode占用内存，如果存在大量的小文件，它们会吃掉NameNode节点的大量内存。Hadoop Archives可以有效的处理以上问题，它可以把多个文件归档成为一个文件，归档成一个文件后还可以透明的访问每一个文件。</p> 
<h5><a id="_310"></a>注意事项</h5> 
<ul><li>Archive可以理解为将HDFS上已经存在的小文件进行"打包"，打包成一个文件，只占一条元数据，后缀是*.har</li><li>Archive在打包之后可以透明的访问其中的每一个小文件</li><li>Archive归档之后，原来的小文件不会被删除</li><li>Archive归档之后，会生成一个文件夹（test.har）,该文件夹下有四个文件</li><li>将归档文件之后，原来的归档文件不会自动删除，需要手动删除</li><li>归档文件只是打包，并没有压缩，不会减少空间的占用</li><li>当原来的小文件内容发生改变，则归档文件不会跟着改变，除非创建新的归档文件</li><li>归档的过程实际上是执行MapReduce任务</li></ul> 
<h5><a id="_320"></a>操作</h5> 
<pre><code class="prism language-shell"><span class="token comment">#1、创建Archive文件</span>
<span class="token comment">#将/config目录下的所有文件进行打包归档，归档之后的包命名为test.har，将打包后的归档文件放在/outputdir</span>
hadoop fs <span class="token parameter variable">-mkdir</span> /config
<span class="token builtin class-name">cd</span> /export/server/hadoop-3.1.4/etc/hadoop/
hadoop fs <span class="token parameter variable">-put</span>  ./* /config   

hadoop archive <span class="token parameter variable">-archiveName</span> test.har <span class="token parameter variable">-p</span> /config  /outputdir

<span class="token comment">#2、查看归档后的part-0文件</span>
<span class="token comment">#将所有小文件的内容都显示到终端上（没有意义）</span>
hadoop fs <span class="token parameter variable">-cat</span> /outputdir/test.har/part-0



<span class="token comment">## 注意：har访问只显示原文件，索引标识都隐藏起来。</span>
har://scheme-hostname:port/archivepath/
<span class="token comment">## 如果没有提供scheme-hostname，那么会使用默认文件系统</span>
har:///archivepath/
<span class="token comment">#3、查看归档包中有哪些小文件（显示小文件名）</span>
hadoop fs <span class="token parameter variable">-ls</span> har://hdfs-node1:8020/outputdir/test.har

<span class="token comment">#4、单独显示或者操作其中的某个小文件</span>
hadoop fs <span class="token parameter variable">-cat</span>  har://hdfs-node1:8020/outputdir/test.har/capacity-scheduler.xml
hadoop fs <span class="token parameter variable">-cat</span>  har:///outputdir/test.har/capacity-scheduler.xml

<span class="token comment">#5、将归档包中的小文件还原,</span>
 <span class="token function">mkdir</span> /config2
 hadoop fs <span class="token parameter variable">-cp</span> har:///outputdir/test.har/* /config2
</code></pre> 
<h5><a id="SnapShot_351"></a>SnapShot快照</h5> 
<h5><a id="_352"></a>概念</h5> 
<ul><li>HDFS的快照就是将HDFS系统的某一个目录或者某一个文件一个时间点的状态和内容保存下来</li><li>如果以后源数据发生了误删除或者丢失、错乱，则可以使用快照来恢复原来的数据</li></ul> 
<h5><a id="_356"></a>操作</h5> 
<pre><code class="prism language-shell"><span class="token comment">#1、开启指定目录的快照功能</span>
hdfs dfsadmin <span class="token parameter variable">-allowSnapshot</span> /config

<span class="token comment">#2、关闭指定目录的快照功能</span>
hdfs dfsadmin  <span class="token parameter variable">-disallowSnapshot</span>  /config

<span class="token comment">#3、创建快照,使用默认的名字，快照在/config/.snapshot/s20220212-164145.611目录</span>
hdfs dfs <span class="token parameter variable">-createSnapshot</span> /config

<span class="token comment">#4、创建快照，自己来指定名字</span>
hdfs dfs <span class="token parameter variable">-createSnapshot</span> /config mysnap1

<span class="token comment">#5、给快照重命名</span>
hdfs dfs <span class="token parameter variable">-renameSnapshot</span> /config  mysnap1 mysnap2

<span class="token comment">#6、比较两个快照的差异</span>
<span class="token comment">#  + 表示增加文件    - 减少文件  M 修改文件</span>
 hdfs snapshotDiff  /config  s20220212-164145.611 mysnap2 
 
<span class="token comment">#7、恢复快照</span>
hdfs dfs <span class="token parameter variable">-cp</span> <span class="token parameter variable">-ptopax</span> /config/.snapshot/mysnap1  /config3

<span class="token comment">#、删除快照</span>
<span class="token comment">#如果一个目录有快照，则该目录不能直接被删除，必须把所有快照都删除了，才能删除该目录</span>
hdfs dfs <span class="token parameter variable">-deleteSnapshot</span> /config  mysnap2
</code></pre> 
<h4><a id="HDFS_384"></a>HDFS的权限</h4> 
<pre><code class="prism language-shell">
hadoop fs <span class="token parameter variable">-chmod</span> <span class="token number">777</span> /config       <span class="token comment">#变更目录或文件的权限位</span>
hadoop fs <span class="token parameter variable">-chmod</span> <span class="token parameter variable">-R</span> <span class="token number">777</span> /config     <span class="token comment">#变更目录及子目录的权限位</span>

hadoop fs <span class="token parameter variable">-chown</span> lft: /config <span class="token comment">#变更目录或文件的所属用户</span>
hadoop fs <span class="token parameter variable">-chown</span>  :lft /config  <span class="token comment">#变更目录或文件的所属用户组</span>

hadoop fs <span class="token parameter variable">-chown</span>  lft:lft /config  <span class="token comment">#变更目录或文件的所属用户和用户组</span>
</code></pre> 
<h4><a id="HDFS_395"></a>HDFS的动态扩容和缩容</h4> 
<h5><a id="_396"></a>扩容</h5> 
<ol><li>关闭原来Hadoop集群，添加在hdfs-site.xml添加以下内容,然后讲hdfs-site.xml分发到node2，node3</li></ol> 
<pre><code class="prism language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>dfs.hosts.exclude<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>/export/server/hadoop-3.1.4/etc/hadoop/excludes<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> 
<ol start="2"><li>关闭node3</li><li>根据node3克隆node4</li><li>修改node4的Mac地址</li><li>修改node4的ip地址：192.168.88.164</li><li>修改node4的主机名：node4</li></ol> 
<hr> 
<ol start="7"><li>启动node1,node2,node3主机，并打开hadoop集群</li><li>配置主机名和ip地址的映射，在node1，node2，node3，node4的/etc/hosts文件修改内容如下:</li></ol> 
<pre><code class="prism language-shell"><span class="token number">192.168</span>.88.161 node1 node1.itcast.cn
<span class="token number">192.168</span>.88.162 node2 node2.itcast.cn
<span class="token number">192.168</span>.88.163 node3 node3.itcast.cn
<span class="token number">192.168</span>.88.164 node4 node4.itcast.cn
</code></pre> 
<ol start="9"><li>配置免密登录</li></ol> 
<ul><li>删除从node3克隆的秘钥信息：rm /root/.ssh/ -fr</li><li>在node4上生成公钥和私钥:ssh-keygen -t rsa</li><li>将node4的公钥发送给node1:ssh-copy-id node1</li><li>将node1的所有主机公钥分发给其他主机</li></ul> 
<pre><code class="prism language-shell"><span class="token function">scp</span> /root/.ssh/authorized_keys node2:/root/.ssh
<span class="token function">scp</span> /root/.ssh/authorized_keys node3:/root/.ssh
<span class="token function">scp</span> /root/.ssh/authorized_keys node4:/root/.ssh
</code></pre> 
<p>10.在node1的/export/server/hadoop-3.1.4/etc/hadoop/workers修改以下内容：</p> 
<pre><code class="prism language-shell">node1
node2
node3
node4
</code></pre> 
<p>11.在node4上重新配置hadoop</p> 
<ul><li>这里node4上已经有了从node3上克隆过来的hadoop，必须将/export/server/hadoop-3.1.4/data目录下的内容全部清空,这样node4的hadoop就配置完了</li></ul> 
<pre><code class="prism language-shell"><span class="token function">rm</span> <span class="token parameter variable">-fr</span> /export/server/hadoop-3.1.4/data/*
</code></pre> 
<p>12.在node4上启动hdfs</p> 
<pre><code class="prism language-shell">hdfs <span class="token parameter variable">--daemon</span> start datanode
</code></pre> 
<p>13.设置负载均衡</p> 
<pre><code class="prism language-shell">hdfs dfsadmin <span class="token parameter variable">-setBalancerBandwidth</span> <span class="token number">104857600</span>  <span class="token comment">#设置贷款</span>
hdfs balancer <span class="token parameter variable">-threshold</span> <span class="token number">5</span>  <span class="token comment">#设置block数量</span>
</code></pre> 
<p>14、观察webUI页面是否添加成功<br> <img src="https://images2.imgbox.com/22/e6/PISh0HQi_o.png" alt=""></p> 
<h5><a id="_459"></a>缩容</h5> 
<ol><li>在node1编辑：vim /export/server/hadoop-3.1.4/etc/hadoop/excludes，在该文件中添加要退役的主机名</li><li>在node1上刷新namenode：hdfs dfsadmin -refreshNodes</li><li>在node4上手动关闭datanode</li><li>在node1上执行负载均衡命令：hdfs balancer -threshold 5</li></ol> 
<h4><a id="HDFSID_465"></a>HDFS的ID</h4> 
<ul><li> <p>namespaceID：namenode名称空间id<br> 在单节点或者高可用情况下，整个集群只有一个namespaceID<br> 在联邦机制下，当横向扩展namenode时，扩展几个namenode，就会有几个namespaceID</p> </li><li> <p>clusterID：集群id，不管是单节点，高可用，联邦都属于同一个集群，所有clusterID都相同</p> </li><li> <p>blockpoolID:数据池ID<br> 在单节点或者高可用情况下，整个id都相同</p> <pre><code>在联邦机制下，当有多个namenode同时工作，每一个namenode都有自己的数据池，都有自己的blockpoolID
</code></pre> </li></ul> 
<h4><a id="HDFS_475"></a>HDFS的高可用搭建</h4> 
<pre><code class="prism language-shell">
集群部署节点角色的规划（3节点）
------------------
	node1   namenode    resourcemanager  zkfc   nodemanager  datanode   zookeeper   journal <span class="token function">node</span>
	node2   namenode    resourcemanager  zkfc   nodemanager  datanode   zookeeper   journal <span class="token function">node</span>
	node3                                       nodemanager  datanode   zookeeper    journal <span class="token function">node</span>
------------------

安装步骤：
<span class="token number">1</span>.安装配置zooekeeper集群<span class="token punctuation">(</span>可选<span class="token punctuation">)</span>
	<span class="token number">1.1</span>解压
		<span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /opt/export/server
		<span class="token function">tar</span> <span class="token parameter variable">-zxvf</span> zookeeper-3.4.5.tar.gz <span class="token parameter variable">-C</span> /opt/export/server
	<span class="token number">1.2</span>修改配置
		<span class="token builtin class-name">cd</span> /opt/export/server/zookeeper-3.4.5/conf/
		<span class="token function">cp</span> zoo_sample.cfg zoo.cfg
		<span class="token function">vim</span> zoo.cfg
		修改：dataDir<span class="token operator">=</span>/opt/export/data/zkdata
		在最后添加：
		<span class="token assign-left variable">server.1</span><span class="token operator">=</span>node1:2888:3888
		<span class="token assign-left variable">server.2</span><span class="token operator">=</span>node2:2888:3888
		<span class="token assign-left variable">server.3</span><span class="token operator">=</span>node3:2888:3888
		保存退出
		然后创建一个tmp文件夹
		<span class="token function">mkdir</span> /opt/export/data/zkdata
		<span class="token builtin class-name">echo</span> <span class="token number">1</span> <span class="token operator">&gt;</span> /opt/export/data/zkdata/myid
		
	<span class="token number">1.3</span>将配置好的zookeeper拷贝到其他节点
		<span class="token function">scp</span> <span class="token parameter variable">-r</span> /opt/export/server/zookeeper-3.4.5 node2:/opt/export/server
		<span class="token function">scp</span> <span class="token parameter variable">-r</span> /opt/export/server/zookeeper-3.4.5 node3:/opt/export/server
		
		编辑node2、node3对应/opt/export/data/zkdata/myid内容
		node2：
			<span class="token function">mkdir</span> /opt/export/data/zkdata
			<span class="token builtin class-name">echo</span> <span class="token number">2</span> <span class="token operator">&gt;</span> /opt/export/data/zkdata/myid
		node3：
			<span class="token function">mkdir</span> /opt/export/data/zkdata
			<span class="token builtin class-name">echo</span> <span class="token number">3</span> <span class="token operator">&gt;</span> /opt/export/data/zkdata/myid

<span class="token number">2</span>.安装配置hadoop集群
    <span class="token number">2.0</span> node1和node2安装以下软件
	  yum <span class="token function">install</span> psmisc <span class="token parameter variable">-y</span>
	  
	 
	<span class="token number">2.1</span>解压
	    在三台主机创建以下目录：
		<span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /opt/export/server
		<span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /opt/export/software
		<span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /opt/export/opt/export/data/zkdata
		
		在node1上将hadoop安装包上传到/opt/export/software目录,并解压
		
		<span class="token function">tar</span> zxvf hadoop-3.1.4-bin-snappy-CentOS7.tar.gz  <span class="token parameter variable">-C</span> /opt/export/server
		
		在node1上创建以下目录
		<span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /opt/export/server/hadoop-3.1.4/data
		<span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /opt/export/server/hadoop-3.1.4/journaldata
		
	<span class="token number">2.2</span>配置3台主机的修改hadoop环境变量
		<span class="token function">vim</span> /etc/profile

		<span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_HOME</span><span class="token operator">=</span>/opt/export/server/hadoop-3.1.4
		<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$PATH</span><span class="token builtin class-name">:</span><span class="token variable">$HADOOP_HOME</span>/bin:<span class="token variable">$HADOOP_HOME</span>/sbin

		<span class="token builtin class-name">source</span> /etc/profile
<span class="token number">3</span>.在node1上修改配置文件
<span class="token comment">##############################################################################</span>
<span class="token number">3.1</span> 修改hadoop-env.sh

<span class="token builtin class-name">cd</span> /opt/export/server/hadoop-3.1.4/etc/hadoop
<span class="token function">vim</span> hadoop-env.sh

<span class="token builtin class-name">export</span> <span class="token assign-left variable">JAVA_HOME</span><span class="token operator">=</span>/export/server/jdk1.8.0_241
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HDFS_NAMENODE_USER</span><span class="token operator">=</span>root
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HDFS_DATANODE_USER</span><span class="token operator">=</span>root
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HDFS_JOURNALNODE_USER</span><span class="token operator">=</span>root
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HDFS_ZKFC_USER</span><span class="token operator">=</span>root


<span class="token comment">###############################################################################</span>
				
<span class="token number">3.2</span> 修改core-site.xml
<span class="token operator">&lt;</span>configuration<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span><span class="token operator">!</span>-- HA集群名称，该值要和hdfs-site.xml中的配置保持一致 --<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>fs.defaultFS<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>hdfs://mycluster<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>

	<span class="token operator">&lt;</span><span class="token operator">!</span>-- hadoop本地磁盘存放数据的公共目录 --<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>hadoop.tmp.dir<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>/opt/export/server/hadoop-3.1.4/data<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>

	<span class="token operator">&lt;</span><span class="token operator">!</span>-- ZooKeeper集群的地址和端口--<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>ha.zookeeper.quorum<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>node1:2181,node2:2181,node3:218<span class="token operator"><span class="token file-descriptor important">1</span>&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>/configuration<span class="token operator">&gt;</span>

<span class="token comment">###############################################################################</span>
				
<span class="token number">3.3</span> 修改hdfs-site.xml
<span class="token operator">&lt;</span>configuration<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span><span class="token operator">!</span>--指定hdfs的nameservice为mycluster，需要和core-site.xml中的保持一致 --<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>dfs.nameservices<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>mycluster<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
	
	<span class="token operator">&lt;</span><span class="token operator">!</span>-- mycluster下面有两个NameNode，分别是nn1，nn2 --<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>dfs.ha.namenodes.mycluster<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>nn1,nn<span class="token operator"><span class="token file-descriptor important">2</span>&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>

	<span class="token operator">&lt;</span><span class="token operator">!</span>-- nn1的RPC通信地址 --<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>dfs.namenode.rpc-address.mycluster.nn<span class="token operator"><span class="token file-descriptor important">1</span>&lt;</span>/name<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>node1:802<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>

	<span class="token operator">&lt;</span><span class="token operator">!</span>-- nn1的http通信地址 --<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>dfs.namenode.http-address.mycluster.nn<span class="token operator"><span class="token file-descriptor important">1</span>&lt;</span>/name<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>node1:987<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>

	<span class="token operator">&lt;</span><span class="token operator">!</span>-- nn2的RPC通信地址 --<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>dfs.namenode.rpc-address.mycluster.nn<span class="token operator"><span class="token file-descriptor important">2</span>&lt;</span>/name<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>node2:802<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
	
	<span class="token operator">&lt;</span><span class="token operator">!</span>-- nn2的http通信地址 --<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>dfs.namenode.http-address.mycluster.nn<span class="token operator"><span class="token file-descriptor important">2</span>&lt;</span>/name<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>node2:987<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>

	<span class="token operator">&lt;</span><span class="token operator">!</span>-- 指定NameNode的edits元数据在JournalNode上的存放位置 --<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>dfs.namenode.shared.edits.dir<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>qjournal://node1:8485<span class="token punctuation">;</span>node2:8485<span class="token punctuation">;</span>node3:8485/mycluster<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
	
	<span class="token operator">&lt;</span><span class="token operator">!</span>-- 指定JournalNode在本地磁盘存放数据的位置 --<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>dfs.journalnode.edits.dir<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>/opt/export/server/hadoop-3.1.4/journaldata<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>

	<span class="token operator">&lt;</span><span class="token operator">!</span>-- 开启NameNode失败自动切换 --<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>dfs.ha.automatic-failover.enabled<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>true<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>

	<span class="token operator">&lt;</span><span class="token operator">!</span>-- 指定该集群出故障时，哪个实现类负责执行故障切换 --<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>dfs.client.failover.proxy.provider.mycluster<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>

	<span class="token operator">&lt;</span><span class="token operator">!</span>-- 配置隔离机制方法--<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>dfs.ha.fencing.methods<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>sshfence<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
	
	<span class="token operator">&lt;</span><span class="token operator">!</span>-- 使用sshfence隔离机制时需要ssh免登陆 --<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>/root/.ssh/id_rsa<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
	
	<span class="token operator">&lt;</span><span class="token operator">!</span>-- 配置sshfence隔离机制超时时间 --<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>dfs.ha.fencing.ssh.connect-timeout<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span><span class="token number">3000</span><span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>/configuration<span class="token operator">&gt;</span>

<span class="token comment">###############################################################################</span>
			
<span class="token number">3.4</span> 修改mapred-site.xml
<span class="token operator">&lt;</span>configuration<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span><span class="token operator">!</span>-- 指定mr框架为yarn方式 --<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>mapreduce.framework.name<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>yarn<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>/configuration<span class="token operator">&gt;</span>	

<span class="token comment">###############################################################################</span>
			
<span class="token number">3.5</span> 修改yarn-site.xml
<span class="token operator">&lt;</span>configuration<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span><span class="token operator">!</span>-- 开启RM高可用 --<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>yarn.resourcemanager.ha.enabled<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>true<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span><span class="token operator">!</span>-- 指定RM的cluster <span class="token function">id</span> --<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>yarn.resourcemanager.cluster-id<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>yrc<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span><span class="token operator">!</span>-- 指定RM的名字 --<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>rm1,rm<span class="token operator"><span class="token file-descriptor important">2</span>&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span><span class="token operator">!</span>-- 分别指定RM的地址 --<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>yarn.resourcemanager.hostname.rm<span class="token operator"><span class="token file-descriptor important">1</span>&lt;</span>/name<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>node<span class="token operator"><span class="token file-descriptor important">1</span>&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>yarn.resourcemanager.hostname.rm<span class="token operator"><span class="token file-descriptor important">2</span>&lt;</span>/name<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>node<span class="token operator"><span class="token file-descriptor important">2</span>&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span><span class="token operator">!</span>-- 指定zk集群地址 --<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>yarn.resourcemanager.zk-address<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>node1:2181,node2:2181,node3:218<span class="token operator"><span class="token file-descriptor important">1</span>&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>yarn.nodemanager.aux-services<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>mapreduce_shuffle<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>/configuration<span class="token operator">&gt;</span>
			
				
<span class="token number">3.6</span> 修改workers
	node1
	node2
	node3
	
	
<span class="token number">3.7</span> 修改start-yarn.sh和stop-yarn.sh,在第二行添加以下内容

<span class="token function">vim</span>  /opt/export/server/hadoop-3.1.4/sbin/start-yarn.sh 

<span class="token assign-left variable">YARN_RESOURCEMANAGER_USER</span><span class="token operator">=</span>root
<span class="token assign-left variable">HADOOP_SECURE_DN_USER</span><span class="token operator">=</span>yarn
<span class="token assign-left variable">YARN_NODEMANAGER_USER</span><span class="token operator">=</span>root

<span class="token function">vim</span>  /opt/export/server/hadoop-3.1.4/sbin/stop-yarn.sh 

<span class="token assign-left variable">YARN_RESOURCEMANAGER_USER</span><span class="token operator">=</span>root
<span class="token assign-left variable">HADOOP_SECURE_DN_USER</span><span class="token operator">=</span>yarn
<span class="token assign-left variable">YARN_NODEMANAGER_USER</span><span class="token operator">=</span>root



<span class="token number">3.8</span> 分发hadoop包
<span class="token builtin class-name">cd</span> /opt/export/server/
<span class="token function">scp</span> <span class="token parameter variable">-r</span> hadoop-3.1.4 root@node2:<span class="token environment constant">$PWD</span>
<span class="token function">scp</span> <span class="token parameter variable">-r</span> hadoop-3.1.4 root@node3:<span class="token environment constant">$PWD</span>


			
			
<span class="token number">4</span>.启动集群			
<span class="token comment">###注意：严格按照下面的步骤!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</span>
<span class="token number">4.1</span>启动zookeeper集群（分别在node1、node2、node3上启动zk）
	
	zkServer.sh start
	<span class="token comment">#查看状态：一个leader，两个follower</span>
	zkServer.sh status
	
<span class="token number">4.2</span>手动启动journalnode（分别在在node1、node2、node3上执行）
	/opt/export/server/hadoop-3.1.4/sbin/hadoop-daemon.sh start journalnode
	<span class="token comment">#运行jps命令检验,多了JournalNode进程</span>

<span class="token number">4.3</span>格式化namenode
	<span class="token comment">#在node1上执行命令:</span>
	/opt/export/server/hadoop-3.1.4/bin/hdfs namenode <span class="token parameter variable">-format</span>
	
	<span class="token comment">#格式化后会在根据core-site.xml中的hadoop.tmp.dir配置的目录下生成个hdfs初始化文件，</span>
	
	<span class="token comment">#在node1启动namenode进程</span>
	/opt/export/server/hadoop-3.1.4/bin/hdfs <span class="token parameter variable">--daemon</span> start namenode

	
	<span class="token comment">##在node2上启动备用namenode并进行元数据同步</span>
	/opt/export/server/hadoop-3.1.4/bin/hdfs namenode <span class="token parameter variable">-bootstrapStandby</span>

<span class="token number">4.4</span>格式化ZKFC<span class="token punctuation">(</span>在active<span class="token punctuation">(</span>node1<span class="token punctuation">)</span>上执行即可<span class="token punctuation">)</span>
	/opt/export/server/hadoop-3.1.4/bin/hdfs zkfc <span class="token parameter variable">-formatZK</span>

<span class="token number">4.5</span>启动HDFS<span class="token punctuation">(</span>在node1上执行<span class="token punctuation">)</span>
	/opt/export/server/hadoop-3.1.4/sbin/start-dfs.sh

<span class="token number">4.6</span>启动YARN（在node1上执行）   
	/opt/export/server/hadoop-3.1.4/sbin/start-yarn.sh
	如果node2上没有启动ResourceManage，则还需要手动在standby上手动启动备份的  resourcemanager
	/opt/export/server/hadoop-3.1.4/sbin/yarn-daemon.sh start resourcemanager

			
			
测试集群工作状态的一些指令 ：
       hdfs dfsadmin <span class="token parameter variable">-report</span>	 查看hdfs的各节点状态信息
</code></pre>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a1f4c471e49a1ba4bc3f23b32137ea35/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">java用户名和密码授权链接elasticsearch</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e088e342369fc7513f7f4b3aacee3cbe/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【ThinkPHP6 - 连接 SQLite 及遇到的一些问题】</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>