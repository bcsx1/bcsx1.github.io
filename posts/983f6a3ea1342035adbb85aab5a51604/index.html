<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>K8S学习圣经：大白话说K8S底层原理，14W字实现K8S自由 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="K8S学习圣经：大白话说K8S底层原理，14W字实现K8S自由" />
<meta property="og:description" content="Kubernets 底层原理和实操 (学习圣经) Kubernets 简称 k8s，用于自动部署，扩展和管理容器化应用程序的开源系统。也就是能帮我们部署和管理分布式系统。
学习 Kubernets 的相关资料 中文官网：https://kubernetes.io/zh-cn/docs/
官方文档：https://kubernetes.io/zh/docs/home/（推荐）
中文社区：https://www.kubernetes.org.cn/
社区文档：http://docs.kubernetes.org.cn/
Kubernetes API 规约：community/api-conventions.md at master · kubernetes/community (github.com)
Kubernetes kubectl 命令表: http://docs.kubernetes.org.cn/683.html
说在前面： 现在拿到offer超级难，甚至连面试电话，一个都搞不到。
尼恩的技术社群中（50&#43;），很多小伙伴凭借 “左手云原生&#43;右手大数据”的绝活，拿到了offer，并且是非常优质的offer，据说年终奖都足足18个月。
从Java高薪岗位和就业岗位来看，K8S 现在对于 高级工程师， 架构师，越来越重要，下面是一个高薪Java岗位的K8S技能要求：
但是 K8S 又很难。在这里，尼恩从架构师视角出发，基于自己的尼恩Java 架构师知识体系和知识宇宙，对K8S的核心原理做一个宏观的介绍， 一共十二部分， 组成一本《K8S学习圣经》
《K8S学习圣经》 带大家穿透K8S，实现K8S自由，让大家不迷路。
《K8S学习圣经》的组成 第一部分：云原生(Cloud Native)的原理与演进第二部分：穿透K8S的8大宏观架构第三部分：最小化K8s环境实操第四部分：Kubernetes 基本概念第五部分：Kubernetes 工作负载第六部分：Kubernetes 的资源控制第七部分: SVC负载均衡底层原理第八部分: Ingress底层原理和实操第九部分： 蓝绿发布、金丝雀发布、滚动发布、A/B测试 实操第十部分： 服务网格Service Mesh 宏观架构模式和实操第十一部分： 使用K8S&#43;Harber 手动部署 Springboot 应用第十二部分： SpringCloud&#43;Jenkins&#43; K8s Ingress 自动化灰度发布第十三部分： k8s springboot 生产实践（高可用部署、基于qps动态扩缩容、prometheus监控）第十四部分：k8s生产环境容器内部JVM参数配置解析及优化 米饭要一口一口的吃，不能急。
结合《K8S学习圣经》，尼恩从架构师视角出发，左手云原生&#43;右手大数据 &#43;SpringCloud Alibaba 微服务 核心原理做一个宏观的介绍。由于内容确实太多， 所以写多个pdf 电子书：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/983f6a3ea1342035adbb85aab5a51604/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-25T20:58:35+08:00" />
<meta property="article:modified_time" content="2023-05-25T20:58:35+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">K8S学习圣经：大白话说K8S底层原理，14W字实现K8S自由</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="Kubernets___4"></a>Kubernets 底层原理和实操 (学习圣经)</h3> 
<p>Kubernets 简称 k8s，用于自动部署，扩展和管理容器化应用程序的开源系统。也就是能帮我们部署和管理分布式系统。</p> 
<img src="https://images2.imgbox.com/94/9a/kFF1ly0T_o.png" width="400"> 
<h3><a id="_Kubernets__12"></a>学习 Kubernets 的相关资料</h3> 
<p>中文官网：https://kubernetes.io/zh-cn/docs/</p> 
<p>官方文档：https://kubernetes.io/zh/docs/home/（推荐）</p> 
<p>中文社区：https://www.kubernetes.org.cn/</p> 
<p>社区文档：http://docs.kubernetes.org.cn/</p> 
<p>Kubernetes API 规约：<a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md">community/api-conventions.md at master · kubernetes/community (github.com)</a></p> 
<p>Kubernetes kubectl 命令表: http://docs.kubernetes.org.cn/683.html</p> 
<h3><a id="_32"></a>说在前面：</h3> 
<p>现在<strong>拿到offer超级难</strong>，甚至连面试电话，一个都搞不到。</p> 
<p>尼恩的技术社群中（50+），很多小伙伴凭借 “左手云原生+右手大数据”的绝活，拿到了offer，并且是非常优质的offer，<strong>据说年终奖都足足18个月</strong>。</p> 
<p>从Java高薪岗位和就业岗位来看，K8S 现在对于 高级工程师， 架构师，越来越重要，下面是一个高薪Java岗位的K8S技能要求：</p> 
<p><img src="https://images2.imgbox.com/b7/ba/kzjSilSf_o.png" alt=""></p> 
<p>但是 K8S 又很难。在这里，尼恩从架构师视角出发，基于自己的尼恩Java 架构师知识体系和知识宇宙，对K8S的核心原理做一个宏观的介绍， 一共十二部分， 组成一本《K8S学习圣经》</p> 
<p>《K8S学习圣经》 带大家穿透K8S，实现K8S自由，让大家不迷路。</p> 
<h3><a id="K8S_48"></a>《K8S学习圣经》的组成</h3> 
<ul><li>第一部分：云原生(Cloud Native)的原理与演进</li><li>第二部分：穿透K8S的8大宏观架构</li><li>第三部分：最小化K8s环境实操</li><li>第四部分：Kubernetes 基本概念</li><li>第五部分：Kubernetes 工作负载</li><li>第六部分：Kubernetes 的资源控制</li><li>第七部分: SVC负载均衡底层原理</li><li>第八部分: Ingress底层原理和实操</li><li>第九部分： 蓝绿发布、金丝雀发布、滚动发布、A/B测试 实操</li><li>第十部分： 服务网格Service Mesh 宏观架构模式和实操</li><li>第十一部分： 使用K8S+Harber 手动部署 Springboot 应用</li><li>第十二部分： SpringCloud+Jenkins+ K8s Ingress 自动化灰度发布</li><li>第十三部分： k8s springboot 生产实践（高可用部署、基于qps动态扩缩容、prometheus监控）</li><li>第十四部分：k8s生产环境容器内部JVM参数配置解析及优化</li></ul> 
<p>米饭要一口一口的吃，不能急。</p> 
<p>结合《K8S学习圣经》，尼恩从架构师视角出发，左手云原生+右手大数据 +SpringCloud Alibaba 微服务 核心原理做一个宏观的介绍。由于内容确实太多， 所以写多个pdf 电子书：</p> 
<p>(1) 《 <a href="https://blog.csdn.net/crazymakercircle/article/details/129482349">Docker 学习圣经</a> 》PDF （V1已经完成）</p> 
<p>(2) 《 <a href="https://blog.csdn.net/crazymakercircle/article/details/129559428?spm=1001.2014.3001.5502">SpringCloud Alibaba 微服务 学习圣经</a> 》PDF （V1已经完成）</p> 
<p>(3) 《 K8S 学习圣经 》PDF （coding…）</p> 
<p>(4) 《 flink + hbase 学习圣经 》PDF （planning …）</p> 
<p>以上学习圣经，并且后续会持续升级，从V1版本一直迭代发布。 就像咱们的《 <a href="https://blog.csdn.net/crazymakercircle/article/details/124790425">尼恩 Java 面试宝典</a> 》一样， 已经迭代到V60啦。</p> 
<p>40岁老架构师尼恩的掏心窝： 通过一系列的学习圣经，带大家穿透“左手云原生+右手大数据 +SpringCloud Alibaba 微服务“ ，实现技术 自由 ，走向颠覆人生，让大家不迷路。</p> 
<p>本PDF 《K8S 学习圣经》完整版PDF的 V1版本，后面会持续迭代和升级。供后面的小伙伴参考，提升大家的 3高 架构、设计、开发水平。</p> 
<p>以上学习圣经的 基础知识是 尼恩的 《 <a href="https://blog.csdn.net/crazymakercircle/article/details/115677785">高并发三部曲</a> 》，建议在看 学习圣经之前，一定把尼恩的《 <a href="https://blog.csdn.net/crazymakercircle/article/details/115677785">Java高并发三部曲</a> 》过一遍，切记，切记。</p> 
<blockquote> 
 <p>注：本文以 PDF 持续更新，最新尼恩 架构笔记、面试题 的PDF文件，请到《技术自由圈》公众号获取</p> 
</blockquote> 
<h3><a id="_90"></a>本书目录</h3> 
<ul><li>学习 Kubernets 的相关资料</li><li>说在前面：</li><li>《K8S学习圣经》的组成</li><li>本书目录</li><li>第1部分：云原生(Cloud Native)的原理与演进 
  <ul><li>1、什么是云原生(Cloud Native)？ 
    <ul><li>云原生的四要素：</li></ul> </li><li>2、云原生发展历史时间轴 
    <ul><li>2.1 从微服务到服务网格</li><li>2.2 微服务架构的问题 
      <ul><li>解决方案：</li></ul> </li><li>2.3 SideCar 旁车模式（边车模式）</li></ul> </li><li>3、2018 年云原生被CNCF重新定义 
    <ul><li>3.1 服务网格 （Service Mesh）</li><li>3.2 不可变基础设施（Immutable Infrastructure）</li><li>3.3 声明式API ( declarative APIs)</li></ul> </li><li>4、CNCF云原生组织发展和介绍 
    <ul><li>什么是CNCF</li><li>CNCF解决了什么问题</li><li>介绍几个常用的已经毕业的云原生项目</li><li>孵化中的项目</li></ul> </li><li>5、从微服务演进到Service Mesh（服务网格）的过程 
    <ul><li>5.1 单体服务时代</li><li>5.2 微服务时代</li><li>5.3 服务网格新时期 （service mesh）</li></ul> </li><li>6、服务网格（service mesh）原理和价值 
    <ul><li>什么是服务网格（service mesh）</li><li>Service Mesh的价值</li><li>Linkerd</li><li>istio</li><li>国内兴起的服务网格</li></ul> </li><li>7、云原生应用和传统应用的区别</li><li>8、云原生涉及的核心项目</li></ul> </li><li>第2部分：穿透K8S的8大宏观架构 
  <ul><li>尼恩的K8S的独特视角：不是普通的视角</li><li>K8S的核心价值：带领大家从 容器管理的石器时代，进入工业时代 
    <ul><li>图0：K8S 的宏观组件架构图</li><li>图1：K8S 业务架构图</li><li>图2：K8S 元数据架构图</li><li>图3：K8S 容器管理流程架构</li><li>图4：容器元数据的数据传输架构</li><li>图5：容器对外暴露架构图</li><li>图6：总的架构图</li><li>图7：master 上APIServer 内部架构图</li><li>图8：worker上 kubelet内部架构图</li></ul> </li><li>学习 云原生+ 微服务的神器</li></ul> </li><li>第3部分：K8s运行时 实操 
  <ul><li>什么是minikube</li><li>minikube 背景</li><li>Kubernetes集群架构 与minikube架构对比 
    <ul><li>1、Kubernetes集群架构</li><li>2、Minikube架构</li></ul> </li><li>minikube安装前准备</li><li>docker安装和环境检查 
    <ul><li>docker 版本要求</li><li>关闭虚拟机swap、selinux、firewalld</li><li>编辑虚拟机hosts文件</li><li>登录阿里云</li></ul> </li><li>创建用户，加入docker用户组 
    <ul><li>让用户minikube获得root权限</li></ul> </li><li>安装与启动minikube 
    <ul><li>软件版本说明</li><li>安装minikube</li><li>启动minikube</li><li>命令清单：血泪的安装史，尼恩用过的 命令list</li><li>minikube start 参数</li><li>示例 
      <ul><li>–vm-driver=kvm2</li><li>–vm-driver=hyperv</li><li>–vm-driver=none</li></ul> </li></ul> </li><li>解决拉取镜像的问题 
    <ul><li>错误日志查看</li><li>Q1：解决minikube拉取镜像速度缓慢的问题 
      <ul><li>解决 minikube <code>start</code> 过程中拉取镜像慢的问题</li></ul> </li><li>Q2：基础镜像拉不下来 
      <ul><li>错误日志查看</li><li>基础镜像拉不下来</li><li>指定镜像启动</li><li>终于开始创建容器，开始启动了</li></ul> </li><li>Q3：新的问题来了：coredns 镜像找不到</li><li>Q4：继续下载镜像</li><li>Q5：使用阿里云代理http://k8s.gcr.io镜像仓库</li></ul> </li><li>Virtual Box 使用的问题 
    <ul><li>Q1：嵌套虚拟化问题 
      <ul><li>什么是嵌套 虚拟化特性？</li><li>虚拟机启用嵌套VT-x/AMD-V</li></ul> </li><li>Q2：conntrack依赖</li><li>Q3：依赖kubectl、kubelet</li><li>Q4：桥接问题</li><li>Q5：初始化失败报错，升级内核</li><li>启动Dashboard</li><li>终于全部启动了</li><li>如何从宿主机也就是我们的Windows中访问dashborad呢</li><li>直接使用minikube</li><li>minikube重建</li></ul> </li><li>docker-compose to minikube</li><li>部署minikube 遇到的问题 
    <ul><li>外部访问问题</li><li>pull image问题</li></ul> </li><li>POD 容器的问题 
    <ul><li>K8s的常用命令</li><li>查看所有的pod，看看哪些有问题</li><li>storage-provisioner 的ImagePullBackOff 状态</li></ul> </li><li>启动minikube时指定harber仓库 
    <ul><li>minikube 复制证书</li><li>进入minikube虚拟机 
      <ul><li>增加私仓地址配置</li><li>重启虚拟机的Docker</li><li>测试</li></ul> </li></ul> </li><li>命令清单：尼恩用过的 启动 命令清单 （都是血和泪）</li><li>minikube常用命令 
    <ul><li>一、基本命令</li><li>二、镜像命令</li><li>三、配置和管理命令</li><li>四、网络和连接命令</li><li>五、高级命令</li><li>六、疑难解答命令</li><li>七、其它命令</li></ul> </li><li>启动minikube完整的命令清单</li><li>Helm 的原理、安装、使用 
    <ul><li>Helm 组件及相关术语</li><li>Helm 工作原理</li><li>安装Helm</li></ul> </li></ul> </li><li>第4部分：Kubernetes 基本概念 
  <ul><li>1、基础概念理解 
    <ul><li>K8S集群</li><li>Node的核心组件</li><li>Pod</li><li>Label</li><li>Deployment</li><li>Service</li><li>Addons</li><li>DNS</li><li>Web UI (Dashboard)</li></ul> </li><li>2、k8s对象（kubernetes Objects） 
    <ul><li>对象的yaml结构</li><li>关于 yaml文件的分割</li><li>2.1 一个简单的Kubernetes API 对象</li><li>2.1 什么是k8s 资源对象 
      <ul><li>kubernetes 对象必需字段</li></ul> </li><li>2.2 kubernetes API的版本 
      <ul><li>一、查看apiversion 可用版本</li><li>二、各种apiVersion的含义</li></ul> </li><li>2.3 获取各个属性字段的含义 
      <ul><li>方式1：从命令行获取方式</li><li>方式2：官方 API 文档方式</li></ul> </li><li>2.4、管理k8s对象 
      <ul><li>1、命令式</li><li>2、指令性</li><li>3、声明式</li></ul> </li><li>2.5、对象名称与ID 
      <ul><li>Names</li><li>UID</li></ul> </li><li>2.6 对象规约（Spec）与状态（Status） 
      <ul><li>对象规约（Spec）与状态（Status）</li><li>spec 规约</li><li>status 状态</li></ul> </li><li>2.7、名称空间 
      <ul><li>名称空间的使用参考：</li><li>如何访问其他名称空间的东西？</li><li>查看名称空间</li><li>在执行请求的时设定namespace</li><li>设置名称偏好</li><li>名称空间与DNS</li><li>名称空间的作用</li><li>并非所有对象都在命名空间中</li></ul> </li><li>2.8 、标签和选择器</li><li>2.9、注解annotation</li><li>2.8、字段选择器</li><li>2.10、认识kubectl 客户端命令 
      <ul><li>kubectl的所有命令参考：</li></ul> </li><li>2.11、自动补全</li><li>2.12、给idea安装kubernetes插件 
      <ul><li>1、plugins kubernetes</li><li>2、快速生成kubernetes 资源文件模板</li><li>3、输入<code>k</code>即可触发自动命令提示</li></ul> </li></ul> </li></ul> </li><li>第5部分：Kubernetes 工作负载 
  <ul><li>什么是Workloads?</li><li>Pod的原理与生命周期 
    <ul><li>1、什么是Pod</li><li>2、Pod使用</li><li>3、Pod生命周期</li></ul> </li><li>“根容器” : Pause 容器 
    <ul><li>Pod中容器也有分类</li><li>Pod生命周期的 两个阶段</li></ul> </li><li>Pod的Init 初始化容器 (附Demo) 
    <ul><li>Pod 钩子Hook方法 (附Demo)</li><li>Pod 健康检查（探针）机制</li><li>Pod 的状态和重启策略 
      <ul><li>Pod 常见的状态</li><li>Pod重启策略</li><li>Pod常见状态转换场景</li></ul> </li></ul> </li><li>临时容器：线上排错 
    <ul><li>使用方法：</li><li>通过临时容器查看Java容器log日志</li><li>临时容器的参数说明：</li><li>使用临时容器进行 dump 转储 
      <ul><li>基于制作Java Debug 镜像</li><li>临时容器不能用jmap、jps而只能用jattach</li><li>jattach指令集:</li></ul> </li><li>临时容器的配置</li><li>临时容器的使用场景</li></ul> </li><li>静态Pod 
    <ul><li>如何找到 静态pod 的文件路径</li><li>静态Pod实操</li><li>Pod 资源需求和限制</li></ul> </li><li>Pod的Probe 探针机制（健康检查机制） 
    <ul><li>Probe 探针 背景</li><li>K8S 的3种探针</li><li>livenessProbe和readinessProbe 的区别 
      <ul><li>（1）livenessProbe</li><li>（2）readinessProbe</li><li>（3）就绪、存活两种探针的区别</li></ul> </li><li>存活探针 (liveness)的三种探测方法 
      <ul><li>exec 方式示例</li><li>httpGet 方式示例</li><li>TCP 方式示例</li><li>使用命名的端口</li></ul> </li><li>就绪探针 (readiness) 
      <ul><li>ReadinessProbe 探针使用示例</li></ul> </li><li>ReadinessProbe + LivenessProbe 配合使用示例 
      <ul><li>以上案例中存活探针 参数意思：</li><li>以上案例中就绪探针 参数意思：</li></ul> </li><li>startupProbe探针 
      <ul><li>1、startupProbe探针介绍</li><li>2、startupProbe探针与另两种区别</li><li>3、startupProbe探针方法、属性</li><li>4、为什么要使用startupProbe、使用场景</li></ul> </li></ul> </li></ul> </li><li>第6部分：Kubernetes 的资源控制 
  <ul><li>动态的扩容缩容的重要性</li><li>1：Deployment 资源对象 
    <ul><li>Deployment的创建</li><li>Deployment 资源、replicaset资源、Pod资源 三者之间的关系</li><li>Deployment 基本操作 
      <ul><li>副本扩容</li><li>副本缩容</li></ul> </li><li>Deployment 更新机制 
      <ul><li>准备：升级镜像</li><li>在线修改yaml</li><li>滚动机制相关的命令</li><li>暂停和恢复</li><li>回滚策略 
        <ul><li>记录保留</li><li>滚动更新数量</li></ul> </li></ul> </li><li>使用Deployment 进行灰度发布</li></ul> </li><li>2：副本资源 RC、副本集RS 资源对象 
    <ul><li>RC（Replication Controller）</li><li>RS（Replication Set）</li><li>ReplicatSet的三个部分</li><li>RS扩容缩容</li></ul> </li><li>3：DaemonSet 守护集 
    <ul><li>DaemonSet 组成结构详解</li><li>调度节点的选择 
      <ul><li>方式一：nodeSelector方式</li><li>方式二：nodeAffinity方式</li><li>方式三：podAffinity方式</li></ul> </li><li>Toleration</li></ul> </li><li>4：StatefulSet 有状态集 
    <ul><li>StatefulSet 使用场景</li><li>StatefulSet的一些限制和要求</li><li>StatefulSet示例</li><li>DNS解析</li><li>StatefulSet 的几个要点 
      <ul><li>（1）pod管理策略（podManagementPolicy）</li><li>（2）updateStrategy： 更新策略</li><li>（3）对应的headless service</li></ul> </li></ul> </li><li>5：Job 任务、CronJob 定时任务 
    <ul><li>Job 任务</li><li>并行 job 示例</li><li>CronJob 定时任务 
      <ul><li>CronJob示例</li></ul> </li></ul> </li><li>6：HPA(Horizontal Pod Autoscaling)水平自动伸缩 
    <ul><li>收集指标插件</li><li>Metrics API</li><li>前置条件: 开启聚合路由 
      <ul><li>metrics安装方式一：minikube 中启用指标服务器作为插件</li><li>metrics server安装方式二：手动安装</li><li>执行安装和检查</li><li>执行安装的命令清单</li></ul> </li></ul> </li><li>7：使用HPA对SpringCloud微服务进行自动伸缩 
    <ul><li>autoscale命令</li><li>Metrics支持的指标</li><li>创建hpa、查看hpa、删除hpa</li><li>HPA扩容实操</li><li>基于自定义指标的自动扩容</li><li>扩展阅读: GC机制</li><li>什么是垃圾回收</li></ul> </li><li>什么是OCI、CRI、CNI、CSI 
    <ul><li>OCI、CRI、CNI、CSI、CRD、CNM规范基本概念：</li><li>实现OCI、CRI、CNI、CSI组件介绍 
      <ul><li>1、OCI、CRI组件</li><li>2、CNI组件 
        <ul><li>CNI 和 CNM 的对比：</li></ul> </li><li>3、CSI组件</li></ul> </li><li>CRI、OCI的关系</li><li>CRD是什么</li><li>CR是什么</li><li>CRD与CR的关系</li><li>CRD在API Server中的设计和实现机制</li><li>如何创建 CRD？</li></ul> </li></ul> </li><li>第7部分：SVC负载均衡底层原理 
  <ul><li>Pod的IP 漂移问题</li><li>service 概念</li><li>四大service 类型</li><li>kubernetes暴露端口的方式 
    <ul><li>1：集群内部实现访问：Clusterip</li><li>2：集群外部方式访问：NodePort</li><li>3: LoadBalancer</li><li>4: Ingress</li></ul> </li><li>DNS解析案例 
    <ul><li>案例</li></ul> </li><li>SVC 流量分发的底层原理 
    <ul><li>VIP 和 Service 代理</li><li>代理模式分类</li><li>Ⅰ、userspace 代理模式</li><li>Ⅱ、Iptables 代理模式</li><li>Ⅲ、ipvs 代理模式 
      <ul><li>ipvs为负载均衡提供算法：</li><li>ipvs 对比 iptables</li></ul> </li><li>iptables/netfilter 介绍 
      <ul><li>1，iptables/netfilter介绍</li><li>2，iptables 基础：</li><li>3，链的概念：</li><li>4，表的概念：</li><li>5，数据经过防火墙的流程图：</li><li>6，规则：</li></ul> </li><li>总结： iptables包含4个表，5个链</li></ul> </li><li>iptables 在 K8s 中的应用剖析 
    <ul><li>路由的流程分析:</li><li>service 的路由分析实例 
      <ul><li>先看iptables 中的DNAT</li><li>再看SNAT</li></ul> </li></ul> </li><li>基于客户端地址的会话保持模式的svc负载分发策略</li><li>k8s集群中service的域名解析、pod的域名解析 
    <ul><li>service的域名 
      <ul><li>1、创建namespace.yaml文件</li><li>2、创建deployment.yaml文件</li><li>3、创建service.yaml文件</li><li>4、测试</li></ul> </li><li>总结</li></ul> </li><li>应用持久化存储（PV和PVC） 
    <ul><li>1、Volume 
      <ul><li>1.1、emptyDir</li><li>1.2、hostPath</li><li>1.3、外部存储（以NFS为例）</li></ul> </li><li>2、PV与PVC 
      <ul><li>持久卷的存储插件类型</li><li>2.1、Static PV 
        <ul><li>1）先搭建好NFS服务器（192.168.100.172）</li><li>2）创建PV</li><li>3）创建PVC</li><li>4）创建Pod</li></ul> </li><li>2.2、Dynamic PV 
        <ul><li>Dynamic PV属于PV的自动化：</li><li>什么是StorageClass</li><li>为什么需要StorageClass</li><li>StorageClass案例</li><li>运行原理</li></ul> </li></ul> </li></ul> </li></ul> </li><li>第8部分：K8S Ingress原理和实操 
  <ul><li>背景： 
    <ul><li>svc的作用与不足</li><li>Ingress 的来源</li></ul> </li><li>Ingress的构成 
    <ul><li>什么是Ingress Controller？ 
      <ul><li>Ingress 资源对象</li><li>ingress-controller组件</li></ul> </li><li>nginx-ingress-controller</li></ul> </li><li>部署ingress-controller 
    <ul><li>Minikube安装Ingress</li><li>手工部署ingress-controller pod及相关资源</li><li>ingress版本的问题</li><li>调整RBAC api-versions 版本</li><li>部署 ingress-nginx</li><li>配置ingress资源</li><li>报错 failed calling webhook</li><li>获取 ingress 启动的svc 
      <ul><li>加上本地的host</li></ul> </li><li>测试访问</li><li>实操的善后工作</li></ul> </li><li>k8s ingress的工作原理 
    <ul><li>（1）Ingress Controller 控制器</li><li>（2） Ingress 资源对象</li></ul> </li><li>详解ingress资源 
    <ul><li>ingress规则</li><li>DefaultBackend</li><li>资源后端</li><li>路径类型 
      <ul><li>路径类型示例</li></ul> </li><li>路径多重匹配</li></ul> </li><li>主机名通配符</li><li>ingress类 
    <ul><li>名字空间域的参数</li><li>默认ingress类</li></ul> </li><li>ingress部署的三种模式 
    <ul><li>模式一：NodePort模式的Service</li><li>模式二：DaemonSet+nodeSelector+HostNetwork</li><li>模式三：Deployment+LoadBalancer模式的Service</li></ul> </li><li>模式一的问题：service暴露服务的问题</li><li>DaemonSet+HostNetwork+nodeselector 实操 
    <ul><li>nodePort的NAT性能问题</li><li>指定nginx-ingress-controller运行的node节点</li><li>修改Deployment为Daemonset,指定节点运行，并开启 hostNetwork</li><li>启动nginx-ingress-controller</li><li>查看pod的IP和端口</li><li>配置ingress资源</li><li>命令清单</li></ul> </li><li>生产环境 LVS+keepalive 做高可用和负载均衡 
    <ul><li>边缘节点</li><li>生产环境可以使用 HA + LB + DaemonSet hostNetwork 架构</li></ul> </li><li>四种port底层原理：nodePort、port、targetPort、containerPort 的核心 
    <ul><li>1、nodePort</li><li>2、port</li><li>3、targetPort</li><li>4、containerPort</li></ul> </li><li>Ingress 动态域名配置底层原理 
    <ul><li>ingress生产建议</li></ul> </li><li>ingress总结</li><li>K8s的常用命令 
    <ul><li>kubectl命令 要求</li></ul> </li></ul> </li><li>第9部分：蓝绿发布、金丝雀发布、滚动发布、A/B测试 原理和实操 
  <ul><li>背景：</li><li>蓝绿发布、金丝雀发布、滚动发布、A/B测试 核心原理 
    <ul><li>蓝绿发布（Blue-green Deployments） 核心原理</li><li>金丝雀发布（anCanary Releases） 核心原理</li><li>滚动发布的 核心原理</li><li>A/B测试（A/B Testing） 核心原理</li></ul> </li><li>蓝绿发布、金丝雀发布、滚动发布实操 
    <ul><li>spring cloud 灰度实操</li><li>反向代理网关灰度实操</li><li>Kubernetes 中的灰度策略</li><li>Deployment金丝雀部署：按照流量比例 
      <ul><li>step1 ：启动 V1 服务，查看服务是否正确，然后观察一下服务。</li><li>step2：启动 V2 的服务版本：1 个复本</li><li>step3：观察 V2 流量正常的情况的话，那么启动 V2 的 2 个复本。</li><li>step4：删除 V1 的 2 个复本，流量全部到 V2 上。</li><li>实操总结</li></ul> </li><li>Deployment实现滚动发布 
      <ul><li>step1：启动 V1 服务，查看服务是否正确，然后观察一下服务。</li><li>step2：启动 app-v2-rolling 进行滚动发布</li><li>step3：观察所有容器版本变为 V2 版本</li><li>实操总结</li></ul> </li><li>Deployment实现蓝绿部署 
      <ul><li>step1 ：启动 V1 服务，查看服务是否正确，然后观察一下服务。</li><li>step2：启动 V2 的服务版本</li><li>step3：将版本 1 服务切换到版本 2，观察服务情况</li><li>patch</li><li>实操总结</li></ul> </li><li>Ingress Annotations实现金丝雀发布 
      <ul><li>Ingress Annotations实现金丝雀发布 实操</li></ul> </li><li>华为云的金丝雀发布</li><li>参考</li></ul> </li></ul> </li><li>第10部分：服务网格Service Mesh 宏观架构模式 
  <ul><li>第1大模式：Sidecar （边车）模式 架构 
    <ul><li>sidecar的（边车）负责的功能</li><li>sidecar模式好处、坏处</li><li>如何解决依赖的复杂性和性能问题呢？</li><li>优化之后的sidecar模式优点：</li></ul> </li><li>第2大模式：代理模式</li><li>服务网格 istio 框架微服务与SpringCloud 对比 
    <ul><li>SpringCloud 和 istio 中间组件的对比</li><li>云原生Sidecar分体架构微服务Provider一体架构对比</li><li>两大基础组件的对比</li></ul> </li><li>角色sidecar 对应到啥组件？</li><li>小结：</li><li>Istio 架构 
    <ul><li>控制平和数据面</li><li>Istio 的运转流程</li><li>（1）Sidecar自动注入：</li><li>（2）流量拦截：</li><li>（3）服务发现：</li><li>（4）负载均衡：</li><li>（5）流量治理：</li><li>（6）访问安全：</li><li>（7）服务遥测：</li><li>（8）策略执行：</li><li>（9）外部访问：</li></ul> </li><li>Istio组件介绍 
    <ul><li>2.1 Pilot</li><li>2.2 Mixer</li><li>2.3 Citadel</li><li>2.4 Galley</li><li>2.5 Sidecar-injector</li><li>2.6 Proxy(Envoy)</li><li>2.7 Ingress gateway</li><li>2.8 其他组件</li></ul> </li><li>Istio安装 
    <ul><li>在本地搭建Istio环境</li><li>Kubernetes集群环境</li></ul> </li><li>安装Istio 
    <ul><li>快速部署Istio</li><li>初步感受istio</li><li>手动注入</li><li>自动注入sidecar</li></ul> </li><li>istio项目案例：bookinfo 
    <ul><li>什么是bookinfo</li></ul> </li><li>sidecar自动注入到微服务</li><li>启动bookinfo</li><li>通过ingress方式访问</li><li>通过istio的ingress gateway访问 
    <ul><li>确定 Ingress 的 IP 和端口</li></ul> </li></ul> </li><li>第11部分：使用K8S+Harber 手动部署 SpringCloud 应用 
  <ul><li>启动minikube时指定私有仓库</li><li>minikube 复制证书</li><li>进入minikube虚拟机 
    <ul><li>增加私仓地址配置</li><li>重启虚拟机的Docker</li><li>测试</li></ul> </li><li>完整的命令清单</li><li>Docker构建镜像后通过K8S部署</li><li>镜像与容器关联 
    <ul><li>Docker构建镜像并且推送Harber 
      <ul><li>前提：停止或者重启 Harbor</li></ul> </li><li>创建Dockerfile</li><li>镜像构建：将本地镜像打包 
      <ul><li>docker build语法</li></ul> </li><li>镜像打tag：镜像添加版本号</li><li>镜像推送：镜像推送到远程仓库</li><li>测试</li><li>完整的命令清单</li></ul> </li><li>k8s的Secrets： 
    <ul><li>创建一个k8s的Secrets：</li><li>1.Secret对象配置过程</li><li>2.容器镜像拉取的两种策略 
      <ul><li>2.1 ImagePullPolicy</li><li>2.2 ImgaePullSecrets</li></ul> </li></ul> </li><li>k8s的configMap对象 
    <ul><li>Pod可以通过三种方式来使用ConfigMap，分别为：</li><li>configMap是什么 
      <ul><li>1.单pod使用configmap示例图</li><li>2.多pod使用configmap示例图</li></ul> </li><li>configmap来配置环境变量 
      <ul><li>步骤1：创建configmap 
        <ul><li>方法1：yaml档来设定configmap</li><li>方法1：命令行设定configmap</li></ul> </li><li>步骤3：使用configmap来配置</li></ul> </li></ul> </li><li>docker-compose to minikube 
    <ul><li>简单案例</li><li>实操案例 
      <ul><li>参考的原始的docker-compose编排文件</li></ul> </li><li>设置env环境变量</li><li>k8s向etc/hosts里添加内容 
      <ul><li>k8s默认被重写/etc/hosts</li><li>Dockerfile里的配置被覆盖</li><li>将你的配置写到k8s yml里</li></ul> </li><li>Kubernetes volume hostPath explained with examples 
      <ul><li>hostPath</li><li>Example</li></ul> </li></ul> </li><li>部署和创建服务 
    <ul><li>ImagePullBackOff错误排查 
      <ul><li>解决问题：停止后重启 Harbor</li></ul> </li><li>CrashLoopBackOff 错误 
      <ul><li>坑1：镜像拉取是ok，但是依赖包没有</li><li>坑2：java 进程异常结束了</li><li>坑3：minikube是套娃虚拟机，套娃里边没有映射目录，远程复制</li><li>坑4：检查域名和环境变量</li></ul> </li><li>解决问题有感</li></ul> </li><li>完整的命令清单</li><li>openresty 镜像 
    <ul><li>dockerfile</li></ul> </li></ul> </li><li>第12部分：SpringCloud+Jenkins+ K8s Ingress 自动化灰度发布 
  <ul><li>如何进行SpringCloud+Jenkins+ K8s Ingress 灰度发布？</li><li>回顾Nginx-ingress 架构和原理</li><li>灰度实操之前的准备 
    <ul><li>部署和测试 stable 版本的 deployment 和 svc</li><li>部署和测试 canary版本 的 deployment 和 svc</li></ul> </li><li>基于用户的灰度场景 
    <ul><li>接下来，开始基于 用户的灰度实操</li></ul> </li><li>基于权重的灰度场景 
    <ul><li>基于权重的 Canary 规则</li><li>基于权重的发布实操</li></ul> </li><li>如何进行自动化灰度？</li><li>jenkins安装和pipeline 流水线的使用 
    <ul><li>下载和启动jenkins</li><li>登录jenkins</li><li>使用pipeline插件 
      <ul><li>安装Pipeline 插件</li></ul> </li><li>pipeline 的hello world</li><li>pipeline 语法介绍</li><li>Jenkins插件SSH Pipeline Steps 
      <ul><li>sshCommand 在远程节点上执行给定的命令并响应输出</li><li>sshGet 从远程主机获取文件或目录</li><li>sshPut 将文件或目录放入远程主机</li><li>sshRemove 删除远程主机上的文件或目录</li><li>sshScript 在远程节点上执行给定的脚本(文件)并响应输出</li><li>结合 withCredentials 从 Jenkins 凭证存储中读取私钥</li></ul> </li><li>pipeline支持的指令</li></ul> </li><li>ingress 灰度发布流水线设计 
    <ul><li>CICD流水线预览</li><li>step1：自动化的制品发布 
      <ul><li>1、克隆 springboot代码项目到本地</li><li>2、maven构建springboot代码项目</li><li>3、构建docker镜像</li><li>4、推送docker镜像到harber上，完成制品发布</li></ul> </li><li>step2：生产环境进行 A/B 测试 
      <ul><li>AB测试原理：</li></ul> </li><li>step3：生产环境进行 A/B 测试</li><li>step4：生产环境进行版本正式切换</li></ul> </li><li>最后总结一下</li></ul> </li><li>第13部分：springboot 基于 qps 动态扩缩容 
  <ul><li>步骤</li><li>1、springboot安装prometheus依赖并获取metric</li><li>2、安装prometheus operator 实现 kubernetes的监控指标 
    <ul><li>2.1 helm安装prometheus operator、prometheus adapter（custom metric）</li><li>2.2 kubernetes的监控指标</li><li>Prometheus Operator 极简配置Prometheus 监控 
      <ul><li>Operator</li><li>Operator介绍</li><li>Prometheus Operator vs. kube-prometheus vs. community helm chart</li></ul> </li><li>Prometheus Operator CRD介绍</li><li>命令清单: 安装的Prometheus</li><li>查看安装后的CRD、svc、pods：</li></ul> </li><li>从外部访问promethus 
    <ul><li>访问Prometheus</li><li>访问Alertmanager</li><li>访问Grafana</li></ul> </li><li>移除kube-prometheus 
    <ul><li>解决prometheus-adapter创建失败的问题 
      <ul><li>查看 安装文件</li></ul> </li><li>解决kube-state-metrics 创建失败的问题</li><li>解决namespace 删除不来</li></ul> </li><li>配置prometheus -adapter获取应用qps</li><li>prometheus采集到的metrics适配给kubernetes用 
    <ul><li>K8s中 kubernetes的监控指标分为两种：</li><li>Metrics-Server 简介</li><li>prometheus-adpater</li></ul> </li><li>获取 metrics 定制化指标 
    <ul><li>checkout 仓库 k8s-prometheus-adapter</li><li>脚本进行部署 k8s-prometheus-adapter 
      <ul><li>请求metrics.k8s.io的 api</li><li>直接访问 api 的方式来测试部署是否 ok</li></ul> </li><li>排错 no matches for kind “APIService” in version "apiregistration.k8s.io/v1 
      <ul><li>查看版本 kubectl api-versions</li></ul> </li><li>Prometheus adapter 的文件介绍</li><li>配置文件</li><li>resource metrics API</li><li>命令清单</li></ul> </li><li>自定义alertmanager的PrometheusRule</li><li>配置prometheus及adapter获取应用qps 
    <ul><li>启动springboot应用</li><li>添加自定义指标</li><li>架构图原理</li><li>添加一个自定义监控的步骤</li><li>部署prometheus-adapter</li></ul> </li><li>custom-metrics-server 规则配置 
    <ul><li>指标发现和配置展示(Metrics Discovery and Presentation Configuration)</li><li>http_requests（每秒请求数QPS)监控指标</li><li>坑： /metrics 路径的定制化修改</li><li>查询与 http_requests 相关的指标</li></ul> </li><li>自定义metric rules、配置HPA 
    <ul><li>配置自定义prometheus-adapter-config配置</li><li>配置 qps 请求量指标</li><li>查看 “pods/http_server_requests_per_second” 指标</li><li>HPA配置 
      <ul><li>基于Pod做HPA</li><li>基于cpu或者memory做HPA</li></ul> </li><li>启动HPA的伸缩控制器 
      <ul><li>hpa验证测试</li></ul> </li></ul> </li><li>配置grafana展示qps监控数据</li><li>hpa命令清单</li><li>高阶知识：Adapter 的Discovery规则如何配置？ 
    <ul><li>以获取Per-pod HTTP Requests为例 
      <ul><li>1 demo问题场景</li><li>2 配置适配器</li><li>3 查询api</li></ul> </li></ul> </li><li>prometheus operator 不足之处 
    <ul><li>数据持久化</li><li>tsdb 保留天数</li><li>告警方式不方便</li><li>加监控target也不方便</li></ul> </li></ul> </li><li>第14部分：k8s生产环境容器内部JVM参数配置解析及优化 
  <ul><li>Java Heap基础知识</li><li>容器环境的Java Heap 
    <ul><li>UseContainerSupport</li></ul> </li><li>最佳实践 
    <ul><li>常用容器内存大小对应的jvm内存配置 
      <ul><li>容器启动常用配置</li></ul> </li><li>配置项的具体介绍 
      <ul><li>1.堆总内存初始化大小分配和最大值分配</li><li>2.非堆总内存初始化大小分配和最大值分配（1.8为metaspace）</li><li>3.堆内存之年轻代年老代大小设置</li><li>4.线程栈大小</li><li>5.GC日志输出</li><li>6.每个线程堆栈大小</li></ul> </li></ul> </li><li>Kubernetes（k8s）配置Java服务自动Dump</li><li>参考：Go语音解决 java 内存溢出时候 dump 文件的存储问题 
    <ul><li>问题</li><li>方案</li><li>其他</li></ul> </li></ul> </li><li>参考文献</li></ul> 
<h3><a id="1Cloud_Native_785"></a>第1部分：云原生(Cloud Native)的原理与演进</h3> 
<p>云原生之所以解释不清楚，是因为云原生没有确切的定义，</p> 
<p>云原生一直在发展变化之中，解释权不归某个人或组织所有。</p> 
<p>如果要理解 Cloud Native， 首先就要理解字面意思 ，大概是：</p> 
<ul><li>首先生在云上 ，不是在本地</li><li>本身生在云上 ，不是在本地</li><li>天生长在云上 ，不是在本地</li><li>土生土长在云上 ，不是在本地</li></ul> 
<p>所以，这个概念，是建立在 云基础设施基础上的。 云基础设施有： 虚拟服务器、虚拟容器 等，代表作就是K8S。</p> 
<p>好，接下来，就看看 官方介绍的 什么是云原生(Cloud Native)。</p> 
<h4><a id="1Cloud_Native_802"></a>1、什么是云原生(Cloud Native)？</h4> 
<p>云原生之所以解释不清楚，是因为云原生没有确切的定义，</p> 
<p>云原生一直在发展变化之中，解释权不归某个人或组织所有。</p> 
<p>Pivotal公司的Matt Stine于2013年首次提出云原生（Cloud Native）的概念；</p> 
<p>2015年，云原生刚推广时，</p> 
<p>Matt Stine在《迁移到云原生架构》一书中定义了符合云原生架构的几个特征：</p> 
<ul><li>符合 12 因素应用、</li><li>面向微服务架构、</li><li>自敏捷架构、</li><li>基于API协作、</li><li>扛脆弱性；</li></ul> 
<blockquote> 
 <p>Pivotal 推出过 Pivotal Cloud Foundry 云原生应用平台和 Spring 开源 Java 开发框架，成为云原生应用架构中先驱者和探路者。</p> 
 <p>Pivotal 是云原生应用平台第一股，2018 年在纽交所上市，2019 年底被 VMWare 以 27 亿美元收购，加入到 VMware 新的产品线 Tanzu。</p> 
</blockquote> 
<p>到了 2015 年 Google 主导成立了云原生计算基金会（CNCF），开始围绕云原生的概念打造云原生生态体系，起初CNCF对云原生的定义包含以下三个方面：</p> 
<ul><li>应用容器化(software stack to be Containerized)</li><li>面向微服务架构(Microservices oriented)</li><li>应用支持容器的编排调度(Dynamically Orchestrated)</li></ul> 
<p>2017年, 云原生应用的提出者之一的Pivotal在其官网上将云原生的定义概况为DevOps、持续交付、微服务、容器这四大特征，这也成了很多人对 Cloud Native的基础印象。</p> 
<p><img src="https://images2.imgbox.com/9b/ac/rvlhN5eE_o.png" alt=""></p> 
<h5><a id="_836"></a>云原生的四要素：</h5> 
<ul><li>持续交付、</li><li>DevOps、</li><li>微服务、</li><li>容器：</li></ul> 
<p><img src="https://images2.imgbox.com/c1/a0/KXwoEZ2Z_o.png" alt=""></p> 
<p>1） DevOps</p> 
<p>DevOps（Development和Operations的组合词）即开发、运维一体化。</p> 
<p>涉及软件在整个开发生命周期中的持续开发，持续测试，持续集成，持续部署和持续监控。<br> 最佳实践：Git，Jenkins，Bamboo，Docker，Kubernetes</p> 
<p>2） 持续交付</p> 
<p>持续交付：持续交付是不误时开发，不停机更新，小步快跑，反传统瀑布式开发模型，这要求开发版本和稳定版本并存，其实需要很多流程和工具支撑。</p> 
<p>最佳实践：CI/CD, gitlab， Jenkins，流水线pipeline，tekton等</p> 
<p>3） 微服务 （Microservice）</p> 
<p>几乎每个云原生的定义都包含微服务，跟微服务相对的是单体应用，</p> 
<p>微服务有理论基础，那就是康威定律，指导服务怎么切分。</p> 
<p>4） 容器 （Container）</p> 
<p>2013年，Docker项目正式发布，2014年，K8s项目也正式发布。</p> 
<p>Docker是应用最为广泛的容器引擎，在思科谷歌等公司的基础设施中大量使用。</p> 
<p>K8S是容器编排系统，用于容器管理，容器间的负载均衡</p> 
<h4><a id="2_884"></a>2、云原生发展历史时间轴</h4> 
<h5><a id="21__888"></a>2.1 从微服务到服务网格</h5> 
<p><img src="https://images2.imgbox.com/e7/3c/2Jp75iW3_o.png" alt=""></p> 
<ul><li>微服务</li></ul> 
<p>马丁大师在2014年定义了微服务</p> 
<ul><li>Kubernetes</li></ul> 
<p>从2014年6月由Google宣布开源，</p> 
<p>到2015年7月发布1.0这个正式版本并进入CNCF基金会，再到2018年3月从CNCF基金会正式毕业，迅速成为容器编排领域的标准，是开源历史上发展最快的项目之一</p> 
<ul><li>Linkerd</li></ul> 
<p>Scala语言编写，运行在JVM中，Service Mesh名词的创造者</p> 
<p>2016年01月15号，0.0.7发布</p> 
<p>2017年01月23号，加入CNCF组织</p> 
<p>2017年04月25号，1.0版本发布</p> 
<ul><li>Envoy</li></ul> 
<p>envoy 是一个开源的服务代理，为云原生设计的程序，由C++语言编程[Lyft]<br> 2016年09月13号，1.0发布<br> 2017年09月14号，加入CNCF组织</p> 
<ul><li>Istio</li></ul> 
<p>Google、IBM、Lyft发布0.1版本</p> 
<p>Istio是开源的微服务管理、保护和监控框架。Istio为希腊语，意思是”起航“。</p> 
<h5><a id="22__928"></a>2.2 微服务架构的问题</h5> 
<p><strong>微服务时代有了Spring Cloud就完美了吗？不妨想一想会有哪些问题?</strong></p> 
<p>（1）最初是为了业务而写代码，比如登录功能、支付功能等，到后面会发现要解决网络通信的问题，虽然有 Spring Cloud里面的组件帮我们解决了，但是细想一下它是怎么解决的？</p> 
<p>在业务代码里面加上spring cloud maven依赖，加上spring cloud组件注解，写配置，打成jar的时候还必须要把非业务的代码也要融合在一起，称为“侵入式框架”；</p> 
<p>（2）微服务中的服务支持不同语言开发，也需要维护不同语言和非业务代码的成本；</p> 
<p>（3）业务代码开发者应该把更多的精力投入到业务熟悉度上，而不应该是非业务上，Spring Cloud虽然能解决微服务领域的很多问题，但是学习成本还是较大的；</p> 
<p>（4）互联网公司产品的版本升级是非常频繁的，为了维护各个版本的兼容性、权限、流量等，</p> 
<p>因为Spring Cloud是“代码侵入式的框架”，这时候版本的升级就注定要让非业务代码一起，一旦出现问题，再加上多语言之间的调用，工程师会非常痛苦；</p> 
<p>（5）其实我们到目前为止应该感觉到了，服务拆分的越细，只是感觉上轻量级解耦了，但是维护成本却越高了，那么怎么 办呢？</p> 
<p>我们不是说spring cloud不好，只是为了引出service mesh,</p> 
<p>目前spring cloud微服务还是比较主流的， 我们指出spring cloud的不好也只是为了突出service mesh的优点</p> 
<p><strong>问题解决思路</strong></p> 
<ul><li> <p>本质上是要解决服务之间通信的问题，不应该将非业务的代码融合到业务代码中</p> </li><li> <p>也就是从客户端发出的请求，要能够顺利到达对应的服务，这中间的网络通信的过程要和业务代码尽量无关</p> <p>服务通信无非就是服务发现、负载均衡、版本控制等等</p> </li><li> <p>在很早之前的单体架构中，其实通信问题也是需要写在业务代码中的，那时候怎么解决的呢？</p> <p><img src="https://images2.imgbox.com/e2/d3/ubVvnga6_o.png" alt=""></p> </li></ul> 
<h6><a id="_962"></a>解决方案：</h6> 
<p>把网络通信，流量转发等问题放到了计算机网络模型中的TCP/UDP层，也就是非业务功能代码下沉，</p> 
<p>把这些网络的问题下沉到计算机网络模型当中，也就是网络七层模型</p> 
<p>网络七层模型：应用层、表示层、会话层、传输层、网络层、数据链路层、物理层</p> 
<p><img src="https://images2.imgbox.com/7d/d6/kGJy7oMj_o.png" alt=""></p> 
<p><strong>思考：</strong></p> 
<p>我们是否也可以把每个服务配置一个代理，所有通信的问题都交给这个代理去做，</p> 
<p>就好比大家熟悉的nginx,haproxy其实它们做反向代理把请求转发给其他服务器,也就为 Service Mesh的诞生和功能实现提供了一个解决思路</p> 
<h5><a id="23_SideCar__982"></a>2.3 SideCar 旁车模式（边车模式）</h5> 
<p>Sidecar模式是一种将应用功能从应用本身剥离出来作为单独进程的方式。</p> 
<p>SideCar降低了与微服务架构相关的复杂性，并且提供了负载平衡、服务发现、流量管理、电路中断、遥测、故障注入等基础特性。</p> 
<p><img src="https://images2.imgbox.com/73/ab/sxYfnZJZ_o.png" alt=""></p> 
<p>服务业务代码与Sidecar绑定在一起，每个服务都配置了一个Sidecar代理，每个服务所有的流量都经过sidecar,</p> 
<p>sidecar帮我们屏蔽了通信的细节，我的业务开发人员只需要关注业务就行了，而通信的事情交给sidecar处理</p> 
<p>sidecar是为了通用基础设施而设计，可以做到与公司框架技术无侵入性</p> 
<p>该模式允许我们向应用无侵入添加多种功能，避免了为满足第三方组件需求而向应用添加额外的配置代码。</p> 
<p>很多公司借鉴了Proxy模式，推出了Sidecar的产品，比如像Netflix的Prana，蚂蚁金服的SofaMesh</p> 
<p>很多公司借鉴了Proxy模式，推出了Sidecar的产品， 比如像Netflix的Prana，蚂蚁金服的SofaMesh</p> 
<p>2014年 Netflix发布的Prana</p> 
<p>2015年 唯品会发布local proxy</p> 
<p>2016年 Twitter的基础设施工程师发布了第一款Service Mesh项目：Linkerd (所以下面介绍Linkerd)</p> 
<h4><a id="32018_CNCF_1016"></a>3、2018 年云原生被CNCF重新定义</h4> 
<p>到了 2018 年，随着近几年来云原生生态的不断壮大，</p> 
<p>所有主流云计算供应商都加入了该基金会，</p> 
<p>且从 Cloud Native Landscape 中可以看出, 云原生有意蚕食原先非云原生应用的部分。</p> 
<p>CNCF 基金会中的会员以及容纳的项目越来越多，该定义已经限制了云原生生态的发展，CNCF 为云原生进行了重新定位。</p> 
<p>2018年6月，CNCF正式对外公布了更新之后的云原生的定义（包含中文版本）v1.0版本：</p> 
<p><a href="https://github.com/cncf/toc/blob/main/DEFINITION.md">CNCF官方对Cloud Native的定义</a>:</p> 
<p>Cloud native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach.</p> 
<p>These techniques enable loosely coupled systems that are resilient, manageable, and observable. Combined with robust automation, they allow engineers to make high-impact changes frequently and predictably with minimal toil.</p> 
<p>The Cloud Native Computing Foundation seeks to drive adoption of this paradigm by fostering and sustaining an ecosystem of open source, vendor-neutral projects. We democratize state-of-the-art patterns to make these innovations accessible for everyone.</p> 
<p><strong>中文版本：</strong></p> 
<p>云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。</p> 
<p><strong>云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式API。</strong></p> 
<p>这些技术能够构建容错性好、易于管理和便于观察的松耦合系统。结合可靠的自动化手段，云原生技术使工程师能够轻松地对系统作出频繁和可预测的重大变更。</p> 
<p>云原生计算基金会（CNCF）致力于培育和维护一个厂商中立的开源生态系统，来推广云原生技术。</p> 
<p>我们通过将最前沿的模式民主化，让这些创新为大众所用。</p> 
<p><img src="https://images2.imgbox.com/28/65/wILMg3Hi_o.png" alt=""></p> 
<p>新的定义中，继续保持原有的核心内容：容器和微服务，</p> 
<p>但是非常特别的是：</p> 
<p>将服务网格单独列出来，而不是将服务网格作为微服务的一个子项或者实现模式，体现了云原生中服务网格这一个新生技术的重要性。</p> 
<p>而不可变基础设施和声明式API这两个设计指导理念的加入，则强调了这两个概念对云原生架构的影响和对未来发展的指导作用。</p> 
<h5><a id="31__Service_Mesh_1060"></a>3.1 服务网格 （Service Mesh）</h5> 
<p>微服务技术架构实践中主要有侵入式架构和非侵入式架构两种实现形式。</p> 
<p>侵入式架构是指服务框架嵌入程序代码，开发者组合各种组件，如RPC、负载均衡、熔断等，实现微服务架构。</p> 
<p>非侵入式架构则是以代理的形式，与应用程序部署在一起，接管应用程序的网络且对其透明，开发者只需要关注自身业务即可，以服务网格为代表。</p> 
<p>为了解决微服务框架的侵入性问题，引入Service Mesh。</p> 
<p>Serice Mesh提供了专业化的解决方案，其中所涉及的服务通信、容错、认证等功能，都是专业度极高的领城，这些领城应该出现工业级成熟度的制成品，这对于中小企业来说是一个降低成本的选择。</p> 
<pre><code>Service Mesh的开源软件包括Istio、Linkerd、 Envoy、 Dubbo Mesh等。

同时，为了让Service Mesh有更好的底层支撑，我们又将Service Mesh运行在Kubernetes上。
</code></pre> 
<h5><a id="32_Immutable_Infrastructure_1080"></a>3.2 不可变基础设施（Immutable Infrastructure）</h5> 
<p>What is “Immutable Infrastructure”?</p> 
<p>参考URL：https://www.armory.io/blog/what-is-immutable-infrastructure/</p> 
<p>不可变基础设施里的“不可变”非常类似于程序设计中的“不可变”概念。</p> 
<p>程序设计中，不可变变量（Immutable Variable）就是在完成赋值后就不能发生更改，只能创建新的来整体替换旧的。</p> 
<p><strong>对于基础设施的不可变性，最基本的就是指运行服务的服务器在完成部署后，就不再进行更改。</strong></p> 
<p>可变基础设施通常会导致以下问题：</p> 
<ul><li>在灾难发生的时候，难以重新构建服务。持续过多的手工操作，缺乏记录，会导致很难由标准初始化后的服务器来重新构建起等效的服务。</li><li>在服务运行过程中，持续的修改服务器，就犹如程序中的可变变量的值发生变化而引入的状态不一致的并发风险。这些对于服务器的修改，同样会引入中间状态，从而导致不可预知的问题。</li></ul> 
<p>总结：不可变基础设施其实就是生产环境下基础设施不能改动，如果改动，已切都是被记录的，是可以回溯的。</p> 
<h5><a id="33_API__declarative_APIs_1105"></a>3.3 声明式API ( declarative APIs)</h5> 
<p>Declarative APIs<br> https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/</p> 
<p>声明式 API 的 “声明式” 是什么意思？</p> 
<p>对于我们使用 Kubernetes API 对象的方式，一般会编写对应 API 对象的 YAML 文件交给 Kubernetes（而不是使用一些命令来直接操作 API）。<br> 所谓“声明式”，指的就是我只需要提交一个定义好的 API 对象来“声明”（这个 YAML 文件其实就是一种“声明”），表示所期望的最终状态是什么样子就可以了。而如果提交的是一个个命令，去指导怎么一步一步达到期望状态，这就是“命令式”了。<br> “命令式 API”接收的请求只能一个一个实现，否则会有产生冲突的可能；“声明式API”一次能处理多个写操作，并且具备 Merge 能力。<br> Kubernetes 有很多能力，这些能力都是通过各种 API 对象来提供。也就是说，API 对象正是我们使用 Kubernetes 的接口，我们正是通过操作这些提供的 API 对象来使用 Kubernetes 能力的。</p> 
<p>总结：声明式API其实就是所有资源抽象，抽象成api。这些api 标准化，相当于规范标准了。</p> 
<h4><a id="4CNCF_1121"></a>4、CNCF云原生组织发展和介绍</h4> 
<p>CNCF 是一个开源软件基金会，致力于使云原生计算具有普遍性和可持续性。</p> 
<p>云原生计算使用开源软件技术栈将应用程序部署为微服务，将每个部分打包到自己的容器中，并动态编排这些容器以优化资源利用率。</p> 
<p>云原生技术使软件开发人员能够更快地构建出色的产品。</p> 
<h5><a id="CNCF_1133"></a>什么是CNCF</h5> 
<p>cncf官网：https://www.cncf.io/</p> 
<p>CNCF基金会是 2015年由Linux基金会发起了一个 The Cloud Native Computing Foundation（CNCF）基金组织，</p> 
<p>CNCF基金会的成立标志着云原生正式进入高速发展轨道，Google、Cisco、Docker各大厂纷纷加入，并逐步构建出围绕 Cloud Native 的具体工具，而云原生这个的概念也逐渐变得更具体化。</p> 
<p>CNCF最初对云原生定义是也是狭窄的，当时把云原生定位为容器化封装+自动化管理+面向微服务。</p> 
<p>CNCF的目的不一样，他成立的目的是希望打破云巨头的垄断，实际上是希望通过容器和k8s，将提供底层资源的云服务商变得无差异化。</p> 
<p>这主要因为CNCF基金会在当时的核心拳头软件就是k8s，因此在概念定义上主要是围绕着容器编排建立起来的生态。</p> 
<p>这也是为什么我们感觉CNCF 定义云原生的时候就是在说容器生态。</p> 
<p>CNCF 是非营利性 Linux 基金会的一部分。</p> 
<p>官网介绍：<br> Building sustainable ecosystems for cloud native software</p> 
<p><em>为云原生软件建立可持续的生态系统</em></p> 
<p>Cloud Native Computing Foundation (CNCF) serves as the vendor-neutral home for many of the fastest-growing open source projects, including Kubernetes, Prometheus, and Envoy.</p> 
<p><em>云本地计算基金会(CNCF)是许多增长最快的开放源码项目的供应商中立主机，其中包括 Kubernetes、 Prometheus 和 truster。</em></p> 
<blockquote> 
 <p>CNCF 致力于培育和维护一个厂商中立的开源社区生态，<strong>用以推广云原生技术</strong>。</p> 
</blockquote> 
<p>从 2015 年 Google 牵头成立 CNCF 以来，云原生技术开始进入公众的视线并取得快速的发展，到 2018 年包括 Google、AWS、Azure、Alibaba Cloud 等大型云计算供应商都加入了云原生基金会 CNCF，云原生技术也从原来的应用容器化发展出包括容器、Service Mesh、微服务、不可变基础设施、Serverless、FaaS 等众多技术方向。</p> 
<h5><a id="CNCF_1168"></a>CNCF解决了什么问题</h5> 
<ul><li>统一基础平台：kubernetes</li><li>如果我们需要日志监控：Prometheus</li><li>需要代理：Envoy</li><li>需要分布式链路跟踪：Jaeger</li><li>…</li></ul> 
<p><code>地址</code>：https://www.cncf.io/</p> 
<h5><a id="_1182"></a>介绍几个常用的已经毕业的云原生项目</h5> 
<p><strong>Kubernetes</strong></p> 
<p>Kubernetes 是世界上最受欢迎的容器编排平台也是第一个 CNCF 项目。 Kubernetes 帮助用户构建、扩展和管理应用程序及其动态生命周期。</p> 
<p><strong>Prometheus</strong></p> 
<p>Prometheus 为云原生应用程序提供实时监控、警报包括强大的查询和可视化能力，并与许多流行的开源数据导入、导出工具集成。</p> 
<p><strong>Jaeger</strong></p> 
<p>Jaeger 是由 Uber 开发的分布式追踪系统，用于监控其大型微服务环境。 Jaeger 被设计为具有高度可扩展性和可用性，它具有现代 UI，旨在与云原生系统（如 OpenTracing、Kubernetes 和 Prometheus）集成。</p> 
<p><strong>Containerd</strong></p> 
<p>Containerd 是由 Docker 开发并基于 Docker Engine 运行时的行业标准容器运行时组件。<br> 作为容器生态系统的选择，Containerd 通过提供运行时，可以将 Docker 和 OCI 容器镜像作为新平台或产品的一部分进行管理。</p> 
<p><strong>Envoy</strong></p> 
<p>Envoy 是最初在 Lyft 创建的 Service Mesh（服务网格），现在用于Google、Apple、Netflix等公司内部。 Envoy 是用 C++ 编写的，旨在最大限度地减少内存和 CPU 占用空间，同时提供诸如负载均衡、网络深度可观察性、微服务环境中的跟踪和数据库活动等功能。</p> 
<p><strong>Fluentd</strong></p> 
<p>Fluentd 是一个统一的日志记录工具，可收集来自任何数据源（包括数据库、应用程序服务器、最终用户设备）的数据，并与众多警报、分析和存储工具配合使用。<br> Fluentd 通过提供一个统一的层来帮助用户更好地了解他们的环境中发生的事情，以便收集、过滤日志数据并将其路由到许多流行的源和目的地。</p> 
<h5><a id="_1222"></a>孵化中的项目</h5> 
<p><strong>Open Tracing</strong></p> 
<p>OpenTracing：为不同的平台，供应中立的API，使开发人员可以轻松地应用分布式跟踪。</p> 
<p><strong>GRPC</strong></p> 
<p>gRPC 是一个高性能、开源和通用的 RPC 框架,语言中立，支持多种语言。</p> 
<p><strong>CNI</strong></p> 
<p>CNI 就是这样一个标准，它旨在为容器平台提供网络的标准化。不同的容器平台能够通过相同的接口调用不同的网络组件。</p> 
<p><strong>Helm</strong></p> 
<p>Helm 是 Kubernetes 的包管理器。包管理器类似于我们在Centos中使用的yum一样，能快速查找、下载和安装软件包。</p> 
<p><strong>Etcd</strong></p> 
<p>一个高可用的分布式键值(key-value)数据库。etcd内部采用raft协议作为一致性算法，etcd基于Go语言实现。一般用的最多的就是作为一个注册中心来使用</p> 
<h4><a id="5Service_Mesh_1258"></a>5、从微服务演进到Service Mesh（服务网格）的过程</h4> 
<p>CNCF非常特别的是：</p> 
<p>将服务网格单独列出来，而不是将服务网格作为微服务的一个子项或者实现模式，体现了云原生中服务网格这一个新生技术的重要性。</p> 
<p>来看看 从微服务演进到Service Mesh（服务网格）的过程</p> 
<h5><a id="51__1268"></a>5.1 单体服务时代</h5> 
<p>2010年前，大多数服务业务单一且简单，采用典型的单机+数据库模式，</p> 
<p>所有的功能都写在一个应用里并进行集中部署</p> 
<p><img src="https://images2.imgbox.com/69/a3/n1ibzrhr_o.png" alt=""></p> 
<p>论坛业务、聊天室业务、邮箱业务全部都耦合在一台小型机上面，所有的业务数据也都存储在一台数据库上。</p> 
<p>随着应用的日益复杂与多样化，开发者对系统的容灾，伸缩以及业务响应能力有了更高的要求</p> 
<p>如果小型机和数据库中任何一个出现故障，整个系统都会崩溃，</p> 
<p>若某个板块的功能需要更新，那么整个系统都需要重新发布，</p> 
<p>如何保障可用性的同时快速响应业务的变化，需要将系统进行拆分，将上面的应用拆分出多个子应用。</p> 
<p><img src="https://images2.imgbox.com/4b/18/GAvsA86P_o.png" alt=""></p> 
<p>优点：应用跟应用解耦，系统容错提高了，也解决了独立应用发布的问题</p> 
<p>应用垂直拆分解决了应用发布的问题，但是随着用户数量的增加，单机的计算能力依旧是杯水车薪</p> 
<p>用户量越来越大，同时也要进行高可用保证， 大部分系统，需要进行横向扩展（水平扩展）</p> 
<p>在接入层，引入负载均衡组件，有了负载均衡之后，架构图如下</p> 
<p><img src="https://images2.imgbox.com/5e/77/fv9joMpB_o.png" alt=""></p> 
<p>阿里巴巴在2008提出去“IOE”，也就是IBM小型机、Oracle数据库，EMC存储，</p> 
<p>全部改成集群化负载均衡架构，在2013年支付宝最后一台IBM小型机下线</p> 
<p>优点：</p> 
<p>应用跟应用解耦，系统容错提高了，也解决了独立应用发布的问题，同时可以水平扩展来提供应用的并发量</p> 
<h5><a id="52__1318"></a>5.2 微服务时代</h5> 
<p><strong>分布式微服务时代</strong></p> 
<p>微服务是在2012年提出的概念，微服务的希望的重点是一个服务只负责一个独立的功能。</p> 
<p>拆分原则，任何一个需求不会因为发布或者维护而影响到不相关的服务，一切可以做到独立部署运维。</p> 
<p>比如传统的“用户中心”服务，对于微服务来说，需要根据业务再次拆分，可能需要拆分成“买家服务”、“卖家服务”、“商家服务”等。</p> 
<p>典型代表：</p> 
<p>Spring Cloud，相对于传统分布式架构，SpringCloud使用的是HTTP作为RPC远程调用，配合上注册中心Eureka和API网关Zuul，可以做到细分内部服务的同时又可以对外暴露统一的接口，让外部对系统内部架构无感，此外Spring Cloud的config组件还可以把配置统一管理。</p> 
<p>马丁大师对微服务的定义：https://martinfowler.com/articles/microservices.html</p> 
<p>微服务真正定义的时间是在2014年</p> 
<pre><code>The term "Microservice Architecture" has sprung up over the last few years to describe a particular way of designing software applications as suites of independently deployable services. While there is no precise definition of this architectural style, there are certain common characteristics around organization around business capability, automated deployment, intelligence in the endpoints, and decentralized control of languages and data.
</code></pre> 
<p>大概意思：可独立部署服务，服务会越来越细</p> 
<p>spring cloud<code>地址</code>：https://spring.io/projects/spring-cloud</p> 
<p>集群部署多了，这些重复的功能无疑会造成资源浪费，所以会把重复功能抽取出来，名字叫"XX服务（Service）"</p> 
<p><img src="https://images2.imgbox.com/78/ac/mrz07Pya_o.png" alt=""></p> 
<p>为了解决服务跟服务如何相互调用，需要一个程序之间的通信协议，所以就有了远程过程调用（RPC），作用就是让服务之间的程序调用变得像本地调用一样的简单</p> 
<h5><a id="53__service_mesh_1356"></a>5.3 服务网格新时期 （service mesh）</h5> 
<p>服务网格和微服务的本质区别，主要在于 单体服务进行 业务服务和 非业务基础 服务进行解耦</p> 
<p>非业务基础 服务，交给基础框架进行 管理和控制</p> 
<p>非业务基础 （sidecar）服务 + 服务治理 中间件 （Pilot/mixor），组成一个网络结构， 被称之为服务网格</p> 
<p><strong>如下图所示</strong></p> 
<p><img src="https://images2.imgbox.com/58/7c/MuapmThc_o.png" alt=""></p> 
<p>为啥叫做服务网格，主要 从架构层面看起来跟网格很像，</p> 
<p>如果每一个格子都是一个sidecar数据单位（单元格），然后sidecar进行彼此通信，所以这些单元格 组成 <strong>数据面</strong></p> 
<p>这些 sidecar数据单位（单元格），由统一的控制/配置中间组件（类似nacos注册中心）进行配置和控制， 那些控制组件 组成 了 <strong>控制面</strong></p> 
<p>宏观上 ，服务网格 =<strong>数据面 + 控制面</strong></p> 
<p><strong>特点：</strong></p> 
<ul><li>基础设施：</li></ul> 
<p>服务网格是一种处理服务之间通信的基础设施层。</p> 
<ul><li>支撑云原生：</li></ul> 
<p>服务网格尤其适用于在云原生场景下帮助应用程序在复杂的服务间可靠地传递请求。</p> 
<ul><li>网络代理：</li></ul> 
<p>在实际使用中，服务网格一般是通过一组轻量级网络代理来执行治理逻辑的。</p> 
<ul><li>对应用透明：</li></ul> 
<p>轻量网络代理与应用程序部署在一起，但应用感知不到代理的存在，还是使用原来的方式工作。</p> 
<h4><a id="6service_mesh_1404"></a>6、服务网格（service mesh）原理和价值</h4> 
<h5><a id="service_mesh_1408"></a>什么是服务网格（service mesh）</h5> 
<p><strong>istio官网 也对什么是service mesh给出了定义</strong></p> 
<p><code>地址</code>：https://istio.io/docs/concepts/what-is-istio/#what-is-a-service-mesh</p> 
<pre><code>Istio addresses the challenges developers and operators face as monolithic applications transition towards a distributed microservice architecture. To see how, it helps to take a more detailed look at Istio’s service mesh. 
</code></pre> 
<p>翻译：</p> 
<p>解决开发与运维部署分布式微服务面临的问题</p> 
<pre><code>The term service mesh is used to describe the network of microservices that make up such applications and the interactions between them. As a service mesh grows in size and complexity, it can become harder to understand and manage. Its requirements can include discovery, load balancing, failure recovery, metrics, and monitoring. A service mesh also often has more complex operational requirements, like A/B testing, canary rollouts, rate limiting, access control, and end-to-end authentication.
</code></pre> 
<p>翻译：</p> 
<p>也是解决微服务之间服务跟服务之间通信的问题，可以包括服务发现、负载平衡、故障恢复、度量和监视，服务网格通常还具有更复杂的操作需求，如A/B测试、速率限制、访问控制和端到端身份验证</p> 
<p>服务网格：指的是微服务网络应用之间的交互，随着规模和复杂性增加，服务跟服务调用错综复杂</p> 
<p><strong>如下图所示</strong></p> 
<p><img src="https://images2.imgbox.com/e3/d2/HclD0uYv_o.png" alt=""></p> 
<p>如果每一个格子都是一个sidecar数据单元，然后sidecar进行彼此通信，从架构层面看起来跟网格很像，整体组成一个服务网格 service mech</p> 
<p>服务网格将 “业务服务”和“基础设施”解耦，将一个微服务进程一分为二：</p> 
<p><img src="https://images2.imgbox.com/cf/3d/WLWJbGuk_o.png" alt="图片"></p> 
<p>特点：</p> 
<ul><li>基础设施：服务网格是一种处理服务之间通信的基础设施层。</li><li>支撑云原生：服务网格尤其适用于在云原生场景下帮助应用程序在复杂的服务间可靠地传递请求。</li><li>网络代理：在实际使用中，服务网格一般是通过一组轻量级网络代理来执行治理逻辑的。</li><li>对应用透明：轻量网络代理与应用程序部署在一起，但应用感知不到代理的存在，还是使用原来的方式工作。</li></ul> 
<h5><a id="Service_Mesh_1461"></a>Service Mesh的价值</h5> 
<p>服务网格将 “业务服务”和“基础设施”解耦，</p> 
<p>Service Mesh上的sidecar支撑了所有的上层应用，业务开发者无须关心底层构成，可以用Java，也可以用Go等语言完成自己的业务开发。</p> 
<p>Istio的理论概念是Service Mesh（服务网络），我们不必纠结于概念实际也是微服务的一种落地形式有点类似上面的SideCar模式。</p> 
<p>Service Mesh 的主要思想是职责解耦、责任清晰，即不像SpringCloud一样将服务治理交给研发来做，也不集成到k8s中产生职责混乱，</p> 
<p>Istio是通过为服务配 Agent代理来提供服务发现、负截均衡、限流、链路跟踪、鉴权等微服务治理手段。</p> 
<p>Istio开始就是与k8s结合设计的，Istio结合k8s可以牛逼的落地微服务架构。</p> 
<p>istio 超越 spring cloud等传统开发框架之处, 就在于不仅仅带来了远超这些框架所能提供的功能, 而且也不需要应用程序为此做大量的改动，开发人员也不必为上面的功能实现进行大量的知识储备。</p> 
<h5><a id="Linkerd_1481"></a>Linkerd</h5> 
<p>2016年1月，离开Twitter的基础设施工程师打造的一个服务网格项目 Linkerd,</p> 
<p>第一个Service Mesh项目由此诞生，解决通用性。</p> 
<p>Linkerd很好地结合了Kubernetes所提供的功能，</p> 
<p>以此为基础，在每个Kubernetes Node上都部署运行一个Linkerd实例，用代理的方式将加入Mesh的Pod通信转接给Linkerd，这样Linkerd就能在通信链路中完成对通信的控制和监控。</p> 
<p><strong>Linkerd设计思想</strong></p> 
<p><img src="https://images2.imgbox.com/0f/c0/ViXBnXf2_o.png" alt=""></p> 
<p>Linderd的思想跟sidecar很类似，目标也是屏蔽网络通信细节</p> 
<p>Linkerd除了完成对Service Mesh的命名，以及Service Mesh各主要功能的落地，还有以下重要创举：</p> 
<ul><li>无须侵入工作负载的代码，直接进行通信监视和管理；</li><li>提供了统一的配置方式，用于管理服务之间的通信和边缘通信；</li><li>除了支持Kubernetes，还支持多种底层平台。</li></ul> 
<p><strong>总结：</strong></p> 
<ul><li>跟我们前面sidecar很类似，以前的调用方式都是服务来调用服务，在Linkerd思想要求所有的流量都走sidecar，Linkerd帮业务人员屏蔽了通信细节，通信不需要侵入到业务代码内部了，这样业务开发者就专注于业务开发的本身</li><li>Linkerd在面世之后，迅速获得用户的关注，并在多个用户的生产环境上成功部署、运行。2017年，Linkerd加入CNCF，随后宣布完成对千亿次生产环境请求的处理，紧接着发布了1.0版本，并且具备一定数量的商业用户，一时间风光无限，一直持续到Istio横空出世。</li></ul> 
<p><strong>问题:</strong></p> 
<p>在早期的时候又要部署服务，又要部署sidecar，对于运维人员来说比较困难的，</p> 
<p>所以没有得到很好的发展，其实主要的 问题是Linkerd只是实现了数据层面的问题，但没有对其进行很好的管理。</p> 
<p>或者说， Linkerd 没有实现一套好的 <strong>控制面</strong> 组件。</p> 
<h5><a id="istio_1520"></a>istio</h5> 
<p>由Google、IBM和Lyft共同发起的开源项目</p> 
<p>istio，是由go语言编写的</p> 
<p><strong>什么是istio?</strong></p> 
<p><code>地址</code>：https://istio.io/docs/concepts/what-is-istio/#why-use-istio</p> 
<pre><code> Istio makes it easy to create a network of deployed services with load balancing, service-to-service authentication, monitoring, and more, with few or no code changes in service code. You add Istio support to services by deploying a special sidecar proxy throughout your environment that intercepts all network communication between microservices, then configure and manage Istio using its control plane functionality
</code></pre> 
<p>通过Istio，可以轻松创建带有负载平衡，服务到服务的身份验证，监视等功能的已部署服务网络，使得服务中的代码更改很少或没有更改。</p> 
<p>通过在整个环境中部署一个特殊的sidecar代理来拦截微服务之间的所有网络通信，然后使用其控制平面功能配置和管理，可以为服务添加Istio支持。</p> 
<p><strong>注意这句话：</strong></p> 
<p><em>通过Istio，使得服务中的代码更改很少或没有更改, 可以轻松创建 微服务。</em></p> 
<p>如果我们用的是spring cloud，要加依赖、加注解、改配置 依赖组件，才能完成 创建 微服务</p> 
<p>istio 提供了很好的 控制面 组件。</p> 
<p><strong>什么是控制平面？</strong> 控制平面就是来管理数据平面，也就是管理sideCar</p> 
<p><img src="https://images2.imgbox.com/13/8f/iihd9C9T_o.png" alt=""></p> 
<p><strong>所以istio既有数据平面也有很好控制平面</strong></p> 
<p><strong>istio能干什么?</strong></p> 
<pre><code> Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic.
 Fine-grained control of traffic behavior with rich routing rules, retries, failovers, and fault injection.
 A pluggable policy layer and configuration API supporting access controls, rate limits and quotas.
 Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress.
 Secure service-to-service communication in a cluster with strong identity-based authentication and authorization.
</code></pre> 
<ul><li>1.HTTP、gRPC、WebSocket和TCP流量的自动负载平衡。</li><li>2.对流量行为的 路由、重试、故障转移和错误 进行细粒度控制。</li><li>3.支持访问控制、速率限制、配置API。</li><li>4.集群内所有流量的自动衡量、日志和跟踪，包括集群入口和出口。</li><li>5.使用基于身份验证和授权来保护集群中服务跟服务之间的通信。</li></ul> 
<p>**总结：**很明显Istio不仅拥有“数据平面（Data Plane）”，而且还拥有“控制平面（Control Plane），也就是拥有了数据 接管与集中控制能力。</p> 
<h5><a id="_1576"></a>国内兴起的服务网格</h5> 
<p>前面提到，在Service Mesh这个概念得到具体定义之前，实际上已经有很多厂商开始了微服务新的尝试，这一动作势必引发对微服务治理的强劲需求。</p> 
<p>在Service Mesh概念普及之后，有的厂商意识到自身产品也具备Service Mesh的特点，也有厂商受其启发，将自有的服务治理平台进行完善和改造，推出自己的Service Mesh产品。</p> 
<p>例如，蚂蚁金服、腾讯和华为都推出自己的网格产品，华为的产品甚至已被投入公有云进行商业应用。</p> 
<ul><li>蚂蚁金服 sofa Mesh</li></ul> 
<p><img src="https://images2.imgbox.com/a8/b1/hvRKWISp_o.png" alt=""></p> 
<p>前身是SOFA RPC ，2018年07月正式开源</p> 
<ul><li>腾讯 Tencent Service Mesh</li></ul> 
<p><img src="https://images2.imgbox.com/70/24/nWKE1fCK_o.png" alt=""></p> 
<ul><li>华为 CSE Mesher</li></ul> 
<p><img src="https://images2.imgbox.com/2d/d8/SNnSh37u_o.png" alt=""></p> 
<p>总结：基本上都借鉴了Sidecar、Envoy和Istio等设计思想</p> 
<h4><a id="7_1604"></a>7、云原生应用和传统应用的区别</h4> 
<p>云原生是一个很宽泛的概念，想要开发一个支持云原生的应用并不难，</p> 
<p>可能就是简单的实现可基于容器部署、使用Kubernetes进行编排与调度，集成CI/CD工具以及Prometheus监控工具等。</p> 
<p><img src="https://images2.imgbox.com/e5/30/Z8cvtccP_o.png" alt=""><img src="https://images2.imgbox.com/69/9c/YdM8pIAE_o.png" alt=""><br> <img src="https://images2.imgbox.com/6c/d5/35Th9yaD_o.png" alt=""></p> 
<p>云原生在一个更好的基础平台与设施上提供了更多的应用。</p> 
<p>因为做了容器化就不需要指定操作系统，K8S 的资源调度更有弹性，之前需要通过代码来协调实现伸缩策略，比较麻烦，借助DevOps 会容易达成协作，因为它整个流程都是自动的，能够敏捷开发。还有微服是都是各自独立的，具有高内聚、低耦合的原则，具有自动化运维、快速恢复的特点，自愈能力强。当集群宕掉了，它会自动拉起。</p> 
<blockquote> 
 <p>总结： 云原生与传统应用有比较明显的区别，云原生更倡导敏捷、自动化、容错，而传统应用则大多还处于原生的瀑布开发模型和人工运维阶段。</p> 
</blockquote> 
<h4><a id="8_1619"></a>8、云原生涉及的核心项目</h4> 
<p><img src="https://images2.imgbox.com/49/e7/rRMf2KJ3_o.png" alt=""></p> 
<h3><a id="2K8S8_1633"></a>第2部分：穿透K8S的8大宏观架构</h3> 
<p>在这里，尼恩用自己 20 年陈酿的 3高架构知识宇宙， 给大家从宏观上，梳理一下穿透K8S的8大宏观架构</p> 
<h4><a id="K8S_1637"></a>尼恩的K8S的独特视角：不是普通的视角</h4> 
<p>尼恩从架构师视角，带大家通过8个图，穿透K8S，实现K8S自由</p> 
<ul><li>图0：K8S 的宏观组件架构图</li><li>图1：K8S 业务架构图</li><li>图2：K8S 元数据架构图</li><li>图3：K8S 容器管理流程架构</li><li>图4：容器元数据的数据传输架构</li><li>图5：容器对外暴露架构图</li><li>图6：总的架构图</li><li>第7：master 上APIServer 内部架构图</li><li>第8：worker上 kubelet内部架构图</li></ul> 
<h4><a id="K8S__1655"></a>K8S的核心价值：带领大家从 容器管理的石器时代，进入工业时代</h4> 
<p>首先，回顾一下， 在石器时代，我们是怎么管理容器的：</p> 
<ul><li>docker 命令 （石器时代早期）</li><li>docker 编排 （石器时代晚期期）</li></ul> 
<p><img src="https://images2.imgbox.com/d1/41/jg9ENo12_o.png" alt=""></p> 
<h5><a id="0K8S__1668"></a>图0：K8S 的宏观组件架构图</h5> 
<p>K8S，是一个围绕容器打造的分布式系统，和其他的分布式系统比如rocketmq、kafka、elasticsearch，其实宏观上非常类似</p> 
<p>两个大组件：</p> 
<ul><li>master ： 集群管理+元数据管理</li><li>worker（node）： 容器的生命周期管理</li></ul> 
<p>master上的主要组件是 api-server + 一大堆的控制器</p> 
<p>node上主要的组件就是 kubelet （容器管理）+ kube-proxy（流量负载均衡）</p> 
<p><img src="https://images2.imgbox.com/65/a0/7xMt6z5Q_o.png" alt=""></p> 
<p><em>具体介绍，请参见前面的视频。</em></p> 
<h5><a id="1K8S__1689"></a>图1：K8S 业务架构图</h5> 
<p>在尼恩的 3高架构知识宇宙中， 系统的架构，都是从业务架构开始的。</p> 
<p>K8S 业务架构：</p> 
<ul><li>容器元数据管理 （管理镜像地址、资源信息、部署的副本数量、部署的节点信息，对外暴露的端口信息等等）</li><li>容器什么周期管理 （ 管死管生管过程）</li></ul> 
<p>具体如下：</p> 
<p><img src="https://images2.imgbox.com/55/39/NKSBbOak_o.png" alt=""></p> 
<p><em>具体介绍，请参见前面的视频。</em></p> 
<h5><a id="2K8S__1710"></a>图2：K8S 元数据架构图</h5> 
<p>包括：镜像地址、资源信息、部署的副本数量、部署的节点信息，对外暴露的端口信息等等</p> 
<p><img src="https://images2.imgbox.com/19/d4/fKQWv1hg_o.png" alt=""></p> 
<p><strong>Pod</strong>：kubernetes的最小控制单元，容器都是运行在pod中的，一个pod中可以有1个或者多个容器<br> <strong>Service</strong>：pod对外服务的统一入口，下面可以维护者同一类的多个pod<br> <strong>Label</strong>：标签，用于对pod进行分类，同一类pod会拥有相同的标签</p> 
<p><strong>Master</strong>：集群控制节点，每个集群需要至少一个master节点负责集群的管控<br> <strong>Node</strong>：工作负载节点，由master分配容器到这些node工作节点上，然后node节点上的docker负责容器的运行</p> 
<p><strong>Replication Controller</strong>：</p> 
<p>Replication Controller 保证了在所有时间内，都有特定数量的Pod副本正在运行，如果太多了，Replication Controller就杀死几个，如果太少了，Replication Controller会新建几个。</p> 
<p>和直接创建的pod不同的是，Replication Controller会替换掉那些删除的或者被终止的pod，不管删除的原因是什么（维护阿，更新啊，Replication Controller都不关心）。</p> 
<p>基于这个理由，我们建议即使是只创建一个pod，我们也要使用Replication Controller。</p> 
<p>Replication Controller 就像一个进程管理器，监管着不同node上的多个pod,而不是单单监控一个node上的pod,Replication Controller 会委派本地容器来启动一些节点上服务（Kubelet ,Docker）。</p> 
<p><strong>ReplicaSet</strong>：</p> 
<p>ReplicaSet是下一代复本控制器。ReplicaSet和 Replication Controller之间的唯一区别是现在的选择器支持。<em>Replication Controller</em>只支持基于等式的selector（env=dev或environment!=qa），但ReplicaSet还支持基于集合的selector（version in (v1.0, v2.0)或env notin (dev, qa)）。</p> 
<p>在使用时官方推荐ReplicaSet。</p> 
<p><strong>Deployment</strong>：</p> 
<p>为Pod和ReplicaSet提供了一个声明式定义(declarative)方法，用来替代以前的ReplicationController来方便的管理应用。</p> 
<p>典型的应用场景包括：</p> 
<ul><li>定义Deployment来创建Pod和ReplicaSet</li><li>滚动升级和回滚应用</li><li>扩容和缩容</li><li>暂停和继续Deployment</li></ul> 
<p><em>具体介绍，请参见前面的视频。</em></p> 
<h5><a id="3K8S__1754"></a>图3：K8S 容器管理流程架构</h5> 
<p>从容器管理的入口， 到最终容器的创建， 具体流程如下图所示：</p> 
<p><img src="https://images2.imgbox.com/5e/06/xxVCwMvb_o.png" alt=""></p> 
<p>第一步： 用户向APIServer （数据总线）提交一份 部署文件，里边有用户视角的 容器元数据信息</p> 
<p>第一步： APIServer 通知 一系列的控制器进行 部署文件处理， 得到最终的 元数据 对象，也就是 资源对象，持久化到 etcd，并且发布到 Node节点的 kubelet组件</p> 
<p>第三部： Node节点的 kubelet组件判断是否属于自己管理的容器，如果是，则进行容器的创建</p> 
<p>创建容器时，主要用到下面三个接口</p> 
<p><strong>(1)CRI（Container Runtime Interface）：</strong></p> 
<p>容器运行时接口，提供计算资源。</p> 
<p>kubernetes1.5版本之后，kubernetes项目推出了自己的运行时接口api–CRI(container runtime interface)。</p> 
<p><strong>(2)CNI（Container Network Interface）</strong>：</p> 
<p>容器网络接口，提供网络资源。</p> 
<p>是和 CoreOS 主导制定的容器网络标准，它本身并不是实现或者代码，可以理解成一个协议。CNI旨在为容器平台提供网络的标准化。容器平台可以从CNI获取到满足网络互通条件的网络参数(如IP地址、网关、路由、DNS等)。</p> 
<p><strong>(3)CSI（Container Storage Interface）：</strong></p> 
<p>容器存储接口，提供存储资源。</p> 
<p>由 kubernetes、Mesos、Docker 等社区成员联合制定的一个行业标准接口规范，旨在将任意存储系统暴露给容器化应用程序。</p> 
<p>kubernetes 1.9 版为alpha阶段–&gt;kubernetes 1.10版为beta阶段–&gt;kubernetes 1.13 GA。</p> 
<p><em>具体介绍，请参见前面的视频。</em></p> 
<h5><a id="4_1798"></a>图4：容器元数据的数据传输架构</h5> 
<p>内部的 api server 和 kublet 客户端 组件之间， 通过 长短链接 相结合的方式，完成高性能数据传输：</p> 
<ul><li>长连接： 进行增量元数据 推送 ， 推模式</li><li>短连接： 进行全量元数据 拉取 ， 推模式</li></ul> 
<p><img src="https://images2.imgbox.com/77/70/K2fQhhtU_o.png" alt=""></p> 
<p><em>具体介绍，请参见前面的视频。</em></p> 
<h5><a id="5_1809"></a>图5：容器对外暴露架构图</h5> 
<p>两个阶段：</p> 
<ul><li>元数据对象创建的阶段： 完成容器的选择， 容器清理的建立，以及端口的映射</li><li>流量路由阶段： 由 kube-proxy组件 进行 流量的分发，和 pod 之间的负载均衡</li></ul> 
<p><img src="https://images2.imgbox.com/ef/18/NAYiFhb7_o.png" alt=""></p> 
<p><em>具体介绍，请参见前面的视频。</em></p> 
<h5><a id="6_1822"></a>图6：总的架构图</h5> 
<p>有了前面的基础之后，来一个总的架构图</p> 
<p>K8S，是一个围绕容器打造的分布式系统，和其他的分布式系统比如rocketmq、kafka、elasticsearch，其实宏观上非常类似</p> 
<p>两个大组件：</p> 
<ul><li>master ： 集群管理+元数据管理</li><li>worker（node）： 容器的生命周期管理</li></ul> 
<p>master上的主要组件是 api-server + 一大堆的控制器</p> 
<p>node上主要的组件就是 kubelet （容器管理）+ kube-proxy（流量负载均衡）</p> 
<p>另外通过etcd 进行元数据的持久化。</p> 
<p><img src="https://images2.imgbox.com/7c/bb/OekQzoyM_o.png" alt=""></p> 
<p><em>具体介绍，请参见前面的视频。</em></p> 
<h5><a id="7master_APIServer__1849"></a>图7：master 上APIServer 内部架构图</h5> 
<p>Kube-APIServer是Kubernetes最重要的核心组件之一，主要提供以下功能：</p> 
<ul><li>提供集群管理的RESTAPI接口，包括∶认证Authentication、 授权Authorization、准入Admission（Mutating&amp;Valiating）。</li><li>提供其他模块之间的数据交互和通信的枢纽（其他模块通过APIServer查询或修改数据，只有APIServer才直接操作etcd）。</li><li>APIServer提供etcd数据缓存以减少集群对etcd的访问。</li></ul> 
<p><img src="https://images2.imgbox.com/2c/3c/3vVRcFIE_o.png" alt=""></p> 
<p>APIServer对请求的处理流程，采用了类似 责任链 模式的架构， 由很多的处理器组成：</p> 
<p>（1）APIHandler：APIServer 本质上是一个 RestServer，那么就需要注册不同对象的Handler</p> 
<p>（2）AuthN：认证，除了可以使用Kubernetes 自带的认证机制外，还可以使用 webhook自己定义一些认证机制。当配置了外部的认证机制后，认证请求就可以被转发到外部去，这样就可以去集成企业统一的认证平台</p> 
<p>（3）Rate Limit：限流</p> 
<p>（4）Auditing：审计，所有的操作都会生成一条日志记录</p> 
<p>（5）AuthZ：认证，可以使用 k8s 自带的 RBAC（role-base access control），也可以使用 webhook 自定义</p> 
<p>（6）Aggregator：</p> 
<p>可以像 nginx 一样做路由配置，如果 APIServer 是标准的 K8S APIServer，就会走默认的 K8S APIServer，包括Mutating、Validation等；</p> 
<p>如果是自定义的 APIServer，就会走Aggregated APIServer（自定义、独立部署的 APIServer），包括自定义的Mutating、Validation等。</p> 
<p>（7）Mutating：变形，可以加一些属性</p> 
<p>（8）Validating：验证，加完属性后再做一些验证</p> 
<h5><a id="8worker_kubelet_1885"></a>图8：worker上 kubelet内部架构图</h5> 
<p><img src="https://images2.imgbox.com/d2/8b/ouIiUYUY_o.png" alt=""></p> 
<p>Kubelet 将 容器运行时， 容器网络和 容器存储抽象成了CRI，CNI,CSI。</p> 
<p>主要的工作为：</p> 
<p>（1）Kubelet 是node上　Kubernetes的初始化系统（init system）</p> 
<p>（2）Kubelet 从不同源获取Pod清单，并按需求启停Pod的核心组件：</p> 
<ul><li>可从本地文件目录获取Pod 清单</li><li>从给定的HTTPServer或Kube-APIServer等源头获取Pod 清单；</li></ul> 
<p>（3）Kubelet负责汇报当前节点的资源信息和健康状态；</p> 
<p>（4）Kubelet负责Pod的健康检查和状态汇报。</p> 
<h4><a id="___1914"></a>学习 云原生+ 微服务的神器</h4> 
<p>本地、轻量级 K8S 环境，一键启动， 学习 云原生+ 微服务 ， 非常方便</p> 
<p>尼恩会给大家准备好 虚拟机的box文件，可以直接用，省去 折腾的烦恼</p> 
<p><img src="https://images2.imgbox.com/a7/1a/MEHYcNLX_o.png" alt=""></p> 
<p><img src="https://images2.imgbox.com/9f/33/azmMkEQY_o.png" alt=""></p> 
<h3><a id="3K8s__1926"></a>第3部分：K8s运行时 实操</h3> 
<p>k8s有多种部署方式，目前主流的方式有3种：kubeadm、minikube、二进制包。</p> 
<p>1）kubeadm：一个用于快速搭建单节点kubernetes的工具</p> 
<p>2）minikube：一个用于快速搭建kubernetes集群的工具</p> 
<p>3）二进制包：从官网下载每个组件的二进制包，依次去安装，此方式对于理解kubernetes组件更加有效</p> 
<h4><a id="minikube_1936"></a>什么是minikube</h4> 
<p>minikube 是本地 Kubernetes，</p> 
<p>优点是：快速启动，消耗机器资源较少，非常适合新手体验与开发。</p> 
<p>minikube 最大特点就是“小而美”，可执行文件仅有不到 100MB，运行镜像也不过 1GB，</p> 
<p>但就在这么小的空间里却集成了 Kubernetes 的绝大多数功能特性，不仅有核心的容器编排功能，还有丰富的插件，例如 Dashboard、GPU、Ingress、Istio、Kong、Registry 等等，综合来看非常完善。</p> 
<h4><a id="minikube__1948"></a>minikube 背景</h4> 
<p>徒手搭建过k8s的同学都晓得其中的煎熬，复杂的认证，配置环节相当折磨人，出错率相当高，</p> 
<p>而minikube就是为解决这个问题而衍生出来的工具，它基于go语言开发，</p> 
<p>minikube可以在单机环境下快速搭建可用的k8s集群，非常适合测试和本地开发，现有的大部分在线k8s实验环境也是基于minikube</p> 
<p>可以在minikube上体验kubernetes的相关功能。</p> 
<p>minikube基于go语言开发， 是一个易于在本地运行 Kubernetes 的工具，可在你的笔记本电脑上的虚拟机内轻松创建单机版 Kubernetes 集群。</p> 
<p>便于尝试 Kubernetes 或使用 Kubernetes 日常开发。</p> 
<p>可以在单机环境下快速搭建可用的k8s集群，非常适合测试和本地开发。</p> 
<p>所以，可以在本地实验环境来安装minikube，来入门学习kubernetes相关的知识；</p> 
<h4><a id="Kubernetes_minikube_1984"></a>Kubernetes集群架构 与minikube架构对比</h4> 
<h5><a id="1Kubernetes_1986"></a>1、Kubernetes集群架构</h5> 
<p>通常情况下，一套完整的Kubernetes集群至少需要包括master节点和node节点，</p> 
<p>下图是常规k8s的集群架构，master节点一般是独立的，用于协调调试其它节点之用，而容器实际运行都是在node节点上，kubectl位于 master节点。</p> 
<p><img src="https://images2.imgbox.com/84/6d/Klgm1a77_o.png" alt=""></p> 
<h5><a id="2Minikube_2000"></a>2、Minikube架构</h5> 
<p>下图是 Minikube的架构，可以看出，master 节点与其它节点合为一体，而整体则通过宿主机上的 kubectl 进行管理，这样可以更加节省资源。</p> 
<p><img src="https://images2.imgbox.com/b7/b2/X1QVvw3Y_o.png" alt=""></p> 
<p>其支持大部分kubernetes的功能，列表如下</p> 
<ul><li>DNS</li><li>NodePorts</li><li>ConfigMaps and Secrets</li><li>Dashboards</li><li>Container Runtime: Docker, and rkt</li><li>Enabling CNI (Container Network Interface)</li><li>Ingress</li><li>…</li></ul> 
<p><a href="https://github.com/kubernetes/minikube">Minikube</a> 支持 Windows、macOS、Linux 三种 OS，会根据平台不同，下载对应的虚拟机镜像，并在镜像内安装 k8s。</p> 
<h4><a id="minikube_2023"></a>minikube安装前准备</h4> 
<p>推荐在linux主机上安装，我本地用的是 centos。</p> 
<p>安装minikube的主机必要配置：</p> 
<ul><li>2 CPUs or more</li><li>2GB of free memory</li><li>20GB of free disk space</li><li>Internet connection</li><li>Container or virtual machine manager, such as: <a href="https://minikube.sigs.k8s.io/docs/drivers/docker/" rel="nofollow">Docker</a>, <a href="https://minikube.sigs.k8s.io/docs/drivers/hyperkit/" rel="nofollow">Hyperkit</a>, <a href="https://minikube.sigs.k8s.io/docs/drivers/hyperv/" rel="nofollow">Hyper-V</a>, <a href="https://minikube.sigs.k8s.io/docs/drivers/kvm2/" rel="nofollow">KVM</a>, <a href="https://minikube.sigs.k8s.io/docs/drivers/parallels/" rel="nofollow">Parallels</a>, <a href="https://minikube.sigs.k8s.io/docs/drivers/podman/" rel="nofollow">Podman</a>, <a href="https://minikube.sigs.k8s.io/docs/drivers/virtualbox/" rel="nofollow">VirtualBox</a>, or <a href="https://minikube.sigs.k8s.io/docs/drivers/vmware/" rel="nofollow">VMware</a></li></ul> 
<p>Container 容器我本地安装是docker；</p> 
<h4><a id="docker_2039"></a>docker安装和环境检查</h4> 
<p>安装过程，请参见下面的文档：</p> 
<p>尼恩编著，《 docker 学习圣经 》PDF</p> 
<p>注：</p> 
<p>由于国内访问docker镜像库很是缓慢，所以建议配置阿里云的代理，通过修改daemon配置文件/etc/docker/daemon.json来使用加速器：</p> 
<pre><code class="prism language-bash">$ <span class="token builtin class-name">cd</span> /etc/docker
<span class="token comment"># 在daemon.json文件末尾追加如下配置：</span>
$ <span class="token function">sudo</span> <span class="token function">tee</span> /etc/docker/daemon.json <span class="token operator">&lt;&lt;-</span><span class="token string">'EOF'
{
  "registry-mirrors": [
    "https://bjtzu1jb.mirror.aliyuncs.com",
    "http://f1361db2.m.daocloud.io",
    "https://hub-mirror.c.163.com",
    "https://docker.mirrors.ustc.edu.cn",
    "https://reg-mirror.qiniu.com",
    "https://dockerhub.azk8s.cn",
    "https://registry.docker-cn.com"
  ]
}

EOF</span>

<span class="token comment"># 重启docker</span>
<span class="token function">sudo</span> systemctl daemon-reload
<span class="token function">sudo</span> systemctl restart docker
</code></pre> 
<h5><a id="docker__2072"></a>docker 版本要求</h5> 
<pre><code class="prism language-python">For improved Docker performance<span class="token punctuation">,</span> Upgrade Docker to a newer version <span class="token punctuation">(</span>Minimum recommended version <span class="token keyword">is</span> <span class="token number">18.09</span><span class="token number">.0</span><span class="token punctuation">)</span>
! docker <span class="token keyword">is</span> currently using the devicemapper storage driver<span class="token punctuation">,</span> consider switching to overlay2 <span class="token keyword">for</span> better performance
<span class="token operator">*</span> Using image repository registry<span class="token punctuation">.</span>cn<span class="token operator">-</span>hangzhou<span class="token punctuation">.</span>aliyuncs<span class="token punctuation">.</span>com<span class="token operator">/</span>google_containers
<span class="token operator">*</span> Starting control plane node minikube <span class="token keyword">in</span> cluster minikube
<span class="token operator">*</span> Pulling base image <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token operator">*</span> Creating docker container <span class="token punctuation">(</span>CPUs<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> Memory<span class="token operator">=</span>2200MB<span class="token punctuation">)</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token operator">*</span> Preparing Kubernetes v1<span class="token punctuation">.</span><span class="token number">23.1</span> on Docker <span class="token number">20.10</span><span class="token number">.8</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
</code></pre> 
<p>docker 版本：</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>root@cdh1 ~<span class="token punctuation">]</span><span class="token comment">#  docker version</span>
Client: Docker Engine - Community
 Version:           <span class="token number">20.10</span>.23
 API version:       <span class="token number">1.41</span>
 Go version:        go1.18.10
 Git commit:        <span class="token number">7155243</span>
 Built:             Thu Jan <span class="token number">19</span> <span class="token number">17</span>:36:21 <span class="token number">2023</span>
 OS/Arch:           linux/amd64
 Context:           default
 Experimental:      <span class="token boolean">true</span>

Server: Docker Engine - Community
 Engine:
  Version:          <span class="token number">20.10</span>.23
  API version:      <span class="token number">1.41</span> <span class="token punctuation">(</span>minimum version <span class="token number">1.12</span><span class="token punctuation">)</span>
  Go version:       go1.18.10
  Git commit:       6051f14
  Built:            Thu Jan <span class="token number">19</span> <span class="token number">17</span>:34:26 <span class="token number">2023</span>
  OS/Arch:          linux/amd64
  Experimental:     <span class="token boolean">false</span>
 containerd:
  Version:          <span class="token number">1.6</span>.15
  GitCommit:        5b842e528e99d4d4c1686467debf2bd4b88ecd86
 runc:
  Version:          <span class="token number">1.1</span>.4
  GitCommit:        v1.1.4-0-g5fd4c4d
 docker-init:
  Version:          <span class="token number">0.19</span>.0
  GitCommit:        de40ad0
</code></pre> 
<h5><a id="swapselinuxfirewalld_2122"></a>关闭虚拟机swap、selinux、firewalld</h5> 
<pre><code class="prism language-bash"><span class="token comment"># 临时关闭swap</span>
swapoff -a
 
<span class="token comment"># 临时关闭selinux，如永久关闭请配置为permissive</span>
setenforce <span class="token number">0</span>
 
<span class="token comment"># 关闭防火墙</span>
systemctl stop firewalld
systemctl disable firewalld
</code></pre> 
<p>永久关闭swap可注释掉/etc/fstab中的swap行，然后重启。</p> 
<p>永久关闭selinux可编辑/etc/sysconfig/selinux，配置为SELINUX=permissive，然后重启。</p> 
<p>此处为常规操作不详述。</p> 
<h5><a id="hosts_2144"></a>编辑虚拟机hosts文件</h5> 
<p>与安装k8s类似，需要添加主机名解析</p> 
<pre><code class="prism language-bash"><span class="token builtin class-name">echo</span> <span class="token string">"127.0.0.1 test1"</span> <span class="token operator">&gt;&gt;</span> /etc/hosts
</code></pre> 
<p>其中test1为虚拟机主机名。</p> 
<p>如果不添加该解析，启动minikube时会有如下报错：</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>WARNING Hostname<span class="token punctuation">]</span>: <span class="token function">hostname</span> <span class="token string">"test1"</span> could not be reached<span class="token punctuation">[</span>WARNING Hostname<span class="token punctuation">]</span>: <span class="token function">hostname</span> <span class="token string">"test1"</span><span class="token builtin class-name">:</span> lookup test1 on <span class="token number">172.18</span>.3.4:53: no such <span class="token function">host</span>
</code></pre> 
<h5><a id="_2160"></a>登录阿里云</h5> 
<p>注册阿里云账号， 开通容器镜像服务</p> 
<pre><code class="prism language-bash">docker login --username<span class="token operator">=</span>修改成你自己的账号 registry.cn-hangzhou.aliyuncs.com

docker tag <span class="token punctuation">[</span>ImageId<span class="token punctuation">]</span> registry.cn-hangzhou.aliyuncs.com/kaigejava/my_kaigejava:<span class="token punctuation">[</span>镜像版本号<span class="token punctuation">]</span>

docker push registry.cn-hangzhou.aliyuncs.com/kaigejava/my_kaigejava:<span class="token punctuation">[</span>镜像版本号<span class="token punctuation">]</span> 
</code></pre> 
<h4><a id="docker_2176"></a>创建用户，加入docker用户组</h4> 
<p>新建一个minikube用户</p> 
<p><code>useradd minikube</code></p> 
<p>新建一个用户组</p> 
<p><code>groupadd docker</code></p> 
<p>将minikube添加到docker组</p> 
<p><code>usermod -aG docker minikube</code></p> 
<p>将当前用户添加到该docker组(root)</p> 
<p><code>usermod -aG docker $USER</code></p> 
<p>更新用户组</p> 
<p><code>newgrp docker</code></p> 
<p>重启docker</p> 
<p><code>sudo systemctl daemon-reload</code></p> 
<p><code>sudo systemctl restart docker</code></p> 
<h5><a id="minikuberoot_2212"></a>让用户minikube获得root权限</h5> 
<p>1、添加用户，首先用adduser命令添加一个普通用户，命令如下：</p> 
<p>添加一个名为minikube的用户</p> 
<ul><li>adduser minikube 添加用户</li><li>passwd minikube//修改密码</li></ul> 
<p>2、赋予root权限 （三种方法，推荐第三）</p> 
<p><strong>方法一</strong>： 修改 /etc/sudoers 文件，找到下面一行，把前面的注释（#）去掉</p> 
<pre><code class="prism language-bash"><span class="token comment">## Allows people in group wheel to run all commands</span>
%wheel    <span class="token assign-left variable">ALL</span><span class="token operator">=</span><span class="token punctuation">(</span>ALL<span class="token punctuation">)</span>    ALL
</code></pre> 
<p>然后修改用户，使其属于root组（wheel），命令如下：</p> 
<p><code>usermod -g root minikube</code></p> 
<p>修改完毕，现在可以用minikube帐号登录，然后用命令 <strong>su –</strong> ，即可获得root权限进行操作。</p> 
<p><strong>方法二</strong>： 修改 /etc/sudoers 文件，找到下面一行，在root下面添加一行，如下所示：</p> 
<pre><code class="prism language-bash"><span class="token comment">## Allow root to run any commands anywhere</span>
root    <span class="token assign-left variable">ALL</span><span class="token operator">=</span><span class="token punctuation">(</span>ALL<span class="token punctuation">)</span>     ALL
minikube   <span class="token assign-left variable">ALL</span><span class="token operator">=</span><span class="token punctuation">(</span>ALL<span class="token punctuation">)</span>     ALL
</code></pre> 
<p>修改完毕，现在可以用minikube帐号登录，然后用命令 su – ，即可获得root权限进行操作。</p> 
<p>su minikube</p> 
<p>su -</p> 
<p><strong>方法三：</strong> 修改 /etc/passwd 文件，直接修改用户id为0，就是root的用户id</p> 
<p><code>cat /etc/passwd</code></p> 
<p><img src="https://images2.imgbox.com/fa/45/SUqFKZQw_o.png" alt=""></p> 
<p>7 个字段的详细信息如下：</p> 
<p>（1）用户名 （user1）： 已创建用户的用户名，字符长度 1 个到 12 个字符。如果是“*”的话，那么就表示该账号被查封了，系统不允许持有该账号的用户登录。</p> 
<p>（2）密码（x）：代表加密密码，保存在 /etc/shadow 文件中。</p> 
<p>（3）用户 ID（1001）：代表用户的 ID 号，每个用户都要有一个唯一的 ID 。UID 号为 0 的是为 root 用户保留的，UID 号 1 到99 是为系统用户保留的，UID 号 100-999 是为系统账户和群组保留的。</p> 
<p>（4）群组 ID （100）：代表user1用户所属群组的 ID 号，每个群组都要有一个唯一的 GID ，群组信息保存在 /etc/group文件中。</p> 
<p>（5）用户信息（用户1）：代表描述字段，可以用来描述用户的信息。</p> 
<p>（6）家目录（/usr/testUser）：代表用户的主目录。</p> 
<p>（7）Shell（/bin/bash）：代表用户使用的 shell 类型。</p> 
<p>找到如下行，把用户ID修改为 0 ，如下所示：</p> 
<pre><code class="prism language-bash">minikube:x:1001:1001::/hminikubekube:/bin/bash
</code></pre> 
<p>修改后如下</p> 
<pre><code class="prism language-bash">minikube:x:0:1001::/home/minikube:/bin/bash
</code></pre> 
<p>保存，用minikube账户登录后，直接获取的就是root帐号的权限。</p> 
<h4><a id="minikube_2298"></a>安装与启动minikube</h4> 
<p>minikube的官网：<a href="https://minikube.sigs.k8s.io/docs/start/" rel="nofollow">minikube start | minikube (k8s.io)</a></p> 
<p>官网上的安装minikube网速实在太慢了，推荐使用阿里云的镜像来进行安装minikube，</p> 
<h5><a id="_2310"></a>软件版本说明</h5> 
<ul><li>minikube：v1.23.1</li><li>Kubernetes：v1.23.1</li><li>kube-prometheus：v0.11.0</li></ul> 
<h5><a id="minikube_2316"></a>安装minikube</h5> 
<pre><code class="prism language-bash"><span class="token comment">#阿里云镜像</span>
<span class="token function">curl</span> -Lo minikube https://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/releases/v1.23.1/minikube-linux-amd64 <span class="token operator">&amp;&amp;</span> <span class="token function">chmod</span> +x minikube <span class="token operator">&amp;&amp;</span> <span class="token function">sudo</span> <span class="token function">mv</span> minikube /usr/local/bin/

<span class="token comment">#官方二进制包下载</span>

<span class="token function">curl</span> -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64

<span class="token function">sudo</span> <span class="token function">install</span> minikube-linux-amd64 /usr/local/bin/minikube
</code></pre> 
<p>65M</p> 
<p><img src="https://images2.imgbox.com/9a/07/BS8ilvUG_o.png" alt=""></p> 
<p>linux 下载太慢，我用浏览器下载后， 放在虚拟机共享目录</p> 
<p>然后 复制到 /usr/local/bin</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>root@cdh1 ~<span class="token punctuation">]</span><span class="token comment"># su kube</span>
<span class="token punctuation">[</span>root@cdh1 root<span class="token punctuation">]</span><span class="token comment"># cp /vagrant/minikube-linux-amd64 /usr/local/bin/</span>
<span class="token punctuation">[</span>root@cdh1 root<span class="token punctuation">]</span><span class="token comment"># cp /vagrant/minikube-linux-amd64 /usr/local/bin</span>
<span class="token punctuation">[</span>root@cdh1 root<span class="token punctuation">]</span><span class="token comment"># chmod +x /usr/local/bin/minikube-linux-amd64</span>
<span class="token punctuation">[</span>root@cdh1 root<span class="token punctuation">]</span><span class="token comment"># mv  /usr/local/bin/minikube-linux-amd64  /usr/local/bin/minikube</span>
</code></pre> 
<h5><a id="minikube_2353"></a>启动minikube</h5> 
<pre><code class="prism language-bash">minikube start
</code></pre> 
<p>执行<code>minikube start</code>出现 <strong>The “docker” driver should not be used with root privileges</strong> 的报错.</p> 
<p><img src="https://images2.imgbox.com/a7/b3/mQLsc44t_o.png" alt=""></p> 
<p>如果是本地测试环境，根本就不需要考虑那么多，直接执行以下命令，强制使用docker：</p> 
<pre><code class="prism language-bash">minikube start --force --driver<span class="token operator">=</span>docker
<span class="token comment"># 或者使用阿里云镜像启动</span>
minikube start --force --driver<span class="token operator">=</span>docker --image-mirror-country cn --iso-url<span class="token operator">=</span>https://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/iso/minikube-v1.5.0.iso --registry-mirror<span class="token operator">=</span>https://xxxxxx.mirror.aliyuncs.com 

minikube start --force --driver<span class="token operator">=</span>docker --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span> --iso-url<span class="token operator">=</span>https://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/iso/minikube-v1.5.0.iso --registry-mirror<span class="token operator">=</span>https://xxxxxx.mirror.aliyuncs.com 
</code></pre> 
<p>还是有问题， docker image 下载不了</p> 
<p><img src="https://images2.imgbox.com/ff/17/m6lXK2Lh_o.png" alt=""></p> 
<h5><a id="_list_2381"></a>命令清单：血泪的安装史，尼恩用过的 命令list</h5> 
<p>通过下面的命令，可以看出血泪的安装史</p> 
<p>建议大家用尼恩的镜像，不用二次安装了</p> 
<pre><code class="prism language-bash">minikube start --image-repository<span class="token operator">=</span>registry.cn-hangzhou.aliyuncs.com/google_containers --registry-mirror<span class="token operator">=</span>https://ovfftd6p.mirror.aliyuncs.com  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>docker 

<span class="token function">sudo</span> minikube start --kubernetes-version<span class="token operator">=</span>v1.23.1 --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span> --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span>


minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span> --registry-mirror<span class="token operator">=</span><span class="token string">'https://ovfftd6p.mirror.aliyuncs.com'</span>  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>docker


minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span> --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>docker


minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span> --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>docker


minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span> --registry-mirror<span class="token operator">=</span><span class="token string">'https://ovfftd6p.mirror.aliyuncs.com'</span>  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>docker


minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-repository<span class="token operator">=</span><span class="token string">'registry.aliyuncs.com/google_containers'</span> --registry-mirror<span class="token operator">=</span><span class="token string">'https://ovfftd6p.mirror.aliyuncs.com'</span>  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>docker


minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1 --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>docker


minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-repository<span class="token operator">=</span><span class="token string">'registry.aliyuncs.com/google_containers'</span> --registry-mirror<span class="token operator">=</span><span class="token string">'https://ovfftd6p.mirror.aliyuncs.com'</span>  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>none


<span class="token function">su</span> minikube

minikube delete

minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span> --image-repository<span class="token operator">=</span><span class="token string">'registry.aliyuncs.com/google_containers'</span> --registry-mirror<span class="token operator">=</span><span class="token string">'https://ovfftd6p.mirror.aliyuncs.com'</span>    --force --driver<span class="token operator">=</span>docker


minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span> --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span>   --force --driver<span class="token operator">=</span>docker  --cpus <span class="token number">4</span>  --memory <span class="token number">5120</span>

 minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span> --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span>   --force --driver<span class="token operator">=</span>docker  --cpus <span class="token number">4</span>  --memory <span class="token number">5120</span> --feature-gates<span class="token operator">=</span>EphemeralContainers<span class="token operator">=</span>true

 minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1     --force --driver<span class="token operator">=</span>docker --cpus <span class="token number">4</span>  --memory <span class="token number">5120</span> --feature-gates<span class="token operator">=</span>EphemeralContainers<span class="token operator">=</span>true
</code></pre> 
<p><img src="https://images2.imgbox.com/82/01/dYKcCBby_o.png" alt=""></p> 
<h5><a id="minikube_start__2434"></a>minikube start 参数</h5> 
<p>启动命令：<code>minikube start "参数"</code></p> 
<ul><li><code>--image-mirror-country cn</code> 将缺省利用 <code>registry.cn-hangzhou.aliyuncs.com/google_containers</code> 作为安装Kubernetes的容器镜像仓库，</li><li><code>--iso-url=***</code> 利用阿里云的镜像地址下载相应的 .iso 文件</li><li><code>--cpus=2</code>：为minikube虚拟机分配CPU核数</li><li><code>--memory=2000mb</code>：为minikube虚拟机分配内存数</li><li><code>--kubernetes-version=***</code>：minikube 虚拟机将使用的 kubernetes 版本 ,e.g. --kubernetes-version v 1.17.3</li><li><code>--docker-env http_proxy</code> 传递代理地址</li></ul> 
<pre><code class="prism language-bash">默认启动使用的是 VirtualBox 驱动，使用 --vm-driver 参数可以指定其它驱动
<span class="token comment"># https://minikube.sigs.k8s.io/docs/drivers/</span>
- --vm-driver<span class="token operator">=</span>none 表示用容器；
- --vm-driver<span class="token operator">=</span>virtualbox 表示用虚拟机；
</code></pre> 
<p><strong>注意:</strong> To use kubectl or minikube commands as your own user, you may need to relocate them. For example, to overwrite your own settings, run:</p> 
<pre><code class="prism language-bash"><span class="token function">sudo</span> <span class="token function">mv</span> /root/.kube /root/.minikube <span class="token environment constant">$HOME</span>
<span class="token function">sudo</span> <span class="token function">chown</span> -R <span class="token environment constant">$USER</span> <span class="token environment constant">$HOME</span>/.kube <span class="token environment constant">$HOME</span>/.minikube
</code></pre> 
<h5><a id="_2461"></a>示例</h5> 
<h6><a id="vmdriverkvm2_2463"></a>–vm-driver=kvm2</h6> 
<p>参考: https://minikube.sigs.k8s.io/docs/drivers/kvm2/</p> 
<pre><code class="prism language-bash">minikube start --image-mirror-country cn --image-repository<span class="token operator">=</span>registry.cn-hangzhou.aliyuncs.com/google_containers --registry-mirror<span class="token operator">=</span>https://ovfftd6p.mirror.aliyuncs.com --driver<span class="token operator">=</span>kvm2
</code></pre> 
<h6><a id="vmdriverhyperv_2471"></a>–vm-driver=hyperv</h6> 
<pre><code class="prism language-shell"><span class="token comment"># 创建基于Hyper-V的Kubernetes测试环境</span>
minikube.exe start --image-mirror-country cn <span class="token punctuation">\</span>
    --iso-url<span class="token operator">=</span>https://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/iso/minikube-v1.5.0.iso <span class="token punctuation">\</span>
    --registry-mirror<span class="token operator">=</span>https://xxxxxx.mirror.aliyuncs.com <span class="token punctuation">\</span>
    --vm-driver<span class="token operator">=</span><span class="token string">"hyperv"</span> <span class="token punctuation">\</span>
    --hyperv-virtual-switch<span class="token operator">=</span><span class="token string">"MinikubeSwitch"</span> <span class="token punctuation">\</span>
    --memory<span class="token operator">=</span><span class="token number">4096</span>
</code></pre> 
<h6><a id="vmdrivernone_2483"></a>–vm-driver=none</h6> 
<pre><code class="prism language-bash"><span class="token function">sudo</span> minikube start --image-mirror-country cn --vm-driver<span class="token operator">=</span>none
</code></pre> 
<p><img src="https://images2.imgbox.com/7e/e2/m01nv5tV_o.png" alt=""></p> 
<pre><code class="prism language-bash"><span class="token function">sudo</span> minikube start --vm-driver<span class="token operator">=</span>none --docker-env <span class="token assign-left variable">http_proxy</span><span class="token operator">=</span>http://<span class="token variable">$host_IP</span>:8118 --docker-env <span class="token assign-left variable">https_proxy</span><span class="token operator">=</span>https:// <span class="token variable">$host_IP</span>:8118
</code></pre> 
<p>其中$host_IP指的是host的IP，可以通过ifconfig查看；比如在我这台机器是10.0.2.15，用virtualbox部署，则用下列命令启动minikube</p> 
<pre><code class="prism language-bash"><span class="token function">sudo</span> minikube start --vm-driver<span class="token operator">=</span>none --docker-env <span class="token assign-left variable">http_proxy</span><span class="token operator">=</span>http://10.0.2.15:8118 --docker-env <span class="token assign-left variable">https_proxy</span><span class="token operator">=</span>https://10.0.2.15:8118


<span class="token function">sudo</span> minikube start --kubernetes-version<span class="token operator">=</span>v1.23.1 --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span> --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span>

<span class="token function">sudo</span> minikube start --kubernetes-version<span class="token operator">=</span>v1.23.1 --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span> --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span> --extra-config<span class="token operator">=</span>kubelet.cgroup 
</code></pre> 
<h4><a id="_2512"></a>解决拉取镜像的问题</h4> 
<h5><a id="_2516"></a>错误日志查看</h5> 
<pre><code class="prism language-bash">minikube logs
</code></pre> 
<h5><a id="Q1minikube_2526"></a>Q1：解决minikube拉取镜像速度缓慢的问题</h5> 
<p>需要进入minikube进程内部，修改远程镜像仓库</p> 
<pre><code class="prism language-bash">minikube <span class="token function">ssh</span>
<span class="token function">sudo</span> <span class="token function">mkdir</span> -p /etc/docker
<span class="token function">sudo</span> <span class="token function">tee</span> /etc/docker/daemon.json <span class="token operator">&lt;&lt;-</span><span class="token string">'EOF'
{"registry-mirrors": ["http://hub-mirror.c.163.com"]
}
EOF</span>
<span class="token function">sudo</span> systemctl daemon-reload
<span class="token function">sudo</span> systemctl restart docker
</code></pre> 
<h6><a id="_minikube_start__2543"></a>解决 minikube <code>start</code> 过程中拉取镜像慢的问题</h6> 
<p>之前下载失败后的minikube，想要重新下载记得先删除</p> 
<pre><code class="prism language-bash">minikube delete --all
</code></pre> 
<p>拉取镜像慢可以拉取国内仓库，<code>minikube start</code> 的时候会帮我们下载新版的kubernetes，但是我这里不太支持最新版的，所以需要指定kubernetes版本</p> 
<pre><code class="prism language-bash">minikube start --kubernetes-version<span class="token operator">=</span>v1.23.1 --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span> --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span>

minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span> --registry-mirror<span class="token operator">=</span><span class="token string">'https://ovfftd6p.mirror.aliyuncs.com'</span>  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>docker

minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span> --registry-mirror<span class="token operator">=</span><span class="token string">'https://ovfftd6p.mirror.aliyuncs.com'</span>  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>docker --extra-config<span class="token operator">=</span>kubelet.cgroup-driver<span class="token operator">=</span>systemd

minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-repository<span class="token operator">=</span><span class="token string">'registry.aliyuncs.com/google_containers'</span> --registry-mirror<span class="token operator">=</span>https://kfwkfulq.mirror.aliyuncs.com  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>docker --extra-config<span class="token operator">=</span>kubelet.cgroup-driver<span class="token operator">=</span>systemd

--registry-mirror<span class="token operator">=</span>https://bjtzu1jb.mirror.aliyuncs.com

minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span> --registry-mirror<span class="token operator">=</span><span class="token string">'https://bjtzu1jb.mirror.aliyuncs.com'</span>  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>docker --extra-config<span class="token operator">=</span>kubelet.cgroup-driver<span class="token operator">=</span>systemd


minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span> --registry-mirror<span class="token operator">=</span><span class="token string">'https://bjtzu1jb.mirror.aliyuncs.com'</span>  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>docker --extra-config<span class="token operator">=</span>kubelet.cgroup-driver<span class="token operator">=</span>systemd  --base-image<span class="token operator">=</span><span class="token string">"registry.cn-hangzhou.aliyuncs.com/google_containers/kicbase:v0.0.27"</span>
</code></pre> 
<h5><a id="Q2_2572"></a>Q2：基础镜像拉不下来</h5> 
<h6><a id="_2578"></a>错误日志查看</h6> 
<pre><code class="prism language-bash">minikube logs
</code></pre> 
<h6><a id="_2586"></a>基础镜像拉不下来</h6> 
<p><img src="https://images2.imgbox.com/80/7e/o3LnDm3W_o.png" alt=""></p> 
<pre><code class="prism language-bash">registry.cn-hangzhou.aliyuncs.com/google_containers/kicbase:v0.0.27

镜像地址调整为 registry.aliyuncs.com

registry.aliyuncs.com/google_containers/kicbase:v0.0.27
</code></pre> 
<p>单独下载</p> 
<pre><code class="prism language-bash"> docker pull registry.aliyuncs.com/google_containers/kicbase:v0.0.27
</code></pre> 
<p>打tag</p> 
<pre><code class="prism language-bash">docker tag registry.aliyuncs.com/google_containers/kicbase:v0.0.27   registry.cn-hangzhou.aliyuncs.com/google_containers/kicbase:v0.0.27
</code></pre> 
<p>删除老的tag</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>root@cdh1 ~<span class="token punctuation">]</span><span class="token comment"># docker image ls</span>
REPOSITORY                                                    TAG                 IMAGE ID            CREATED             SIZE
registry.aliyuncs.com/google_containers/kicbase               v0.0.27             9fa1cc16ad6d        <span class="token number">16</span> months ago       <span class="token number">1</span>.08GB
registry.cn-hangzhou.aliyuncs.com/google_containers/kicbase   v0.0.27             9fa1cc16ad6d        <span class="token number">16</span> months ago       <span class="token number">1</span>.08GB

<span class="token punctuation">[</span>root@cdh1 ~<span class="token punctuation">]</span><span class="token comment"># docker rmi  registry.aliyuncs.com/google_containers/kicbase:v0.0.27</span>
Untagged: registry.aliyuncs.com/google_containers/kicbase:v0.0.27
Untagged: registry.aliyuncs.com/google_containers/kicbase@sha256:89b4738ee74ba28684676e176752277f0db46f57d27f0e08c3feec89311e22de
</code></pre> 
<h6><a id="_2631"></a>指定镜像启动</h6> 
<p>因为 <code>registry.cn-hangzhou.aliyuncs.com/google_containers/kicbase:v0.0.27</code> 自动下载、sha校验失败，而无法启动集群！</p> 
<p>手动下载后，打tag后</p> 
<p>还有验证环节，需要指定镜像，忽略SHA校验</p> 
<table><thead><tr><th align="center">参数</th><th>值</th></tr></thead><tbody><tr><td align="center">–base-image</td><td>指定镜像，忽略SHA校验</td></tr></tbody></table> 
<p>使用以下命令启动minikube：</p> 
<pre><code class="prism language-bash">minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span> --registry-mirror<span class="token operator">=</span><span class="token string">'https://bjtzu1jb.mirror.aliyuncs.com'</span>  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>docker --extra-config<span class="token operator">=</span>kubelet.cgroup-driver<span class="token operator">=</span>systemd  --base-image<span class="token operator">=</span><span class="token string">"registry.cn-hangzhou.aliyuncs.com/google_containers/kicbase:v0.0.27"</span>

minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span> --registry-mirror<span class="token operator">=</span><span class="token string">'https://bjtzu1jb.mirror.aliyuncs.com'</span>  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>docker  --base-image<span class="token operator">=</span><span class="token string">"registry.cn-hangzhou.aliyuncs.com/google_containers/kicbase:v0.0.27"</span>
</code></pre> 
<h6><a id="_2651"></a>终于开始创建容器，开始启动了</h6> 
<p><img src="https://images2.imgbox.com/51/0b/aec9LCWE_o.png" alt=""></p> 
<h5><a id="Q3coredns__2661"></a>Q3：新的问题来了：coredns 镜像找不到</h5> 
<pre><code class="prism language-bash">X Unable to load cached images: loading cached images: Docker load /var/lib/minikube/images/coredns_v1.8.4: loadimage docker.: /bin/bash -c <span class="token string">"sudo cat /var/lib/minikube/images/coredns_v1.8.4 | docker load"</span><span class="token builtin class-name">:</span> Process exited with status <span class="token number">1</span>
</code></pre> 
<p>直接下载</p> 
<pre><code class="prism language-bash">docker pull coredns/coredns:1.8.4

改tag

docker tag coredns/coredns:1.8.4  registry.cn-hangzhou.aliyuncs.com/google_containers/coredns/coredns:v1.8.4 

删除旧tag

docker rmi coredns/coredns:1.8.4

docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.8.4  registry.cn-hangzhou.aliyuncs.com/google_containers/coredns/coredns:v1.8.4 


docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/coredns/coredns:v1.8.4  registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.8.4  
</code></pre> 
<p>再次启动</p> 
<p><img src="https://images2.imgbox.com/df/0e/op6cEzV0_o.png" alt=""></p> 
<h5><a id="Q4_2704"></a>Q4：继续下载镜像</h5> 
<pre><code class="prism language-bash"><span class="token comment">#从国内镜像拉取</span>

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.1-0
docker pull coredns/coredns:1.8.6


docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.23.1
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.23.1
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.23.1
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.23.1
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.5


docker pull  registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.23.1 
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.23.1 
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.23.1 
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.23.1 
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.5 
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.0-0 
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns/coredns:v1.8.4 
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/k8s-minikube/storage-provisioner:v5 
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetesui/dashboard:v2.3.1
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetesui/metrics-scraper:v1.0.7


docker pull k8s-minikube/storage-provisioner:v5 
docker pull kubernetesui/dashboard:v2.3.1

docker pull kubernetesui/metrics-scraper:v1.0.7


docker pull  registry.aliyuncs.com/k8s-minikube/storage-provisioner:v5 
</code></pre> 
<pre><code class="prism language-bash">registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.8.6

registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.8.6

registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.1-0
 
docker pull etcd/etcd:3.5.1-0

改tag

docker tag coredns/coredns:1.8.4  registry.cn-hangzhou.aliyuncs.com/google_containers/coredns/coredns:v1.8.4 
</code></pre> 
<h5><a id="Q5_httpk8sgcrio__2763"></a>Q5：使用阿里云代理 http://k8s.gcr.io 镜像仓库</h5> 
<p>国内根本访问不了k8s的镜像库：<code>k8s.gsc.io</code>。</p> 
<p>​ 比如下载<code>k8s.gcr.io/coredns:1.6.5</code>镜像，在国内默认是下载失败的！</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>root@k8s-vm03 ~<span class="token punctuation">]</span><span class="token comment"># docker pull k8s.gcr.io/coredns:1.6.5</span>
Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled <span class="token keyword">while</span> waiting <span class="token keyword">for</span> connection <span class="token punctuation">(</span>Client.Timeout exceeded <span class="token keyword">while</span> awaiting headers<span class="token punctuation">)</span>
</code></pre> 
<p>部署K8S最大的难题是镜像下载，在国内无翻墙环境情况下很难从k8s.gcr.io等镜像源里下载镜像。<br> 这种情况下正确做法是：</p> 
<ol><li>直接指定国内镜像代理仓库（如阿里云代理仓库）进行镜像拉取下载。</li><li>成功拉取代理仓库中的镜像后，再将其tag打标签成为k8s.gcr.io对应镜像。</li><li>最后再删除从代理仓库中拉取下来的镜像。</li><li>要确保imagePullPolicy策略是IfNotPresent，即本地有镜像则使用本地镜像，不拉取！<br> 或者将下载的镜像放到harbor私有仓库里，然后将image下载源指向harbor私仓地址。</li></ol> 
<pre><code class="prism language-bash"><span class="token comment"># 阿里云代理仓库地址为：registry.aliyuncs.com/google_containers</span>
<span class="token comment"># 比如下载</span>
k8s.gcr.io/coredns:1.6.5
<span class="token comment"># 可以代理为：</span>
registry.aliyuncs.com/google_containers/coredns:1.6.5
</code></pre> 
<p>下面以阿里云代理仓库为例进行说明：</p> 
<pre><code class="prism language-bash"><span class="token comment"># 比如下载k8s.gcr.io/coredns:1.6.5镜像，在国内默认是下载失败的！</span>
 
<span class="token punctuation">[</span>root@k8s-vm01 coredns<span class="token punctuation">]</span><span class="token comment"># pwd</span>
/opt/k8s/work/kubernetes/cluster/addons/dns/coredns
<span class="token punctuation">[</span>root@k8s-vm01 coredns<span class="token punctuation">]</span><span class="token comment"># fgrep "image" ./*</span>
./coredns.yaml:        image: k8s.gcr.io/coredns:1.6.5
./coredns.yaml:        imagePullPolicy: IfNotPresent
 
<span class="token punctuation">[</span>root@k8s-vm03 ~<span class="token punctuation">]</span><span class="token comment"># docker pull k8s.gcr.io/coredns:1.6.5</span>
Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled <span class="token keyword">while</span> waiting <span class="token keyword">for</span> connection <span class="token punctuation">(</span>Client.Timeout exceeded <span class="token keyword">while</span> awaiting headers<span class="token punctuation">)</span>
 
<span class="token comment"># 这时候去指定国内的阿里云镜像代理仓库进行下载</span>
<span class="token punctuation">[</span>root@k8s-vm03 ~<span class="token punctuation">]</span><span class="token comment"># docker pull registry.aliyuncs.com/google_containers/coredns:1.6.5</span>
<span class="token number">1.6</span>.5: Pulling from google_containers/coredns
c6568d217a00: Pull complete
fc6a9081f665: Pull complete
Digest: sha256:608ac7ccba5ce41c6941fca13bc67059c1eef927fd968b554b790e21cc92543c
Status: Downloaded newer image <span class="token keyword">for</span> registry.aliyuncs.com/google_containers/coredns:1.6.5
registry.aliyuncs.com/google_containers/coredns:1.6.5
 
<span class="token comment"># 然后打tag，并删除之前从代理仓库下载的镜像</span>
<span class="token punctuation">[</span>root@k8s-vm03 ~<span class="token punctuation">]</span><span class="token comment"># docker tag registry.aliyuncs.com/google_containers/coredns:1.6.5 k8s.gcr.io/coredns:1.6.5</span>
 
<span class="token punctuation">[</span>root@k8s-vm03 ~<span class="token punctuation">]</span><span class="token comment"># docker images</span>
REPOSITORY                                        TAG                 IMAGE ID            CREATED             SIZE
k8s.gcr.io/coredns                                <span class="token number">1.6</span>.5               70f311871ae1        <span class="token number">5</span> months ago        <span class="token number">41</span>.6MB
registry.aliyuncs.com/google_containers/coredns   <span class="token number">1.6</span>.5               70f311871ae1        <span class="token number">5</span> months ago        <span class="token number">41</span>.6MB
 
<span class="token punctuation">[</span>root@k8s-vm03 ~<span class="token punctuation">]</span><span class="token comment"># docker rmi registry.aliyuncs.com/google_containers/coredns:1.6.5</span>
Untagged: registry.aliyuncs.com/google_containers/coredns:1.6.5
Untagged: registry.aliyuncs.com/google_containers/coredns@sha256:608ac7ccba5ce41c6941fca13bc67059c1eef927fd968b554b790e21cc92543c
 
<span class="token punctuation">[</span>root@k8s-vm03 ~<span class="token punctuation">]</span><span class="token comment"># docker images</span>
REPOSITORY                               TAG                 IMAGE ID            CREATED             SIZE
k8s.gcr.io/coredns                       <span class="token number">1.6</span>.5               70f311871ae1        <span class="token number">5</span> months ago        <span class="token number">41</span>.6MB
 
<span class="token comment"># 最终发现我们想要的k8s.gcr.io/coredns:1.6.5镜像被成功下载下来了！</span>
 
<span class="token comment"># 最后要记得：</span>
<span class="token comment"># 确定imagePullPolicy镜像下载策略是IfNotPresent，即本地有镜像则使用本地镜像，不拉取！</span>
<span class="token comment"># 或者将下载好的镜像放到harbor私有仓库里，然后将image下载地址指向harbor仓库地址。</span>
</code></pre> 
<p>以上总结三个步骤：</p> 
<pre><code class="prism language-bash">docker pull registry.aliyuncs.com/google_containers/coredns:1.6.5
docker tag registry.aliyuncs.com/google_containers/coredns:1.6.5 k8s.gcr.io/coredns:1.6.5
docker rmi registry.aliyuncs.com/google_containers/coredns:1.6.5
</code></pre> 
<h4><a id="Virtual_Box__2851"></a>Virtual Box 使用的问题</h4> 
<h5><a id="Q1_2855"></a>Q1：嵌套虚拟化问题</h5> 
<h6><a id="__2857"></a>什么是嵌套 虚拟化特性？</h6> 
<p>我们知道,在Intel处理器上，Vitural box使用Intel的vmx(virtul machine eXtensions)来提高虚拟机性能, 即硬件辅助虚拟化技术，</p> 
<p>现在如果我们需要需要多台具备"vmx"支持的主机，但是又没有太多物理服务器可使用，</p> 
<p>如果我们的虚拟机能够和物理机一样支持"vmx"，那么问题就解决了，</p> 
<p>而正常情况下,一台虚拟机无法使自己成为一个hypervisors并在其上再次安装虚拟机,因为这些虚拟机并不支持"vmx"，此时，可以使用 嵌套式虚拟nested</p> 
<p>嵌套式虚拟nested是一个可通过内核参数来启用的功能。</p> 
<p>它能够使一台虚拟机具有物理机CPU特性,支持vmx或者svm(AMD)硬件虚拟化,</p> 
<h6><a id="VTxAMDV_2871"></a>虚拟机启用嵌套VT-x/AMD-V</h6> 
<p>嵌套 虚拟化特性在VirtualBox虚拟机中默认是不启用的（设置-系统-处理器）：</p> 
<p><img src="https://images2.imgbox.com/47/9f/wyHtkfTU_o.png" alt=""></p> 
<p>打开Windows Powershell，进入VirtualBox安装目录，将要安装minikube的虚拟机启用嵌套VT-x/AMD-V。</p> 
<pre><code class="prism language-bash"><span class="token comment"># 进入安装目录</span>
<span class="token builtin class-name">cd</span> 'C:<span class="token punctuation">\</span>Program Files<span class="token punctuation">\</span>Oracle<span class="token punctuation">\</span>VirtualBox<span class="token punctuation">\</span>'
 
<span class="token comment"># 列出所有虚拟机</span>
C:<span class="token punctuation">\</span>Program Files<span class="token punctuation">\</span>Oracle<span class="token punctuation">\</span>VirtualBox<span class="token operator">&gt;</span>.<span class="token punctuation">\</span>VBoxManage.exe list vms
<span class="token string">"cdh1"</span> <span class="token punctuation">{<!-- --></span>309cd81a-248c-4184-9f99-8fe72d01c1f0<span class="token punctuation">}</span>

 
<span class="token comment"># 打开嵌套虚拟化功能</span>
.<span class="token punctuation">\</span>VBoxManage.exe modifyvm <span class="token string">"cdh1"</span>  --nested-hw-virt on
</code></pre> 
<p>启用完成后可以看到界面中该选项已勾选：</p> 
<p><img src="https://images2.imgbox.com/7d/12/5VOwUPRs_o.png" alt=""></p> 
<h5><a id="Q2conntrack_2906"></a>Q2：conntrack依赖</h5> 
<p>安装conntrack（后面使用–driver=none启动，依赖此包）</p> 
<pre><code class="prism language-bash">yum <span class="token function">install</span> conntrack -y
</code></pre> 
<p>使用如下命令启动minikube</p> 
<pre><code class="prism language-bash">minikube start --registry-mirror<span class="token operator">=</span><span class="token string">"https://na8xypxe.mirror.aliyuncs.com"</span> --driver<span class="token operator">=</span>none
</code></pre> 
<p>使用–driver=none的好处是可以直接使用root运行minikube，无需再配置其他用户。</p> 
<p>缺点是安全性降低、稳定性降低、数据丢失风险、无法使用–cpus、–memory进行资源限制等等，</p> 
<p>但这不是我们需要考虑的，因为本身安装minikube就是测试学习用的。</p> 
<p>关于driver的选择，详细可以参看：<a href="https://minikube.sigs.k8s.io/docs/drivers/none/" rel="nofollow">none | minikube (k8s.io)</a></p> 
<p>启动时我们看到如下报错：</p> 
<pre><code class="prism language-bash">stderr:
error execution phase preflight: <span class="token punctuation">[</span>preflight<span class="token punctuation">]</span> Some fatal errors occurred:
        <span class="token punctuation">[</span>ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables<span class="token punctuation">]</span>: /proc/sys/net/bridge/bridge-nf-call-iptables contents are not <span class="token builtin class-name">set</span> to <span class="token number">1</span>
<span class="token punctuation">[</span>preflight<span class="token punctuation">]</span> If you know what you are doing, you can <span class="token function">make</span> a check non-fatal with <span class="token variable"><span class="token variable">`</span>--ignore-preflight-errors<span class="token operator">=</span><span class="token punctuation">..</span>.<span class="token variable">`</span></span>
</code></pre> 
<p>根据提示进行解决即可：</p> 
<pre><code class="prism language-bash"><span class="token builtin class-name">echo</span> <span class="token string">'1'</span> <span class="token operator">&gt;</span> /proc/sys/net/bridge/bridge-nf-call-iptables
</code></pre> 
<p>再次尝试启动，启动成功：</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>root@test1 ~<span class="token punctuation">]</span><span class="token comment"># minikube start --registry-mirror="https://na8xypxe.mirror.aliyuncs.com" --driver=none</span>
* minikube v1.18.1 on Centos <span class="token number">7.6</span>.1810
* Using the none driver based on existing profile
* Starting control plane node minikube <span class="token keyword">in</span> cluster minikube
* Restarting existing none bare metal machine <span class="token keyword">for</span> <span class="token string">"minikube"</span> <span class="token punctuation">..</span>.
* OS release is CentOS Linux <span class="token number">7</span> <span class="token punctuation">(</span>Core<span class="token punctuation">)</span>
* Preparing Kubernetes v1.20.2 on Docker <span class="token number">1.13</span>.1 <span class="token punctuation">..</span>.
  - Generating certificates and keys <span class="token punctuation">..</span>.
  - Booting up control plane <span class="token punctuation">..</span>.
  - Configuring RBAC rules <span class="token punctuation">..</span>.
* Configuring <span class="token builtin class-name">local</span> <span class="token function">host</span> environment <span class="token punctuation">..</span>.
* 
<span class="token operator">!</span> The <span class="token string">'none'</span> driver is designed <span class="token keyword">for</span> experts <span class="token function">who</span> need to integrate with an existing VM
* Most <span class="token function">users</span> should use the newer <span class="token string">'docker'</span> driver instead, <span class="token function">which</span> does not require root<span class="token operator">!</span>
* For <span class="token function">more</span> information, see: https://minikube.sigs.k8s.io/docs/reference/drivers/none/
* 
<span class="token operator">!</span> kubectl and minikube configuration will be stored <span class="token keyword">in</span> /root
<span class="token operator">!</span> To use kubectl or minikube commands as your own user, you may need to relocate them. For example, to overwrite your own settings, run:
* 
  - <span class="token function">sudo</span> <span class="token function">mv</span> /root/.kube /root/.minikube <span class="token environment constant">$HOME</span>
  - <span class="token function">sudo</span> <span class="token function">chown</span> -R <span class="token environment constant">$USER</span> <span class="token environment constant">$HOME</span>/.kube <span class="token environment constant">$HOME</span>/.minikube
* 
* This can also be <span class="token keyword">done</span> automatically by setting the <span class="token function">env</span> var <span class="token assign-left variable">CHANGE_MINIKUBE_NONE_USER</span><span class="token operator">=</span>true
* Verifying Kubernetes components<span class="token punctuation">..</span>.
  - Using image registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v4 <span class="token punctuation">(</span>global image repository<span class="token punctuation">)</span>
* Enabled addons: storage-provisioner, default-storageclass
* Done<span class="token operator">!</span> kubectl is now configured to use <span class="token string">"minikube"</span> cluster and <span class="token string">"default"</span> namespace by default
</code></pre> 
<h5><a id="Q3kubectlkubelet_2977"></a>Q3：依赖kubectl、kubelet</h5> 
<p>kubectl 是和 kubeneters 交互的一个客户端，方便的和 k8s 进行交互，提交作业，查询状态等等。</p> 
<p>添加阿里云kubenetes yum源</p> 
<pre><code class="prism language-bash"><span class="token comment"># /etc/yum.repos.d/kubenetes.repo</span>
<span class="token punctuation">[</span>kubernetes<span class="token punctuation">]</span>
<span class="token assign-left variable">name</span><span class="token operator">=</span>Kubernetes
<span class="token assign-left variable">baseurl</span><span class="token operator">=</span>https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
<span class="token assign-left variable">enabled</span><span class="token operator">=</span><span class="token number">1</span>
<span class="token assign-left variable">gpgcheck</span><span class="token operator">=</span><span class="token number">1</span>
<span class="token assign-left variable">repo_gpgcheck</span><span class="token operator">=</span><span class="token number">1</span>
<span class="token assign-left variable">gpgkey</span><span class="token operator">=</span>https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
</code></pre> 
<p>生成元数据缓存</p> 
<pre><code class="prism language-bash"><span class="token comment"># 生成元数据缓存</span>
yum makecache
</code></pre> 
<p>安装kubectl、kubelet</p> 
<pre><code class="prism language-bash">yum <span class="token function">install</span> kubectl -y
yum <span class="token function">install</span> kubelet -y
systemctl <span class="token builtin class-name">enable</span> kubelet
</code></pre> 
<p>带着版本安装</p> 
<pre><code class="prism language-bash">yum list available kubectl

kubectl version

yum remove kubectl

yum <span class="token function">install</span> -y kubelet-1.23.1 kubectl-1.23.1 kubeadm-1.23.1
</code></pre> 
<p>查看版本</p> 
<pre><code class="prism language-bash">$ kubectl version
Client Version: version.Info<span class="token punctuation">{<!-- --></span>Major:<span class="token string">"1"</span>, Minor:<span class="token string">"23"</span>, GitVersion:<span class="token string">"v1.23.1"</span>, GitCommit:<span class="token string">"86ec240af8cbd1b60bcc4c03c20da9b98005b92e"</span>, GitTreeState:<span class="token string">"clean"</span>, BuildDate:<span class="token string">"2021-12-16T11:41:01Z"</span>, GoVersion:<span class="token string">"go1.17.5"</span>, Compiler:<span class="token string">"gc"</span>, Platform:<span class="token string">"linux/amd64"</span><span class="token punctuation">}</span>
The connection to the server localhost:8080 was refused - did you specify the right <span class="token function">host</span> or port?
</code></pre> 
<h5><a id="Q4_3035"></a>Q4：桥接问题</h5> 
<p>启动时我们看到如下报错：</p> 
<pre><code class="prism language-bash">stderr:error execution phase preflight: <span class="token punctuation">[</span>preflight<span class="token punctuation">]</span> Some fatal errors occurred:        <span class="token punctuation">[</span>ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables<span class="token punctuation">]</span>: /proc/sys/net/bridge/bridge-nf-call-iptables contents are not <span class="token builtin class-name">set</span> to <span class="token number">1</span><span class="token punctuation">[</span>preflight<span class="token punctuation">]</span> If you know what you are doing, you can <span class="token function">make</span> a check non-fatal with <span class="token variable"><span class="token variable">`</span>--ignore-preflight-errors<span class="token operator">=</span><span class="token punctuation">..</span>.<span class="token variable">`</span></span>
</code></pre> 
<p>根据提示进行解决即可：</p> 
<pre><code class="prism language-bash"><span class="token builtin class-name">echo</span> <span class="token string">'1'</span> <span class="token operator">&gt;</span> /proc/sys/net/bridge/bridge-nf-call-iptables
</code></pre> 
<h5><a id="Q5_3053"></a>Q5：初始化失败报错，升级内核</h5> 
<pre><code class="prism language-bash">error execution phase preflight: <span class="token punctuation">[</span>preflight<span class="token punctuation">]</span> Some fatal errors occurred:
<span class="token punctuation">[</span>ERROR SystemVerification<span class="token punctuation">]</span>: unexpected kernel config: CONFIG_CGROUP_PIDS
<span class="token punctuation">[</span>ERROR SystemVerification<span class="token punctuation">]</span>: missing required cgroups: pids
<span class="token punctuation">[</span>preflight<span class="token punctuation">]</span> If you know what you are doing, you can <span class="token function">make</span> a check non-fatal with --ignore-preflight-errors<span class="token operator">=</span><span class="token punctuation">..</span>.
To see the stack trace of this error execute with --v<span class="token operator">=</span><span class="token number">5</span> or higher
</code></pre> 
<p><strong>首先，你要在cat /boot/config-<code>uname -r</code> | grep CGROUP这个文件里面加CONFIG_CGROUP_PIDS=y，</strong></p> 
<p>然后你再升级一下内核就可以了。</p> 
<p><a href="https://blog.csdn.net/wulinpingailxr/article/details/96480526">内核升级参考</a></p> 
<pre><code class="prism language-bash"><span class="token function">rpm</span> --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
<span class="token function">rpm</span> -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm
yum --disablerepo<span class="token operator">=</span><span class="token string">"*"</span> --enablerepo<span class="token operator">=</span><span class="token string">"elrepo-kernel"</span> list available
yum --enablerepo<span class="token operator">=</span>elrepo-kernel <span class="token function">install</span> kernel-ml
<span class="token function">cp</span> /etc/default/grub  /etc/default/grub_bak
<span class="token function">vi</span> /etc/default/grub
grub2-mkconfig -o /boot/grub2/grub.cfg
 systemctl <span class="token builtin class-name">enable</span> docker.service
re
</code></pre> 
<p>查看内核版本</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>root@cdh1 ~<span class="token punctuation">]</span><span class="token comment"># awk -F\' ' $1=="menuentry " {print i++ " : " $2}' /etc/grub2.cfg</span>
<span class="token number">0</span> <span class="token builtin class-name">:</span> CentOS Linux <span class="token punctuation">(</span><span class="token number">6.1</span>.8-1.el7.elrepo.x86_64<span class="token punctuation">)</span> <span class="token number">7</span> <span class="token punctuation">(</span>Core<span class="token punctuation">)</span>
<span class="token number">1</span> <span class="token builtin class-name">:</span> CentOS Linux <span class="token punctuation">(</span><span class="token number">3.10</span>.0-327.4.5.el7.x86_64<span class="token punctuation">)</span> <span class="token number">7</span> <span class="token punctuation">(</span>Core<span class="token punctuation">)</span>
<span class="token number">2</span> <span class="token builtin class-name">:</span> CentOS Linux <span class="token punctuation">(</span><span class="token number">3.10</span>.0-327.el7.x86_64<span class="token punctuation">)</span> <span class="token number">7</span> <span class="token punctuation">(</span>Core<span class="token punctuation">)</span>
<span class="token number">3</span> <span class="token builtin class-name">:</span> CentOS Linux <span class="token punctuation">(</span><span class="token number">0</span>-rescue-e147b422673549a3b4fda77127bd4bcd<span class="token punctuation">)</span> <span class="token number">7</span> <span class="token punctuation">(</span>Core<span class="token punctuation">)</span>
</code></pre> 
<p>编辑 /etc/default/grub 文件</p> 
<p>设置 GRUB_DEFAULT=0，通过上面查询显示的编号为 0 的内核作为默认内核：</p> 
<pre><code class="prism language-bash"><span class="token punctuation">(</span>sed ‘s, release .*$,g’ /etc/system-release<span class="token punctuation">)</span>"
<span class="token assign-left variable">GRUB_DEFAULT</span><span class="token operator">=</span><span class="token number">0</span>
<span class="token assign-left variable">GRUB_DISABLE_SUBMENU</span><span class="token operator">=</span>true
<span class="token assign-left variable">GRUB_TERMINAL_OUTPUT</span><span class="token operator">=</span>“console”
<span class="token assign-left variable">GRUB_CMDLINE_LINUX</span><span class="token operator">=</span>“crashkernel<span class="token operator">=</span>auto rd.lvm.lv<span class="token operator">=</span>cl/root rhgb quiet”
<span class="token assign-left variable">GRUB_DISABLE_RECOVERY</span><span class="token operator">=</span>“true”
</code></pre> 
<p>生成 grub 配置文件并重启</p> 
<pre><code class="prism language-bash">$ grub2-mkconfig -o /boot/grub2/grub.cfg
Generating grub configuration <span class="token function">file</span> …
Found linux image: /boot/vmlinuz-5.12.1-1.el7.elrepo.x86_64
Found initrd image: /boot/initramfs-5.12.1-1.el7.elrepo.x86_64.img
Found linux image: /boot/vmlinuz-3.10.0-1160.25.1.el7.x86_64
Found initrd image: /boot/initramfs-3.10.0-1160.25.1.el7.x86_64.img
Found linux image: /boot/vmlinuz-3.10.0-1160.el7.x86_64
Found initrd image: /boot/initramfs-3.10.0-1160.el7.x86_64.img
Found linux image: /boot/vmlinuz-0-rescue-16ba4d58b7b74338bfd60f5ddb0c8483
Found initrd image: /boot/initramfs-0-rescue-16ba4d58b7b74338bfd60f5ddb0c8483.img
<span class="token keyword">done</span>

$ <span class="token function">reboot</span>
</code></pre> 
<p>查看内核版本</p> 
<pre><code class="prism language-bash">$ <span class="token function">uname</span> -sr
Linux <span class="token number">6.1</span>.8-1.el7.elrepo.x86_64
</code></pre> 
<h5><a id="Dashboard_3141"></a>启动Dashboard</h5> 
<p>使用如下命令启动dashboard：</p> 
<pre><code class="prism language-bash">$ minikube addons <span class="token builtin class-name">enable</span> dashboard
    ▪ Using image kubernetesui/dashboard:v2.3.1
    ▪ Using image kubernetesui/metrics-scraper:v1.0.7
   Some dashboard features require the metrics-server addon. To <span class="token builtin class-name">enable</span> all features please run:

        minikube addons <span class="token builtin class-name">enable</span> metrics-server


   The <span class="token string">'dashboard'</span> addon is enabled

$ minikube addons <span class="token builtin class-name">enable</span> metrics-server
    ▪ Using image k8s.gcr.io/metrics-server/metrics-server:v0.4.2
   The <span class="token string">'metrics-server'</span> addon is enabled

$ minikube dashboard
   Verifying dashboard health <span class="token punctuation">..</span>.
   Launching proxy <span class="token punctuation">..</span>.
   Verifying proxy health <span class="token punctuation">..</span>.
   Opening http://127.0.0.1:39887/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ <span class="token keyword">in</span> your default browser<span class="token punctuation">..</span>.
   http://127.0.0.1:39887/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/
</code></pre> 
<p>直接用minikube addons启动Dashboard，提示还需要一并启用metrics-server，都enable下</p> 
<h5><a id="_3174"></a>终于全部启动了</h5> 
<p><img src="https://images2.imgbox.com/4d/44/vuy3YXI7_o.png" alt=""></p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>minikube@cdh1 root<span class="token punctuation">]</span>$ kubectl get pod -A
</code></pre> 
<p><img src="https://images2.imgbox.com/f2/a8/vXeOzAu7_o.png" alt=""></p> 
<h5><a id="Windowsdashborad_3190"></a>如何从宿主机也就是我们的Windows中访问dashborad呢</h5> 
<p>从上面输出的信息可以看到，dashboard绑定的IP地址为本地回环地址127.0.0.1，这意味着该地址只能在本地访问。</p> 
<p>虚拟机是没有GUI的，那么如何从宿主机也就是我们的Windows中访问dashborad呢？</p> 
<p>可以进行如下操作：</p> 
<p>使用 nginx、openresty 进行 反向代理</p> 
<pre><code class="prism language-bash">http://127.0.0.1:39887/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/

http://k8s:80/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/
</code></pre> 
<h5><a id="minikube_3210"></a>直接使用minikube</h5> 
<p>此时的<code>minikube kubectl --</code>就相当于k8s里的<code>kubectl</code>命令，当然我们实际不会这样使用，</p> 
<p>我们可以minikube的命令给alias一下：</p> 
<pre><code class="prism language-bash">$ <span class="token builtin class-name">alias</span> <span class="token assign-left variable">kubectl</span><span class="token operator">=</span><span class="token string">"minikube kubectl --"</span>
</code></pre> 
<p>此时再直接运行<code>kubectl</code>命令：</p> 
<pre><code class="prism language-bash">$ kubectl
kubectl controls the Kubernetes cluster manager.

 Find <span class="token function">more</span> information at: https://kubernetes.io/docs/reference/kubectl/overview/

Basic Commands <span class="token punctuation">(</span>Beginner<span class="token punctuation">)</span>:
  create        Create a resource from a <span class="token function">file</span> or from stdin
  expose        Take a replication controller, service, deployment or pod and expose it as a new
Kubernetes <span class="token function">service</span>
  run           在集群中运行一个指定的镜像
  <span class="token builtin class-name">set</span>           为 objects 设置一个指定的特征

Basic Commands <span class="token punctuation">(</span>Intermediate<span class="token punctuation">)</span>:
  explain       Get documentation <span class="token keyword">for</span> a resource
  get           显示一个或更多 resources
  edit          在服务器上编辑一个资源
  delete        Delete resources by <span class="token function">file</span> names, stdin, resources and names, or by resources and
label selector
<span class="token punctuation">..</span>.
</code></pre> 
<p>好了，就可以愉快的k8s玩耍了。</p> 
<h5><a id="minikube_3252"></a>minikube重建</h5> 
<p>如果环境搞乱了想重新部署，很简单就可以实现</p> 
<pre><code class="prism language-bash">$ minikube delete
$ minikube start --driver<span class="token operator">=</span>docker // 这里指定了用docker，不指定也会自动检测
</code></pre> 
<h4><a id="dockercompose_to_minikube_3261"></a>docker-compose to minikube</h4> 
<p>需要将docker-compose.yaml转变为k8s deploy、svc、configmap，以swagger-ui为例</p> 
<p>docker-compose.yaml</p> 
<pre><code class="prism language-yaml"><span class="token key atrule">version</span><span class="token punctuation">:</span> <span class="token string">"3.0"</span>
<span class="token key atrule">services</span><span class="token punctuation">:</span>
  <span class="token key atrule">swiagger-ui</span><span class="token punctuation">:</span>
    <span class="token key atrule">image</span><span class="token punctuation">:</span> swaggerapi/swagger<span class="token punctuation">-</span>ui
    <span class="token key atrule">container_name</span><span class="token punctuation">:</span> swagger_ui_container
    <span class="token key atrule">ports</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token string">"9092:8080"</span>
    <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> ../docs/openapi<span class="token punctuation">:</span>/usr/share/nginx/html/doc
    <span class="token key atrule">environment</span><span class="token punctuation">:</span>
      <span class="token key atrule">API_URL</span><span class="token punctuation">:</span> ./doc/api.yaml
</code></pre> 
<p>k8s deploy、svc、configmap</p> 
<pre><code class="prism language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">io.minikube.service</span><span class="token punctuation">:</span> swagger<span class="token punctuation">-</span>ui
  <span class="token key atrule">name</span><span class="token punctuation">:</span> swagger<span class="token punctuation">-</span>ui
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">1</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
      <span class="token key atrule">io.minikube.service</span><span class="token punctuation">:</span> swagger<span class="token punctuation">-</span>ui
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">io.minikube.service</span><span class="token punctuation">:</span> swagger<span class="token punctuation">-</span>ui
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">env</span><span class="token punctuation">:</span>
            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> SWAGGER_JSON
              <span class="token key atrule">value</span><span class="token punctuation">:</span> /openapi/api.yaml
          <span class="token key atrule">image</span><span class="token punctuation">:</span> swaggerapi/swagger<span class="token punctuation">-</span>ui
          <span class="token key atrule">name</span><span class="token punctuation">:</span> swagger<span class="token punctuation">-</span>ui
          <span class="token key atrule">ports</span><span class="token punctuation">:</span>
            <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">8080</span>
          <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> IfNotPresent
          <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
            <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /openapi
              <span class="token key atrule">name</span><span class="token punctuation">:</span> swagger<span class="token punctuation">-</span>ui<span class="token punctuation">-</span>cm
      <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> swagger<span class="token punctuation">-</span>ui<span class="token punctuation">-</span>cm
          <span class="token key atrule">configMap</span><span class="token punctuation">:</span>
            <span class="token key atrule">name</span><span class="token punctuation">:</span> swagger<span class="token punctuation">-</span>ui<span class="token punctuation">-</span>cm

<span class="token punctuation">---</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> swagger<span class="token punctuation">-</span>ui
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">io.minikube.service</span><span class="token punctuation">:</span> swagger<span class="token punctuation">-</span>ui
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">ports</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">8080</span>
      <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP
      <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">8080</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">io.minikube.service</span><span class="token punctuation">:</span> swagger<span class="token punctuation">-</span>ui

<span class="token punctuation">---</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ConfigMap
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> swagger<span class="token punctuation">-</span>ui<span class="token punctuation">-</span>cm
<span class="token key atrule">data</span><span class="token punctuation">:</span>
  <span class="token key atrule">api.yaml</span><span class="token punctuation">:</span> <span class="token punctuation">|</span><span class="token scalar string">
    openapi: 3.0.0
      version: "1.0"</span>
<span class="token punctuation">...</span>
</code></pre> 
<h4><a id="minikube__3344"></a>部署minikube 遇到的问题</h4> 
<h5><a id="_3346"></a>外部访问问题</h5> 
<p>minikube内的k8s网络明显与host不一样，不论使用不使用Type=NodePort，都无法直接访问，都需要用port-forward</p> 
<p>这里查了些资料，可能的一个解释是，minikube使用docker-machine为底层，</p> 
<p>实现可以部署到vm、container、host等基础设施上，docker-machine会构建自身的docker环境，与host不同，网络也不在一个平面，所以使用NodePort，从host也无法访问，</p> 
<p>需要借助</p> 
<p><code>kubectl port-forward --address=0.0.0.0 service/hello-minikube 7080:8080</code>。</p> 
<p>------------ 2021-11-26 update---------------</p> 
<p>可以使用以下命令获取minikube的ip，然后通过该ip+nodeport访问</p> 
<pre><code class="prism language-bash">$ minikube <span class="token function">ip</span>
<span class="token number">192.168</span>.49.2
</code></pre> 
<p>也可以通过一下命令直接获取对应service的url</p> 
<pre><code class="prism language-bash">$ minikube <span class="token function">service</span> hello-minikube --url
http://192.168.49.2:30660
</code></pre> 
<h5><a id="pull_image_3374"></a>pull image问题</h5> 
<p>minikube内的docker daemon与host docker daemon不一样，且k8s不与host上的docker共享信息，</p> 
<p>host上的docker images和daemon.json配置对minikube内的docker daemon不可见，</p> 
<p>minikube内的docker daemon总是从dockerhub pull image，会遇到</p> 
<pre><code class="prism language-bash">You have reached your pull rate limit. 
You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limits. 
You must authenticate your pull requests.
</code></pre> 
<p>解决办法</p> 
<p>可以先用<code>host docker pull images</code>，然后load到minikube</p> 
<pre><code class="prism language-bash">$ docker pull <span class="token operator">&lt;</span>image name<span class="token operator">&gt;</span>
$ minikube image load <span class="token operator">&lt;</span>image name<span class="token operator">&gt;</span>
</code></pre> 
<p>注意k8s默认的imagePullPolicy</p> 
<blockquote> 
 <p>Default image pull policy<br> When you (or a controller) submit a new Pod to the API server, your cluster sets the <code>imagePullPolicy</code> field when specific conditions are met:<br> if you omit the <code>imagePullPolicy</code> field, and the tag for the container image is <code>:latest</code>, <code>imagePullPolicy</code> is automatically set to <code>Always</code>;<br> if you omit the <code>imagePullPolicy</code> field, and you don’t specify the tag for the container image, <code>imagePullPolicy</code> is automatically set to <code>Always</code>;<br> if you omit the <code>imagePullPolicy</code> field, and you specify the tag for the container image that isn’t <code>:latest</code>, the <code>imagePullPolicy</code> is automatically set to <code>IfNotPresent</code>.</p> 
</blockquote> 
<ol><li>如果没有设置imagePullPolicy，但image tag是latest，那么默认就是imagePullPolicy: Always</li><li>如果没有设置imagePullPolicy，也没有设置image tag，那么默认也是imagePullPolicy: Always</li><li>如果设置了image tag，默认imagePullPolicy: IfNotPresent</li></ol> 
<p>为了不出错，建议直接指定 imagePullPolicy: IfNotPresent</p> 
<h4><a id="POD__3415"></a>POD 容器的问题</h4> 
<h5><a id="K8s_3417"></a>K8s的常用命令</h5> 
<p><code>kubectl get pods -A</code> 查看所有的命令空间下的pods</p> 
<p><code>kubectl describe node</code> 查看所有节点的cpu和内存使用情况</p> 
<p><code>kubectl describe node nodename |grep Taints</code> 查看该节点是否可达，是否可以部署内容；一般三种情况</p> 
<p><code>kubectl -n namespace名 logs -f --tail 200 pod名 -n namespace</code> 查看命名空间下的 pods日志（运行后才有日志,此命令查看实时的200条日志）</p> 
<p><code>kubectl exec -it -n namespace名 pod名 sh</code> 进入pod</p> 
<p><code>kubectl get services,pods -o wide</code> 查看所有的pods和services， -o 输出格式为wide或者yaml</p> 
<p><code>kubectl describe pod pod名 -n namespace名</code> 查看pod的描述状态</p> 
<p><code>kubectl describe job/ds/deployment pod名 -n namespace名</code> 查看三个控制器下pod描述</p> 
<p><code>kubectl exec -it pod名 -c 容器名 -- /bin/bash</code></p> 
<p><code>kubectl get pod pod名 -n namespace名 -oyaml | kubectl replace --force -f -</code> 重启pod命令</p> 
<p><code>kubectl get pods -n namespace名</code></p> 
<p><code>kubectl get pods pod名 -o yaml -n namespace名</code></p> 
<p><code>kubectl get ds -n namespace名</code> 查看命名空间下daemonset的信息</p> 
<p><code>kubectl get ds ds名 -o yaml -n namespace名</code></p> 
<p><code>kubectl get deployment -n namespace名</code></p> 
<p><code>kubectl get deployment deployment名 -o yaml -n namespace名</code><br> 后面加–force --grace-period=0；立刻强制删除与下面的一起用</p> 
<p>删除当前的应用：<code>kubectl delete ds daemonset名 -n namespace名</code>、<code>kubectl delete deployment deployment名 -n namespace名</code>（备注：如果是没删除ds/deployment/job，直接删除对应的pod（<code>kubectl delete pod pod名 -n namespace名</code>，pod会一直重启）</p> 
<p>查看容器实时最新的10条日志 <code>docker logs -f -t --tail 10 容器名</code></p> 
<p><code>kubectl delete job jobname -n namespace名</code>（job任务也是如此）</p> 
<h5><a id="pod_3460"></a>查看所有的pod，看看哪些有问题</h5> 
<p><code>kubectl get pods -A</code></p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>root@cdh1 ~<span class="token punctuation">]</span><span class="token comment"># kubectl get pods -A</span>
NAMESPACE              NAME                                         READY   STATUS             RESTARTS   AGE
kube-system            coredns-6d8c4cb4d-grphf                      <span class="token number">1</span>/1     Running            <span class="token number">0</span>          8m16s
kube-system            etcd-cdh1                                    <span class="token number">1</span>/1     Running            <span class="token number">1</span>          8m29s
kube-system            kube-apiserver-cdh1                          <span class="token number">1</span>/1     Running            <span class="token number">1</span>          8m31s
kube-system            kube-controller-manager-cdh1                 <span class="token number">1</span>/1     Running            <span class="token number">1</span>          8m29s
kube-system            kube-proxy-78trt                             <span class="token number">1</span>/1     Running            <span class="token number">0</span>          8m16s
kube-system            kube-scheduler-cdh1                          <span class="token number">1</span>/1     Running            <span class="token number">1</span>          8m29s
kube-system            storage-provisioner                          <span class="token number">0</span>/1     ImagePullBackOff   <span class="token number">0</span>          8m28s
kubernetes-dashboard   dashboard-metrics-scraper-5496b5d99f-llh9t   <span class="token number">0</span>/1     ImagePullBackOff   <span class="token number">0</span>          6m28s
kubernetes-dashboard   kubernetes-dashboard-58b48666f8-hsn8g        <span class="token number">0</span>/1     ImagePullBackOff   <span class="token number">0</span>          6m28s
</code></pre> 
<h5><a id="storageprovisioner_ImagePullBackOff__3484"></a>storage-provisioner 的ImagePullBackOff 状态</h5> 
<p><code>kubectl get pods -A</code></p> 
<p><code>kubectl describe pod XXX -n kube-system</code></p> 
<p>通过kubectl describe命令详细查看redis-master-0这个pod：</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>root@cdh1 ~<span class="token punctuation">]</span><span class="token comment"># kubectl describe pod  storage-provisioner -n kube-system</span>
Name:         storage-provisioner
Namespace:    kube-system
Priority:     <span class="token number">0</span>
Node:         cdh1/10.0.2.15
Start Time:   Sun, <span class="token number">29</span> Jan <span class="token number">2023</span> 05:17:14 +0800
Labels:       addonmanager.kubernetes.io/mode<span class="token operator">=</span>Reconcile
              integration-test<span class="token operator">=</span>storage-provisioner
Annotations:  <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
Status:       Pending
IP:           <span class="token number">10.0</span>.2.15
IPs:
  IP:  <span class="token number">10.0</span>.2.15
Containers:
  storage-provisioner:
    Container ID:
    Image:         registry.aliyuncs.com/google_containers/k8s-minikube/storage-provisioner:v5
    Image ID:
    Port:          <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
    Host Port:     <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
    Command:
      /storage-provisioner
    State:          Waiting
      Reason:       ImagePullBackOff
    Ready:          False
    Restart Count:  <span class="token number">0</span>
    Environment:    <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
    Mounts:
      /tmp from tmp <span class="token punctuation">(</span>rw<span class="token punctuation">)</span>
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-l5h2b <span class="token punctuation">(</span>ro<span class="token punctuation">)</span>
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  tmp:
    Type:          HostPath <span class="token punctuation">(</span>bare <span class="token function">host</span> directory volume<span class="token punctuation">)</span>
    Path:          /tmp
    HostPathType:  Directory
  kube-api-access-l5h2b:
    Type:                    Projected <span class="token punctuation">(</span>a volume that contains injected data from multiple sources<span class="token punctuation">)</span>
    TokenExpirationSeconds:  <span class="token number">3607</span>
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <span class="token operator">&lt;</span>nil<span class="token operator">&gt;</span>
    DownwardAPI:             <span class="token boolean">true</span>
QoS Class:                   BestEffort
Node-Selectors:              <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute <span class="token assign-left variable">op</span><span class="token operator">=</span>Exists <span class="token keyword">for</span> 300s
                             node.kubernetes.io/unreachable:NoExecute <span class="token assign-left variable">op</span><span class="token operator">=</span>Exists <span class="token keyword">for</span> 300s
Events:
  Type     Reason            Age                    From               Message
  ----     ------            ----                   ----               -------
  Warning  FailedScheduling  27m                    default-scheduler  <span class="token number">0</span>/1 nodes are available: <span class="token number">1</span> node<span class="token punctuation">(</span>s<span class="token punctuation">)</span> had taint <span class="token punctuation">{<!-- --></span>node.kubernetes.io/not-ready: <span class="token punctuation">}</span>, that the pod didn<span class="token string">'t tolerate.
  Normal   Scheduled         27m                    default-scheduler  Successfully assigned kube-system/storage-provisioner to cdh1
  Normal   Pulling           26m (x4 over 27m)      kubelet            Pulling image "registry.aliyuncs.com/google_containers/k8s-minikube/storage-provisioner:v5"
  Warning  Failed            26m (x4 over 27m)      kubelet            Failed to pull image "registry.aliyuncs.com/google_containers/k8s-minikube/storage-provisioner:v5": rpc error: code = Unknown desc = Error response from daemon: pull access denied for registry.aliyuncs.com/google_containers/k8s-minikube/storage-provisioner, repository does not exist or may require '</span>docker login': denied: requested access to the resource is denied
  Warning  Failed            26m <span class="token punctuation">(</span>x4 over 27m<span class="token punctuation">)</span>      kubelet            Error: ErrImagePull
  Warning  Failed            25m <span class="token punctuation">(</span>x6 over 27m<span class="token punctuation">)</span>      kubelet            Error: ImagePullBackOff
  Normal   BackOff           2m39s <span class="token punctuation">(</span>x109 over 27m<span class="token punctuation">)</span>  kubelet            Back-off pulling image <span class="token string">"registry.aliyuncs.com/google_containers/k8s-minikube/storage-provisioner:v5"</span>
</code></pre> 
<p>dashboard-metrics-scraper-5496b5d99f-wj2d9</p> 
<p>我们查看一下storage-provisioner pod的imagePullPolicy：</p> 
<pre><code class="prism language-yaml"><span class="token comment"># kubectl get pod dashboard-metrics-scraper-5496b5d99f-wj2d9  -n kubernetes-dashboard -o yaml</span>
<span class="token punctuation">...</span> <span class="token punctuation">...</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">command</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> /storage<span class="token punctuation">-</span>provisioner
    <span class="token key atrule">image</span><span class="token punctuation">:</span> registry.cn<span class="token punctuation">-</span>hangzhou.aliyuncs.com/google_containers/k8s<span class="token punctuation">-</span>minikube/storage<span class="token punctuation">-</span>provisioner<span class="token punctuation">:</span>v5
    <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> IfNotPresent
    <span class="token key atrule">name</span><span class="token punctuation">:</span> storage<span class="token punctuation">-</span>provisioner
</code></pre> 
<p>我们发现storage-provisioner的imagePullPolicy为ifNotPresent，这意味着如果本地有storage-provisioner:v5这个镜像的话，minikube不会再去远端下载该image。这样我们可以先将storage-provisioner:v5下载到本地并重新tag为registry.cn-hangzhou.aliyuncs.com/google_containers/k8s-minikube/storage-provisioner:v5。</p> 
<h4><a id="minikubeharber_3578"></a>启动minikube时指定harber仓库</h4> 
<p>如果minikube已经创建过、</p> 
<p>则需要先 <code>minikube delete</code></p> 
<p>再执行下述语句</p> 
<pre><code class="prism language-bash">minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>docker  --registry-mirror<span class="token operator">=</span>https://harbor.daemon.io --insecure-registry<span class="token operator">=</span><span class="token number">192.168</span>.56.121:85

下面的老的启动语句

minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span> --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span>   --force --driver<span class="token operator">=</span>docker
</code></pre> 
<p><code>kubectl get pods -A </code></p> 
<p><img src="https://images2.imgbox.com/13/c3/0trpaVJQ_o.png" alt=""></p> 
<p>验证:</p> 
<pre><code class="prism language-bash">$ minikube <span class="token function">ssh</span>
$ <span class="token function">curl</span> http://192.168.56.121:85/v2/_catalog
</code></pre> 
<p><img src="https://images2.imgbox.com/0f/b9/3hhig0C5_o.png" alt=""></p> 
<p>在里边拉取镜像</p> 
<pre><code class="prism language-bash">docker pull harbor.daemon.io/demo/nginx:latest

docker pull <span class="token number">192.168</span>.56.121:85/demo/nginx:latest
</code></pre> 
<h5><a id="minikube___3625"></a>minikube 复制证书</h5> 
<p>您应该可以使用 minikube ssh 登录机器，然后按照此处的说明进行操作</p> 
<p>https://docs.docker.com/engine/security/certificates/#understanding-the-configuration</p> 
<p>将 CA 放在适当的目录 (/etc/docker/certs.d/[registry hostname]/) 中。</p> 
<pre><code class="prism language-bash">  /etc/docker/certs.d/        <span class="token operator">&lt;</span>-- Certificate directory
    └── localhost:5000          <span class="token operator">&lt;</span>-- Hostname:port
       ├── client.cert          <span class="token operator">&lt;</span>-- Client certificate
       ├── client.key           <span class="token operator">&lt;</span>-- Client key
       └── ca.crt               <span class="token operator">&lt;</span>-- Root CA that signed
                                    the registry certificate, <span class="token keyword">in</span> PEM
                                    
                                    
centos

<span class="token function">cp</span> /usr/local/harbor/ssl/harbor.daemon.io.cert /etc/docker/certs.d/harbor.daemon.io/harbor.daemon.io.cert 


<span class="token function">cp</span> /usr/local/harbor/ssl/harbor.daemon.io.key /etc/docker/certs.d/harbor.daemon.io/harbor.daemon.io.key 
</code></pre> 
<p>您不需要重新启动守护程序即可使其工作。</p> 
<p>具体的操作如下：</p> 
<p>我通过以下步骤处理了它：<br> 进入minikube的ssh</p> 
<p><code>minikube ssh</code></p> 
<p>使用sudo更改docker密码</p> 
<p><code>sudo passwd docker</code></p> 
<p>并创建新密码，因此现在我知道了docker用户密码 123456</p> 
<p>使用scp命令将文件复制到minikube</p> 
<p><code>scp /local/path/to/file/ docker@minikube IP:/your/destination/folder/</code></p> 
<pre><code class="prism language-bash">minikube <span class="token function">ssh</span>

<span class="token punctuation">[</span>minikube@centos1 logs<span class="token punctuation">]</span>$ minikube <span class="token function">ip</span>
<span class="token number">192.168</span>.49.2

从 centos 到 minikube
<span class="token function">scp</span> -r /etc/docker/certs.d  docker@192.168.49.2:/etc/docker/

在 minikube 复制 centos 的文件
<span class="token function">scp</span> -r   root@192.168.56.121:/etc/docker/certs.d  /etc/docker/



minikube 用户需要切换到 root， 密码也是 root
然后才能复制

</code></pre> 
<p>之后，它只要求minikube docker用户密码，123456</p> 
<p><img src="https://images2.imgbox.com/b6/c9/xq129p0n_o.png" alt=""></p> 
<p>再拉</p> 
<pre><code class="prism language-bash">docker pull harbor.daemon.io/demo/nginx:latest


报证书错误

 response from daemon: Get <span class="token string">"https://harbor.daemon.io/v2/"</span><span class="token builtin class-name">:</span> x509: certificate is valid <span class="token keyword">for</span> spasipark.ru, not harbor.daemon.io
</code></pre> 
<pre><code class="prism language-bash">minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1     --force --driver<span class="token operator">=</span>docker


<span class="token comment"># 进入minikube虚拟机</span>
minikube <span class="token function">ssh</span>

<span class="token function">sudo</span> <span class="token function">su</span>
</code></pre> 
<h5><a id="minikube_3723"></a>进入minikube虚拟机</h5> 
<pre><code class="prism language-bash">minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1     --force --driver<span class="token operator">=</span>docker


<span class="token comment"># 进入minikube虚拟机</span>
minikube <span class="token function">ssh</span>

<span class="token comment"># 编辑Docker配置文件</span>
<span class="token function">sudo</span> <span class="token function">su</span>
<span class="token function">sudo</span> <span class="token function">vi</span> /etc/docker/daemon.json
</code></pre> 
<h6><a id="_3737"></a>增加私仓地址配置</h6> 
<p>增加registry-mirrors属性配置到JSON，其中<code>host.minikube.internal:5000</code>是宿主电脑搭建的docker私有仓库地址，也可以配置阿里公网或者公司的私有仓库地址</p> 
<pre><code class="prism language-bash"><span class="token function">cat</span> <span class="token operator">&lt;&lt;</span> <span class="token string">EOF<span class="token bash punctuation"> <span class="token operator">&gt;&gt;</span> /etc/docker/daemon.json</span>
{
  "registry-mirrors": [
    "http://192.168.56.121:85"
    ]
}
EOF</span>


<span class="token function">cat</span> <span class="token operator">&lt;&lt;</span> <span class="token string">EOF<span class="token bash punctuation"> <span class="token operator">&gt;&gt;</span> /etc/docker/daemon.json</span>
{
  "registry-mirrors": [
    "https://bjtzu1jb.mirror.aliyuncs.com",
    "http://f1361db2.m.daocloud.io",
    "https://hub-mirror.c.163.com",
    "https://docker.mirrors.ustc.edu.cn",
    "https://reg-mirror.qiniu.com",
    "https://dockerhub.azk8s.cn",
    "https://harbor.daemon.io"
    ]
}    
EOF</span>


  
  在 minikube 复制 centos 的文件
<span class="token function">scp</span>  root@192.168.56.121:/etc/docker/daemon.json  /etc/docker/


<span class="token function">cat</span> <span class="token operator">&lt;&lt;</span> <span class="token string">EOF<span class="token bash punctuation"> <span class="token operator">&gt;&gt;</span> /etc/hosts</span>
192.168.56.121  harbor.daemon.io
EOF</span>


在 minikube 复制 centos 的文件
<span class="token function">scp</span> -r   root@192.168.56.121:/etc/docker/certs.d  /etc/docker/

</code></pre> 
<h6><a id="Docker_3781"></a>重启虚拟机的Docker</h6> 
<pre><code class="prism language-bash"><span class="token function">sudo</span> systemctl daemon-reload
<span class="token function">sudo</span> systemctl restart docker
</code></pre> 
<h6><a id="_3788"></a>测试</h6> 
<p>依旧是在minikube虚拟机里测试，是否可以拉取你push到私有仓库到镜像</p> 
<pre><code class="prism language-bash">docker pull test/springboot:1.0.0
</code></pre> 
<h4><a id="____3800"></a>命令清单：尼恩用过的 启动 命令清单 （都是血和泪）</h4> 
<p>通过下面的命令，可以看出血泪的安装史</p> 
<p>建议大家用尼恩的镜像，不用二次安装了</p> 
<pre><code class="prism language-bash">minikube start --image-repository<span class="token operator">=</span>registry.cn-hangzhou.aliyuncs.com/google_containers --registry-mirror<span class="token operator">=</span>https://ovfftd6p.mirror.aliyuncs.com  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>docker 

<span class="token function">sudo</span> minikube start --kubernetes-version<span class="token operator">=</span>v1.23.1 --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span> --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span>


minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span> --registry-mirror<span class="token operator">=</span><span class="token string">'https://ovfftd6p.mirror.aliyuncs.com'</span>  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>docker



minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span> --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>docker



minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span> --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>docker


minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span> --registry-mirror<span class="token operator">=</span><span class="token string">'https://ovfftd6p.mirror.aliyuncs.com'</span>  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>docker


minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-repository<span class="token operator">=</span><span class="token string">'registry.aliyuncs.com/google_containers'</span> --registry-mirror<span class="token operator">=</span><span class="token string">'https://ovfftd6p.mirror.aliyuncs.com'</span>  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>docker


minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1 --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>docker


minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-repository<span class="token operator">=</span><span class="token string">'registry.aliyuncs.com/google_containers'</span> --registry-mirror<span class="token operator">=</span><span class="token string">'https://ovfftd6p.mirror.aliyuncs.com'</span>  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span>   --force --driver<span class="token operator">=</span>none


<span class="token function">su</span> minikube

minikube delete

minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span> --image-repository<span class="token operator">=</span><span class="token string">'registry.aliyuncs.com/google_containers'</span> --registry-mirror<span class="token operator">=</span><span class="token string">'https://ovfftd6p.mirror.aliyuncs.com'</span>    --force --driver<span class="token operator">=</span>docker



minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span> --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span>   --force --driver<span class="token operator">=</span>docker  --cpus <span class="token number">4</span>  --memory <span class="token number">5120</span>

 minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1  --image-mirror-country<span class="token operator">=</span><span class="token string">'cn'</span> --image-repository<span class="token operator">=</span><span class="token string">'registry.cn-hangzhou.aliyuncs.com/google_containers'</span>   --force --driver<span class="token operator">=</span>docker  --cpus <span class="token number">4</span>  --memory <span class="token number">5120</span> --feature-gates<span class="token operator">=</span>EphemeralContainers<span class="token operator">=</span>true

 minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1     --force --driver<span class="token operator">=</span>docker --cpus <span class="token number">4</span>  --memory <span class="token number">5120</span> --feature-gates<span class="token operator">=</span>EphemeralContainers<span class="token operator">=</span>true
</code></pre> 
<h4><a id="minikube_3852"></a>minikube常用命令</h4> 
<h5><a id="_3854"></a>一、基本命令</h5> 
<ol><li>启动集群：<code>minikube start</code></li><li>获取集群状态：<code>minikube status</code></li><li>停止集群：<code>minikube stop</code></li><li>删除集群：<code>minikube delete</code></li><li>暂停k8s：<code>minikube pause</code></li><li>恢复暂停的k8s：<code>minikube unpause</code></li></ol> 
<h5><a id="_3865"></a>二、镜像命令</h5> 
<ol><li>配置环境以使用minikube的Docker守护程序：<code>minikube docker-env</code></li><li>配置环境以使用minikube的Podman服务：<code>minikube podman-env</code></li><li>在minikube中添加、删除或推送本地镜像：<code>minikube cache add|delete|list|reload</code></li></ol> 
<h5><a id="_3873"></a>三、配置和管理命令</h5> 
<ol><li>启用或禁用minikube插件：<code>minikube addons</code></li><li>修改持久化的配置值：<code>minikube config</code></li><li>获取或列出当前配置文件(集群)：<code>minikube profile</code></li><li>在IP或端口发生更改时更新kubeconfig：<code>minikube update-context</code></li></ol> 
<h5><a id="_3882"></a>四、网络和连接命令</h5> 
<ol><li>返回连接到一个服务的URL： <code>minikube service hello-minikube</code></li><li>连接到LoadBalancer服务：<code>minikube tunnel</code></li></ol> 
<h5><a id="_3889"></a>五、高级命令</h5> 
<ol><li>将指定目录装入minikube：<code>minikube mount &lt;source directory&gt;:&lt;target directory&gt;</code></li><li>进入minikube虚拟机，整个k8s集群跑在这里面：<code>minikube ssh</code></li><li>运行与集群版本匹配的kubectl二进制文件：<code>minikube kubectl</code></li><li>添加、删除或列出其他节点：<code>minikube node add|start|stop|delete|list</code></li></ol> 
<h5><a id="_3898"></a>六、疑难解答命令</h5> 
<ol><li>检索指定群集的ssh标识密钥路径：<code>minikube ssh-key</code></li><li>查看集群IP：<code>minikube ip</code></li><li>查看集群日志：<code>minikube logs</code></li><li>打印当前和最新的版本号：<code>minikube update-check</code></li><li>打印minikube版本信息：<code>minikube version</code></li></ol> 
<h5><a id="_3908"></a>七、其它命令</h5> 
<p>为shell生成命令补全：<code>minikube completion</code></p> 
<h4><a id="minikube_3914"></a>启动minikube完整的命令清单</h4> 
<pre><code class="prism language-bash">harber  admin <span class="token number">12345</span>
 
https://cdh1/harbor/projects
  
minikube delete
 
minikube start  --kubernetes-version<span class="token operator">=</span>v1.23.1     --force --driver<span class="token operator">=</span>docker --cpus <span class="token number">4</span>  --memory <span class="token number">5120</span>
 
kubectl get pods -A  


<span class="token comment"># 进入minikube虚拟机</span>
minikube <span class="token function">ssh</span>

<span class="token comment"># 切换到root</span>
<span class="token function">sudo</span> <span class="token function">su</span>

<span class="token function">cat</span> <span class="token operator">&lt;&lt;</span> <span class="token string">EOF<span class="token bash punctuation"> <span class="token operator">&gt;&gt;</span> /etc/docker/daemon.json</span>
{
  "registry-mirrors": [
    "https://bjtzu1jb.mirror.aliyuncs.com",
    "http://f1361db2.m.daocloud.io",
    "https://hub-mirror.c.163.com",
    "https://docker.mirrors.ustc.edu.cn",
    "https://reg-mirror.qiniu.com",
    "https://dockerhub.azk8s.cn",
    "https://harbor.daemon.io"
    ]
}    
EOF</span>

 
<span class="token function">cat</span> <span class="token operator">&lt;&lt;</span> <span class="token string">EOF<span class="token bash punctuation"> <span class="token operator">&gt;&gt;</span> /etc/hosts</span>
192.168.56.121  harbor.daemon.io
EOF</span>

 
在 minikube 复制 centos 的文件
<span class="token function">scp</span> -r   root@192.168.56.121:/etc/docker/certs.d  /etc/docker/
   
 
<span class="token number">3</span>.重启虚拟机的Docker

<span class="token function">sudo</span> systemctl daemon-reload
<span class="token function">sudo</span> systemctl restart docker


<span class="token number">4</span>.测试

依旧是在minikube虚拟机里测试，是否可以拉取你push到私有仓库到镜像


docker pull test/springboot:1.0.0


<span class="token builtin class-name">exit</span>
 
minikube addons <span class="token builtin class-name">enable</span> dashboard
 
minikube addons <span class="token builtin class-name">enable</span> metrics-server
    
minikube dashboard
</code></pre> 
<p>使用 nginx、openresty 进行 反向代理</p> 
<pre><code class="prism language-bash">http://127.0.0.1:38225/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/
onnect to <span class="token number">127.0</span>.0.1 port <span class="token number">38225</span>: C
niginx 启动

http://k8s:1099/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/
</code></pre> 
<pre><code class="prism language-bash">kubectl get pods -A  
   
kubectl describe pod  kubernetes-dashboard-6ddd477f4f-vhdgg   -n kubernetes-dashboard
    
kubectl get pods --namespace<span class="token operator">=</span>kubernetes-dashboard <span class="token operator">|</span> <span class="token function">grep</span> dashboard
</code></pre> 
<h4><a id="Helm__4009"></a>Helm 的原理、安装、使用</h4> 
<p>java 使用 maven；前端使用 npm；python 使用 pip；运维使用 yum 或 apt。</p> 
<p>分工不同，诉求却相同，都希望有一种资源管理工具，可以方便查找、下载、安装、使用和分发程序包。</p> 
<p>helm 也一样，它是 k8s 的资源包管理工具。它使我们操作的对象不再是单个资源，而是一个实体。</p> 
<p>比如我们需要一个负载均衡的 web 服务，如果不使用 helm，我们需要写 deployment，service 和 ingress 才可以让集群外部的客户使用。但是如果使用 helm，直接使用一个 install 命令便可以实现相同的功能。</p> 
<p>Helm 是 Deis 开发的一个用于 Kubernetes 应用的包管理工具，主要用来管理 Charts。有点类似于 Ubuntu 中的 APT 或 CentOS 中的 YUM。</p> 
<p>Helm Chart 是用来封装 Kubernetes 原生应用程序的一系列 YAML 文件。可以在你部署应用的时候自定义应用程序的一些 Metadata，以便于应用程序的分发。</p> 
<p>对于应用发布者而言，可以通过 Helm 打包应用、管理应用依赖关系、管理应用版本并发布应用到软件仓库。</p> 
<p>对于使用者而言，使用 Helm 后不用需要编写复杂的应用部署文件，可以以简单的方式在 Kubernetes 上查找、安装、升级、回滚、卸载应用程序。</p> 
<p>推荐使用 helm 的 3.0 版本。</p> 
<h5><a id="Helm__4029"></a>Helm 组件及相关术语</h5> 
<p><strong>Helm</strong></p> 
<p>Helm 是一个命令行下的客户端工具。主要用于 Kubernetes 应用程序 Chart 的创建、打包、发布以及创建和管理本地和远程的 Chart 仓库。</p> 
<p><strong>Tiller</strong></p> 
<p>Tiller 是 Helm 的服务端，部署在 Kubernetes 集群中。Tiller 用于接收 Helm 的请求，并根据 Chart 生成 Kubernetes 的部署文件（ Helm 称为 Release ），然后提交给 Kubernetes 创建应用。Tiller 还提供了 Release 的升级、删除、回滚等一系列功能。</p> 
<p><strong>Chart</strong></p> 
<p>Helm 的软件包，采用 TAR 格式。类似于 APT 的 DEB 包或者 YUM 的 RPM 包，其包含了一组定义 Kubernetes 资源相关的 YAML 文件。</p> 
<p><strong>Repoistory</strong></p> 
<p>Helm 的软件仓库，Repository 本质上是一个 Web 服务器，该服务器保存了一系列的 Chart 软件包以供用户下载，并且提供了一个该 Repository 的 Chart 包的清单文件以供查询。Helm 可以同时管理多个不同的 Repository。</p> 
<p><strong>Release</strong></p> 
<p>使用 helm install 命令在 Kubernetes 集群中部署的 Chart 称为 Release。</p> 
<h5><a id="Helm__4051"></a>Helm 工作原理</h5> 
<p><strong>Chart Install 过程</strong></p> 
<p>Helm 从指定的目录或者 TAR 文件中解析出 Chart 结构信息。<br> Helm 将指定的 Chart 结构和 Values 信息通过 gRPC 传递给 Tiller。<br> Tiller 根据 Chart 和 Values 生成一个 Release。<br> Tiller 将 Release 发送给 Kubernetes 用于生成 Release。</p> 
<p><strong>Chart Update 过程</strong></p> 
<p>Helm 从指定的目录或者 TAR 文件中解析出 Chart 结构信息。<br> Helm 将需要更新的 Release 的名称、Chart 结构和 Values 信息传递给 Tiller。<br> Tiller 生成 Release 并更新指定名称的 Release 的 History。<br> Tiller 将 Release 发送给 Kubernetes 用于更新 Release。</p> 
<p><strong>Chart Rollback 过程</strong></p> 
<p>Helm 将要回滚的 Release 的名称传递给 Tiller。<br> Tiller 根据 Release 的名称查找 History。<br> Tiller 从 History 中获取上一个 Release。<br> Tiller 将上一个 Release 发送给 Kubernetes 用于替换当前 Release。</p> 
<p><strong>helm使用</strong></p> 
<p>1.创建一个空charts<br> helm create test</p> 
<h5><a id="Helm_4079"></a>安装Helm</h5> 
<p>1、先下载helm3压缩包下载地地址</p> 
<p>或者执行 <code>wget https://get.helm.sh/helm-v3.6.3-linux-amd64.tar.gz</code></p> 
<p>2、解压<br> <code>tar -zxvf helm-v3.6.3-linux-amd64.tar.gz</code></p> 
<p>3、拷贝<br> <code>cp linux-amd64/helm /usr/bin</code></p> 
<p>4、是否安装成功<br> <code>helm version</code></p> 
<p>5、添加chart源<br> <code>helm repo add aliyuncs https://apphub.aliyuncs.com</code><br> <code>helm repo add bitnami https://charts.bitnami.com/bitnami</code><br> <code>helm repo update</code></p> 
<p>添加仓库</p> 
<pre><code class="prism language-bash">helm repo <span class="token function">add</span> apphub https://apphub.aliyuncs.com
helm repo <span class="token function">add</span> azure http://mirror.azure.cn/kubernetes/charts/
</code></pre> 
<p>6、查看当前集群有那些chart 库<br> <code>helm repo list</code>查看仓库</p> 
<pre><code class="prism language-bash">helm repo list
NAME        URL
apphub      https://apphub.aliyuncs.com
azure       http://mirror.azure.cn/kubernetes/charts/
</code></pre> 
<p>查询有哪些程序包</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>root@web-test-01 k8s<span class="token punctuation">]</span><span class="token comment"># helm search repo mysql</span>
NAME                                CHART VERSION   APP VERSION DESCRIPTION
apphub/mysql                        <span class="token number">6.8</span>.0           <span class="token number">8.0</span>.19      Chart to create a Highly available MySQL cluster
apphub/mysqldump                    <span class="token number">2.6</span>.0           <span class="token number">2.4</span>.1       A Helm chart to <span class="token builtin class-name">help</span> backup MySQL databases usi<span class="token punctuation">..</span>.
apphub/mysqlha                      <span class="token number">1.0</span>.0           <span class="token number">5.7</span>.13      MySQL cluster with a single master and zero or <span class="token punctuation">..</span>.
apphub/prometheus-mysql-exporter    <span class="token number">0.5</span>.2           v0.11.0     A Helm chart <span class="token keyword">for</span> prometheus mysql exporter with<span class="token punctuation">..</span>.
azure/mysql                         <span class="token number">1.6</span>.2           <span class="token number">5.7</span>.28      Fast, reliable, scalable, and easy to use open-<span class="token punctuation">..</span>.
azure/mysqldump                     <span class="token number">2.6</span>.0           <span class="token number">2.4</span>.1       A Helm chart to <span class="token builtin class-name">help</span> backup MySQL databases usi<span class="token punctuation">..</span>.
</code></pre> 
<p>7、查找安装程序<br> <code>helm search repo nginx</code></p> 
<p>8、安装一个程序<br> <code>helm install nginx bitnami/nginx</code></p> 
<p>9、查询 svc<br> <code>kubectl get svc -n default</code></p> 
<p>10、访问集群</p> 
<p>11、查看当前安装的应用<br> <code>helm list</code></p> 
<p>12、删除应用<br> <code>helm uninstall nginx</code></p> 
<h3><a id="_4166"></a><strong>…省略</strong></h3> 
<p><strong>由于公众号最多可以发布5W字</strong></p> 
<p><strong>还有10W字放不下…</strong></p> 
<p><strong>以下几个部分的具体内容，</strong></p> 
<p><strong>请参见《K8S学习圣经》PDF</strong></p> 
<p><strong>请在公众号《技术自由圈》回复“领电子书”</strong></p> 
<h3><a id="4Kubernetes__4186"></a>第4部分：Kubernetes 基本概念</h3> 
<h4><a id="4_4188"></a>第4部分目录</h4> 
<ul><li>1、基础概念理解 
  <ul><li>K8S集群</li><li>Node的核心组件</li><li>Pod</li><li>Label</li><li>Deployment</li><li>Service</li><li>Addons</li><li>DNS</li><li>Web UI (Dashboard)</li></ul> </li><li>2、k8s对象（kubernetes Objects） 
  <ul><li>对象的yaml结构</li><li>关于 yaml文件的分割</li><li>2.1 一个简单的Kubernetes API 对象</li><li>2.1 什么是k8s 资源对象 
    <ul><li>kubernetes 对象必需字段</li></ul> </li><li>2.2 kubernetes API的版本 
    <ul><li>一、查看apiversion 可用版本</li><li>二、各种apiVersion的含义</li></ul> </li><li>2.3 获取各个属性字段的含义 
    <ul><li>方式1：从命令行获取方式</li><li>方式2：官方 API 文档方式</li></ul> </li><li>2.4、管理k8s对象 
    <ul><li>1、命令式</li><li>2、指令性</li><li>3、声明式</li></ul> </li><li>2.5、对象名称与ID 
    <ul><li>Names</li><li>UID</li></ul> </li><li>2.6 对象规约（Spec）与状态（Status） 
    <ul><li>对象规约（Spec）与状态（Status）</li><li>spec 规约</li><li>status 状态</li></ul> </li><li>2.7、名称空间 
    <ul><li>名称空间的使用参考：</li><li>如何访问其他名称空间的东西？</li><li>查看名称空间</li><li>在执行请求的时设定namespace</li><li>设置名称偏好</li><li>名称空间与DNS</li><li>名称空间的作用</li><li>并非所有对象都在命名空间中</li></ul> </li><li>2.8 、标签和选择器</li><li>2.9、注解annotation</li><li>2.8、字段选择器</li><li>2.10、认识kubectl 客户端命令 
    <ul><li>kubectl的所有命令参考：</li></ul> </li><li>2.11、自动补全</li><li>2.12、给idea安装kubernetes插件 
    <ul><li>1、plugins kubernetes</li><li>2、快速生成kubernetes 资源文件模板</li><li>3、输入<code>k</code>即可触发自动命令提示</li></ul> </li></ul> </li></ul> 
<h3><a id="5Kubernetes__4246"></a>第5部分：Kubernetes 工作负载</h3> 
<h4><a id="5_4248"></a>第5部分目录</h4> 
<ul><li>什么是Workloads?</li><li>Pod的原理与生命周期 
  <ul><li>1、什么是Pod</li><li>2、Pod使用</li><li>3、Pod生命周期</li></ul> </li><li>“根容器” : Pause 容器 
  <ul><li>Pod中容器也有分类</li><li>Pod生命周期的 两个阶段</li></ul> </li><li>Pod的Init 初始化容器 (附Demo) 
  <ul><li>Pod 钩子Hook方法 (附Demo)</li><li>Pod 健康检查（探针）机制</li><li>Pod 的状态和重启策略 
    <ul><li>Pod 常见的状态</li><li>Pod重启策略</li><li>Pod常见状态转换场景</li></ul> </li></ul> </li><li>临时容器：线上排错 
  <ul><li>使用方法：</li><li>通过临时容器查看Java容器log日志</li><li>临时容器的参数说明：</li><li>使用临时容器进行 dump 转储 
    <ul><li>基于制作Java Debug 镜像</li><li>临时容器不能用jmap、jps而只能用jattach</li><li>jattach指令集:</li></ul> </li><li>临时容器的配置</li><li>临时容器的使用场景</li></ul> </li><li>静态Pod 
  <ul><li>如何找到 静态pod 的文件路径</li><li>静态Pod实操</li><li>Pod 资源需求和限制</li></ul> </li><li>Pod的Probe 探针机制（健康检查机制） 
  <ul><li>Probe 探针 背景</li><li>K8S 的3种探针</li><li>livenessProbe和readinessProbe 的区别 
    <ul><li>（1）livenessProbe</li><li>（2）readinessProbe</li><li>（3）就绪、存活两种探针的区别</li></ul> </li><li>存活探针 (liveness)的三种探测方法 
    <ul><li>exec 方式示例</li><li>httpGet 方式示例</li><li>TCP 方式示例</li><li>使用命名的端口</li></ul> </li><li>就绪探针 (readiness) 
    <ul><li>ReadinessProbe 探针使用示例</li></ul> </li><li>ReadinessProbe + LivenessProbe 配合使用示例 
    <ul><li>以上案例中存活探针 参数意思：</li><li>以上案例中就绪探针 参数意思：</li></ul> </li><li>startupProbe探针 
    <ul><li>1、startupProbe探针介绍</li><li>2、startupProbe探针与另两种区别</li><li>3、startupProbe探针方法、属性</li><li>4、为什么要使用startupProbe、使用场景</li></ul> </li></ul> </li></ul> 
<h3><a id="6Kubernetes__4306"></a>第6部分：Kubernetes 的资源控制</h3> 
<h4><a id="6_4308"></a>第6部分目录</h4> 
<ul><li>动态的扩容缩容的重要性</li><li>1：Deployment 资源对象 
  <ul><li>Deployment的创建</li><li>Deployment 资源、replicaset资源、Pod资源 三者之间的关系</li><li>Deployment 基本操作 
    <ul><li>副本扩容</li><li>副本缩容</li></ul> </li><li>Deployment 更新机制 
    <ul><li>准备：升级镜像</li><li>在线修改yaml</li><li>滚动机制相关的命令</li><li>暂停和恢复</li><li>回滚策略 
      <ul><li>记录保留</li><li>滚动更新数量</li></ul> </li></ul> </li><li>使用Deployment 进行灰度发布</li></ul> </li><li>2：副本资源 RC、副本集RS 资源对象 
  <ul><li>RC（Replication Controller）</li><li>RS（Replication Set）</li><li>ReplicatSet的三个部分</li><li>RS扩容缩容</li></ul> </li><li>3：DaemonSet 守护集 
  <ul><li>DaemonSet 组成结构详解</li><li>调度节点的选择 
    <ul><li>方式一：nodeSelector方式</li><li>方式二：nodeAffinity方式</li><li>方式三：podAffinity方式</li></ul> </li><li>Toleration</li></ul> </li><li>4：StatefulSet 有状态集 
  <ul><li>StatefulSet 使用场景</li><li>StatefulSet的一些限制和要求</li><li>StatefulSet示例</li><li>DNS解析</li><li>StatefulSet 的几个要点 
    <ul><li>（1）pod管理策略（podManagementPolicy）</li><li>（2）updateStrategy： 更新策略</li><li>（3）对应的headless service</li></ul> </li></ul> </li><li>5：Job 任务、CronJob 定时任务 
  <ul><li>Job 任务</li><li>并行 job 示例</li><li>CronJob 定时任务 
    <ul><li>CronJob示例</li></ul> </li></ul> </li><li>6：HPA(Horizontal Pod Autoscaling)水平自动伸缩 
  <ul><li>收集指标插件</li><li>Metrics API</li><li>前置条件: 开启聚合路由 
    <ul><li>metrics安装方式一：minikube 中启用指标服务器作为插件</li><li>metrics server安装方式二：手动安装</li><li>执行安装和检查</li><li>执行安装的命令清单</li></ul> </li></ul> </li><li>7：使用HPA对SpringCloud微服务进行自动伸缩 
  <ul><li>autoscale命令</li><li>Metrics支持的指标</li><li>创建hpa、查看hpa、删除hpa</li><li>HPA扩容实操</li><li>基于自定义指标的自动扩容</li><li>扩展阅读: GC机制</li><li>什么是垃圾回收</li></ul> </li><li>什么是OCI、CRI、CNI、CSI 
  <ul><li>OCI、CRI、CNI、CSI、CRD、CNM规范基本概念：</li><li>实现OCI、CRI、CNI、CSI组件介绍 
    <ul><li>1、OCI、CRI组件</li><li>2、CNI组件 
      <ul><li>CNI 和 CNM 的对比：</li></ul> </li><li>3、CSI组件</li></ul> </li><li>CRI、OCI的关系</li><li>CRD是什么</li><li>CR是什么</li><li>CRD与CR的关系</li><li>CRD在API Server中的设计和实现机制</li><li>如何创建 CRD？</li></ul> </li></ul> 
<h3><a id="7SVC_4387"></a>第7部分：SVC负载均衡底层原理</h3> 
<h4><a id="7_4389"></a>第7部分目录</h4> 
<ul><li>Pod的IP 漂移问题</li><li>service 概念</li><li>四大service 类型</li><li>kubernetes暴露端口的方式 
  <ul><li>1：集群内部实现访问：Clusterip</li><li>2：集群外部方式访问：NodePort</li><li>3: LoadBalancer</li><li>4: Ingress</li></ul> </li><li>DNS解析案例 
  <ul><li>案例</li></ul> </li><li>SVC 流量分发的底层原理 
  <ul><li>VIP 和 Service 代理</li><li>代理模式分类</li><li>Ⅰ、userspace 代理模式</li><li>Ⅱ、Iptables 代理模式</li><li>Ⅲ、ipvs 代理模式 
    <ul><li>ipvs为负载均衡提供算法：</li><li>ipvs 对比 iptables</li></ul> </li><li>iptables/netfilter 介绍 
    <ul><li>1，iptables/netfilter介绍</li><li>2，iptables 基础：</li><li>3，链的概念：</li><li>4，表的概念：</li><li>5，数据经过防火墙的流程图：</li><li>6，规则：</li></ul> </li><li>总结： iptables包含4个表，5个链</li></ul> </li><li>iptables 在 K8s 中的应用剖析 
  <ul><li>路由的流程分析:</li><li>service 的路由分析实例 
    <ul><li>先看iptables 中的DNAT</li><li>再看SNAT</li></ul> </li></ul> </li><li>基于客户端地址的会话保持模式的svc负载分发策略</li><li>k8s集群中service的域名解析、pod的域名解析 
  <ul><li>service的域名 
    <ul><li>1、创建namespace.yaml文件</li><li>2、创建deployment.yaml文件</li><li>3、创建service.yaml文件</li><li>4、测试</li></ul> </li><li>总结</li></ul> </li><li>应用持久化存储（PV和PVC） 
  <ul><li>1、Volume 
    <ul><li>1.1、emptyDir</li><li>1.2、hostPath</li><li>1.3、外部存储（以NFS为例）</li></ul> </li><li>2、PV与PVC 
    <ul><li>持久卷的存储插件类型</li><li>2.1、Static PV 
      <ul><li>1）先搭建好NFS服务器（192.168.100.172）</li><li>2）创建PV</li><li>3）创建PVC</li><li>4）创建Pod</li></ul> </li><li>2.2、Dynamic PV 
      <ul><li>Dynamic PV属于PV的自动化：</li><li>什么是StorageClass</li><li>为什么需要StorageClass</li><li>StorageClass案例</li><li>运行原理</li></ul> </li></ul> </li></ul> </li></ul> 
<h3><a id="8K8S_Ingress_4451"></a>第8部分：K8S Ingress原理和实操</h3> 
<h4><a id="8_4453"></a>第8部分目录</h4> 
<ul><li>背景： 
  <ul><li>svc的作用与不足</li><li>Ingress 的来源</li></ul> </li><li>Ingress的构成 
  <ul><li>什么是Ingress Controller？ 
    <ul><li>Ingress 资源对象</li><li>ingress-controller组件</li></ul> </li><li>nginx-ingress-controller</li></ul> </li><li>部署ingress-controller 
  <ul><li>Minikube安装Ingress</li><li>手工部署ingress-controller pod及相关资源</li><li>ingress版本的问题</li><li>调整RBAC api-versions 版本</li><li>部署 ingress-nginx</li><li>配置ingress资源</li><li>报错 failed calling webhook</li><li>获取 ingress 启动的svc 
    <ul><li>加上本地的host</li></ul> </li><li>测试访问</li><li>实操的善后工作</li></ul> </li><li>k8s ingress的工作原理 
  <ul><li>（1）Ingress Controller 控制器</li><li>（2） Ingress 资源对象</li></ul> </li><li>详解ingress资源 
  <ul><li>ingress规则</li><li>DefaultBackend</li><li>资源后端</li><li>路径类型 
    <ul><li>路径类型示例</li></ul> </li><li>路径多重匹配</li></ul> </li><li>主机名通配符</li><li>ingress类 
  <ul><li>名字空间域的参数</li><li>默认ingress类</li></ul> </li><li>ingress部署的三种模式 
  <ul><li>模式一：NodePort模式的Service</li><li>模式二：DaemonSet+nodeSelector+HostNetwork</li><li>模式三：Deployment+LoadBalancer模式的Service</li></ul> </li><li>模式一的问题：service暴露服务的问题</li><li>DaemonSet+HostNetwork+nodeselector 实操 
  <ul><li>nodePort的NAT性能问题</li><li>指定nginx-ingress-controller运行的node节点</li><li>修改Deployment为Daemonset,指定节点运行，并开启 hostNetwork</li><li>启动nginx-ingress-controller</li><li>查看pod的IP和端口</li><li>配置ingress资源</li><li>命令清单</li></ul> </li><li>生产环境 LVS+keepalive 做高可用和负载均衡 
  <ul><li>边缘节点</li><li>生产环境可以使用 HA + LB + DaemonSet hostNetwork 架构</li></ul> </li><li>四种port底层原理：nodePort、port、targetPort、containerPort 的核心 
  <ul><li>1、nodePort</li><li>2、port</li><li>3、targetPort</li><li>4、containerPort</li></ul> </li><li>Ingress 动态域名配置底层原理 
  <ul><li>ingress生产建议</li></ul> </li><li>ingress总结</li><li>K8s的常用命令 
  <ul><li>kubectl命令 要求</li></ul> </li></ul> 
<h3><a id="9AB___4520"></a>第9部分：蓝绿发布、金丝雀发布、滚动发布、A/B测试 原理和实操</h3> 
<h4><a id="9_4522"></a>第9部分目录</h4> 
<ul><li>背景：</li><li>蓝绿发布、金丝雀发布、滚动发布、A/B测试 核心原理 
  <ul><li>蓝绿发布（Blue-green Deployments） 核心原理</li><li>金丝雀发布（anCanary Releases） 核心原理</li><li>滚动发布的 核心原理</li><li>A/B测试（A/B Testing） 核心原理</li></ul> </li><li>蓝绿发布、金丝雀发布、滚动发布实操 
  <ul><li>spring cloud 灰度实操</li><li>反向代理网关灰度实操</li><li>Kubernetes 中的灰度策略</li><li>Deployment金丝雀部署：按照流量比例 
    <ul><li>step1 ：启动 V1 服务，查看服务是否正确，然后观察一下服务。</li><li>step2：启动 V2 的服务版本：1 个复本</li><li>step3：观察 V2 流量正常的情况的话，那么启动 V2 的 2 个复本。</li><li>step4：删除 V1 的 2 个复本，流量全部到 V2 上。</li><li>实操总结</li></ul> </li><li>Deployment实现滚动发布 
    <ul><li>step1：启动 V1 服务，查看服务是否正确，然后观察一下服务。</li><li>step2：启动 app-v2-rolling 进行滚动发布</li><li>step3：观察所有容器版本变为 V2 版本</li><li>实操总结</li></ul> </li><li>Deployment实现蓝绿部署 
    <ul><li>step1 ：启动 V1 服务，查看服务是否正确，然后观察一下服务。</li><li>step2：启动 V2 的服务版本</li><li>step3：将版本 1 服务切换到版本 2，观察服务情况</li><li>patch</li><li>实操总结</li></ul> </li><li>Ingress Annotations实现金丝雀发布 
    <ul><li>Ingress Annotations实现金丝雀发布 实操</li></ul> </li><li>华为云的金丝雀发布</li><li>参考</li></ul> </li></ul> 
<h3><a id="10Service_Mesh__4560"></a>第10部分：服务网格Service Mesh 宏观架构模式</h3> 
<h4><a id="10_4562"></a>第10部分目录</h4> 
<ul><li>第1大模式：Sidecar （边车）模式 架构 
  <ul><li>sidecar的（边车）负责的功能</li><li>sidecar模式好处、坏处</li><li>如何解决依赖的复杂性和性能问题呢？</li><li>优化之后的sidecar模式优点：</li></ul> </li><li>第2大模式：代理模式</li><li>服务网格 istio 框架微服务与SpringCloud 对比 
  <ul><li>SpringCloud 和 istio 中间组件的对比</li><li>云原生Sidecar分体架构微服务Provider一体架构对比</li><li>两大基础组件的对比</li></ul> </li><li>角色sidecar 对应到啥组件？</li><li>小结：</li><li>Istio 架构 
  <ul><li>控制平和数据面</li><li>Istio 的运转流程</li><li>（1）Sidecar自动注入：</li><li>（2）流量拦截：</li><li>（3）服务发现：</li><li>（4）负载均衡：</li><li>（5）流量治理：</li><li>（6）访问安全：</li><li>（7）服务遥测：</li><li>（8）策略执行：</li><li>（9）外部访问：</li></ul> </li><li>Istio组件介绍 
  <ul><li>2.1 Pilot</li><li>2.2 Mixer</li><li>2.3 Citadel</li><li>2.4 Galley</li><li>2.5 Sidecar-injector</li><li>2.6 Proxy(Envoy)</li><li>2.7 Ingress gateway</li><li>2.8 其他组件</li></ul> </li><li>Istio安装 
  <ul><li>在本地搭建Istio环境</li><li>Kubernetes集群环境</li></ul> </li><li>安装Istio 
  <ul><li>快速部署Istio</li><li>初步感受istio</li><li>手动注入</li><li>自动注入sidecar</li></ul> </li><li>istio项目案例：bookinfo 
  <ul><li>什么是bookinfo</li></ul> </li><li>sidecar自动注入到微服务</li><li>启动bookinfo</li><li>通过ingress方式访问</li><li>通过istio的ingress gateway访问 
  <ul><li>确定 Ingress 的 IP 和端口</li></ul> </li></ul> 
<h3><a id="11K8SHarber__SpringCloud___4617"></a>第11部分：使用K8S+Harber 手动部署 SpringCloud 应用</h3> 
<h4><a id="11_4619"></a>第11部分目录</h4> 
<ul><li>启动minikube时指定私有仓库</li><li>minikube 复制证书</li><li>进入minikube虚拟机 
  <ul><li>增加私仓地址配置</li><li>重启虚拟机的Docker</li><li>测试</li></ul> </li><li>完整的命令清单</li><li>Docker构建镜像后通过K8S部署</li><li>镜像与容器关联 
  <ul><li>Docker构建镜像并且推送Harber 
    <ul><li>前提：停止或者重启 Harbor</li></ul> </li><li>创建Dockerfile</li><li>镜像构建：将本地镜像打包 
    <ul><li>docker build语法</li></ul> </li><li>镜像打tag：镜像添加版本号</li><li>镜像推送：镜像推送到远程仓库</li><li>测试</li><li>完整的命令清单</li></ul> </li><li>k8s的Secrets： 
  <ul><li>创建一个k8s的Secrets：</li><li>1.Secret对象配置过程</li><li>2.容器镜像拉取的两种策略 
    <ul><li>2.1 ImagePullPolicy</li><li>2.2 ImgaePullSecrets</li></ul> </li></ul> </li><li>k8s的configMap对象 
  <ul><li>Pod可以通过三种方式来使用ConfigMap，分别为：</li><li>configMap是什么 
    <ul><li>1.单pod使用configmap示例图</li><li>2.多pod使用configmap示例图</li></ul> </li><li>configmap来配置环境变量 
    <ul><li>步骤1：创建configmap 
      <ul><li>方法1：yaml档来设定configmap</li><li>方法1：命令行设定configmap</li></ul> </li><li>步骤3：使用configmap来配置</li></ul> </li></ul> </li><li>docker-compose to minikube 
  <ul><li>简单案例</li><li>实操案例 
    <ul><li>参考的原始的docker-compose编排文件</li></ul> </li><li>设置env环境变量</li><li>k8s向etc/hosts里添加内容 
    <ul><li>k8s默认被重写/etc/hosts</li><li>Dockerfile里的配置被覆盖</li><li>将你的配置写到k8s yml里</li></ul> </li><li>Kubernetes volume hostPath explained with examples 
    <ul><li>hostPath</li><li>Example</li></ul> </li></ul> </li><li>部署和创建服务 
  <ul><li>ImagePullBackOff错误排查 
    <ul><li>解决问题：停止后重启 Harbor</li></ul> </li><li>CrashLoopBackOff 错误 
    <ul><li>坑1：镜像拉取是ok，但是依赖包没有</li><li>坑2：java 进程异常结束了</li><li>坑3：minikube是套娃虚拟机，套娃里边没有映射目录，远程复制</li><li>坑4：检查域名和环境变量</li></ul> </li><li>解决问题有感</li></ul> </li><li>完整的命令清单</li><li>openresty 镜像 
  <ul><li>dockerfile</li></ul> </li></ul> 
<h3><a id="12SpringCloudJenkins_K8s_Ingress__4684"></a>第12部分：SpringCloud+Jenkins+ K8s Ingress 自动化灰度发布</h3> 
<h4><a id="12_4686"></a>第12部分目录</h4> 
<ul><li>如何进行SpringCloud+Jenkins+ K8s Ingress 灰度发布？</li><li>回顾Nginx-ingress 架构和原理</li><li>灰度实操之前的准备 
  <ul><li>部署和测试 stable 版本的 deployment 和 svc</li><li>部署和测试 canary版本 的 deployment 和 svc</li></ul> </li><li>基于用户的灰度场景 
  <ul><li>接下来，开始基于 用户的灰度实操</li></ul> </li><li>基于权重的灰度场景 
  <ul><li>基于权重的 Canary 规则</li><li>基于权重的发布实操</li></ul> </li><li>如何进行自动化灰度？</li><li>jenkins安装和pipeline 流水线的使用 
  <ul><li>下载和启动jenkins</li><li>登录jenkins</li><li>使用pipeline插件 
    <ul><li>安装Pipeline 插件</li></ul> </li><li>pipeline 的hello world</li><li>pipeline 语法介绍</li><li>Jenkins插件SSH Pipeline Steps 
    <ul><li>sshCommand 在远程节点上执行给定的命令并响应输出</li><li>sshGet 从远程主机获取文件或目录</li><li>sshPut 将文件或目录放入远程主机</li><li>sshRemove 删除远程主机上的文件或目录</li><li>sshScript 在远程节点上执行给定的脚本(文件)并响应输出</li><li>结合 withCredentials 从 Jenkins 凭证存储中读取私钥</li></ul> </li><li>pipeline支持的指令</li></ul> </li><li>ingress 灰度发布流水线设计 
  <ul><li>CICD流水线预览</li><li>step1：自动化的制品发布 
    <ul><li>1、克隆 springboot代码项目到本地</li><li>2、maven构建springboot代码项目</li><li>3、构建docker镜像</li><li>4、推送docker镜像到harber上，完成制品发布</li></ul> </li><li>step2：生产环境进行 A/B 测试 
    <ul><li>AB测试原理：</li></ul> </li><li>step3：生产环境进行 A/B 测试</li><li>step4：生产环境进行版本正式切换</li></ul> </li><li>最后总结一下</li></ul> 
<h3><a id="13springboot__qps__4730"></a>第13部分：springboot 基于 qps 动态扩缩容</h3> 
<h4><a id="13_4732"></a>第13部分目录</h4> 
<ul><li>步骤</li><li>1、springboot安装prometheus依赖并获取metric</li><li>2、安装prometheus operator 实现 kubernetes的监控指标 
  <ul><li>2.1 helm安装prometheus operator、prometheus adapter（custom metric）</li><li>2.2 kubernetes的监控指标</li><li>Prometheus Operator 极简配置Prometheus 监控 
    <ul><li>Operator</li><li>Operator介绍</li><li>Prometheus Operator vs. kube-prometheus vs. community helm chart</li></ul> </li><li>Prometheus Operator CRD介绍</li><li>命令清单: 安装的Prometheus</li><li>查看安装后的CRD、svc、pods：</li></ul> </li><li>从外部访问promethus 
  <ul><li>访问Prometheus</li><li>访问Alertmanager</li><li>访问Grafana</li></ul> </li><li>移除kube-prometheus 
  <ul><li>解决prometheus-adapter创建失败的问题 
    <ul><li>查看 安装文件</li></ul> </li><li>解决kube-state-metrics 创建失败的问题</li><li>解决namespace 删除不来</li></ul> </li><li>配置prometheus -adapter获取应用qps</li><li>prometheus采集到的metrics适配给kubernetes用 
  <ul><li>K8s中 kubernetes的监控指标分为两种：</li><li>Metrics-Server 简介</li><li>prometheus-adpater</li></ul> </li><li>获取 metrics 定制化指标 
  <ul><li>checkout 仓库 k8s-prometheus-adapter</li><li>脚本进行部署 k8s-prometheus-adapter 
    <ul><li>请求metrics.k8s.io的 api</li><li>直接访问 api 的方式来测试部署是否 ok</li></ul> </li><li>排错 no matches for kind “APIService” in version "apiregistration.k8s.io/v1 
    <ul><li>查看版本 kubectl api-versions</li></ul> </li><li>Prometheus adapter 的文件介绍</li><li>配置文件</li><li>resource metrics API</li><li>命令清单</li></ul> </li><li>自定义alertmanager的PrometheusRule</li><li>配置prometheus及adapter获取应用qps 
  <ul><li>启动springboot应用</li><li>添加自定义指标</li><li>架构图原理</li><li>添加一个自定义监控的步骤</li><li>部署prometheus-adapter</li></ul> </li><li>custom-metrics-server 规则配置 
  <ul><li>指标发现和配置展示(Metrics Discovery and Presentation Configuration)</li><li>http_requests（每秒请求数QPS)监控指标</li><li>坑： /metrics 路径的定制化修改</li><li>查询与 http_requests 相关的指标</li></ul> </li><li>自定义metric rules、配置HPA 
  <ul><li>配置自定义prometheus-adapter-config配置</li><li>配置 qps 请求量指标</li><li>查看 “pods/http_server_requests_per_second” 指标</li><li>HPA配置 
    <ul><li>基于Pod做HPA</li><li>基于cpu或者memory做HPA</li></ul> </li><li>启动HPA的伸缩控制器 
    <ul><li>hpa验证测试</li></ul> </li></ul> </li><li>配置grafana展示qps监控数据</li><li>hpa命令清单</li><li>高阶知识：Adapter 的Discovery规则如何配置？ 
  <ul><li>以获取Per-pod HTTP Requests为例 
    <ul><li>1 demo问题场景</li><li>2 配置适配器</li><li>3 查询api</li></ul> </li></ul> </li><li>prometheus operator 不足之处 
  <ul><li>数据持久化</li><li>tsdb 保留天数</li><li>告警方式不方便</li><li>加监控target也不方便</li></ul> </li></ul> 
<h3><a id="14k8sJVM_4807"></a>第14部分：k8s生产环境容器内部JVM参数配置解析及优化</h3> 
<h4><a id="14_4809"></a>第14部分目录</h4> 
<ul><li>Java Heap基础知识</li><li>容器环境的Java Heap 
  <ul><li>UseContainerSupport</li></ul> </li><li>最佳实践 
  <ul><li>常用容器内存大小对应的jvm内存配置 
    <ul><li>容器启动常用配置</li></ul> </li><li>配置项的具体介绍 
    <ul><li>1.堆总内存初始化大小分配和最大值分配</li><li>2.非堆总内存初始化大小分配和最大值分配（1.8为metaspace）</li><li>3.堆内存之年轻代年老代大小设置</li><li>4.线程栈大小</li><li>5.GC日志输出</li><li>6.每个线程堆栈大小</li></ul> </li></ul> </li><li>Kubernetes（k8s）配置Java服务自动Dump</li><li>参考：Go语音解决 java 内存溢出时候 dump 文件的存储问题 
  <ul><li>问题</li><li>方案</li><li>其他</li></ul> </li></ul> 
<p><strong>说明：由于公众号最多可以发布5W字</strong></p> 
<p><strong>还有10W字放不下…</strong></p> 
<p><strong>以上几个部分的具体内容，</strong></p> 
<p><strong>请参见《K8S学习圣经》PDF</strong></p> 
<p><strong>请在公众号《技术自由圈》回复“领电子书”</strong></p> 
<h3><a id="_4846"></a>参考文献：</h3> 
<p>https://github.com/kubernetes-sigs/prometheus-adapter/blob/master/docs/config.md</p> 
<p>https://www.jianshu.com/p/ea5bc56211ee</p> 
<p>https://blog.csdn.net/m0_71518373/article/details/127793755</p> 
<p>https://blog.csdn.net/huxiutao/article/details/94968889</p> 
<p>https://blog.csdn.net/lt_xiaodou/article/details/126666270</p> 
<p><a href="https://istio.io/latest/docs/setup/install/istioctl/" rel="nofollow">Istio / Install with Istioctl</a></p> 
<p><a href="https://github.com/istio/istio/releases/">Releases · istio/istio (github.com)</a></p> 
<p>https://blog.csdn.net/weixin_40304387/article/details/127800427</p> 
<p>https://blog.csdn.net/ohalo/article/details/126290829</p> 
<p>http://events.jianshu.io/p/a621d4af6867</p> 
<p>https://www.cnblogs.com/liconglong/p/16905844.html</p> 
<p>https://www.cnblogs.com/xiao987334176/p/14236554.html</p> 
<p>https://kubernetes.io/docs/tasks/tools/</p> 
<p>https://minikube.sigs.k8s.io/docs/start/</p> 
<p>https://developer.aliyun.com/article/221687</p> 
<p>https://blog.csdn.net/fly_leopard/article/details/108790217</p> 
<p>https://www.cnblogs.com/yjmyzz/p/install-k8s-on-mac-using-minikube.html</p> 
<p>https://blog.csdn.net/qq_38340601/article/details/108437017</p> 
<p>https://www.cnblogs.com/zhengchunyuan/p/12598210.html</p> 
<p>https://blog.csdn.net/weixin_46108954/article/details/105717399</p> 
<p>https://blog.csdn.net/swalikh/article/details/117995440</p> 
<p>https://blog.csdn.net/weixin_44729138/article/details/124394257</p> 
<p>https://blog.csdn.net/weixin_44729138/article/details/124394257</p> 
<p>https://andyoung.blog.csdn.net/article/details/127965449</p> 
<p>https://www.jianshu.com/p/8aa4ae9be025</p> 
<p>https://zhuanlan.zhihu.com/p/514799248</p> 
<p>https://www.qikqiak.com/k8s-book/docs/58.Prometheus%20Operator.html</p> 
<p>https://www.cnblogs.com/wangxu01/articles/11652372.html</p> 
<p>https://blog.csdn.net/shida_csdn/article/details/81077972</p> 
<p>https://blog.csdn.net/u010918487/article/details/119052933</p> 
<h3><a id="_4915"></a>技术自由的实现路径：</h3> 
<h5><a id="__4917"></a>实现你的 架构自由：</h5> 
<p>《<a href="https://blog.csdn.net/crazymakercircle/article/details/129204455">吃透8图1模板，人人可以做架构</a>》</p> 
<p>《<a href="https://blog.csdn.net/crazymakercircle/article/details/129410795">10Wqps评论中台，如何架构？B站是这么做的！！！</a>》</p> 
<p>《<a href="https://blog.csdn.net/crazymakercircle/article/details/128848309">阿里二面：千万级、亿级数据，如何性能优化？ 教科书级 答案来了</a>》</p> 
<p>《<a href="https://blog.csdn.net/crazymakercircle/article/details/128725701">峰值21WQps、亿级DAU，小游戏《羊了个羊》是怎么架构的？</a>》</p> 
<p>《<a href="https://blog.csdn.net/crazymakercircle/article/details/129145200">100亿级订单怎么调度，来一个大厂的极品方案</a>》</p> 
<p>《<a href="https://blog.csdn.net/crazymakercircle/article/details/128697096">2个大厂 100亿级 超大流量 红包 架构方案</a>》</p> 
<p><em>… 更多架构文章，正在添加中</em></p> 
<h5><a id="___4935"></a>实现你的 响应式 自由：</h5> 
<p>《<a href="https://blog.csdn.net/crazymakercircle/article/details/129022714">响应式圣经：10W字，实现Spring响应式编程自由</a>》</p> 
<p>这是老版本 《<a href="https://blog.csdn.net/crazymakercircle/article/details/124120506">Flux、Mono、Reactor 实战（史上最全）</a>》</p> 
<h5><a id="_spring_cloud__4943"></a>实现你的 spring cloud 自由：</h5> 
<p><strong>《<a href="https://blog.csdn.net/crazymakercircle/article/details/129559428">Spring cloud Alibaba 学习圣经</a>》 PDF</strong></p> 
<p>《<a href="https://blog.csdn.net/crazymakercircle/article/details/123420859">分库分表 Sharding-JDBC 底层原理、核心实战（史上最全）</a>》</p> 
<p>《<a href="https://blog.csdn.net/crazymakercircle/article/details/125135726">一文搞定：SpringBoot、SLF4j、Log4j、Logback、Netty之间混乱关系（史上最全）</a>》</p> 
<h5><a id="_linux__4954"></a>实现你的 linux 自由：</h5> 
<p>《<a href="https://blog.csdn.net/crazymakercircle/article/details/128932396">Linux命令大全：2W多字，一次实现Linux自由</a>》</p> 
<h5><a id="___4960"></a>实现你的 网络 自由：</h5> 
<p>《<a href="https://blog.csdn.net/crazymakercircle/article/details/114527369">TCP协议详解 (史上最全)</a>》</p> 
<p>《<a href="https://blog.csdn.net/crazymakercircle/article/details/129334254">网络三张表：ARP表, MAC表, 路由表，实现你的网络自由！！</a>》</p> 
<h5><a id="___4968"></a>实现你的 分布式锁 自由：</h5> 
<p>《<a href="https://blog.csdn.net/crazymakercircle/article/details/116425814">Redis分布式锁（图解 - 秒懂 - 史上最全）</a>》</p> 
<p>《<a href="https://blog.csdn.net/crazymakercircle/article/details/85956246">Zookeeper 分布式锁 - 图解 - 秒懂</a>》</p> 
<h5><a id="___4976"></a>实现你的 王者组件 自由：</h5> 
<p>《<a href="https://blog.csdn.net/crazymakercircle/article/details/128264803">队列之王： Disruptor 原理、架构、源码 一文穿透</a>》</p> 
<p>《<a href="https://blog.csdn.net/crazymakercircle/article/details/128123114">缓存之王：Caffeine 源码、架构、原理（史上最全，10W字 超级长文）</a>》</p> 
<p>《<a href="https://blog.csdn.net/crazymakercircle/article/details/113751575">缓存之王：Caffeine 的使用（史上最全）</a>》</p> 
<p>《<a href="https://blog.csdn.net/crazymakercircle/article/details/126579528">Java Agent 探针、字节码增强 ByteBuddy（史上最全）</a>》</p> 
<h5><a id="___4988"></a>实现你的 面试题 自由：</h5> 
<p><a href="https://blog.csdn.net/crazymakercircle/article/details/124790425">4000页《尼恩Java面试宝典 》 40个专题</a></p> 
<p>以上尼恩 架构笔记、面试题 的PDF文件，<strong>请到《技术自由圈》公众号领取</strong> ↓ ↓ ↓</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/22a5389f19bdfc96bb0ff7c9c3bb9804/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">vite pwa项目使用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/79e3147578e29e267931c203622a7e5b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Docker圣经：大白话说Docker底层原理，6W字实现Docker自由</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>