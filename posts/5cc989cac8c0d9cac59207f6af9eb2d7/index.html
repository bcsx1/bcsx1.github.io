<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>DIN模型pytorch代码逐行细讲 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="DIN模型pytorch代码逐行细讲" />
<meta property="og:description" content="DIN模型pytorch代码逐行细讲 文章目录 DIN模型pytorch代码逐行细讲一.DIN模型的结构二.代码介绍三.导入包四.导入数据五.数据处理六.模型定义七.封装训练集，测试集八.模型训练九.效果展示 一.DIN模型的结构 ​ DIN将注意力机制引入模型当中，对用户的历史行为进行注意力分析。虽然如今看对于历史行为的注意力分析方法比较简单，但是也是值得看一下其中逻辑的。
二.代码介绍 ​ 使用的模型：din
​ 模型功能：预测用户是否想要购买目标商品y
​ 模型特点：该模型一般用于分析用户历史行为与预测目标之前的关系
​ 数据集：用户的id，亚马逊的用户购买商品的记录，以及要预测的目标商品（注：一般更好的数据集会带有用户的个人信息，也可以作为模型输入进行分析，但是作为一个模板，不搞这么复制）
​ 模型输入：用户的历史购买商品类别序列
​ 模型输出：用户想购买商品的概率
​ 模型亮点：引入了注意力机制，会去分析商品与推荐商品之间的注意力系数
​ 模型论文链接：https://arxiv.org/pdf/1706.06978.pdf
三.导入包 &#39;&#39;&#39; -*- coding: utf-8 -*- @File : din.py &#39;&#39;&#39; # 1.python自定义包 import os import pandas as pd import numpy as np # 2.pytorch相关包 import torch import torch.nn as nn import torch.optim as optim import torch.utils.data as Data import torch.nn.functional as F # 3.sklearn相关包 from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, KBinsDiscretizer from sklearn." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/5cc989cac8c0d9cac59207f6af9eb2d7/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-12-26T21:00:39+08:00" />
<meta property="article:modified_time" content="2022-12-26T21:00:39+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">DIN模型pytorch代码逐行细讲</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="DINpytorch_0"></a>DIN模型pytorch代码逐行细讲</h3> 
<p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><ul><li><a href="#DINpytorch_0" rel="nofollow">DIN模型pytorch代码逐行细讲</a></li><li><ul><li><a href="#DIN_5" rel="nofollow">一.DIN模型的结构</a></li><li><a href="#_12" rel="nofollow">二.代码介绍</a></li><li><a href="#_32" rel="nofollow">三.导入包</a></li><li><a href="#_56" rel="nofollow">四.导入数据</a></li><li><a href="#_68" rel="nofollow">五.数据处理</a></li><li><a href="#_141" rel="nofollow">六.模型定义</a></li><li><a href="#_297" rel="nofollow">七.封装训练集，测试集</a></li><li><a href="#_337" rel="nofollow">八.模型训练</a></li><li><a href="#_421" rel="nofollow">九.效果展示</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h4><a id="DIN_5"></a>一.DIN模型的结构</h4> 
<p>​ DIN将注意力机制引入模型当中，对用户的历史行为进行注意力分析。虽然如今看对于历史行为的注意力分析方法比较简单，但是也是值得看一下其中逻辑的。<br> <img src="https://images2.imgbox.com/fd/b8/htGsavSa_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="_12"></a>二.代码介绍</h4> 
<p>​ 使用的模型：din</p> 
<p>​ 模型功能：预测用户是否想要购买目标商品y</p> 
<p>​ 模型特点：该模型一般用于分析用户历史行为与预测目标之前的关系</p> 
<p>​ 数据集：用户的id，亚马逊的用户购买商品的记录，以及要预测的目标商品（注：一般更好的数据集会带有用户的个人信息，也可以作为模型输入进行分析，但是作为一个模板，不搞这么复制）</p> 
<p>​ 模型输入：用户的历史购买商品类别序列</p> 
<p>​ 模型输出：用户想购买商品的概率</p> 
<p>​ 模型亮点：引入了注意力机制，会去分析商品与推荐商品之间的注意力系数</p> 
<p>​ 模型论文链接：https://arxiv.org/pdf/1706.06978.pdf</p> 
<h4><a id="_32"></a>三.导入包</h4> 
<pre><code class="prism language-python"><span class="token triple-quoted-string string">'''
-*- coding: utf-8 -*-
@File  : din.py
'''</span>
<span class="token comment"># 1.python自定义包</span>
<span class="token keyword">import</span> os
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token comment"># 2.pytorch相关包</span>
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">as</span> Data
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token comment"># 3.sklearn相关包</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> LabelEncoder<span class="token punctuation">,</span> OrdinalEncoder<span class="token punctuation">,</span> KBinsDiscretizer
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> roc_auc_score
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> metrics
</code></pre> 
<h4><a id="_56"></a>四.导入数据</h4> 
<p>​ 1.数据集读取，read_csv()里面的内容改成你的本地路径</p> 
<p>​ 2.数据集的内容-label：实际上用户有没有购买商品，userid：用户唯一id，itemid：商品唯一id，cateid：商品类别，hist_item_list：购买的商品id序列，hist_cate_list：购买的商品类别序列</p> 
<pre><code class="prism language-python">data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">'amazon-books-100k.txt'</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/25/d0/tDdSXtoJ_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="_68"></a>五.数据处理</h4> 
<p>​ 1.由于该数据集只有10w条，很多商品id只出现了一次，故编码的时候是以类别作为编码和预测的targe</p> 
<p>​ 2.如果你要用学生做的试题序列作为训练集，且这些试题被不同的学生来回做过，可以用试题作为唯一的编码</p> 
<p>​ 3.这里数据处理的目的是形成学生答题序列，把文本数据转化为唯一的数值编码，作为模型的输入，用于预测的目标试题</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">AmazonBookPreprocess</span><span class="token punctuation">(</span>dataframe<span class="token punctuation">,</span> seq_len<span class="token operator">=</span><span class="token number">40</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    数据集处理
    :param dataframe: 未处理的数据集
    :param seq_len: 数据序列长度
    :return data: 处理好的数据集
    """</span>
    <span class="token comment"># 1.按'|'切割，用户历史购买数据，获取item的序列和类别的序列</span>
    data <span class="token operator">=</span> dataframe<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>
    data<span class="token punctuation">[</span><span class="token string">'hist_item_list'</span><span class="token punctuation">]</span> <span class="token operator">=</span> dataframe<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token string">'hist_item_list'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'|'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
    data<span class="token punctuation">[</span><span class="token string">'hist_cate_list'</span><span class="token punctuation">]</span> <span class="token operator">=</span> dataframe<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token string">'hist_cate_list'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'|'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token comment"># 2.获取cate的所有种类，为每个类别设置一个唯一的编码</span>
    cate_list <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token string">'cateID'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    _ <span class="token operator">=</span> <span class="token punctuation">[</span>cate_list<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> data<span class="token punctuation">[</span><span class="token string">'hist_cate_list'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">]</span>
    <span class="token comment"># 3.将编码去重</span>
    cate_set <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span>cate_list <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token string">'0'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 用 '0' 作为padding的类别</span>

    <span class="token comment"># 4.截取用户行为的长度,也就是截取hist_cate_list的长度，生成对应的列名</span>
    cols <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'hist_cate_{}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>seq_len<span class="token punctuation">)</span><span class="token punctuation">]</span>

    <span class="token comment"># 5.截取前40个历史行为，如果历史行为不足40个则填充0</span>
    <span class="token keyword">def</span> <span class="token function">trim_cate_list</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">&gt;</span> seq_len<span class="token punctuation">:</span>
            <span class="token comment"># 5.1历史行为大于40, 截取后40个行为</span>
            <span class="token keyword">return</span> pd<span class="token punctuation">.</span>Series<span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token operator">-</span>seq_len<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> index<span class="token operator">=</span>cols<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># 5.2历史行为不足40, padding到40个行为</span>
            pad_len <span class="token operator">=</span> seq_len <span class="token operator">-</span> <span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            x <span class="token operator">=</span> x <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token string">'0'</span><span class="token punctuation">]</span> <span class="token operator">*</span> pad_len
            <span class="token keyword">return</span> pd<span class="token punctuation">.</span>Series<span class="token punctuation">(</span>x<span class="token punctuation">,</span> index<span class="token operator">=</span>cols<span class="token punctuation">)</span>

    <span class="token comment"># 6.预测目标为试题的类别</span>
    labels <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">'label'</span><span class="token punctuation">]</span>
    data <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">'hist_cate_list'</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>trim_cate_list<span class="token punctuation">)</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token string">'cateID'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token comment"># 7.生成类别对应序号的编码器，如book-&gt;1,Russian-&gt;2这样</span>
    cate_encoder <span class="token operator">=</span> LabelEncoder<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fit<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span>cate_set<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># 8.这里分为两步，第一步为把类别转化为数值，第二部为拼接上label</span>
    data <span class="token operator">=</span> data<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>cate_encoder<span class="token punctuation">.</span>transform<span class="token punctuation">)</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>labels<span class="token punctuation">)</span>
    <span class="token keyword">return</span> data
</code></pre> 
<pre><code class="prism language-python"><span class="token comment"># 对数据进行处理</span>
cate_encoder <span class="token operator">=</span> <span class="token boolean">None</span>
data <span class="token operator">=</span> AmazonBookPreprocess<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token comment"># 形成历史购买序列和label的数据集</span>
data
</code></pre> 
<p><img src="https://images2.imgbox.com/8c/cc/ENWxWeCK_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python"><span class="token comment"># 查看是否有gpu进行运算，如果没有则使用cpu运算（注：cpu计算很慢很慢，最好开个gpu进行计算）</span>
device<span class="token operator">=</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda:0"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span>
<span class="token comment"># 计算出现的最大类别编码是多少，目的为统计一共有多少个商品类别</span>
fields <span class="token operator">=</span> data<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="_141"></a>六.模型定义</h4> 
<p>​ 1.这块代码主要是，定义din模型，模型的内容可以看https://arxiv.org/pdf/1706.06978.pdf 了解din模型是个啥</p> 
<p>​ 2.pytorch定义的模型主要是看init和forward，</p> 
<p>​ 3.init的功能是初始化一些变量，和别的类定义一样；</p> 
<p>​ 4.forward的功能是向前传播，是调用类时直接将参数输入forward中，进行计算；</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Dice</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    自定义的dice激活函数，原论文有公式介绍，有点复杂我也没看懂，别的地方用的不多，不介绍了。
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Dice<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>alpha <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>epsilon <span class="token operator">=</span> <span class="token number">1e-9</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>

        norm_x <span class="token operator">=</span> <span class="token punctuation">(</span>x <span class="token operator">-</span> x<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>x<span class="token punctuation">.</span>var<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>epsilon<span class="token punctuation">)</span>
        p <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>norm_x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>alpha <span class="token operator">*</span> x<span class="token punctuation">.</span>mul<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>p<span class="token punctuation">)</span> <span class="token operator">+</span> x<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>p<span class="token punctuation">)</span>
    
        <span class="token keyword">return</span> x
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">ActivationUnit</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    激活函数单元
    功能是计算用户购买行为与推荐目标之间的注意力系数，比如说用户虽然用户买了这个东西，但是这个东西实际上和推荐目标之间没啥关系，也不重要，所以要乘以一个小权重
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> fc_dims <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>ActivationUnit<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 1.初始化fc层</span>
        fc_layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token comment"># 2.输入特征维度</span>
        input_dim <span class="token operator">=</span> embedding_dim<span class="token operator">*</span><span class="token number">4</span>     
        <span class="token comment"># 3.fc层内容：全连接层（4*embedding,32）—&gt;激活函数-&gt;dropout-&gt;全连接层（32,16）-&gt;.....-&gt;全连接层（16,1）</span>
        <span class="token keyword">for</span> fc_dim <span class="token keyword">in</span> fc_dims<span class="token punctuation">:</span>
            fc_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> fc_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
            fc_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Dice<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            fc_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p <span class="token operator">=</span> dropout<span class="token punctuation">)</span><span class="token punctuation">)</span>
            input_dim <span class="token operator">=</span> fc_dim
        
        fc_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 4.将上面定义的fc层，整合到sequential中</span>
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>fc_layers<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> query<span class="token punctuation">,</span> user_behavior<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
            :param query:targe目标的embedding -&gt;（输入维度） batch*1*embed 
            :param user_behavior:行为特征矩阵 -&gt;（输入维度） batch*seq_len*embed
            :return out:预测目标与历史行为之间的注意力系数
        """</span>
        <span class="token comment"># 1.获取用户历史行为序列长度</span>
        seq_len <span class="token operator">=</span> user_behavior<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token comment"># 2.序列长度*embedding</span>
        queries <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>query<span class="token punctuation">]</span> <span class="token operator">*</span> seq_len<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 3.前面的把四个embedding合并成一个（4*embedding）的向量，</span>
        <span class="token comment">#  第一个向量是目标商品的向量，第二个向量是用户行为的向量，</span>
        <span class="token comment">#  至于第三个和第四个则是他们的相减和相乘（这里猜测是为了添加一点非线性数据用于全连接层，充分训练）</span>
        attn_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>queries<span class="token punctuation">,</span> user_behavior<span class="token punctuation">,</span> queries <span class="token operator">-</span> user_behavior<span class="token punctuation">,</span> 
                                queries <span class="token operator">*</span> user_behavior<span class="token punctuation">]</span><span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>attn_input<span class="token punctuation">)</span>
        <span class="token keyword">return</span> out
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">AttentionPoolingLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
      注意力序列层
      功能是计算用户行为与预测目标之间的系数，并将所有的向量进行相加，这里的目的是计算出用户的兴趣的能力向量
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span>  dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>AttentionPoolingLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>active_unit <span class="token operator">=</span> ActivationUnit<span class="token punctuation">(</span>embedding_dim <span class="token operator">=</span> embedding_dim<span class="token punctuation">,</span> 
                                          dropout <span class="token operator">=</span> dropout<span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> query_ad<span class="token punctuation">,</span> user_behavior<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
          :param query_ad:targe目标x的embedding   -&gt; （输入维度） batch*1*embed
          :param user_behavior:行为特征矩阵     -&gt; （输入维度） batch*seq_len*embed
          :param mask:被padding为0的行为置为false  -&gt; （输入维度） batch*seq_len*1
          :return output:用户行为向量之和，反应用户的爱好
        """</span>
        <span class="token comment"># 1.计算目标和历史行为之间的相关性</span>
        attns <span class="token operator">=</span> self<span class="token punctuation">.</span>active_unit<span class="token punctuation">(</span>query_ad<span class="token punctuation">,</span> user_behavior<span class="token punctuation">)</span>     
        <span class="token comment"># 2.注意力系数乘以行为 </span>
        output <span class="token operator">=</span> user_behavior<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>attns<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>mask<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 3.历史行为向量相加</span>
        output <span class="token operator">=</span> user_behavior<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> output
    
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">DeepInterestNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
      模型主体
      功能是用户最近的历史40个购买物品是xxx时，购买y的概率是多少
    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> feature_dim<span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> mlp_dims<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>DeepInterestNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 1.特征维度，就是输入的特征有多少个类</span>
        self<span class="token punctuation">.</span>feature_dim <span class="token operator">=</span> feature_dim
        <span class="token comment"># 2.embeding层，将特征数值转化为向量</span>
        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>feature_dim<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> embed_dim<span class="token punctuation">)</span>
        <span class="token comment"># 3.注意力计算层（论文核心）</span>
        self<span class="token punctuation">.</span>AttentionActivate <span class="token operator">=</span> AttentionPoolingLayer<span class="token punctuation">(</span>embed_dim<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>
        <span class="token comment"># 4.定义fc层</span>
        fc_layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token comment"># 5.该层的输入为历史行为的embedding，和目标的embedding，所以输入维度为2*embedding_dim</span>
        <span class="token comment">#  全连接层（2*embedding,fc_dims[0]）—&gt;激活函数-&gt;dropout-&gt;全连接层（fc_dims[0],fc_dims[1]）-&gt;.....-&gt;全连接层（fc_dims[n],1）</span>
        input_dim <span class="token operator">=</span> embed_dim <span class="token operator">*</span> <span class="token number">2</span>      
        <span class="token keyword">for</span> fc_dim <span class="token keyword">in</span> mlp_dims<span class="token punctuation">:</span>
            fc_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> fc_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
            fc_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            fc_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p <span class="token operator">=</span> dropout<span class="token punctuation">)</span><span class="token punctuation">)</span>
            input_dim <span class="token operator">=</span> fc_dim
        fc_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 6.将所有层封装</span>
        self<span class="token punctuation">.</span>mlp <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>fc_layers<span class="token punctuation">)</span>        
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
            x输入(behaviors*40,ads*1) -&gt;（输入维度） batch*(behaviors+ads)
            
        """</span>
        <span class="token comment"># 1.排除掉推荐目标</span>
        behaviors_x <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token comment"># 2.记录之前填充为0的行为位置</span>
        mask <span class="token operator">=</span> <span class="token punctuation">(</span>behaviors_x <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 3.获取推荐的目标</span>
        ads_x <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token comment"># 4.对推荐目标进行向量嵌入</span>
        query_ad <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>ads_x<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 5.对用户行为进行embeding，注意这里的维度为(batch*历史行为长度*embedding长度)</span>
        user_behavior <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>behaviors_x<span class="token punctuation">)</span>
        <span class="token comment"># 6.矩阵相乘，将那些行为为空的地方全部写为0</span>
        user_behavior <span class="token operator">=</span> user_behavior<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>mask<span class="token punctuation">)</span>
        <span class="token comment"># 7.将用户行为乘上注意力系数,再把所有行为记录向量相加</span>
        user_interest <span class="token operator">=</span> self<span class="token punctuation">.</span>AttentionActivate<span class="token punctuation">(</span>query_ad<span class="token punctuation">,</span> user_behavior<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>
        <span class="token comment"># 8.将计算后的用户行为行为记录和推荐的目标进行拼接</span>
        concat_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>user_interest<span class="token punctuation">,</span> query_ad<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 9.输入用户行为和目标向量，计算预测得分</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span>concat_input<span class="token punctuation">)</span>
        <span class="token comment"># 10.sigmoid激活函数</span>
        out <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>out<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        
        <span class="token keyword">return</span> out
</code></pre> 
<h4><a id="_297"></a>七.封装训练集，测试集</h4> 
<p>​ 这里的目的是为了划分训练集，测试集</p> 
<p>​ 再把数据封装到data_loader里面，方便后面按batch获取数据，训练模型</p> 
<pre><code class="prism language-python"><span class="token comment">#模型输入</span>
data_X <span class="token operator">=</span> data<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
<span class="token comment">#模型输出</span>
data_y <span class="token operator">=</span> data<span class="token punctuation">.</span>label<span class="token punctuation">.</span>values
<span class="token comment">#划分训练集，测试集，验证集</span>
tmp_X<span class="token punctuation">,</span> test_X<span class="token punctuation">,</span> tmp_y<span class="token punctuation">,</span> test_y <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>data_X<span class="token punctuation">,</span> data_y<span class="token punctuation">,</span> test_size <span class="token operator">=</span> <span class="token number">0.2</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">,</span> stratify<span class="token operator">=</span>data_y<span class="token punctuation">)</span>
train_X<span class="token punctuation">,</span> val_X<span class="token punctuation">,</span> train_y<span class="token punctuation">,</span> val_y <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>tmp_X<span class="token punctuation">,</span> tmp_y<span class="token punctuation">,</span> test_size <span class="token operator">=</span> <span class="token number">0.25</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">,</span> stratify<span class="token operator">=</span>tmp_y<span class="token punctuation">)</span>
dis_test_x <span class="token operator">=</span> test_X
dis_test_y <span class="token operator">=</span> test_y
<span class="token comment"># numpy转化为torch</span>
train_X <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>train_X<span class="token punctuation">.</span>values<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
val_X <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>val_X<span class="token punctuation">.</span>values<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
test_X <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>test_X<span class="token punctuation">.</span>values<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

train_y <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>train_y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
val_y <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>val_y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
test_y <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>test_y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 设置dataset</span>
train_set <span class="token operator">=</span> Data<span class="token punctuation">.</span>TensorDataset<span class="token punctuation">(</span>train_X<span class="token punctuation">,</span> train_y<span class="token punctuation">)</span>
val_set <span class="token operator">=</span> Data<span class="token punctuation">.</span>TensorDataset<span class="token punctuation">(</span>val_X<span class="token punctuation">,</span> val_y<span class="token punctuation">)</span>
test_set <span class="token operator">=</span> Data<span class="token punctuation">.</span>TensorDataset<span class="token punctuation">(</span>test_X<span class="token punctuation">,</span> test_y<span class="token punctuation">)</span>
<span class="token comment"># 设置数据集加载器，用于模型训练，按批次输入数据</span>
train_loader <span class="token operator">=</span> Data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token operator">=</span>train_set<span class="token punctuation">,</span>
                               batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>
                               shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
val_loader <span class="token operator">=</span> Data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token operator">=</span>val_set<span class="token punctuation">,</span>
                             batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>
                             shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
test_loader <span class="token operator">=</span> Data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token operator">=</span>test_set<span class="token punctuation">,</span>
                             batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>
                             shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="_337"></a>八.模型训练</h4> 
<p>​ 模型训练的一般步骤：</p> 
<p>​ 1.定义损失函数</p> 
<p>​ 2.定义优化器</p> 
<p>​ 3.定义模型参数可更新</p> 
<p>​ 4.遍历数据集训练模型</p> 
<p>​ *4.1输入数据，获得预测结果</p> 
<p>​ *4.2计算损失</p> 
<p>​ *4.3反向传播</p> 
<p>​ *4.4参数更新</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 1.设置迭代次数训练模型</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epoches<span class="token punctuation">)</span><span class="token punctuation">:</span>
        train_loss <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token comment"># 1.1设置二分类交叉熵损失函数</span>
        criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 1.2设置adam优化器</span>
        optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr <span class="token operator">=</span> <span class="token number">0.001</span><span class="token punctuation">)</span>
        <span class="token comment"># 1.3设置模型训练，此时模型参数可以更新</span>
        model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 1.4遍历训练数据集，获取每个梯度的大小，输入输出</span>
        <span class="token keyword">for</span> batch<span class="token punctuation">,</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 1.4.1如果有gpu则把数据放入显存中计算，没有的话用cpu计算</span>
            x<span class="token operator">=</span>x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
            y<span class="token operator">=</span>y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
            <span class="token comment"># 1.4.2数据输入模型</span>
            pred <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            <span class="token comment"># 1.4.3计算损失</span>
            loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> y<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token comment"># 1.4.4优化器梯度清空</span>
            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment"># 1.4.5方向传播，计算梯度</span>
            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment"># 1.4.6优化器迭代模型参数</span>
            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment"># 1.4.7记录模型损失数据</span>
            train_loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 1.5模型固化，不修改梯度</span>
        model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        val_loss <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        prediction <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        y_true <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
          <span class="token comment"># 1.6遍历验证数据集，获取每个梯度的大小，输入输出</span>
          <span class="token keyword">for</span> batch<span class="token punctuation">,</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>val_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>
              <span class="token comment"># 1.6.1如果有gpu则把数据放入显存中计算，没有的话用cpu计算</span>
              x<span class="token operator">=</span>x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
              y<span class="token operator">=</span>y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
              <span class="token comment"># 1.6.2模型预测输入</span>
              pred <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
              <span class="token comment"># 1.6.3计算损失函数</span>
              loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> y<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
              val_loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
              prediction<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>pred<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
              y_true<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>y<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 1.7计算auc得分</span>
        val_auc <span class="token operator">=</span> roc_auc_score<span class="token punctuation">(</span>y_true<span class="token operator">=</span>y_true<span class="token punctuation">,</span> y_score<span class="token operator">=</span>prediction<span class="token punctuation">)</span>
        <span class="token comment"># 1.8输出模型训练效果</span>
        <span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">"EPOCH %s train loss : %.5f   validation loss : %.5f   validation auc is %.5f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>train_loss<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>val_loss<span class="token punctuation">)</span><span class="token punctuation">,</span> val_auc<span class="token punctuation">)</span><span class="token punctuation">)</span>        
    <span class="token keyword">return</span> train_loss<span class="token punctuation">,</span> val_loss<span class="token punctuation">,</span> val_auc
</code></pre> 
<pre><code class="prism language-python"><span class="token comment"># 定义din模型</span>
model <span class="token operator">=</span> DeepInterestNet<span class="token punctuation">(</span>feature_dim<span class="token operator">=</span>fields<span class="token punctuation">,</span> embed_dim<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> mlp_dims<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
<span class="token comment"># 迭代次数</span>
epoches <span class="token operator">=</span> <span class="token number">5</span>
<span class="token comment"># 模型训练</span>
_ <span class="token operator">=</span> train<span class="token punctuation">(</span>model<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/82/fa/yMR7BMAt_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="_421"></a>九.效果展示</h4> 
<p>最后拿一条数据看下效果</p> 
<pre><code class="prism language-python"><span class="token comment">#输入的数据</span>
dis_test_x<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>cate_encoder<span class="token punctuation">.</span>inverse_transform<span class="token punctuation">)</span><span class="token punctuation">.</span>reset_index<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/52/98/4tK2CbRu_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python"><span class="token comment">#模型输入的向量</span>
test_X<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/54/89/FGTSHn6b_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python"><span class="token comment">#预测购买的概率</span>
model<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>test_X<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/43/f1/QVSJQypX_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python"><span class="token comment">#事实上该用户是否购买</span>
test_y<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/97/36/WipP5Sc5_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a6f5dd83befea6fe4d18c49b2ecf7259/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">uniapp 原生Toast弹窗提示（穿透所有界面、穿透原生；自定义颜色、图标 ） Ba-Toast</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d08cb030ad658c6282805a6ed983f656/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">(NoPermissions (FileSystemError): Error: EACCES: permission denied, open</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>