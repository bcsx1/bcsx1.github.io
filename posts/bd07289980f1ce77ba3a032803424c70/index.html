<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>R-FCN论文解读及难点理解 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="R-FCN论文解读及难点理解" />
<meta property="og:description" content="原文：R-FCN： https://arxiv.org/abs/1605.06409 Github链接： https://github.com/daijifeng001/r-fcn(https://github.com/daijifeng001/r-fcn) R-FCN:Object Detection via Region-based Fully Convolutional Networks 1、简介 比较流行的关于目标检测的深度网络可以通过ROI( Region-of-Interest) pooling layer分成两个子网络:
1. a shared,“ fully convolution|” subnetwork independent of rols.(独立于Ro的共享的、全卷积的子网络) 2. an Rol-wise subnetwork that does not share computation.(不共享计算的ROI-wise子网)
这种分解来源于较早之前的分类框架,例如: AlexNet和VGGNets,他们由两个子网组成,一个是以 spatial pooling　layer(空间池化层)结束的卷积子网,其后是若干个 fully-connected layers(全连接层)。因此,在图片分类网络中的 spatial pooling layer很自然的在本实验中被转变成了物体检测网络中的 ROI pooling layer。 但是最近最先进的图片分类网络,例如残差网络( ResEts)和 GoogLeNets都是用全卷积设计的。通过分析,使用所有的卷积层去构建一个进行目标检测的共享的卷积网络是一件十分自然的事,而不把 ROI-wise子网作为隐藏层( hidden layer)。然而,通过实证调查,这个方案的检测精确度极低,以至于无法提高网络的分类精确度。为了解决这个问题,在残差网络( Resnet)[9]文献中,ROI pooling layer of the Faster r- cnn detector很不自然的被插进了两组卷积层从而创造了一个更深的 ROI-wise子网来提高准确度,但是由于每个Ro的计算不共享,因此速度会比原来的要慢。 我们认为上述不合理的设计是由图片分类中増长的平移不变性与目标检测中的平移变换性之间的矛盾引起的。一方面,图像级别的分类任务侧重于平移不变性(在一幅图片中平移一个物体而不改变它的判别结果),因此深度全卷积网络结构在尽可能保持平移不变性是最好的,通过在 ImageNet classification上的良好表现得以证明。 Figure1:基于RFCN的物体检测核心思想。在本插图中,通过全卷积网络产生一个k* k=3*3的 position- sensitive score maps。对于每一个在Ro中的k×k个bins, pooling仅仅在k^2个maps中的其中一个呈现(通过不同的颜色进行标记) 另一方面,目标检测任务需要位置表示,从某种程度上说是平移可变换的(translation-variant）。例如,平移一个候选框里的物体应该产生一个有意义的反馈,用来描述候选框和物体的重叠程度。我们假定在图片分类网络中,卷积层越深,对平移越不敏感。为了解决这个两难境地(图片分类中的平移不变性和目标检测中的平移变换性),在 ResNet文献[]中的检测算法中插入了 ROI pooling layer到卷积层——这个区域特异性操作破坏了平移不变性,而且当评估不同区域时,post-Rol卷积层不再是平移不变的了。然而这种设计牺牲了训练和测试的效率,因为它引入了大量的 region-wise layers(TabIe1)。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/bd07289980f1ce77ba3a032803424c70/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-08-06T19:01:24+08:00" />
<meta property="article:modified_time" content="2018-08-06T19:01:24+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">R-FCN论文解读及难点理解</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<center> 
 <strong>原文：R-FCN</strong>： 
 <a href="https://arxiv.org/abs/1605.06409" rel="nofollow">https://arxiv.org/abs/1605.06409</a> 
 <br> 
 <center> 
  <strong>Github链接</strong>： 
  <a href="https://github.com/daijifeng001/r-fcn%28https://github.com/daijifeng001/r-fcn%29">https://github.com/daijifeng001/r-fcn(https://github.com/daijifeng001/r-fcn)</a> 
 </center> 
</center> 
<p></p> 
<h2 id="r-fcnobject-detection-via-region-based-fully-convolutional-networks"> 
 <center>
   R-FCN:Object Detection via Region-based Fully Convolutional Networks 
 </center></h2> 
<h3 id="1简介">1、简介</h3> 
<p>　　比较流行的关于目标检测的深度网络可以通过ROI( Region-of-Interest) pooling layer分成两个子网络:</p> 
<hr> 
<p>　 <strong>1.</strong> a shared,“ fully convolution|” subnetwork independent of rols.(独立于Ro的共享的、全卷积的子网络) <br> 　 <strong>2.</strong> an Rol-wise subnetwork that does not share computation.(不共享计算的ROI-wise子网)</p> 
<hr> 
<p>　　这种分解来源于较早之前的分类框架,例如: AlexNet和VGGNets,他们由两个子网组成,一个是以 spatial pooling　layer(空间池化层)结束的卷积子网,其后是若干个 fully-connected layers(全连接层)。因此,在图片分类网络中的 spatial pooling layer很自然的在本实验中被转变成了物体检测网络中的 ROI pooling layer。 <br> 　　但是最近最先进的图片分类网络,例如残差网络( ResEts)和 GoogLeNets都是用全卷积设计的。通过分析,使用所有的卷积层去构建一个进行目标检测的共享的卷积网络是一件十分自然的事,而不把 ROI-wise子网作为隐藏层( hidden layer)。然而,通过实证调查,这个方案的检测精确度极低,以至于无法提高网络的分类精确度。为了解决这个问题,在残差网络( Resnet)[9]文献中,ROI pooling layer of the Faster r- cnn detector很不自然的被插进了两组卷积层从而创造了一个更深的 ROI-wise子网来提高准确度,但是由于每个Ro的计算不共享,因此速度会比原来的要慢。 <br> 　　我们认为上述不合理的设计是由图片分类中増长的平移不变性与目标检测中的平移变换性之间的矛盾引起的。一方面,图像级别的分类任务侧重于平移不变性(在一幅图片中平移一个物体而不改变它的判别结果),因此深度全卷积网络结构在尽可能保持平移不变性是最好的,通过在 ImageNet classification上的良好表现得以证明。 <br> <img src="https://images2.imgbox.com/41/67/EiJ1hLZK_o.png" alt="这里写图片描述" title=""></p> 
<p></p> 
<center>
  Figure1:基于RFCN的物体检测核心思想。在本插图中,通过全卷积网络产生一个k* k=3*3的 position- sensitive score maps。对于每一个在Ro中的k×k个bins, pooling仅仅在k^2个maps中的其中一个呈现(通过不同的颜色进行标记) 
</center> 
<p></p> 
<p>　　另一方面,目标检测任务需要位置表示,从某种程度上说是平移可变换的(translation-variant）。例如,平移一个候选框里的物体应该产生一个有意义的反馈,用来描述候选框和物体的重叠程度。我们假定在图片分类网络中,卷积层越深,对平移越不敏感。为了解决这个两难境地(图片分类中的平移不变性和目标检测中的平移变换性),在 ResNet文献[]中的检测算法中插入了 ROI pooling layer到卷积层——这个区域特异性操作破坏了平移不变性,而且当评估不同区域时,post-Rol卷积层不再是平移不变的了。然而这种设计牺牲了训练和测试的效率,因为它引入了大量的 region-wise layers(TabIe1)。</p> 
<p>　　在本篇文献中,我们开发出了一个称之为R-FCN( Region- based Fully Convolution Network)的框架来用于目标检测。我们的网络由共享的全卷积结构组成,就像FCN[15]一样。为了把平移变换特性融合进FCN中,我们通过利用一堆特征化的卷积层作为FCN的输出来创建一个 position-sensitive score maps(位敏得分地图),每个得分图对相对空间位置信息进行编码。在FCN的顶层,我们附加了个 position- sensitive Roi pooling layer来统领这些得分地图的信息,这些得分地图不带任何权重层(卷积层或全连接层)。整个结构是端对端(end- to-end)的学习。所有可学习的层在整幅图片中都是可卷积的并且可共享的,并且仍然可以编码空间位置信息用于目标检测。 Figure1说明了这个核心思路( key idea), Table1比较了区域检测的各种算法 <br> 　　 <br> </p> 
<center>
  Table1:使用 ResNet-101[]的 region- based detectors方法 
 <br> 
 <img src="https://images2.imgbox.com/96/b2/Ed7tDS2C_o.png" alt="这里写图片描述" title=""> 
</center> 
<p></p> 
<p>　　使用101层的 Residua|Ne9作为主干,我们的RFCN在 PASCAL VOC2007测试集上达到了836%的mAP,在2012测试集上达到了820%的mAP。同时,我们的结果实现了170ms/每张图片的速度,比 Faster r-cnn+ ResNet-1019快了25~20倍。这个实验结果说明了我们的方法成功的解决了基于全卷积网络的图像级别的分类问题中的平移不变性和平移变换性之间的矛盾,全卷积图像级别分类器,比如 ResNet,能够被有效的转换成全卷积目标检测器。详细的代码参见:<a href="https://github.com/daijifeng001/r-fcn">https://github.com/daijifeng001/r-fcn</a> <br> 　　</p> 
<h3 id="2我们的方法">2、我们的方法</h3> 
<h4 id="overview概览">Overview(概览)</h4> 
<p>对于下面的RCN,我们采用了流行的二级的目标检测策略: region proposa和 region classification。尽管不依赖于region proposa的方法确实存在(比如[17][14]), region- bases system(基于区域的系统)在一些 benchmark[6][13][20上仍然具有最好的准确率。我们通过region proposal Network(RPN)来抽取候选区域,它自身是一个全卷积结构。接下来[18],我们在RPN和R-FCN中共享这些特性。 Figure2展示了整个系统的概览</p> 
<p><img src="https://images2.imgbox.com/cd/71/pu9XiWpR_o.png" alt="这里写图片描述" title=""> <br> </p> 
<center>
  Figure2:RFCN的整体架构。 Region proposal Network(RPN产生候选Rol,然后Rol会被应用于得分地图。所有可学习的权重层在整幅图片中都是可卷积的并且是可计算的。每个Rol的计算开销都是可以忽略的 
</center> 
<p></p> 
<p>　　考虑到 proposal region,R-FCN结构用来将ROI分类为各种物体和背景。在R-FCN中,所有可学习的权重层都是可卷积的并且是在整幅图片中进行计算。最后一个卷积层产生一堆K^2个针对于每一个物体类别的 position-sensitive score <br> maps。因此有k*k(C+1)个通道输出层(C个物体目录项+1个背景)。这K^2个得分地图对应于用来描述相对位置的KxK空间网格。例如,对于K×K=3×3,这9个得分地图将物体类别编码为9种情况(top-lef, top-center, top-rigth,…,bottom-right)。</p> 
<p>　　R-FCN以 position-sensitive Rol pooling layer作为结束层。他将最后一个卷积层的输出结果聚集起来,然后产生每一个ROI的得分记录。和[8][6]不同,我们的 position- sensitive Rol layer产生的是 selective pooling,并且KxK容器的每个条目仅仅聚集来自KxK得分地图堆里面的一个得分地图的响应。通过端对端的训练,ROI层带领最后一层卷积层去学习特征化的 position-sensitive score maps。Figure1说明了这个过程。 Figure3, Figure4是两个可视化的例子。本算法的具体细节介绍参见后面的条目。 <br> 　　<img src="https://images2.imgbox.com/4b/b2/3vcTpijq_o.png" alt="这里写图片描述" title=""> <br> </p> 
<center> 
 <strong>Figure4:可视化例子:Rol与物体没有完全覆盖</strong> 
</center> 
<p></p> 
<h4 id="backbone-architecture主干架构">Backbone architecture(主干架构)</h4> 
<p>　　本篇文献中的RFCN算法是基于 ResNet-1019]的,虽然其他的深度学习网络[10][23]也可以应用。 RstNet-101有100个带goba| average pooling的卷积层,有一个1000级的f层( fully-connected)。我们去掉了 average pooling和 fc layer, <br> 然后只使用卷积层来计算特征图。我们利用作者[9]发布的 ResNet-101,在Image Net[20]上进行预训练。在 ResNet-101中,最后一个卷积块是2048-d(2048维度)我们附加了一个随机初始化的1024-d的1×1的卷积层来降低维度(更精确的,我们增加了卷积层的深度),然后我们使用了k*k(C+1)个通道的卷积层来产生得分地图,下面会有具体的介绍。</p> 
<h4 id="position-sensitive-socre-maps-position-sentitive-rol-pooling">Position-sensitive socre maps position-sentitive rol pooling</h4> 
<p>　　为了在每个ROI中明确编码位置信息,我们通过一个规则网格把每个ROI分成K×K个bins。对于Wxh的Ro区域,每一个bin的大小≈W/k×h/k[8]6]。在我们的方法中,最后一个卷积层是被用来为每个类别产生K*K个得分地图的。在第i行第j列的bin(0≤i,j≤k-1)中,我们定义了一个 position- sensitive Rol pooling操作,仅仅将结果汇集到在第(i,j)个得分图上: <br> 　</p> 
<center>
  　 
 <img src="https://images2.imgbox.com/2c/ee/LYKjuinY_o.png" width="500" height="80" align="center"> 
 <br> 　　 
 <br> 　　在这个公式中:rc(i,j)是针对于第c个目录的第(i,j)个bin的 pooled response;zi,j,c,是K*K(C+1)个得分地图中的其中一个地图;(x,y)表示Ro的左上角;n表示bin中像素的个数;theta表示网络中的所有可学习的参数。第(i,j)个bn的范围是 
 <br> 
 <img src="https://images2.imgbox.com/ef/72/5aGaRtXf_o.png" alt="这里写图片描述" title=""> 
</center> 
<br> 　　 
<br> 　　关于上述等式,在 Figure1中有解释,其中一个颜色代表一个(i,j)对。上述等式使用的是 average pooling(我们整篇文献使用的都是这个)当然 max pooling也有很好的表现。 
<p></p> 
<p>　　这K^2个得分图用来对ROI区域进行投票,在这篇文献中,我们仅仅通过平均得分进行投票,对于每个RoI产生(C+1)维的向量: 　 <br> 　　</p> 
<center>
  　 
 <img src="https://images2.imgbox.com/38/2e/oIl4Hq5l_o.png" alt="这里写图片描述" title=""> 
</center> 
<br> 　　然后我们计算了每一个目录项的softmax响应： 
<br> 　　 
<center> 
 <img src="https://images2.imgbox.com/d7/dd/hYzTxT53_o.png" alt="这里写图片描述" title=""> 
</center> 
<br> 　　在训练和在推理中对RoI评级的时候,他们被用来估计交叉熵损失(cross-entropy）。 
<br> 　　进一步的,我们用相似的方法定位了边界框回归[][6]( bounding boxregression)。除了上面的k*k(C+1)个卷积层,我们附加了一个4k*k个卷积层用于边界框回归。 Position- sensitive rol poo|ing在4k*k的map中执行,对于每一个Rol产生一个4k*k维的向量,然后通过平均投票被聚合为一个4维向量。这个4维向量用t=(1,2,3,4)参数化一个边框[6]。需要注意的是,为了简单起见我们利用的是类别无关的边界框回归,但是对应的特殊类别也是可使用的(比如,一个4k*k个输出层)。 
<br> 　　Position- sensitive score maps有一部分的灵感来自于实例级的语义分割FCNS[3]。进一步的,我们引入了 position-sensitive Rol pooling layer来统领物体检测得分图的学习。在ROI层之后,由于没有可学习的层,所以能够进行几乎无代 
<br> 　　 
<p></p> 
<h4 id="training训练">Training(训练)</h4> 
<p>　　在预先计算了 region proposals之后,端对端地训练R-FCN结构是非常简单的接下来[6],我们的损失函数( loss fuction)定义为在每个RoI上的交叉熵损失(cross-entropy loss)和边界回归损失( box regression loss)的总和: <br> </p> 
<center> 
 <img src="https://images2.imgbox.com/f2/6a/9NHYF01A_o.png" alt="这里写图片描述" title=""> 
</center> 
<br> 上式中,c*是Rol的 ground-truth labe(c*=0表示的是背景)。Lcls(Sc*)是用于分类的交叉熵损失函数( cross-entropy loss); 
<br> Lneg是边界回归损失函数( bounding box regression loss)如[6中定义的一样;t*表示 ground truth box;[c*&gt;0]指示的是,如果参数为真,则等于1,否则为0。我们设置平衡权重=1[6]。当RoI与实际边框的重叠部分至少有0.5的IOU时,那么我们认为是 positive examples,否则是 negative example。 
<p></p> 
<p>　　在训练时,在我们的方法中采用OHEM( online hard example mining)是非常容易的。每个可忽略的Ro区域计算使得样例挖掘( examp| e mIning)是 cost-free的。假定每张图片有N个 proposals,在前向传播时,我们估算所有N个 proposals的损失。然后我们对所有RoI按照损失进行排序,然后挑选B个具有最高损失的ROl。 Backpropagation(反向传播算法)是基于选择的样例来演算的。因为我们的每个ROI的计算都是近似可以忽略的,所以 forward time基本上不会受N的影响。而 OHEM Fast r-CNN[22]可能是会花费双倍的训练时间。在Table3中,我们提供了一个详细的时间数据。 <br> 　　</p> 
<h4 id="inference测试">Inference(测试)</h4> 
<p>正如 Figure2描述的,RPN和R-FCN共享的特征图( feature maps)被计算(在个单尺度为600的图像上)。然后RPN模块选择出了Ro,而R-FCN模块评估了catagogy-wise scores(针对于每一个物体项的得分)和 regresses bounding boxes。在测试阶段,我们评估了[181中的300个Ro区域,进行了公平的比较。结果通过non- maximum suppression(NMS)来进行后期处理,使用了0.3IOU[7]的阈值,作为标准练习。</p> 
<h4 id="a-trous-and-stride-多孔算法和步长">A trous and stride (多孔算法和步长）</h4> 
<p>　　我们的全卷积网络架构是在用于语义分割的FCN基础上进行修改的，并享有它的好处。特别的，我们将ResNet-101的有效stride从32像素减少到16像素，从而增加了得分图分辨率。在conv4之前的所有层（包括conv4)(步长为16) 保持不变；在第一个conv5块的stride=2改为stride=1;在conv5阶段的所有 的卷积滤波都被“hole algorithm进行修 改，用来补偿步长的减小。为了公平的比较，RPN是在conv4阶段的顶层(和 R-FCN共享)进行计算的，正如在[9]中使用FasterR-CNN一样，所以RPN没有被会 trous trick影响。下表显示的是 R-FCN 的结果。（KxK = 7x7, no hard example mining) trous trick将mAP提高了2.6个点。 <br> 　</p> 
<center> 
 <img src="https://images2.imgbox.com/36/a8/Mr82P7KG_o.png" alt="这里写图片描述" title=""> 
</center> 
<p></p> 
<h4 id="visualization-可视化">Visualization (可视化）</h4> 
<p>　　在Figure 3和Figure 4中，我们展示了当KxK = 3x3时，通过R-FCN学习到的 position-sensitive score maps。这些特征地图受到物体的具体相对位置的强烈影 响。例如：“top-center-sensitive” score map对于那些大致接近目标的top-center位置的显示了较高的分数。如果一个候选框与真实物体精确的重合了 (Figures)， 那么大多数的k*k个bins会被强烈的激活，然后会得到较高的分数。相反的，如 果候选边框与真实物体并没有完全准确的重合(Figure4)，那么在ROI里的有一 些k*k bins不会被激活，从而导致得分很低。 <br> 　　</p> 
<h3 id="3相关工作">3、相关工作</h3> 
<p>　　R-CNN[2]己经演示了带深度网络的region proposals丨271丨281的有效性。R-CNN 评估那些关于裁剪过的和带覆盖区域的卷积网络，并且计算在区域之间是不共享 (Table 1)。SPPnet[§]，Fast R-CNN[§]和 Faster R-CNN[18]是 semi-convolutional的，在卷积子网络中是共享对整图的计算的，在另一个子网络是各自计算独立的区域。 <br> 　　 <br> 　　己经有物体检测器可以被认为是全卷积模型。OverFeat [丑]通过在共享的卷 积特征图上滑动多尺度窗口来检测目标。同样的，在FastR-CNN迫]and[U]中， 也在研究能取代region proposals的滑动窗口。在这些情况下，可以将单尺度的 滑动窗口改造成一个单层的卷积层。在Faster R-CNN中的RPN组件是一个全卷 积检测器，用来预测是一个关于多尺寸参考边框的实际边框。原始的RPN是 class-agnostic (译者注：类别无关的）。但是对应的clss-specific counterpart是可 应用的，接下来我们会演示的。 <br> 　　 <br> 　　另一个用于目标检测的是fclayer (fully-connected)用来基于整幅图片的完 整目标检测。</p> 
<p></p> 
<center>
  Table2:使用 ResNet-101的基于全卷积方法的比较。在这个表格中的所有的比较对象使用的a trous算法。没有进行 Hard sample mining。 
</center> 
<br> 
<center> 
 <img src="https://images2.imgbox.com/e8/45/epzcGGvp_o.png" alt="这里写图片描述" title=""> 
</center> 
<p></p> 
<h3 id="4实验结果">4、实验结果</h3> 
<h4 id="41在-pascal-voc上的实验">4.1在 PASCAL VOC上的实验</h4> 
<p>　　我们在 PASCAL VOC[5]上进行了实验, PASCAL VOC上有20个物体目录项我们在voC2007和voC2012的联合训练集上进行模型的训练,接下来[6],然后在VOC2007测试集上进行了测试。通过 mean Average Precision(mAP)来测量物体检测的精度。 <br> 　　</p> 
<h5 id="comparisons-with-other-fully-convolutional-strategies">Comparisons with other Fully convolutional Strategies</h5> 
<p>　　虽然全卷积检测器很容易获取,但是实验证明它的精度非常髙。我们使用Resnet-101网络研究了如下几种全卷积(或者近似是卷积的,每个ROI仅含有一个fc层)策略: <br> 　　Naive Faster RCNN 正如1、简介里面介绍的,它在 ResNet-101中使用所有的卷积层来计算共享的特征图,并且在最后一个卷积层(在conv5之后)采用了 Rol pooling。在每个Roi只有有少量的21- class fc layer被计算(所以变化的基本上是全卷积层)。 a trous trick被用来进行公平的比较。 <br> 　　Class-specific RPN. RPN在[18]中进行训练,其中,2 class(表明是否是物体)。卷积分类层被替换为21- class积分类层。为了公平的比较,对于 class-specificRPN,我们使用了带 a trous trick的 ResNet-101的con5 layer。 <br> 　　R- FCN without position- sensitivity.通过设置K=1,我们移除了RFCN的position- -sensitivity。这等价于每个Ro中的 global pooling。 <br> 　　分析。Table2显示了分析结果。我们注意到,在 ResNet的论文中,标准 FasterRCNN达到了76.4%的mAP(见Table3),它是由 ResNet-101组成的,并且在conv4和conv5[9]中间插入了一个 Rol pooling layer。作为对比,简单 Faster R-cnn(在 <br> conv5之后添加了 Rol pooling)只有相对较低的68.9%mAP(Iabe2)。这个对比通过实践证明了,通过在 Faster R-Cnn的系统中插入 Rol pooling layer来获取空间信息的重要性。相似的观点在文献[19]中有提到。</p> 
<p></p> 
<center> 
 <strong>Tabe3:使用 ResNet101的 Faster R-cnn和RFCN的比较。在单个 Nvidia k40GPU上进行时间估算。使用OHEM,在前向传播中毎张图片计算N个ROI,在反向传播中,挑选了128个样例。在下面的测试结果中使用了300个ROIs[18]</strong> 
 <img src="https://images2.imgbox.com/cc/60/iTmWvpEy_o.png" alt="这里写图片描述" title=""> 
 <br> 
</center> 
<p></p> 
<p>　　Class-specific RPN有67.6%的mAP( Table2),比标准的 Faster R-cnn的76.4%要低9个百分点。这个对比和文献[6][2]中的结果一致。实际上, class-specific RPN是一种特殊形式的、使用滑动窗口的 Fast R-cnn6],正如[6][12]中指出,它们的结果非常差。 <br> 　　 <br> 　　Position- -sensitivity的重要性,通过设置k=1进一步的展示了出来,当设置k=1时,R-FCN无法收敛。在退化的情况下,在RoI中没有空间信息能够捕获,当Rol pooling的输出解是1×1时,简单 Faster r-CNN是可以收敛的,但是它的mAP却降低到61.7%(见 Table2)</p> 
<h5 id="comparisons-with-faster-r-cnn-using-resnet-101">Comparisons with Faster r-cnn using resNet-101</h5> 
<p>　　接下来,我们比较了标准 Faster R-cnN+ ResNet-101],在 PASCAL VOC、MSCOCO和 mageNet测试集中具有强烈的对比度。接下啦,我们使用了K×K=77的设置。 Table3显示了比较结果。 Faster r-CNN计算一个10层子网的Rol可以达到很好的精度,但是RFCN对每个区域的计算量是可以忽略不计的。对于300个RoI, Faster r-cnn花费了0.42s/ / image。比RFCN的017s/mage(在一个K40GPU;在一个 Titan X GPu上的测试结果是0.11s)慢了2.5倍。并且R-FCN训练的速度 <br> 比 Faster R-CNn要快。同时, hard example mining[22]在RFCN中不增加任何开销(Table3)。当需要从2000个RoI中挖掘信息,训练一个RCN是可行的,对比下, Faster r-CNN要比它慢6倍(2.9sVS0.46s)。但是实验证明,从一个更大的RoI中来挖掘信息是没有意义的(Table3),并没有显著提高mAP。所以在本篇文献中我们使用了300个Rol来进行训练和测试 <br> 　　Table4显示了更多的对比。按照]中的多尺度训练,在每一个训练迭代中,我们随机地将图片归一化为特殊的像素尺寸(400,500,600,700,800)。我们测试的时候,仍然使用600像素的图片,从而没有增加测试时间。mAP的结果是80.5%。另外,我们在MS〔oCo[3]训练集上训练了我们的模型,然后在 PASCAL VOC数据集中微调了这个模型。RFCN达到了83.6%的mAP(Table4),与使用ResNet-101的 Faster R-CNM+-系统的精度非常接近。我们的测试速度是0.17s/ /image,比花费了3.36s/amge的 Faster R-CNN(它使用了迭代框回归、文本、多尺度测试)快了20倍左右。这些比较结果在 PASCAL VOC2012测试集上同样可以看到( Table5) <br> </p> 
<center>
  Table4:使用 ResNet-101,在 PASCAL VOC2007测试集上的比较。 Faster r-cnn+++使用了迭代框回归( (iterative box regression)、文本和多尺度测试。 
 <img src="https://images2.imgbox.com/9a/dd/eGGKxh2u_o.png" alt="这里写图片描述" title=""> 
</center> 
<p></p> 
<p></p> 
<center>
  Table5:使用 ResNet-101,在 PASCAL VOC2012测试集上的比较。“07+12”表示07的训练集和测试集,12的训练集。 
 <img src="https://images2.imgbox.com/7d/f1/Mc05GbvD_o.png" alt="这里写图片描述" title=""> 
</center> 
<p></p> 
<h5 id="on-the-impact-of-depth">On the Impact of Depth</h5> 
<p>　　下面的表格显示了使用不同深度凹]的 ResEts的R-FCN结果。当深度从50增加到101的时候,我们的检测精确度获得了提升。在152层的时候,达到饱和。 <br> 　　</p> 
<center> 
 <img src="https://images2.imgbox.com/59/ce/mlYfOqdw_o.png" alt="这里写图片描述" title=""> 
</center> 
<p></p> 
<h5 id="on-the-impact-of-region-proposals">On the impact of region Proposals</h5> 
<p>　　R-FCN能够被很容易的应用到其他的 region proposal methods,例如 Selective　Search(SS)[27]和 Edge boxes(EB)[28]。下面的表格显示了带不同 proposals的结果(使用了 ResNet-101)。R-FCN在SS和EB中的结果表现的差不多,证明了我们方法的通用性。 <br> </p> 
<center> 
 <img src="https://images2.imgbox.com/0c/ed/QohXozRr_o.png" alt="这里写图片描述" title=""> 
</center> 
<p></p> 
<h4 id="42在-ms-coco上的实验">4.2在 MS COCO上的实验</h4> 
<p>　　接下来,我们在 MS COCO数据集上进行了测试,该数据集拥有80个物体目录项。我们的实验使用了80k个训练集,40 k val set,20 k test-dey set。对于90k的迭代,我们使用了0.001的学习速率;对于30k的迭代,我们使用了0.0001的学习速率,使用的有效min-batch的大小是8。我们将训练从4步扩展到5步,当特征共享时,这种调整轻微地提髙了精确度。当然,2步训练已经足够提高精度了,但是特征是不共享的。 <br> 　　 <br> 　　结果在Table6中展示。我们的单尺度RFCN的基线结果是48.9%/27.6%,和Faster R-cnn的48.4%/27.2%是相近的。但是我们的测试速度比它快了2.5倍。当物体较小时([13]),我们的方法表现的效果更好。我们的多尺度RFCN在 val set <br> 上的结果是49.1%/27.8%;在 test-dey set上的结果是51.5%/29.2%。考虑到CoCo的物体尺度具有很大的范围,我们进一步估算了多尺度测试,使用了{200,400600,800,1000等不同尺度。mAP的结果是53.2%/31.5%。这个结果和带ResNet-101的 Faster r-CNN++的结果(55.7%/34.9%)非常接近。不仅如此,我们的方法更加简单,并且没有添加类似于 context或者 iterative box regression的东西,并且在训练和测试上更快。</p> 
<p></p> 
<center>
  Table6:使用 ResNet101,在 MS COCO数据集上的比较。 The COCO-style AP is evaluated@loU2 [0: 5; 0: 95]. AP@0. 5 is the PASCal-style AP evaluated loU=0: 5 
 <img src="https://images2.imgbox.com/d5/1d/238yYAnX_o.png" alt="这里写图片描述" title=""> 
</center> 
<p></p> 
<h3 id="5总结与展望">5、总结与展望</h3> 
<p>　　我们提出的 Region-based Fully Convolutional Networks是一个简单、精确、有效的用于物体检测的框架。我们的系统很自然的采用了先进的图片分类骨架,就像基于全卷积的Res№ets一样。我们的方法和 Faster r-CN具有可比的精确度,但是在训练和预测上都比它要快。我们特意使本篇论文中给出的R-FCN看起来简单。其实仍然存在一系列的FCNS的正交扩展用来进行语义分割(如[2]),还有一些基于区域方法的扩展用来进行物体检测。期望我们的系统能享受到这个领域进步的好处。</p> 
<hr> 
<h3 id="算法总结">算法总结：</h3> 
<p><strong>RCNN</strong> <br> 　　1. 在图像中确定约1000-2000个候选框 (使用选择性搜索) <br> 　　2. 每个候选框内图像块缩放至相同大小，并输入到CNN内进行特征提取 <br> 　　3. 对候选框中提取出的特征，使用分类器判别是否属于一个特定类 <br> 　　4. 对于属于某一特征的候选框，用回归器进一步调整其位置 <br> <strong>Fast RCNN</strong> <br> 　　1. 在图像中确定约1000-2000个候选框 (使用选择性搜索) <br> 　　2. 对整张图片输进CNN，得到feature map <br> 　　3. 找到每个候选框在feature map上的映射patch，将此patch作为每个候选框的卷积特征输入SPP layer和之后的层 <br> 　　4. 对候选框中提取出的特征，使用分类器判别是否属于一个特定类 <br> 　　5. 对于属于某一特征的候选框，用回归器进一步调整其位置 <br> <strong>Faster RCNN</strong> <br> 　　1. 对整张图片输进CNN，得到feature map <br> 　　2. 卷积特征输入到RPN，得到候选框的特征信息 <br> 　　3. 对候选框中提取出的特征，使用分类器判别是否属于一个特定类 <br> 　　4. 对于属于某一特征的候选框，用回归器进一步调整其位置</p> 
<hr> 
<blockquote> 
 <p><strong>R-FCN是微软亚洲研究院的代季峰在2016年提出的一种全新的目标检测结构。它对传统的Faster R-CNN结构进行了改造，将ROI层后的卷积都移到了ROI层前，并利用一种位置敏感的特征图来评估各个类别的概率，使其在保持较高定位准确度的同时，大幅提高检测速率。</strong></p> 
</blockquote> 
<h6 id="网上很容易找到关于r-fcn的各种文章所以不再重复介绍它的结构只是选几个容易引起误解的点做深入解读">网上很容易找到关于R-FCN的各种文章，所以不再重复介绍它的结构，只是选几个容易引起误解的点做深入解读。</h6> 
<p><strong><a href="https://blog.csdn.net/qq_30622831/article/details/81459365">理解难点：平移不变性和平移可变性</a></strong></p> 
<p><strong>理解难点：R-FCN结构的优点</strong></p> 
<p>　　R-FCN要解决的根本问题是Faster R-CNN检测速度慢的问题，速度慢是因为ROI层后的结构对不同的proposal是不共享的，试想下如果有300个proposal，ROI后的全连接网络就要计算300次，这个耗时就太吓人了。所以作者把ROI后的结构往前挪来提升速度，但光是挪动下还不行，ROI在conv5后会引起上节提到的平移可变性问题，必须通过其他方法加强结构的可变性，所以作者就想出了通过添加Position-sensitive score map来达到这个目的。 <br> 　　 <br> <strong>理解难点：Position-sensitive score map的结构</strong></p> 
<p>图1的ResNet-101应用到R-FCN时会把最后的average pool和1000-d fc全连接层都去掉了，仅保留前100层，再新加一个1x1x1024的卷积层用来降维（从2048维降到1024维），和一个很特殊的卷积来生成k2 * (C+1)维的Position-sensitive score map。其中的C是要分类的类别数，比如PASCAL VOC类别就是20，加上1表示加上一个背景分类；k是之后的ROI Pooling中对ROI区域要划分的小格数，比如论文中k=3就是对ROI在长宽方向各三等分形成9个小区域(如图2)。Position-sensitive score map的值对小区域相对于ROI中的位置很敏感，为什么这么说后面会解释。 <br> <img src="https://images2.imgbox.com/47/a1/ESoWgOgh_o.png" alt="这里写图片描述" title=""> <br> 图2中最后一个特殊卷积输出Position-sensitive score map后，就要做ROI Pooling了，和Faster R-CNN中的ROI Pooling一样要对9个小区域分别进行pooling，要注意的是R-FCN中９个小区域并不是在所有k2 * (C+1)维度上都做pooling，每个小区域只会在对应的(C+1)个维度上作pooling，比如ROI左上角的区域就在前C+1个维度上pooling，左中位置的区域就在C+２到２C+２间的维度上作pooling，以此类推。pooling后输出的是C+1维度的k*k数据，每个维度上的k*k个数据再加到一起(图２的vote过程)形成C+1个单点数据，就代表了C+1个类别的分类概率。对于目标定位的输出和上面的分类输出过程类似，只是维度不再是k2 * (C+1)，而是k2*4，表示９个小区域的［dx,dy,dw,dh］4个偏移坐标。 <br> <strong><a href="https://blog.csdn.net/qq_30622831/article/details/81459407">理解难点：Position-sensitive score map影响平移可变性的本质</a></strong></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/54a63428181156ea45b311d7f1cba47b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">PyTorch基础入门七：PyTorch搭建循环神经网络(RNN)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/3aa005251efcb519742da07e63d922cd/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">A Simple Problem with Integers （叶子节点 循环节）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>