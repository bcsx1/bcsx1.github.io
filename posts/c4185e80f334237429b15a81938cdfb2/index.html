<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>[深度学习]note for Machine Learning: An Algorithmic Perspective, Second Edition（Ch04-Ch06）【日常更新】 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="[深度学习]note for Machine Learning: An Algorithmic Perspective, Second Edition（Ch04-Ch06）【日常更新】" />
<meta property="og:description" content="Machine Learning: An Algorithmic Perspective, Second Edition——Part 2 4. 多层感知器4.1 前向4.1.1 偏置 4.2 后向：误差的反向传播4.2.1 多层感知器算法4.2.2 初始化权重4.2.3 不同的输出激活函数4.2.4 顺序和批量训练4.2.5 局部最小4.2.6 利用冲量4.2.7 小批量和随机梯度下降4.2.8 其他改善方法 4.3 实践中的MLP4.3.1 训练数据的量4.3.2 隐藏层的数目4.3.3 什么时候停止学习 4.4 MLP应用示例4.4.1 回归问题 4. 多层感知器 我们感兴趣的大部分分体是非线性可分的——找到一种合适的转换特征的方式使问题线性可分；
神经网络中的学习与权重有关——为了进行更多的运算，就要加入更多的权重——① 加入后向的连接，以便于输出神经与输入联系起来——循环网络（recurrent network），已有研究，但并不经常使用；② 加入更多神经元——在输入节点与输出节点之间加入神经元，使神经网络更复杂；
加入额外的层会使神经网络更有用——解决XOR问题：
检查不同的输入结果可以看出上述网络可以实现XOR函数问题。
如何训练这种多层网络使得权重产生合适的正确目标——计算输出的误差——但不知道哪一个权重是错误的：在第一层还是在第二层；同时也不知道网络中间的神经元的正确激活是什么——隐藏层（hidden layer）：不可能去检查并且直接修正它们的值——多层感知器MLP（Rumelhart、Hinton、MeClelland，1986）；
MLP是最常用的神经网络之一，通常被视为“黑匣子”——人们在不了解其工作原理的情况下使用它，这通常会导致相当差的结果。
4.1 前向 训练MLP由两部分组成：① 前向（forward）：对于给定的输入和当前的权重，计算输出；② 后向（backward）：根据误差对权重进行更新——误差：表示输出和目标之间差别的函数；
前向——再现阶段：沿着网络计算每层神经元是否激活，并用与下一层的输入。
4.1.1 偏置 需要使每一个神经元包含一个偏置输入（-1），并且调整每个神经元的权重作为训练的一部分——在网络中的每个神经元（无论是隐藏层还是输出层）都有一个格外 的输入，且为定值。
4.2 后向：误差的反向传播 误差的反向传播（back-propagation of error）：误差通过网络向后传播，是梯度下降（gradient descent）的一种形式——用数学来表述反向传播的性质。
在感知器中，改变权重以便于当目标认为一个神经元应该激活时这个神经元激活，而目标认为这个神经元不能激活时，它就不激活——为每一个神经元k选择预估误差函数Ek=yk-tk，并试图让其最小——此时网络只有一组权重，所以足以训练网络。
加入额外层（隐藏层）的权重，使得最小化误差难以安排——在尝试调整多层感知器的权重时，需要知道是哪一个权重引起的误差：可能是连接输入层和隐藏层的权重，或者是连接隐藏层和输出层的权重——对于更复杂的网络，在隐藏层中节点之间会有额外的权重，可以用相同的方法解决，但更难讨论，此处只考虑一层隐藏层。
感知器使用的误差函数为： ∑ k = 1 N E k = ∑ k = 1 N y k − t k \sum\limits_{k=1}^{N}{{{E}_{k}}}=\sum\limits_{k=1}^{N}{{{y}_{k}}}-{{t}_{k}} k=1∑N​Ek​=k=1∑N​yk​−tk​，N为输出节点的数量，假设存在两个误差点：一个目标大于输出，一个输出大于目标，而这两个误差的大小相同，则和为0——算法认为没有误差——需要让所有的误差都具有相同的符号——平方和（sum-of square）误差函数：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/c4185e80f334237429b15a81938cdfb2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-27T14:05:31+08:00" />
<meta property="article:modified_time" content="2022-04-27T14:05:31+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">[深度学习]note for Machine Learning: An Algorithmic Perspective, Second Edition（Ch04-Ch06）【日常更新】</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>Machine Learning: An Algorithmic Perspective, Second Edition——Part 2</h4> 
 <ul><li><a href="#4___2" rel="nofollow">4. 多层感知器</a></li><li><ul><li><a href="#41___12" rel="nofollow">4.1 前向</a></li><li><ul><li><a href="#411___15" rel="nofollow">4.1.1 偏置</a></li></ul> 
   </li><li><a href="#42___17" rel="nofollow">4.2 后向：误差的反向传播</a></li><li><ul><li><a href="#421___48" rel="nofollow">4.2.1 多层感知器算法</a></li><li><a href="#422___132" rel="nofollow">4.2.2 初始化权重</a></li><li><a href="#423___138" rel="nofollow">4.2.3 不同的输出激活函数</a></li><li><a href="#424___165" rel="nofollow">4.2.4 顺序和批量训练</a></li><li><a href="#425___175" rel="nofollow">4.2.5 局部最小</a></li><li><a href="#426___179" rel="nofollow">4.2.6 利用冲量</a></li><li><a href="#427___189" rel="nofollow">4.2.7 小批量和随机梯度下降</a></li><li><a href="#428___195" rel="nofollow">4.2.8 其他改善方法</a></li></ul> 
   </li><li><a href="#43__MLP_197" rel="nofollow">4.3 实践中的MLP</a></li><li><ul><li><a href="#431___199" rel="nofollow">4.3.1 训练数据的量</a></li><li><a href="#432___202" rel="nofollow">4.3.2 隐藏层的数目</a></li><li><a href="#433___214" rel="nofollow">4.3.3 什么时候停止学习</a></li></ul> 
   </li><li><a href="#44__MLP_217" rel="nofollow">4.4 MLP应用示例</a></li><li><ul><li><a href="#441___218" rel="nofollow">4.4.1 回归问题</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="4___2"></a>4. 多层感知器</h2> 
<p>我们感兴趣的大部分分体是非线性可分的——找到一种合适的转换特征的方式使问题线性可分；<br> 神经网络中的学习与权重有关——为了进行更多的运算，就要加入更多的权重——① 加入后向的连接，以便于输出神经与输入联系起来——循环网络（recurrent network），已有研究，但并不经常使用；② 加入更多神经元——在输入节点与输出节点之间加入神经元，使神经网络更复杂；<br> <img src="https://images2.imgbox.com/70/48/iaspXy5m_o.png" alt="在这里插入图片描述"><br> 加入额外的层会使神经网络更有用——解决XOR问题：<br> <img src="https://images2.imgbox.com/86/68/WtCkHQFS_o.png" alt="在这里插入图片描述"><br> 检查不同的输入结果可以看出上述网络可以实现XOR函数问题。</p> 
<p>如何训练这种多层网络使得权重产生合适的正确目标——计算输出的误差——但不知道哪一个权重是错误的：在第一层还是在第二层；同时也不知道网络中间的神经元的正确激活是什么——隐藏层（hidden layer）：不可能去检查并且直接修正它们的值——多层感知器MLP（Rumelhart、Hinton、MeClelland，1986）；<br> MLP是最常用的神经网络之一，通常被视为“黑匣子”——人们在不了解其工作原理的情况下使用它，这通常会导致相当差的结果。</p> 
<h3><a id="41___12"></a>4.1 前向</h3> 
<p>训练MLP由两部分组成：① 前向（forward）：对于给定的输入和当前的权重，计算输出；② 后向（backward）：根据误差对权重进行更新——误差：表示输出和目标之间差别的函数；<br> 前向——再现阶段：沿着网络计算每层神经元是否激活，并用与下一层的输入。</p> 
<h4><a id="411___15"></a>4.1.1 偏置</h4> 
<p>需要使每一个神经元包含一个偏置输入（-1），并且调整每个神经元的权重作为训练的一部分——在网络中的每个神经元（无论是隐藏层还是输出层）都有一个格外 的输入，且为定值。</p> 
<h3><a id="42___17"></a>4.2 后向：误差的反向传播</h3> 
<p>误差的反向传播（back-propagation of error）：误差通过网络向后传播，是梯度下降（gradient descent）的一种形式——用数学来表述反向传播的性质。</p> 
<p>在感知器中，改变权重以便于当目标认为一个神经元应该激活时这个神经元激活，而目标认为这个神经元不能激活时，它就不激活——为每一个神经元k选择预估误差函数Ek=yk-tk，并试图让其最小——此时网络只有一组权重，所以足以训练网络。</p> 
<p>加入额外层（隐藏层）的权重，使得最小化误差难以安排——在尝试调整多层感知器的权重时，需要知道是哪一个权重引起的误差：可能是连接输入层和隐藏层的权重，或者是连接隐藏层和输出层的权重——对于更复杂的网络，在隐藏层中节点之间会有额外的权重，可以用相同的方法解决，但更难讨论，此处只考虑一层隐藏层。</p> 
<p>感知器使用的误差函数为：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          ∑ 
         
         
         
           k 
          
         
           = 
          
         
           1 
          
         
        
          N 
         
        
        
        
          E 
         
        
          k 
         
        
       
         = 
        
        
        
          ∑ 
         
         
         
           k 
          
         
           = 
          
         
           1 
          
         
        
          N 
         
        
        
        
          y 
         
        
          k 
         
        
       
         − 
        
        
        
          t 
         
        
          k 
         
        
       
      
        \sum\limits_{k=1}^{N}{<!-- -->{<!-- -->{E}_{k}}}=\sum\limits_{k=1}^{N}{<!-- -->{<!-- -->{y}_{k}}}-{<!-- -->{t}_{k}} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 2.53045em; vertical-align: -1.00211em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.52834em;"><span class="" style="top: -2.09789em; margin-left: 0em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03148em;">k</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class=""><span class="mop op-symbol small-op">∑</span></span></span><span class="" style="top: -3.95em; margin-left: 0em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.00211em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 2.53045em; vertical-align: -1.00211em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.52834em;"><span class="" style="top: -2.09789em; margin-left: 0em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03148em;">k</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class=""><span class="mop op-symbol small-op">∑</span></span></span><span class="" style="top: -3.95em; margin-left: 0em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.00211em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">y</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.76508em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathdefault">t</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span>，N为输出节点的数量，假设存在两个误差点：一个目标大于输出，一个输出大于目标，而这两个误差的大小相同，则和为0——算法认为没有误差——需要让所有的误差都具有相同的符号——平方和（sum-of square）误差函数：<br> <img src="https://images2.imgbox.com/1f/d3/jmTJui7K_o.png" alt="在这里插入图片描述"><br> 上式的1/2是为了便于求导——对一个函数求导，得到函数的梯度，进而得到函数下降或减小最快的方向——对误差函数求导，得到误差的梯度——学习的目的是最小化误差，沿着误差函数下降的方向（负梯度方向）将会得到最小值——梯度下降法。</p> 
<p>求导变量的选择——输入、决定是否要激活节点的激活函数、以及权重：在算法运行过程中前两个不在控制范围内，因此权重为求导的变量；<br> 激活函数：采用阈值函数在阶跃点非连续且不可导——sigmoid函数：导数具有较好的形式。<br> <img src="https://images2.imgbox.com/ca/d6/rBtDjU7j_o.png" alt="在这里插入图片描述"><br> 常用形式（0~1）：<br> <img src="https://images2.imgbox.com/64/5f/Ec4V4BuS_o.png" alt="在这里插入图片描述"><br> 双曲正切函数（-1~+1）：<br> <img src="https://images2.imgbox.com/d9/21/KNyryFxw_o.png" alt="在这里插入图片描述"><br> 双曲正切函数导函数：<br> <img src="https://images2.imgbox.com/96/5f/1DG1PtH5_o.png" alt="在这里插入图片描述"><br> 饱和（saturate）：达到常量，可将双曲正切函数的饱和点，通过0.5（x+1）转换到（0~1）</p> 
<p>根据上述内容，我们可以得到一个新形式的误差函数和一个新的激活函数来决定神经元是否激活——求导，以便改变权重时能使误差下降——改善网络的误差函数；<br> 处理连接输出层的节点，更新完后，将沿着网络后向移动直到回到输入。</p> 
<p>对于上述环节，存在两个问题：<br> ① 对于输出神经元，不知道输入；<br> ② 对于隐藏神经元，不知道目标；对于额外隐藏层，既不知道输入也不知道目标。</p> 
<p>求导的链式法则（chain rule of differentiation）：如果想要知道改变权重时误差如何改变，可以考虑改变权重的输入时误差如何改变，并且用它乘以改变权重时输入值的改变——可以吧输出节点的激活写成隐藏接地那的激活和输出权重的形式——传递误差计算，并且沿着网络后向移动到隐藏层，从而决定对于这些神经元来说其目标输出是什么。<br> 根据权重计算误差的梯度，以便于改变权重，使其下降，从而使得误差变小。通过误差函数对权重求导来实现，但不能直接这样做，所以用链式法则对已知的数据信息求导。从而产生了两个不同的更新函数，一个对应一组去那种，并且仅仅应用它们沿着网络后向传递，从输出开始在输入结束。</p> 
<h4><a id="421___48"></a>4.2.1 多层感知器算法</h4> 
<p>假设有L个输入节点及偏置，M个隐藏节点及偏置，N个输出节点——输入和隐藏层之间有(L+1)M个权重，隐藏层和输出之间有(M+1)N个权重——所描述的算法可以具有任意数量的隐藏层，存在若干M值以及隐藏层之间的额外权重集——i, j, k 索引每层的节点，ι, ζ, κ用于固定索引。<br> <img src="https://images2.imgbox.com/06/a1/mFFmgyDC_o.png" alt="在这里插入图片描述"><br> 使用了误差的反向传播的完整MLP训练算法总结：</p> 
<ol><li>输入向量被输入到输入节点；</li><li>输入沿着网络前向传递：<br> · 利用输入和第一层权重v决定隐藏节点是否激活。激活函数g(·)为上文的sigmoid函数；<br> · 利用这些神经元的输出和第二层权重w决定输出神经元是否激活；</li><li>误差是网络的输出和目标之间的平方和；</li><li>这个误差沿网络后向传递，首先更新第二层权重，然后后向传播更新第一层权重。</li></ol> 
<p>多层感知器算法：<br> · 初始化：</p> 
<ul><li>用很小的随机数（正数或负数）初始化所有权重<br> · 训练：</li><li>重复：<br> 对每一个输入向量：<br> 前向阶段：<br> ① 计算隐藏层中每个神经元j的激活：<br> <img src="https://images2.imgbox.com/51/3e/VuuAsqUI_o.png" alt="在这里插入图片描述"><br> ② 沿着网络直到到达输出层激活的神经元：<br> <img src="https://images2.imgbox.com/c4/8b/2f0sp8yu_o.png" alt="在这里插入图片描述"><br> 后向阶段：<br> ① 计算输出的误差：<br> <img src="https://images2.imgbox.com/4d/b0/iXOf9wh0_o.png" alt="在这里插入图片描述"><br> ② 计算隐藏层的误差：<br> <img src="https://images2.imgbox.com/10/76/8kj1uNAX_o.png" alt="在这里插入图片描述"><br> ③ 更新输出层权重：<br> <img src="https://images2.imgbox.com/1e/a3/ao7JamEo_o.png" alt="在这里插入图片描述"><br> ④ 更新隐藏层权重：<br> <img src="https://images2.imgbox.com/61/06/b8yGtIxf_o.png" alt="在这里插入图片描述"><br> 使输入向量的顺序随机化，以便于每次迭代都不会用相同的顺序训练网络</li><li>直到学习停止</li></ul> 
<p>· 回忆</p> 
<ul><li>使用上面训练部分的向前阶段</li></ul> 
<p>算法的核心权重更新计算可以实现为：</p> 
<pre><code class="prism language-python">deltao <span class="token operator">=</span> <span class="token punctuation">(</span>targets<span class="token operator">-</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">)</span><span class="token operator">*</span>self<span class="token punctuation">.</span>outputs<span class="token operator">*</span><span class="token punctuation">(</span><span class="token number">1.0</span><span class="token operator">-</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">)</span> 
deltah <span class="token operator">=</span> self<span class="token punctuation">.</span>hidden<span class="token operator">*</span><span class="token punctuation">(</span><span class="token number">1.0</span><span class="token operator">-</span>self<span class="token punctuation">.</span>hidden<span class="token punctuation">)</span><span class="token operator">*</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>deltao<span class="token punctuation">,</span>np<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>self<span class="token punctuation">.</span>' 
weights2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> 
updatew1 <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weights1<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> 
updatew2 <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weights2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> 
updatew1 <span class="token operator">=</span> eta<span class="token operator">*</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>np<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token punctuation">,</span>deltah<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> 
updatew2 <span class="token operator">=</span> eta<span class="token operator">*</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>np<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden<span class="token punctuation">)</span><span class="token punctuation">,</span>deltao<span class="token punctuation">)</span><span class="token punctuation">)</span> 
self<span class="token punctuation">.</span>weights1 <span class="token operator">+=</span> updatew1 
self<span class="token punctuation">.</span>weights2 <span class="token operator">+=</span> updatew2
</code></pre> 
<p>MLP学习XOR函数：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np 
<span class="token keyword">import</span> mlp 
anddata <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> 
xordata <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> 
p <span class="token operator">=</span> mlp<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span>anddata<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>anddata<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> 
p<span class="token punctuation">.</span>mlptrain<span class="token punctuation">(</span>anddata<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>anddata<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">0.25</span><span class="token punctuation">,</span><span class="token number">1001</span><span class="token punctuation">)</span> 
p<span class="token punctuation">.</span>confmat<span class="token punctuation">(</span>anddata<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>anddata<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span> 
q <span class="token operator">=</span> mlp<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span>xordata<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>xordata<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> 
q<span class="token punctuation">.</span>mlptrain<span class="token punctuation">(</span>xordata<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>xordata<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">0.25</span><span class="token punctuation">,</span><span class="token number">5001</span><span class="token punctuation">)</span> 
q<span class="token punctuation">.</span>confmat<span class="token punctuation">(</span>xordata<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>xordata<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<p>产生的输出为：<br> MLP学习XOR函数：</p> 
<pre><code class="prism language-python">Iteration<span class="token punctuation">:</span> <span class="token number">0</span> Error<span class="token punctuation">:</span> <span class="token number">0.367917569871</span>
Iteration<span class="token punctuation">:</span> <span class="token number">1000</span> Error<span class="token punctuation">:</span> <span class="token number">0.0204860723612</span> 
Confusion matrix <span class="token keyword">is</span><span class="token punctuation">:</span> 
<span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">3.</span> <span class="token number">0.</span><span class="token punctuation">]</span> 
<span class="token punctuation">[</span> <span class="token number">0.</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">]</span> 
Percentage Correct<span class="token punctuation">:</span> <span class="token number">100.0</span> 
Iteration<span class="token punctuation">:</span> <span class="token number">0</span> Error<span class="token punctuation">:</span> <span class="token number">0.515798627074</span> 
Iteration<span class="token punctuation">:</span> <span class="token number">1000</span> Error<span class="token punctuation">:</span> <span class="token number">0.499568173798</span>
Iteration<span class="token punctuation">:</span> <span class="token number">2000</span> Error<span class="token punctuation">:</span> <span class="token number">0.498271692284</span> 
Iteration<span class="token punctuation">:</span> <span class="token number">3000</span> Error<span class="token punctuation">:</span> <span class="token number">0.480839047738</span> 
Iteration<span class="token punctuation">:</span> <span class="token number">4000</span> Error<span class="token punctuation">:</span> <span class="token number">0.382706753191</span> 
Iteration<span class="token punctuation">:</span> <span class="token number">5000</span> Error<span class="token punctuation">:</span> <span class="token number">0.0537169253359</span> 
Confusion matrix <span class="token keyword">is</span><span class="token punctuation">:</span> 
<span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">2.</span> <span class="token number">0.</span><span class="token punctuation">]</span> 
<span class="token punctuation">[</span> <span class="token number">0.</span> <span class="token number">2.</span><span class="token punctuation">]</span><span class="token punctuation">]</span> 
Percentage Correct<span class="token punctuation">:</span> <span class="token number">100.0</span>
</code></pre> 
<p>比起感知器，即使对于AND也需要迭代很多。所以更加复杂的网络的好处在于它的计算代价——使用更多的计算时间来拟合这些权重从而解决问题。</p> 
<h4><a id="422___132"></a>4.2.2 初始化权重</h4> 
<p>如果初始化权重值靠近1或-1（意味着值很大），那么对于sigmoid函数的输入也很可能会靠近±1，所以神经元的输出是0或1（sigmoid函数是饱和的，达到了最大值或最小值）。如果权重非常小（靠近0），那么输入也靠近0，所以神经元的输出仅仅是线性的，这样我们就得到一个线性模型。这两件事情对于最终的网络都是有用的，但是如果初始值在它们之间，就可以由网络自己来决定。</p> 
<p>每个神经元从n个不同的地方得到输入（如果神经元在隐藏层中，则为输入节点；如果在输出层中，则为隐藏神经元）。如果我们把这些输入值看成都具有均匀的方差，那么对于神经元，典型的输入将是<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         w 
        
        
        
          n 
         
        
       
      
        w\sqrt{n} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.04em; vertical-align: -0.23972em;"></span><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.80028em;"><span class="svg-align" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord" style="padding-left: 0.833em;"><span class="mord mathdefault">n</span></span></span><span class="" style="top: -2.76028em;"><span class="pstrut" style="height: 3em;"></span><span class="hide-tail" style="min-width: 0.853em; height: 1.08em;"> 
           <svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"> 
            <path d="M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,
-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,
-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,
35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,
-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467
s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422
s-65,47,-65,47z M834 80H400000v40H845z"></path> 
           </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.23972em;"><span class=""></span></span></span></span></span></span></span></span></span>，这里w是权重的初始值。所以一个常用的技巧是设置权重在范围<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         − 
        
       
         1 
        
       
         / 
        
        
        
          n 
         
        
       
         &lt; 
        
       
         w 
        
       
         &lt; 
        
       
         1 
        
       
         / 
        
        
        
          n 
         
        
       
      
        -1/\sqrt{n}&lt;w&lt;1/\sqrt{n} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.05028em; vertical-align: -0.25em;"></span><span class="mord">−</span><span class="mord">1</span><span class="mord">/</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.80028em;"><span class="svg-align" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord" style="padding-left: 0.833em;"><span class="mord mathdefault">n</span></span></span><span class="" style="top: -2.76028em;"><span class="pstrut" style="height: 3em;"></span><span class="hide-tail" style="min-width: 0.853em; height: 1.08em;"> 
           <svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"> 
            <path d="M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,
-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,
-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,
35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,
-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467
s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422
s-65,47,-65,47z M834 80H400000v40H845z"></path> 
           </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.23972em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.5782em; vertical-align: -0.0391em;"></span><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.05028em; vertical-align: -0.25em;"></span><span class="mord">1</span><span class="mord">/</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.80028em;"><span class="svg-align" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord" style="padding-left: 0.833em;"><span class="mord mathdefault">n</span></span></span><span class="" style="top: -2.76028em;"><span class="pstrut" style="height: 3em;"></span><span class="hide-tail" style="min-width: 0.853em; height: 1.08em;"> 
           <svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"> 
            <path d="M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,
-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,
-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,
35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,
-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467
s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422
s-65,47,-65,47z M834 80H400000v40H845z"></path> 
           </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.23972em;"><span class=""></span></span></span></span></span></span></span></span></span>之内（n为输入层节点数）。如果权重很大，则神经元的激活可能已经处于或接近0或1，这意味着梯度更小，因此学习速很慢，意味着小的β值（β&lt;3.0）会更有效。</p> 
<p>使用随机值进行初始化，以便每次运行时从不同的地方开始学习，并且保持它们都是相同的大小，因为我们想要所有的权重在同一时刻达到最后的值。这就是所谓的平均学习（uniform learning）</p> 
<h4><a id="423___138"></a>4.2.3 不同的输出激活函数</h4> 
<p>处理回归问题，需要连续输出——使用线性节点来代替输出神经元，也就是把输入加起来，并且把他们当做激活的——只修改输出节点，这些节点不再是神经元的模型，因为它们不再有激活或者不激活的特点；<br> Soft-max函数，使用1-of-N 输出编码——通过计算输入的指数来重新安排输出，并且除以所有神经元输入的和，以便激活加起来是1，并且所有的激活都分布于0到1之间：<br> <img src="https://images2.imgbox.com/9b/4b/PN7DLWO1_o.png" alt="在这里插入图片描述"><br> 如果改变激活函数，学习将会不同，对于线性激活函数：<br> <img src="https://images2.imgbox.com/01/f3/k5azhbKg_o.png" alt="在这里插入图片描述"><br> 其输出误差为：<br> <img src="https://images2.imgbox.com/12/8f/fYKiUR8M_o.png" alt="在这里插入图片描述"><br> 对于soft-max激活，其输出误差为：<br> <img src="https://images2.imgbox.com/94/34/mSekmVM6_o.png" alt="在这里插入图片描述"><br> 如果κ = K，则δκK = 1，否则为0。但如果修改误差函数为交叉熵（cross-entropy）形式：<br> <img src="https://images2.imgbox.com/49/ca/Nub9bIGU_o.png" alt="在这里插入图片描述"><br> 输出误差与线性激活函数一致。</p> 
<p>计算这些更新等式，需要计算被优化的误差函数，然后对其求导，代码如下：</p> 
<pre><code class="prism language-python"><span class="token comment"># Difffferent types of output neurons </span>
<span class="token keyword">if</span> self<span class="token punctuation">.</span>outtype <span class="token operator">==</span> ’linear’<span class="token punctuation">:</span> 
<span class="token keyword">return</span> outputs 
<span class="token keyword">elif</span> self<span class="token punctuation">.</span>outtype <span class="token operator">==</span> ’logistic’<span class="token punctuation">:</span> 
<span class="token keyword">return</span> <span class="token number">1.0</span><span class="token operator">/</span><span class="token punctuation">(</span><span class="token number">1.0</span><span class="token operator">+</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>self<span class="token punctuation">.</span>beta<span class="token operator">*</span>outputs<span class="token punctuation">)</span><span class="token punctuation">)</span> 
<span class="token keyword">elif</span> self<span class="token punctuation">.</span>outtype <span class="token operator">==</span> ’softmax’<span class="token punctuation">:</span> 
normalisers <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>outputs<span class="token punctuation">)</span><span class="token punctuation">,</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">*</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>np<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>outputs<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> 
<span class="token keyword">return</span> np<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>np<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>outputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">/</span>normalisers<span class="token punctuation">)</span> 
<span class="token keyword">else</span><span class="token punctuation">:</span> 
<span class="token keyword">print</span> <span class="token string">"error"</span>
</code></pre> 
<h4><a id="424___165"></a>4.2.4 顺序和批量训练</h4> 
<p>MLP是用于批量训练的算法。所有训练数据都提供给神经网络，计算平均的误差平方和，并且据此来更新权重。对于每次迭代（epoch，即里面所有训练数据）就只有一组权重更新——每一次得带只更新一次权重——权重沿着大多数输入想让它们移动的方向移动，而不是被每一个单独的输入推着走。批量的方法使得误差梯度的估计更精确，并且将会更快收敛到局部最小；<br> 之前的算法是顺序（sequential）版本，即计算误差并且在每一个输入后更新权重，这在学习中并不能保重是有效的，但是使用循环会使得程序简单，因此更常用——收敛性不好，但有时会避开局部最小从而得到更好的解。</p> 
<p>在顺序版本中吗，权重更新的顺序会很重要，因此需要在每次迭代时随机化输入向量顺序，从而提高算法的学习速度。采用np.random.shuffle()：</p> 
<pre><code class="prism language-python">np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>change<span class="token punctuation">)</span>
inputs <span class="token operator">=</span> inputs<span class="token punctuation">[</span>change<span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
targets <span class="token operator">=</span> targets<span class="token punctuation">[</span>change<span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
</code></pre> 
<h4><a id="425___175"></a>4.2.5 局部最小</h4> 
<p>梯度下降法意味着实现了优化（optimisation）：调整权重的值以便最小化误差函数——通过估计误差的梯度并且沿着梯度下降，以便到达局部最小（local minimum）。</p> 
<p>由于不知道误差函数的图像，因此只能在当前所在的位置计算局部特征，也可以尝试几个不同的初始点，从而更有可能找到全局最小——常见方法。</p> 
<h4><a id="426___179"></a>4.2.6 利用冲量</h4> 
<p>通过改变当前点的前一个权重来加入一些贡献，进而用神经网络实现这个想法——冲量使得采用更小的学习速率成为可能，意味学习会更加稳定：<br> <img src="https://images2.imgbox.com/48/16/FFWvnBW1_o.png" alt="在这里插入图片描述"><br> 其中：t代表当前的更新，t-1 是前一个更新。最后一项<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         Δ 
        
        
        
          w 
         
         
         
           ζ 
          
         
           κ 
          
         
         
         
           t 
          
         
           − 
          
         
           1 
          
         
        
       
      
        \Delta w_{\zeta \kappa }^{t-1} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.29165em; vertical-align: -0.437416em;"></span><span class="mord">Δ</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.854239em;"><span class="" style="top: -2.39869em; margin-left: -0.02691em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.07378em;">ζ</span><span class="mord mathdefault mtight">κ</span></span></span></span><span class="" style="top: -3.10313em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.437416em;"><span class=""></span></span></span></span></span></span></span></span></span></span>是权重的前一个更新（所以<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         Δ 
        
        
        
          w 
         
         
         
           ζ 
          
         
           κ 
          
         
        
          t 
         
        
       
         = 
        
       
         η 
        
        
        
          δ 
         
        
          0 
         
        
       
         ( 
        
       
         κ 
        
       
         ) 
        
        
        
          a 
         
        
          ζ 
         
         
         
           h 
          
         
           i 
          
         
           d 
          
         
           d 
          
         
           e 
          
         
           n 
          
         
        
       
         + 
        
       
         a 
        
       
         Δ 
        
        
        
          w 
         
         
         
           ζ 
          
         
           κ 
          
         
         
         
           t 
          
         
           − 
          
         
           1 
          
         
        
       
      
        \Delta w_{\zeta \kappa }^{t}=\eta {<!-- -->{\delta }_{0}}(\kappa )a_{\zeta }^{hidden}+a\Delta w_{\zeta \kappa }^{t-1} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.21277em; vertical-align: -0.419216em;"></span><span class="mord">Δ</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.793556em;"><span class="" style="top: -2.41689em; margin-left: -0.02691em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.07378em;">ζ</span><span class="mord mathdefault mtight">κ</span></span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.419216em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.26832em; vertical-align: -0.419216em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">η</span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03785em;">δ</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">κ</span><span class="mclose">)</span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -2.41689em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.07378em;">ζ</span></span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.419216em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1.29165em; vertical-align: -0.437416em;"></span><span class="mord mathdefault">a</span><span class="mord">Δ</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.854239em;"><span class="" style="top: -2.39869em; margin-left: -0.02691em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.07378em;">ζ</span><span class="mord mathdefault mtight">κ</span></span></span></span><span class="" style="top: -3.10313em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.437416em;"><span class=""></span></span></span></span></span></span></span></span></span></span>），0&lt;a&lt;1是冲量常量，通常取0.9，其代码为：</p> 
<pre><code class="prism language-python">updatew1 <span class="token operator">=</span> eta<span class="token operator">*</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>np<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token punctuation">,</span>deltah<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> ' 
momentum<span class="token operator">*</span>updatew1 
updatew2 <span class="token operator">=</span> eta<span class="token operator">*</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>np<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>hidden<span class="token punctuation">)</span><span class="token punctuation">,</span>deltao<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> momentum<span class="token operator">*</span>updatew2
</code></pre> 
<p>另一个可以加入的是权重衰减（weight decay），即随着迭代次数的增加而减少权重的大小，使得网络更接近于线性，并且只有与非线性学习有关的权重才应该变大——有时会产生糟糕的结果。</p> 
<h4><a id="427___189"></a>4.2.7 小批量和随机梯度下降</h4> 
<p>批量算法可以更好地估计最陡下降方向。</p> 
<p>小批量（minibatch）方法的想法是通过以下方法找到两者之间一些好的中间点：将训练集分成随机批次，根据训练集的一个子集估计梯度，执行去那种更新，然后使用下一个子集估计一个新梯度，并将新梯度用于权重更新，持续进行下去直到使用了所有训练集。然后将训练集随机混合到新批次中，并进行下一次迭代。如果批次很小，那么梯度估计中的误差通常是合理的，因此优化有机会避免局部最小值，尽管要以错误方向前进为代价。</p> 
<p>小批量思想的另一个更极端的版本是仅使用一个数据来估计算法每次迭代时的梯度，并从训练集中随机均匀地挑选该数据。因此，从训练集中选择单个输入向量，计算输出并以此计算该向量的误差，然后据此估计梯度，并更新权重。然后再选择新的随机输入向量（可以与前一个相同）并重复该过程——随机梯度下降。</p> 
<h4><a id="428___195"></a>4.2.8 其他改善方法</h4> 
<p>改善反向传播算法的收敛和行为：① 在算法进行的过程中减小学习速率：当权重是随机值时，网络只有在起初的时候才应该有大的权重改变；如果在以后还使用大的权重改变，那么就会报错；②包括误差函数对于权重的二阶导数的信息，使得学习有更好表现的方法，用来改善网络。</p> 
<h3><a id="43__MLP_197"></a>4.3 实践中的MLP</h3> 
<p>使用MLP来找到四类问题的解决方法：回归、分类、时间序列预测、数据压缩；</p> 
<h4><a id="431___199"></a>4.3.1 训练数据的量</h4> 
<p>对于单层隐藏层的MLP，有(L+1)M+(M+1)N个权重，这里L、M、N是相应的输入层、隐藏层和输出层节点数。额外的 +1 来自于偏置节点。<br> 如果有更多的训练数据，学习效果就会更好，虽然算法学习所花的时间会变长，但是没有办法计算出所需数据的最小值是多少，因为这依赖于问题本身。对于大多数MLP，所使用的训练数据的数目至少是权重数目的10倍。</p> 
<h4><a id="432___202"></a>4.3.2 隐藏层的数目</h4> 
<p>隐藏层节点数量和隐藏层数量的选择——一般MLP学习最多需要两个隐藏层——用数学方式显示一个包含大量隐藏节点的隐藏层就足够了——通用近似定理；<br> 然而没有理论来指导隐藏节点数量的选择：只能用不同隐藏节点数目来训练网络，并且选择给出最好结果的那一个。</p> 
<p>用反向传播算法，在任何给定时间跟踪更新权重变得越来越难。但是不需要超过两层，即一个隐藏层和一个输出层——用局部sigmoid函数的线性组合来估计任何一个光滑的函数映射：<br> 通过sigmoid函数的结合来产生脊状函数，通过脊状函数的结合产生有特定最大值的函数——将其结合起来并用其他神经层来转换，进而得到局部的映射——跃升函数——其线性组合将任何函数映射近似为任意精度。<br> <img src="https://images2.imgbox.com/db/70/SuAf41iA_o.png" alt="MLP的学习可以被分成：a) 单个sigmoid神经元的输出；b) 单个输出的叠加，报考加入反向的图像进而得到“山”的形状；c) 以90°方向加入另一座“山”；d) 把跃升加在输出层，使图像更加尖锐。这样MLP就学习了一个局部的独立输入。"><br> MLP的学习可以被分成：a) 单个sigmoid神经元的输出；b) 单个输出的叠加，报考加入反向的图像进而得到“山”的形状；c) 以90°方向加入另一座“山”；d) 把跃升加在输出层，使图像更加尖锐。这样MLP就学习了一个局部的独立输入。</p> 
<p>上图显示出两个隐藏层就以足够（sufficient），但是并不是必须（necessary）的：虽然可能需要任意数量的隐藏节点，但是一个隐藏层就足够了——通用近似定理（Universal Approximation Theorem）<br> <img src="https://images2.imgbox.com/9f/7c/HQnpZn4G_o.png" alt="在这里插入图片描述"><br> 两个隐藏层对于不同输入足够用于计算这些跃升函数，并且如果我们想要学习（估计）的函数是连续的，网络就可以计算。因此可以估计任意一个决策边界，而不仅仅像感知器计算的线性情形。</p> 
<h4><a id="433___214"></a>4.3.3 什么时候停止学习</h4> 
<p>大多数明显的选择都是不充分的：设置一些预定义的迭代数目T，运行算法直到面临过拟合的风险，或者还没有学习充分，仅在到达预先定义的最小误差时停止——验证集可以用来近视网络在当前学习阶段的泛化能力。如果把训练期间的平方和误差画出来，前几次训练迭代会减小的很快，然后随着学习算法通过微小的改变找到准确的局部最小值，减小的速度会慢下来——预先定义的一段时间训练网络，然后用验证集来估计网络的泛化能力，之后计算训练几次迭代，并且重复整个过程。某个阶段，验证集的误差将会再次增长，此时学习数据本身的噪点，这时需要停止训练——早期停止（early stopping）<br> <img src="https://images2.imgbox.com/a4/03/pgXQz93Z_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="44__MLP_217"></a>4.4 MLP应用示例</h3> 
<h4><a id="441___218"></a>4.4.1 回归问题</h4>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/17651af2dc488141c57718cfeea2b678/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">leetCode_125. 验证回文串</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ecea24feb3b7c807769fc13368dcd6cf/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">C语言——字符串指针篇</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>