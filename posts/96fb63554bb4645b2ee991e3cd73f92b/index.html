<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>近期学习论文总结 2 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="近期学习论文总结 2" />
<meta property="og:description" content="公众号：EDPJ
目录
0. 摘要
1. Artificial Fingerprinting for Generative Models: Rooting Deepfake Attribution in Training Data
1.1 核心思想
1.2 步骤
2. HyperDomainNet: Universal Domain Adaptation for Generative Adversarial Networks
2.1 核心思想
2.2 Training Loss
2.3 泛化
3. Data-Efficient Instance Generation from Instance Discrimination 3.1 核心思想
3.2 步骤
3.3 消融实验
4. MIND THE GAP: Domain Gap Control for Single Shot Domain Adaptation for Generative Adversarial Networks 4.1 核心思想
4.2 步骤" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/96fb63554bb4645b2ee991e3cd73f92b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-04T16:26:23+08:00" />
<meta property="article:modified_time" content="2023-06-04T16:26:23+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">近期学习论文总结 2</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="%E5%85%AC%E4%BC%97%E5%8F%B7%EF%BC%9AEDPJ" style="text-align:center;"><strong>公众号：EDPJ</strong></p> 
<p> </p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="0.%20%E6%91%98%E8%A6%81-toc" style="margin-left:0px;"><a href="#0.%20%E6%91%98%E8%A6%81" rel="nofollow">0. 摘要</a></p> 
<p id="1.%C2%A0Artificial%20Fingerprinting%20for%20Generative%20Models%3A%C2%A0Rooting%20Deepfake%20Attribution%20in%20Training%20Data-toc" style="margin-left:0px;"><a href="#1.%C2%A0Artificial%20Fingerprinting%20for%20Generative%20Models%3A%C2%A0Rooting%20Deepfake%20Attribution%20in%20Training%20Data" rel="nofollow">1. Artificial Fingerprinting for Generative Models: Rooting Deepfake Attribution in Training Data</a></p> 
<p id="1.%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-toc" style="margin-left:40px;"><a href="#1.%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3" rel="nofollow">1.1 核心思想</a></p> 
<p id="2.%20%E6%AD%A5%E9%AA%A4-toc" style="margin-left:40px;"><a href="#2.%20%E6%AD%A5%E9%AA%A4" rel="nofollow">1.2 步骤</a></p> 
<p id="2.%C2%A0HyperDomainNet%3A%20Universal%20Domain%20Adaptation%20for%20Generative%20Adversarial%20Networks-toc" style="margin-left:0px;"><a href="#2.%C2%A0HyperDomainNet%3A%20Universal%20Domain%20Adaptation%20for%20Generative%20Adversarial%20Networks" rel="nofollow">2. HyperDomainNet: Universal Domain Adaptation for Generative Adversarial Networks</a></p> 
<p id="2.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-toc" style="margin-left:40px;"><a href="#2.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3" rel="nofollow">2.1 核心思想</a></p> 
<p id="2.%20Training%20Loss-toc" style="margin-left:40px;"><a href="#2.%20Training%20Loss" rel="nofollow">2.2 Training Loss</a></p> 
<p id="3.%20%E6%B3%9B%E5%8C%96-toc" style="margin-left:40px;"><a href="#3.%20%E6%B3%9B%E5%8C%96" rel="nofollow">2.3 泛化</a></p> 
<p id="3.%C2%A0Data-Efficient%20Instance%20Generation%20from%20Instance%20Discrimination%C2%A0-toc" style="margin-left:0px;"><a href="#3.%C2%A0Data-Efficient%20Instance%20Generation%20from%20Instance%20Discrimination%C2%A0" rel="nofollow">3. Data-Efficient Instance Generation from Instance Discrimination </a></p> 
<p id="3.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-toc" style="margin-left:40px;"><a href="#3.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3" rel="nofollow">3.1 核心思想</a></p> 
<p id="3.2%20%E6%AD%A5%E9%AA%A4-toc" style="margin-left:40px;"><a href="#3.2%20%E6%AD%A5%E9%AA%A4" rel="nofollow">3.2 步骤</a></p> 
<p id="3.%20%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C-toc" style="margin-left:40px;"><a href="#3.%20%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C" rel="nofollow">3.3 消融实验</a></p> 
<p id="4.%C2%A0MIND%20THE%20GAP%3A%20Domain%20Gap%20Control%20for%20Single%20Shot%20Domain%20Adaptation%20for%20Generative%20Adversarial%20Networks%C2%A0-toc" style="margin-left:0px;"><a href="#4.%C2%A0MIND%20THE%20GAP%3A%20Domain%20Gap%20Control%20for%20Single%20Shot%20Domain%20Adaptation%20for%20Generative%20Adversarial%20Networks%C2%A0" rel="nofollow">4. MIND THE GAP: Domain Gap Control for Single Shot Domain Adaptation for Generative Adversarial Networks </a></p> 
<p id="S.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-toc" style="margin-left:40px;"><a href="#S.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3" rel="nofollow">4.1 核心思想</a></p> 
<p id="S.2%20%E6%AD%A5%E9%AA%A4-toc" style="margin-left:40px;"><a href="#S.2%20%E6%AD%A5%E9%AA%A4" rel="nofollow">4.2 步骤</a></p> 
<p id="S.3%20%E9%A3%8E%E6%A0%BC%E6%B7%B7%E5%90%88-toc" style="margin-left:40px;"><a href="#S.3%20%E9%A3%8E%E6%A0%BC%E6%B7%B7%E5%90%88" rel="nofollow">4.3 风格混合</a></p> 
<p id="5.%C2%A0Improved%20StyleGAN%20Embedding%3A%20Where%20are%20the%20good%20latents%C2%A0-toc" style="margin-left:0px;"><a href="#5.%C2%A0Improved%20StyleGAN%20Embedding%3A%20Where%20are%20the%20good%20latents%C2%A0" rel="nofollow">5. Improved StyleGAN Embedding: Where are the good latents </a></p> 
<p id="S.1%20%E9%9A%90%E7%A9%BA%E9%97%B4%EF%BC%88%E5%8F%82%E8%80%83%C2%A0GAN%20Inversion%3A%20A%20Survey%EF%BC%89-toc" style="margin-left:40px;"><a href="#S.1%20%E9%9A%90%E7%A9%BA%E9%97%B4%EF%BC%88%E5%8F%82%E8%80%83%C2%A0GAN%20Inversion%3A%20A%20Survey%EF%BC%89" rel="nofollow">5.1 隐空间（参考 GAN Inversion: A Survey）</a></p> 
<p id="S.2%C2%A0%E6%94%B9%E8%BF%9B%E7%9A%84%20I2S%EF%BC%88Improved%20Image2StyleGAN%EF%BC%8CII2S%EF%BC%89-toc" style="margin-left:40px;"><a href="#S.2%C2%A0%E6%94%B9%E8%BF%9B%E7%9A%84%20I2S%EF%BC%88Improved%20Image2StyleGAN%EF%BC%8CII2S%EF%BC%89" rel="nofollow">5.2 改进的 I2S（Improved Image2StyleGAN，II2S）</a></p> 
<p id="S.3%20%E7%BC%96%E8%BE%91%E8%B4%A8%E9%87%8F%20vs%20%E9%87%8D%E5%BB%BA%E8%B4%A8%E9%87%8F-toc" style="margin-left:40px;"><a href="#S.3%20%E7%BC%96%E8%BE%91%E8%B4%A8%E9%87%8F%20vs%20%E9%87%8D%E5%BB%BA%E8%B4%A8%E9%87%8F" rel="nofollow">5.3 编辑质量 vs 重建质量</a></p> 
<p id="6.%C2%A0DigGAN%3A%20Discriminator%20Gradient%20Gap%20Regularization%20for%20GAN%20Training%20with%20Limited%20Data%C2%A0-toc" style="margin-left:0px;"><a href="#6.%C2%A0DigGAN%3A%20Discriminator%20Gradient%20Gap%20Regularization%20for%20GAN%20Training%20with%20Limited%20Data%C2%A0" rel="nofollow">6. DigGAN: Discriminator Gradient Gap Regularization for GAN Training with Limited Data </a></p> 
<p id="6.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-toc" style="margin-left:40px;"><a href="#6.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3" rel="nofollow">6.1 核心思想</a></p> 
<p id="S.2%20%E5%88%86%E6%9E%90-toc" style="margin-left:40px;"><a href="#S.2%20%E5%88%86%E6%9E%90" rel="nofollow">6.2 分析</a></p> 
<p id="7.%C2%A0Regularizing%20generative%20adversarial%20networks%20under%20limited%20data%C2%A0-toc" style="margin-left:0px;"><a href="#7.%C2%A0Regularizing%20generative%20adversarial%20networks%20under%20limited%20data%C2%A0" rel="nofollow">7. Regularizing generative adversarial networks under limited data </a></p> 
<p id="7.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-toc" style="margin-left:40px;"><a href="#7.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3" rel="nofollow">7.1 核心思想</a></p> 
<p id="7.2%20%E5%88%86%E6%9E%90-toc" style="margin-left:40px;"><a href="#7.2%20%E5%88%86%E6%9E%90" rel="nofollow">7.2 分析</a></p> 
<p id="8.%C2%A0Large%20scale%20gan%20training%20for%20high%20fidelity%20natural%20image%20synthesis%C2%A0-toc" style="margin-left:0px;"><a href="#8.%C2%A0Large%20scale%20gan%20training%20for%20high%20fidelity%20natural%20image%20synthesis%C2%A0" rel="nofollow">8. Large scale gan training for high fidelity natural image synthesis </a></p> 
<p id="8.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-toc" style="margin-left:40px;"><a href="#8.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3" rel="nofollow">8.1 核心思想</a></p> 
<p id="S.2%20%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%EF%BC%88%E8%B0%B1%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%89-toc" style="margin-left:40px;"><a href="#S.2%20%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%EF%BC%88%E8%B0%B1%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%89" rel="nofollow">8.2 解决办法（谱归一化）</a></p> 
<p id="S.3%20%E5%85%B6%E4%BB%96%E8%B4%A1%E7%8C%AE-toc" style="margin-left:40px;"><a href="#S.3%20%E5%85%B6%E4%BB%96%E8%B4%A1%E7%8C%AE" rel="nofollow">8.3 其他贡献</a></p> 
<p id="9.%C2%A0A%20Style-Based%20Generator%20Architecture%20for%20Generative%20Adversarial%20Networks-toc" style="margin-left:0px;"><a href="#9.%C2%A0A%20Style-Based%20Generator%20Architecture%20for%20Generative%20Adversarial%20Networks" rel="nofollow">9. A Style-Based Generator Architecture for Generative Adversarial Networks</a></p> 
<p id="9.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-toc" style="margin-left:40px;"><a href="#9.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3" rel="nofollow">9.1 核心思想</a></p> 
<p id="9.2%20%E5%88%86%E6%9E%90-toc" style="margin-left:40px;"><a href="#9.2%20%E5%88%86%E6%9E%90" rel="nofollow">9.2 分析</a></p> 
<p id="9.3%20%E5%85%B6%E4%BB%96%E8%B4%A1%E7%8C%AE-toc" style="margin-left:40px;"><a href="#9.3%20%E5%85%B6%E4%BB%96%E8%B4%A1%E7%8C%AE" rel="nofollow">9.3 其他贡献</a></p> 
<p id="10.%C2%A0Analyzing%20and%20Improving%20the%20Image%20Quality%20of%20StyleGAN%C2%A0-toc" style="margin-left:0px;"><a href="#10.%C2%A0Analyzing%20and%20Improving%20the%20Image%20Quality%20of%20StyleGAN%C2%A0" rel="nofollow">10. Analyzing and Improving the Image Quality of StyleGAN </a></p> 
<p id="10.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-toc" style="margin-left:40px;"><a href="#10.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3" rel="nofollow">10.1 核心思想</a></p> 
<p id="S.2%20%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-toc" style="margin-left:40px;"><a href="#S.2%20%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84" rel="nofollow">10.2 网络结构</a></p> 
<p id="10.3%20%E5%85%B6%E4%BB%96%E8%B4%A1%E7%8C%AE-toc" style="margin-left:40px;"><a href="#10.3%20%E5%85%B6%E4%BB%96%E8%B4%A1%E7%8C%AE" rel="nofollow">10.3 其他贡献</a></p> 
<p id="11.%C2%A0Alias-Free%20Generative%20Adversarial%20Networks-toc" style="margin-left:0px;"><a href="#11.%C2%A0Alias-Free%20Generative%20Adversarial%20Networks" rel="nofollow">11. Alias-Free Generative Adversarial Networks</a></p> 
<p id="11.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-toc" style="margin-left:40px;"><a href="#11.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3" rel="nofollow">11.1 核心思想</a></p> 
<p id="11.2%20%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-toc" style="margin-left:40px;"><a href="#11.2%20%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84" rel="nofollow">11.2 网络结构</a></p> 
<p id="12.%C2%A0Training%20generative%20adversarial%20networks%20with%20limited%20data%C2%A0-toc" style="margin-left:0px;"><a href="#12.%C2%A0Training%20generative%20adversarial%20networks%20with%20limited%20data%C2%A0" rel="nofollow">12. Training generative adversarial networks with limited data </a></p> 
<p id="12.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-toc" style="margin-left:40px;"><a href="#12.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3" rel="nofollow">12.1 核心思想</a></p> 
<p id="S.2%20%E6%96%B9%E6%B3%95-toc" style="margin-left:40px;"><a href="#S.2%20%E6%96%B9%E6%B3%95" rel="nofollow">12.2 方法</a></p> 
<p id="12.3%20%E5%85%B6%E4%BB%96%E8%B4%A1%E7%8C%AE-toc" style="margin-left:40px;"><a href="#12.3%20%E5%85%B6%E4%BB%96%E8%B4%A1%E7%8C%AE" rel="nofollow">12.3 其他贡献</a></p> 
<p id="S.4%20%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9%E5%88%86%E6%9E%90-toc" style="margin-left:40px;"><a href="#S.4%20%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9%E5%88%86%E6%9E%90" rel="nofollow">12.4 相关内容分析</a></p> 
<p id="13.%C2%A0Differentiable%20augmentation%20for%20data-efficient%20gan%20training%C2%A0-toc" style="margin-left:0px;"><a href="#13.%C2%A0Differentiable%20augmentation%20for%20data-efficient%20gan%20training%C2%A0" rel="nofollow">13. Differentiable augmentation for data-efficient gan training </a></p> 
<p id="13.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-toc" style="margin-left:40px;"><a href="#13.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3" rel="nofollow">13.1 核心思想</a></p> 
<p id="S.2%20%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%C2%A0-toc" style="margin-left:40px;"><a href="#S.2%20%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%C2%A0" rel="nofollow">13.2 网络结构 </a></p> 
<p id="S.3%20%E5%88%86%E6%9E%90-toc" style="margin-left:40px;"><a href="#S.3%20%E5%88%86%E6%9E%90" rel="nofollow">13.3 分析</a></p> 
<p id="14.%C2%A0Masked%20Generative%20Adversarial%20Networks%20are%20Data-Efficient%20Generation%20Learners%C2%A0-toc" style="margin-left:0px;"><a href="#14.%C2%A0Masked%20Generative%20Adversarial%20Networks%20are%20Data-Efficient%20Generation%20Learners%C2%A0" rel="nofollow">14. Masked Generative Adversarial Networks are Data-Efficient Generation Learners </a></p> 
<p id="14.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-toc" style="margin-left:40px;"><a href="#14.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3" rel="nofollow">14.1 核心思想</a></p> 
<p id="S.2%20%E6%B5%81%E7%A8%8B-toc" style="margin-left:40px;"><a href="#S.2%20%E6%B5%81%E7%A8%8B" rel="nofollow">14.2 流程</a></p> 
<p id="14.3%20%E5%88%86%E6%9E%90-toc" style="margin-left:40px;"><a href="#14.3%20%E5%88%86%E6%9E%90" rel="nofollow">14.3 分析</a></p> 
<p id="15.%C2%A0FreGAN_Exploiting%20Frequency%20Components%20for%20Training%20GANs%20under%20Limited%20Data%C2%A0-toc" style="margin-left:0px;"><a href="#15.%C2%A0FreGAN_Exploiting%20Frequency%20Components%20for%20Training%20GANs%20under%20Limited%20Data%C2%A0" rel="nofollow">15. FreGAN_Exploiting Frequency Components for Training GANs under Limited Data </a></p> 
<p id="15.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-toc" style="margin-left:40px;"><a href="#15.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3" rel="nofollow">15.1 核心思想</a></p> 
<p id="15.2%20%E6%96%B9%E6%B3%95-toc" style="margin-left:40px;"><a href="#15.2%20%E6%96%B9%E6%B3%95" rel="nofollow">15.2 方法</a></p> 
<p id="S.3%20%E4%BC%98%E5%8C%96-toc" style="margin-left:40px;"><a href="#S.3%20%E4%BC%98%E5%8C%96" rel="nofollow">15.3 优化</a></p> 
<p id="16.%C2%A0High-frequency%20Component%20Helps%20Explain%20the%20Generalization%20of%20Convolutional%20Neural%20Networks-toc" style="margin-left:0px;"><a href="#16.%C2%A0High-frequency%20Component%20Helps%20Explain%20the%20Generalization%20of%20Convolutional%20Neural%20Networks" rel="nofollow">16. High-frequency Component Helps Explain the Generalization of Convolutional Neural Networks</a></p> 
<p id="S.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-toc" style="margin-left:40px;"><a href="#S.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3" rel="nofollow">16.1 核心思想</a></p> 
<p id="S.2%20%E7%A0%94%E7%A9%B6%E6%96%B9%E6%B3%95-toc" style="margin-left:40px;"><a href="#S.2%20%E7%A0%94%E7%A9%B6%E6%96%B9%E6%B3%95" rel="nofollow">16.2 研究方法</a></p> 
<p id="S.3%20%E5%88%86%E6%9E%90-toc" style="margin-left:40px;"><a href="#S.3%20%E5%88%86%E6%9E%90" rel="nofollow">16.3 分析</a></p> 
<p id="S.4%20%E5%90%AF%E5%8F%91%E5%BC%8F-toc" style="margin-left:40px;"><a href="#S.4%20%E5%90%AF%E5%8F%91%E5%BC%8F" rel="nofollow">16.4 启发式</a></p> 
<p id="S.5%20%E7%A8%B3%E5%81%A5%E6%80%A7%E4%B8%8E%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%B9%B3%E6%BB%91%E5%BA%A6-toc" style="margin-left:40px;"><a href="#S.5%20%E7%A8%B3%E5%81%A5%E6%80%A7%E4%B8%8E%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%B9%B3%E6%BB%91%E5%BA%A6" rel="nofollow">16.5 稳健性与卷积核平滑度</a></p> 
<p id="17.%C2%A0Prototype%20Memory%20and%20Attention%20Mechanisms%20for%20Few%20Shot%20Image%20Generation%C2%A0-toc" style="margin-left:0px;"><a href="#17.%C2%A0Prototype%20Memory%20and%20Attention%20Mechanisms%20for%20Few%20Shot%20Image%20Generation%C2%A0" rel="nofollow">17. Prototype Memory and Attention Mechanisms for Few Shot Image Generation </a></p> 
<p id="S.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-toc" style="margin-left:40px;"><a href="#S.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3" rel="nofollow">17.1 核心思想</a></p> 
<p id="S.2%20%E6%96%B9%E6%B3%95-toc" style="margin-left:40px;"><a href="#S.2%20%E6%96%B9%E6%B3%95" rel="nofollow">17.2 方法</a></p> 
<p id="S.3%20%E9%99%90%E5%88%B6-toc" style="margin-left:40px;"><a href="#S.3%20%E9%99%90%E5%88%B6" rel="nofollow">17.3 限制</a></p> 
<p id="S.4%20%E6%A6%82%E5%BF%B5%E7%B0%87%E5%88%86%E6%9E%90-toc" style="margin-left:40px;"><a href="#S.4%20%E6%A6%82%E5%BF%B5%E7%B0%87%E5%88%86%E6%9E%90" rel="nofollow">17.4 概念簇分析</a></p> 
<p id="18.%C2%A0Towards%20faster%20and%20stabilized%20gan%20training%20for%20high-fidelity%20few-shot%20image%20synthesis-toc" style="margin-left:0px;"><a href="#18.%C2%A0Towards%20faster%20and%20stabilized%20gan%20training%20for%20high-fidelity%20few-shot%20image%20synthesis" rel="nofollow">18. Towards faster and stabilized gan training for high-fidelity few-shot image synthesis</a></p> 
<p id="S.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-toc" style="margin-left:40px;"><a href="#S.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3" rel="nofollow">18.1 核心思想</a></p> 
<p id="S.2%20%E6%A8%A1%E5%9D%97%E6%9E%B6%E6%9E%84-toc" style="margin-left:40px;"><a href="#S.2%20%E6%A8%A1%E5%9D%97%E6%9E%B6%E6%9E%84" rel="nofollow">18.2 模块架构</a></p> 
<p id="18.3%C2%A0%C2%A0Loss-toc" style="margin-left:40px;"><a href="#18.3%C2%A0%C2%A0Loss" rel="nofollow">18.3  Loss</a></p> 
<p id="S.4%20%E5%88%86%E6%9E%90-toc" style="margin-left:40px;"><a href="#S.4%20%E5%88%86%E6%9E%90" rel="nofollow">18.4 分析</a></p> 
<p id="19.%C2%A0Generalized%20One-shot%20Domain%20Adaptation%20of%20Generative%20Adversarial%20Networks-toc" style="margin-left:0px;"><a href="#19.%C2%A0Generalized%20One-shot%20Domain%20Adaptation%20of%20Generative%20Adversarial%20Networks" rel="nofollow">19. Generalized One-shot Domain Adaptation of Generative Adversarial Networks</a></p> 
<p id="S.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-toc" style="margin-left:40px;"><a href="#S.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3" rel="nofollow">19.1 核心思想</a></p> 
<p id="S.2%20%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84-toc" style="margin-left:40px;"><a href="#S.2%20%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84" rel="nofollow">19.2 网络架构</a></p> 
<p id="S.3%20LOSS-toc" style="margin-left:40px;"><a href="#S.3%20LOSS" rel="nofollow">19.3 LOSS</a></p> 
<p id="20.%C2%A0Improving%20GANs%20with%20A%20Dynamic%20Discriminator-toc" style="margin-left:0px;"><a href="#20.%C2%A0Improving%20GANs%20with%20A%20Dynamic%20Discriminator" rel="nofollow">20. Improving GANs with A Dynamic Discriminator</a></p> 
<p id="S.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-toc" style="margin-left:40px;"><a href="#S.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3" rel="nofollow">20.1 核心思想</a></p> 
<p id="S.2%20%E5%85%B7%E4%BD%93%E6%93%8D%E4%BD%9C-toc" style="margin-left:40px;"><a href="#S.2%20%E5%85%B7%E4%BD%93%E6%93%8D%E4%BD%9C" rel="nofollow">20.2 具体操作</a></p> 
<p id="S.3%20%E5%88%86%E6%9E%90-toc" style="margin-left:40px;"><a href="#S.3%20%E5%88%86%E6%9E%90" rel="nofollow">20.3 分析</a></p> 
<p id="%E8%A1%A5%E5%85%85%E7%9F%A5%E8%AF%86-toc" style="margin-left:0px;"><a href="#%E8%A1%A5%E5%85%85%E7%9F%A5%E8%AF%86" rel="nofollow">补充知识</a></p> 
<p id="5.%20%E8%A1%A5%E5%85%85%EF%BC%9A%E8%B0%B1%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%88spectral%20normalization%EF%BC%89-toc" style="margin-left:0px;"><a href="#5.%20%E8%A1%A5%E5%85%85%EF%BC%9A%E8%B0%B1%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%88spectral%20normalization%EF%BC%89" rel="nofollow">1. 谱归一化（spectral normalization）</a></p> 
<p id="5.1%20%E7%90%86%E8%AE%BA%E4%BE%9D%E6%8D%AE-toc" style="margin-left:40px;"><a href="#5.1%20%E7%90%86%E8%AE%BA%E4%BE%9D%E6%8D%AE" rel="nofollow">1.1 理论依据</a></p> 
<p id="5.2%20%E7%AE%97%E6%B3%95-toc" style="margin-left:40px;"><a href="#5.2%20%E7%AE%97%E6%B3%95" rel="nofollow">1.2 算法</a></p> 
<p id="1.3%20%E5%8F%82%E8%80%83%E2%80%8B-toc" style="margin-left:40px;"><a href="#1.3%20%E5%8F%82%E8%80%83%E2%80%8B" rel="nofollow">1.3 参考​</a></p> 
<p id="2.%20%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%EF%BC%88Fusion%EF%BC%89-toc" style="margin-left:0px;"><a href="#2.%20%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%EF%BC%88Fusion%EF%BC%89" rel="nofollow">2. 特征融合（Fusion）</a></p> 
<p id="1.%20%E5%8F%8D%E5%8D%B7%E7%A7%AF-toc" style="margin-left:0px;"><a href="#1.%20%E5%8F%8D%E5%8D%B7%E7%A7%AF" rel="nofollow">3. 反卷积</a></p> 
<p id="2.%20%E6%89%A9%E5%B1%95%20latent%20space%20W%2B-toc" style="margin-left:0px;"><a href="#2.%20%E6%89%A9%E5%B1%95%20latent%20space%20W%2B" rel="nofollow">4. 仿射变换（Affine）</a></p> 
<p id="4.%20%E7%99%BD%E5%8C%96%EF%BC%88Whitening%EF%BC%89-toc" style="margin-left:0px;"><a href="#4.%20%E7%99%BD%E5%8C%96%EF%BC%88Whitening%EF%BC%89" rel="nofollow">5. 白化（Whitening）</a></p> 
<p id="%E5%BE%80%E6%9C%9F%E6%80%BB%E7%BB%93%EF%BC%9A-toc" style="margin-left:0px;"><a href="#%E5%BE%80%E6%9C%9F%E6%80%BB%E7%BB%93%EF%BC%9A" rel="nofollow">往期总结</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="0.%20%E6%91%98%E8%A6%81">0. 摘要</h2> 
<p>改善模型的方法主要分为几大类，例如：</p> 
<ul><li>更改模型架构：HyperDomainNet、StyleGAN、StyleGAN2、MoCA、FastGAN、实体迁移、DynamicD 等</li><li>数据增强：InsGen、ADA、DA</li><li>正则化：DigGAN、R_LC</li><li>利用频域信息：StyleGAN3、MskedGAN、FreGAN、高频泛化 </li><li>从隐空间入手：Domain Gap、I2S、II2S</li></ul> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/e7/b3/4GKYR9C8_o.png"></p> 
<h2 id="1.%C2%A0Artificial%20Fingerprinting%20for%20Generative%20Models%3A%C2%A0Rooting%20Deepfake%20Attribution%20in%20Training%20Data">1. <strong>Artificial Fingerprinting for Generative Models: Rooting Deepfake Attribution in Training Data</strong></h2> 
<p><a href="https://blog.csdn.net/qq_44681809/article/details/130355979" title="（2021，deepfake 溯源）用于生成模型的人工指纹：扎根于训练数据中的深度造假溯源_EDPJ的博客-CSDN博客">（2021，deepfake 溯源）用于生成模型的人工指纹：扎根于训练数据中的深度造假溯源_EDPJ的博客-CSDN博客</a></p> 
<h3 id="1.%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3">1.1 核心思想</h3> 
<p>GAN（生成对抗网络）的核心思想是生成和对抗。该网络的产物（例如：图像）就是基于深度学习获得的虚假产物，称之为深度造假（deepfake）。</p> 
<p>而本文是从对抗的角度来研究：识别 deepfake，并对该 deepfake 溯源（找出该 deepfake 由谁产生）</p> 
<p>核心思想是把唯一指纹信息嵌入到一个 GAN 中，使该 GAN 生成的所有图像都带有该指纹信息。实际上就是把身份信息嵌入到模型中，然后为该模型的输出添加该身份信息。</p> 
<p>该方法类似于为图像添加水印，区别在于，以往的添加水印的方法是在生成模型后加入一个添加水印的模块，与本方法的直接在模型中嵌入信息不同。</p> 
<h3 id="2.%20%E6%AD%A5%E9%AA%A4">1.2 步骤</h3> 
<p>1) 训练获得图像隐写术编码器和译码器</p> 
<p>2) 使用编码器把指纹嵌入到训练数据中</p> 
<p>3) 使用嵌入指纹信息的训练数据训练一个生成模型</p> 
<p>4) 使用译码器从生成数据中解出指纹</p> 
<h3>1.3 <strong>Deepfake 检测和溯源</strong></h3> 
<p>GAN 模型生成的图像具有独特性。生成模型会在生成的样本中留下独特的噪声，从而允许 deepfake 检测。 还可使用神经网络分类器找出不同图像的来源。高频模式不匹配可用于 deepfake 的检测，纹理特征不匹配也可以。 然而，由于检测对策的进步，这些线索是不可持续的。例如，频谱正则化缩小频率不匹配并导致显着的检测恶化。此外，检测器容易受到对抗性规避攻击。</p> 
<h2 id="2.%C2%A0HyperDomainNet%3A%20Universal%20Domain%20Adaptation%20for%20Generative%20Adversarial%20Networks">2. <strong>HyperDomainNet: Universal Domain Adaptation for Generative Adversarial Networks</strong></h2> 
<p><a href="https://blog.csdn.net/qq_44681809/article/details/130498687" title="（2022，HyperDomainNet）生成对抗网络的通用域自适应_EDPJ的博客-CSDN博客">（2022，HyperDomainNet）生成对抗网络的通用域自适应_EDPJ的博客-CSDN博客</a></p> 
<h3 id="2.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3">2.1 核心思想</h3> 
<p>调制/解调背后的想法是将标准自适应实例归一化 (adaptive instance normalization，AdaIN) 替换为基于输入特征图的预期统计数据的归一化。因此，调制部分基本上是一种自适应缩放操作，就像在 AdaIN 中一样，由样式参数 s 控制。域调制流程如图 1 所示。</p> 
<div> 
 <p><img alt="" height="109" src="https://images2.imgbox.com/ec/0a/V8nHlqS9_o.png" width="539">​</p> 
</div> 
<div> 
 <p><img alt="" height="36" src="https://images2.imgbox.com/c9/fd/La6nthAW_o.png" width="635">​</p> 
</div> 
<p><img alt="" height="465" src="https://images2.imgbox.com/29/6d/wCnyPDSY_o.png" width="914"></p> 
<p>这个想法是只优化与样式参数 s 具有相同维度的向量 d。因此，我们只训练向量 d，而不是优化 StyleGAN 合成网络 G_sys 部分的所有权重 θ（d &lt; θ）。该方法极大程度的减少了需要优化的参数。</p> 
<p>我们的目标是，在给定输入目标域的情况下，学习可预测域参数 d = D(B) 的 HyperDomainNet D。</p> 
<h3 id="2.%20Training%20Loss">2.2 Training Loss</h3> 
<p>我们使用三个损失 L_direction（域自适应 loss）、L_tt-direction（提高多样性）和 L_domain-norm（提高多域自适应的多样性）来训练 HyperDomainNet D。</p> 
<p><img alt="" height="534" src="https://images2.imgbox.com/9a/2d/NZYTrgO1_o.png" width="915"></p> 
<p>L_domain-norm：在机器学习中，添加正则化项的作用是为了防止样本数目过少时发生过拟合。在多域自适应时，因为目标域的样本数目较少，为了避免过拟合（目标域贴近），需要添加域参数 d = D(B) 的正则化项，从而使目标域相互远离，即，提高了目标域的多样性。</p> 
<p><span style="color:#fe2c24;">这里有一点我不理解。对于多域自适应，域参数 d 代表目标域的风格，而正则化项限制了域参数的大小。那么对于不同的域，它们的域参数的差别并不大，所以如何使目标域远离？这有点违反直觉，而作者也并未在文献中给出说明。</span></p> 
<h3 id="3.%20%E6%B3%9B%E5%8C%96">2.3 泛化</h3> 
<p>提高 HDN 的泛化能力具有挑战性，因为它倾向于在训练域上过拟合：对于未见过的域，它会预测非常接近训练域的域。解决这个问题的一种方法是大大扩展训练集（Data Augmentation）。为此，我们使用了三种技术：</p> 
<ul><li>通过组合不同的训练域来生成许多训练域；</li><li>从初始训练 embedding 的凸包中采样 CLIP embedding；</li><li>在给定余弦相似性的情况下对初始 CLIP embedding 进行重采样（resampling）。</li></ul> 
<h2 id="3.%C2%A0Data-Efficient%20Instance%20Generation%20from%20Instance%20Discrimination%C2%A0">3. <strong>Data-Efficient Instance Generation from Instance Discrimination</strong> </h2> 
<p><a href="https://blog.csdn.net/qq_44681809/article/details/130581242" title="（2021，InsGen）从实例鉴别中生成数据高效的实例_EDPJ的博客-CSDN博客">（2021，InsGen）从实例鉴别中生成数据高效的实例_EDPJ的博客-CSDN博客</a></p> 
<h3 id="3.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3">3.1 核心思想</h3> 
<p>这是比之前的数据增强算法性能更好的算法，它不止把真实样本用于训练，还把生成器生成的虚假样本用于训练。所以，即使真实样本的数量很少，最终也能获得较好的生成性能（高质量、高多样性）。</p> 
<h3 id="3.2%20%E6%AD%A5%E9%AA%A4">3.2 步骤</h3> 
<p>实例鉴别不止区分真实图像与虚假图像，还要区分开每一个单独的图像。该方法有四个基本组成部分：</p> 
<ul><li>区分真实图像：鉴别器识别每一个真实样本图像。使用对比学习，把一个真实图像的数据增强当做该图像的正例，其它真实图像当做反例</li><li>区分可以无限采样的假图像：鉴别器识别每个单独的假图像。使用对比学习，把一个假图像的数据增强当做该图像的正例，其它假图像当做反例</li><li>噪声扰动策略：由于隐空间的连续性，使得基于邻域内隐编码（z+ε）合成的图像彼此非常接近。因此，它们更适合被视为正对而不是负对。依然使用对比学习。</li><li>鼓励生成器进行多样化生成：多样生成要求所有生成的样本都可以彼此区分，这恰好符合我们实例区分的目标。</li></ul> 
<h3 id="3.%20%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C">3.3 消融实验</h3> 
<p>为了调查实例鉴别中每个组件的重要性，作者进行了消融研究。</p> 
<ul><li>实例鉴别对提升性能很重要。此外，研究结果表明，假样本也可以被视为无监督表示学习的数据源（提升性能）。</li><li>噪声扰动很重要。这种隐空间增强，即隐空间中的小运动，总是导致原始图像发生明显但语义一致的变化，这不容易通过一些几何和颜色变换来实现。同时，由于实例鉴别的目标，要求鉴别器对这种噪声扰动具有不变性。该策略可提升性能。</li><li>合成样本（假样本）很重要。随着合成样本数量的增加，模型性能越来越好。</li></ul> 
<h2 id="4.%C2%A0MIND%20THE%20GAP%3A%20Domain%20Gap%20Control%20for%20Single%20Shot%20Domain%20Adaptation%20for%20Generative%20Adversarial%20Networks%C2%A0">4. <strong>MIND THE GAP: Domain Gap Control for Single Shot Domain Adaptation for Generative Adversarial Networks</strong> </h2> 
<p><a href="https://blog.csdn.net/qq_44681809/article/details/130597224" title="（2021，Domain Gap）MIND THE GAP：生成对抗网络单样本域自适应的域间隙控制_EDPJ的博客-CSDN博客">（2021，Domain Gap）MIND THE GAP：生成对抗网络单样本域自适应的域间隙控制_EDPJ的博客-CSDN博客</a></p> 
<h3 id="S.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3">4.1 核心思想</h3> 
<p><img alt="" height="520" src="https://images2.imgbox.com/be/ec/8cgarKwG_o.png" width="915"></p> 
<p>本文方法的核心是保留域内和域间方向，如图 3 所示。</p> 
<p>域内方向（v_A，v_B）指的是：语义信息 / 语义多样性。若域 A 所有的语义信息 v_A 都被域 B 复制（v_B 与 v_A 平行），当进行域迁移时，域 B 就能保留域 A 的语义多样性。</p> 
<p>域间方向指的是：域间隙 / 域差异。以 StyleGAN-NADA 为例，当域间方向由文本确定时，文本描述的方向就是要进行域迁移的方向。当域间方向由参考图像确定时，需要找出域 B 中的参考图像 I_B 在域 A 中的对应图像 I_A，它们的 embedding 之间的差异就是域差异，也就是进行域迁移的方向。</p> 
<h3 id="S.2%20%E6%AD%A5%E9%AA%A4">4.2 步骤</h3> 
<p>在域 A 中为域 B 中的给定图像查找 I_A 是本方法的核心，也是当前最先进技术 StyleGAN-NADA 的一个重大限制，因为它们使用域 A 的均值。域 A 的均值 是 I_A 的一个非常粗略的近似值。与此相反，我们提出了一个逆域自适应步骤，通过将图像 I_B 投影到域 A 中来找到比域 A 的均值更相似和更具体的参考图像样本。</p> 
<h3 id="S.3%20%E9%A3%8E%E6%A0%BC%E6%B7%B7%E5%90%88">4.3 风格混合</h3> 
<p>W+ 空间中的隐编码可以分为 18 个块，每个块影响 StyleGAN2 的不同层。W+ 编码的后面的块对图像的样式（例如纹理和颜色）有更大的影响，而前面的层会影响图像的粗略结构或内容。可以通过对后面的块（风格块）实行插值（对源域和目标图像的风格块进行加权和），来控制风格混合（迁移）程度。</p> 
<h2 id="5.%C2%A0Improved%20StyleGAN%20Embedding%3A%20Where%20are%20the%20good%20latents%C2%A0">5. <strong>Improved StyleGAN Embedding: Where are the good latents</strong> </h2> 
<p><a href="https://blog.csdn.net/qq_44681809/article/details/130614083" title="（2020，II2S）改进的 StyleGAN Embedding：寻找好的隐编码（latent code）_EDPJ的博客-CSDN博客">（2020，II2S）改进的 StyleGAN Embedding：寻找好的隐编码（latent code）_EDPJ的博客-CSDN博客</a></p> 
<h3 id="S.1%20%E9%9A%90%E7%A9%BA%E9%97%B4%EF%BC%88%E5%8F%82%E8%80%83%C2%A0GAN%20Inversion%3A%20A%20Survey%EF%BC%89">5.1 隐空间（参考 <strong>GAN Inversion: A Survey</strong>）</h3> 
<div> 
 <p><img alt="" height="360" src="https://images2.imgbox.com/e3/54/Dn4i0Wfy_o.png" width="557">​</p> 
</div> 
<div> 
 <p><img alt="" height="653" src="https://images2.imgbox.com/bf/5c/2pCKncS6_o.png" width="1167">​</p> 
</div> 
<p>Z 空间：从简单分布（例如：均匀分布）中随机采样得到的空间。</p> 
<p>W 空间：使用由多层感知器构成的非线性映射网络把 Z 空间映射到 W 空间。相比于 Z 空间，可以缓解简单分布的限制。如图 1 和图 2 所示，W 空间分布无规则。</p> 
<p>W+ 空间：把与网络层数对应数目的 latent code w 串联，然后送入每一层的 AdaIN，得到 W+ 空间。相比于 W 空间，属性有更高的解耦度。在 StyleGAN 中，串联了 18 个维度为 512 的隐编码。</p> 
<p>S 空间：通过对生成器的每一层使用不同的仿射变换，把 W 空间转换为 S 空间。相比于 W 空间，属性有更高的解耦度。</p> 
<p>P 空间：通过反转 StyleGAN 映射网络中的最后一个 Leaky ReLU 层，将 𝑊 空间转换为 𝑃 空间。 由最后一个 Leaky ReLU 使用 1/5 的斜率，我们使用</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/e9/2d/Vsqg21u5_o.png">​</p> 
</div> 
<p>如图 1 所示，P 空间的维度分布是类正态分布，但是不同维度之间是相关的，如图 1 和 图 2 所示，多元分布近似于椭球体。</p> 
<p>P_N 空间：通过 PCA 白化操作把 P 空间转换为 P_N 空间，从而消除依赖性和冗余。如图 1 和 图 2 所示，多元分布是一个超球体，此时，各个维度相互解耦。</p> 
<p>P_N+ 空间：类似于通过串联 W 空间的隐编码，从而实现从 W 空间到 W+ 空间的变换，串联 P_N 空间的隐编码可以得到 P_N+ 空间。</p> 
<h3 id="S.2%C2%A0%E6%94%B9%E8%BF%9B%E7%9A%84%20I2S%EF%BC%88Improved%20Image2StyleGAN%EF%BC%8CII2S%EF%BC%89">5.2 改进的 I2S（Improved Image2StyleGAN，II2S）</h3> 
<p>本文的核心思想是在隐空间中寻找一个好的有利于图像编辑的隐编码（embedding）。具体操作是：为 I2S 的损失函数添加一个正则化器，如果隐编码距离 𝑃_𝑁+ 的原点太远，它就会做出惩罚。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/09/8f/6uE04qDn_o.png">​</p> 
</div> 
<p>其中，v 是 P_N+ 空间的隐编码，||v||^2 也表示为马氏距离（一个点与一个分布之间的距离）。正则化器会将解决方案偏向隐空间分布更密集的部分，在该区域，更有利于图像编辑（对隐编码的更改对图像的影响更大）。</p> 
<h3 id="S.3%20%E7%BC%96%E8%BE%91%E8%B4%A8%E9%87%8F%20vs%20%E9%87%8D%E5%BB%BA%E8%B4%A8%E9%87%8F">5.3 编辑质量 vs 重建质量</h3> 
<p>重建质量受益于与 𝑃_𝑁 空间中原点远离的隐编码，但编辑质量受益于与 𝑃_𝑁 空间中原点靠近的隐编码。这需要在重建质量和编辑质量之间进行权衡。</p> 
<p>对于正态分布，距离原点越近，分布密度越高。而在分布密集区域，对隐编码微小的改变就会使图像产生较大的改变。因此，靠近原点的隐编码更有利于图像编辑。</p> 
<p>然而，这也意味着，对隐编码的微小扰动就会使图像产生较大改变，这是不利于图像重建的。因此，在图像重建时，需要选择远离原点的隐编码。</p> 
<h2 id="6.%C2%A0DigGAN%3A%20Discriminator%20Gradient%20Gap%20Regularization%20for%20GAN%20Training%20with%20Limited%20Data%C2%A0">6. <strong>DigGAN: Discriminator Gradient Gap Regularization for GAN Training with Limited Data</strong> </h2> 
<p><a href="https://blog.csdn.net/qq_44681809/article/details/130634728" title="（2022，DigGAN）用于有限数据 GAN 训练的鉴别器梯度间隙正则化_gan鉴别器_EDPJ的博客-CSDN博客">（2022，DigGAN）用于有限数据 GAN 训练的鉴别器梯度间隙正则化_gan鉴别器_EDPJ的博客-CSDN博客</a></p> 
<h3 id="6.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3">6.1 核心思想</h3> 
<p>本文解决问题的是，训练数据稀缺时，如何使 GAN 能有好的生成性能。</p> 
<p>本文的核心思想是，为 GAN 的鉴别器的损失函数添加一个正则项，如下所示，</p> 
<div> 
 <p><img alt="" height="57" src="https://images2.imgbox.com/45/6f/syyzY4VJ_o.png" width="642">​</p> 
</div> 
<p>该正则项表示鉴别器对真实数据和生成数据梯度的范数的差距。研究表明，随着该差距的减小，生成图像的 FID 值逐渐减小，即 GAN 的生成性能逐渐增加。</p> 
<h3 id="S.2%20%E5%88%86%E6%9E%90">6.2 分析</h3> 
<p>鉴别器对图像的梯度，实际上是鉴别器对图像的各个属性（特征）求梯度，表示的是对各个属性的变化率。当鉴别器对真实数据和生成数据所有属性的变化率相同时，相应的梯度范数差距最小，此时，生成数据更加贴合真实数据。</p> 
<div> 
 <p class="img-center"><img alt="" height="275" src="https://images2.imgbox.com/87/39/ICTXSKUX_o.png" width="332"></p> 
 <p>如图 9 所示，随着正则化权重的增加，生成数据的 FID 值逐渐减小。但当权重过大时（超过 10k），FID 值会增加。对此，我的理解是，当权重过大时，训练更多的关注正则化项，而忽视鉴别器本身的 loss。由于训练数据的稀缺性，过多的关注正则化项，可能会导致过拟合，降低生成数据的多样性。</p> 
</div> 
<h2 id="7.%C2%A0Regularizing%20generative%20adversarial%20networks%20under%20limited%20data%C2%A0">7. <strong>Regularizing generative adversarial networks under limited data</strong> </h2> 
<p><a href="https://blog.csdn.net/qq_44681809/article/details/130656732" title="（2021，R_LC）在有限数据下正则化生成对抗网络_EDPJ的博客-CSDN博客">（2021，R_LC）在有限数据下正则化生成对抗网络_EDPJ的博客-CSDN博客</a></p> 
<h3 id="7.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3">7.1 核心思想</h3> 
<p>有限数据设置下，通过在训练阶段对判别器 loss 添加如下正则化项，使 GAN 模型实现更稳健的训练。</p> 
<div> 
 <p><img alt="" height="36" src="https://images2.imgbox.com/30/5f/ekDephVx_o.png" width="434">​</p> 
</div> 
<div> 
 <p><img alt="" height="40" src="https://images2.imgbox.com/af/2f/G8WdVgdp_o.png" width="524">​</p> 
</div> 
<p>其中，α_R 和 α_F 是指数移动平均变量 ，称为锚点，用于跟踪鉴别器对真实图像和生成图像的预测。</p> 
<p><span style="color:#fe2c24;">很疑惑的一点，交换公式 4 中 α_R 和 α_F 的位置是否更为合理？因为直观地想法是，鉴别器对真实数据的评估应该接近其对真实图像预测。</span></p> 
<h3 id="7.2%20%E5%88%86%E6%9E%90">7.2 分析</h3> 
<div> 
 <p><img alt="" height="473" src="https://images2.imgbox.com/1d/09/Dr1Illz9_o.png" width="545">​</p> 
</div> 
<p>该正则化项有助于 GAN 模型实现更稳健的训练。如图 8 所示，随着训练的进行，鉴别器的预测逐渐收敛到固定点，移动平均变量 α_R 和 α_F 也是如此。 </p> 
<div> 
 <p><img alt="" height="349" src="https://images2.imgbox.com/2c/2e/0yNgCKlt_o.png" width="544">​</p> 
</div> 
<p>实验结果表明，随正则化项的权重的增加，模型性能先增后减，如图 7 所示。这是因为，该正则化项与 LeCam 散度相关，该散度限制正则化权重不宜过大。而直观地理解是，由于训练数据有限，权重过大时，容易发生过拟合。</p> 
<p>另一方面，当模型参数过多时，也容易发生过拟合，造成性能下降，如图 7 所示。</p> 
<h2 id="8.%C2%A0Large%20scale%20gan%20training%20for%20high%20fidelity%20natural%20image%20synthesis%C2%A0">8. <strong>Large scale gan training for high fidelity natural image synthesis</strong> </h2> 
<p><a href="https://blog.csdn.net/qq_44681809/article/details/130672861" title="（2018, BigGAN）用于高保真自然图像合成的大规模 GAN 训练_EDPJ的博客-CSDN博客">（2018, BigGAN）用于高保真自然图像合成的大规模 GAN 训练_EDPJ的博客-CSDN博客</a></p> 
<h3 id="8.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3">8.1 核心思想</h3> 
<p>BigGAN 的架构如图 15 所示。本文的核心思想是，更大的 batch 以及模型规模（更多的参数），可以带来更好的性能。然而，随着模型规模的扩大，训练的不稳定性也越发明显。</p> 
<h3 id="S.2%20%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%EF%BC%88%E8%B0%B1%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%89">8.2 解决办法（谱归一化）</h3> 
<p>谱归一化，它通过使用第一个奇异值的动态估计对其参数进行归一化，在 D 上强制执行 Lipschitz 连续性，从而引入自适应正则化最大奇异方向的 backwards dynamics。</p> 
<p>对生成器的权重谱研究发现，虽然调节 G 可能会提高稳定性，但不足以确保稳定性。</p> 
<p>对鉴别器的权重谱研究发现，虽然可以通过强烈约束 D 来加强稳定性，但这样做会导致性能上的巨大代价。可行的办法是，放松这种约束并允许在训练的后期阶段发生崩溃，然后使用早停来实现更好的最终性能。</p> 
<p>此外，作者认为，D 在记忆训练集符合 D 的作用，因为 D 的作用不是明确地泛化，而是提取训练数据并为 G 提供有用的学习信号。</p> 
<p>我认为，这种说法是合理的。记忆与泛化，其实是并不冲突的。D 记忆训练集，则对训练集的样本，D 应该做出来趋于完美的判决。但与此同时，D 还应具有一定的泛化能力，即对于不属于训练集的样本，D 也需要做出正确的判断。</p> 
<h3 id="S.3%20%E5%85%B6%E4%BB%96%E8%B4%A1%E7%8C%AE">8.3 其他贡献</h3> 
<p><strong>截断。</strong>作者使用截断实现生成多样性和保真度的权衡。</p> 
<p>具体做法是，在训练时，从截断的分布（例如，正态分布）中对 z 进行采样（其中超出范围的值被重新采样以落在该范围内）。</p> 
<p>然而，一些较大模型不适合截断，在输入截断噪声时会产生饱和伪影。为解决问题，作者使用正交正则化，通过将 G 调节为平滑来增加截断的适用性。</p> 
<p><strong>BigGAN-deep</strong>。对于 BigGAN，增加每层的宽度（通道数）可以大幅度提升性能。而增加深度并没有带来改进。为解决这个问题，作者提出了 BigGAN-deep，它使用了不同的 residual 块结构。</p> 
<h2 id="9.%C2%A0A%20Style-Based%20Generator%20Architecture%20for%20Generative%20Adversarial%20Networks">9. <strong>A Style-Based Generator Architecture for Generative Adversarial Networks</strong></h2> 
<p><a href="https://blog.csdn.net/qq_44681809/article/details/130701048" title="（2019, StyleGAN）用于 GAN 的基于样式的生成器架构_stylegan生成器_EDPJ的博客-CSDN博客">（2019, StyleGAN）用于 GAN 的基于样式的生成器架构_stylegan生成器_EDPJ的博客-CSDN博客</a></p> 
<h3 id="9.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3">9.1 核心思想</h3> 
<p class="img-center"><img alt="" height="811" src="https://images2.imgbox.com/4e/51/NSj7D1em_o.png" width="552"></p> 
<p>本文基于风格迁移提出了一个新的生成器，如图 1 所示。该生成器可以在保留高级属性（例如，在人脸上训练时的姿势和身份）的同时，使生成图像产生随机变化（例如，雀斑、头发）。</p> 
<h3 id="9.2%20%E5%88%86%E6%9E%90">9.2 分析</h3> 
<p><strong>生成器的结构</strong>。该生成器移除了传统生成器的输入层，把采样于隐空间 Z 的隐编码 z 输入非线性映射网络（例如：MLP）获得中间隐空间 W 的隐编码 w。再把隐编码通过学到的仿射变换 A，获得风格 y = (y_s, y_b)。再用 y 输入（控制）生成网络每个卷积层的 AdaIN。</p> 
<p class="img-center"><img alt="" height="51" src="https://images2.imgbox.com/a4/75/ZEYuIuzT_o.png" width="455"></p> 
<p>风格 y 控制高级属性（例如，在人脸上训练时的姿势和身份）的生成。由于输入的噪声是随机采样的，所以会使生成图像产生随机变化 （例如，雀斑、头发），但不会对高级属性产生影响。</p> 
<h3 id="9.3%20%E5%85%B6%E4%BB%96%E8%B4%A1%E7%8C%AE">9.3 其他贡献</h3> 
<p><strong>W 空间的属性解耦</strong>。作者表明，W 空间的的各个属性是局部的（解耦的），即，改变一个属性并不会对其他属性产生影响。这是由于非线性映射器的作用，使 Z 空间中相互纠缠的属性在 W 空间中分离。</p> 
<p>解耦性使得基于 W 空间的插值和图像编辑更容易实现。</p> 
<p>当隐空间 W 充分解耦时，其中相邻的属性（区域）可以使用一个超平面分隔开。“<a href="https://blog.csdn.net/qq_44681809/article/details/129582405" title="Interpreting the Latent Space of GANs for Semantic Face Editing">Interpreting the Latent Space of GANs for Semantic Face Editing</a>” 一文就是对此的进一步研究。 </p> 
<p><strong>感知路径长度</strong>。基于感知的成对图像距离，计算为两个 VGG16 embedding 之间的加权差异，其中权重是合适的，因此度量与人类感知相似性判断一致。 如果将隐空间插值路径细分为线性段，可以将此分段路径的总感知长度定义为每个段的感知差异之和。</p> 
<p>隐空间越扭曲（特征的不同纬度相互耦合），例如 Z 空间，感知路径长度越大，反之亦然，例如 W 空间。</p> 
<h2 id="10.%C2%A0Analyzing%20and%20Improving%20the%20Image%20Quality%20of%20StyleGAN%C2%A0">10. <strong>Analyzing and Improving the Image Quality of StyleGAN</strong> </h2> 
<p><a href="https://blog.csdn.net/qq_44681809/article/details/130724319" title="（2020，StyleGAN2）分析和提高 StyleGAN 的图像质量_EDPJ的博客-CSDN博客">（2020，StyleGAN2）分析和提高 StyleGAN 的图像质量_EDPJ的博客-CSDN博客</a></p> 
<h3 id="10.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3">10.1 核心思想</h3> 
<p>由 StyleGAN 生成的图像存在伪影，作者发现这是由生成网络的中的 AdaIN 引起的。通过修改生成器的网络结构，把 AdaIN 转化为调制 / 解调操作，该问题被解决。</p> 
<h3 id="S.2%20%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84">10.2 网络结构</h3> 
<p class="img-center"><img alt="" height="710" src="https://images2.imgbox.com/e1/64/AGRlEntA_o.png" width="1137"></p> 
<p>生成器结构更改的过程如图 2 所示，a → d 显示的是从 AdaIN 转化为对风格进行调制（公式 1）和解调（公式 3）的过程。 </p> 
<p class="img-center"><img alt="" height="33" src="https://images2.imgbox.com/5b/e5/Zs9MN3MS_o.png" width="355"></p> 
<p class="img-center"><img alt="" height="69" src="https://images2.imgbox.com/c7/30/Y5WQIuWR_o.png" width="422"></p> 
<h3 id="10.3%20%E5%85%B6%E4%BB%96%E8%B4%A1%E7%8C%AE">10.3 其他贡献</h3> 
<p><strong>惰性正则化</strong>。损失函数通常为主要损失函数加正则化项的形式，在实现时，可以在多次优化主要损失函数的同时只优化一次正则化项。该方法不会对结果造成影响，但是可以大大降低计算成本和整体内存使用量。</p> 
<p class="img-center"><img alt="" height="38" src="https://images2.imgbox.com/c4/4a/Pa42mgsj_o.png" width="412"></p> 
<p><strong>路径长度正则化</strong>。正则化项如公式 4 所示，其中 W 和 Y 分别表示隐空间和图像空间，J 表示雅克比矩阵。当图像 y 的隐空间任一点对各个方向（属性）的梯度相等时，该正则化项取最小值。</p> 
<p>基于 II2S 一文可知，当属性相互解耦时，隐空间的多元分布近似于一个超球体，在球体表面，任一点对各个方向（属性）的梯度相等。</p> 
<p>基于 StyleGAN 一文可知，属性解耦程度越高，隐空间越不扭曲，感知路径长度越短。</p> 
<p><strong>替代渐进式增长</strong>。在渐进式增长中，每个分辨率都会暂时用作输出分辨率，迫使它生成最大频率细节，然后导致经过训练的网络在中间层中具有过高的频率，从而导致移位不变性（例如，虽然头部姿态改变，但是牙齿在图中的相对位置并没有改变）。</p> 
<p><img alt="" height="698" src="https://images2.imgbox.com/ab/31/TTx09zH7_o.png" width="723">如图 7 所示，可以使用 skip connection 和 residual networks 替代渐进式增长，从而生成高质量图像。</p> 
<p class="img-center"><img alt="" height="483" src="https://images2.imgbox.com/47/15/0hjOb5aG_o.png" width="547"></p> 
<p><strong>图像溯源</strong>。如图 10 所示，对于图像与其投影图像的 LPIPS 距离分布，真实图像的整体数值大于生成图像的整体数值。相比于 StyleGAN，经由 StyleGAN2 生成的图像的差异更明显，更适合进行溯源。</p> 
<p class="img-center"><img alt="" height="29" src="https://images2.imgbox.com/4f/01/xFu8BNIN_o.png" width="211"></p> 
<h2 id="11.%C2%A0Alias-Free%20Generative%20Adversarial%20Networks">11. <strong>Alias-Free Generative Adversarial Networks</strong></h2> 
<p><a href="https://blog.csdn.net/qq_44681809/article/details/130845977" title="（2021，StyleGAN3）无失真（Alias-Free）生成对抗网络_EDPJ的博客-CSDN博客">（2021，StyleGAN3）无失真（Alias-Free）生成对抗网络_EDPJ的博客-CSDN博客</a></p> 
<h3 id="11.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3">11.1 核心思想</h3> 
<p>在现实世界中，不同尺度的细节倾向于分层变换。 例如，移动头部会导致鼻子移动，进而移动其上的皮肤毛孔。 对于典型的 GAN 生成器：粗特征主要控制更精细特征的存在，而不是它们的精确位置。 相反，许多细节似乎都固定在像素坐标中。从而造成像素混叠 / 图像失真。</p> 
<p>像素混叠 / 图像失真的两个来源：</p> 
<ul><li>由非理想的上采样滤波器（例如最近邻、双线性的或跨步的卷积）带有的拖尾导致的像素混叠</li><li>非线性的逐点应用，例如 ReLU 或 swish。连续域中应用 ReLU 可能会引入无法在输出中表示的任意高频。</li></ul> 
<p><img alt="" height="314" src="https://images2.imgbox.com/94/67/r7fh6Blm_o.png" width="911"></p> 
<p>解决办法将：参考 Nyquist–Shannon 采样定理消除失真的办法，把对图像（连续域）的操作（生成图像相对于原始图像的平移或旋转）转移到频域（离散域）中操作，消除所有位置参考源，使得精细特征能随着粗特征的变换而改变（等变）。</p> 
<h3 id="11.2%20%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84">11.2 网络结构</h3> 
<p class="img-center"><img alt="" height="510" src="https://images2.imgbox.com/9d/ed/pyunAua2_o.png" width="559"></p> 
<p>StyleGAN3 的结构如上图所示，生成网络的输入是傅里叶特征。</p> 
<p>上采样，把离散采样与 0 交织，然后与离散滤波器卷积。</p> 
<p>下采样，对连续  representation 进行低通滤波以去除高于输出带宽限制的频率，以便信号可以忠实的重建。</p> 
<h2 id="12.%C2%A0Training%20generative%20adversarial%20networks%20with%20limited%20data%C2%A0">12. <strong>Training generative adversarial networks with limited data</strong> </h2> 
<p><a href="https://blog.csdn.net/qq_44681809/article/details/130875563" title="（2020，ADA）用有限的数据训练生成对抗网络_EDPJ的博客-CSDN博客">（2020，ADA）用有限的数据训练生成对抗网络_EDPJ的博客-CSDN博客</a></p> 
<h3 id="12.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3">12.1 核心思想</h3> 
<p>为了解决用于训练 GAN 的数据少的问题，作者提出了一种新的数据增强方式：自适应鉴别器增强（adaptive discriminator augmentation，ADA）。相比于现有的数据增强方式，该方法研究如何避免把增强泄漏到生成数据中。</p> 
<p>作者发现，当增强的数据出现的概率 p &lt; 1，即有 1-p 的概率用于训练的是未增强的数据时，有可能避免把增强泄漏到生成数据中。对于不同的数据集大小，这个 p 值不是固定的，所以 ADA 就是基于训练，自适应的找出合适的 p。</p> 
<h3 id="S.2%20%E6%96%B9%E6%B3%95">12.2 方法</h3> 
<p>分别用 D_train、D_validation 和 D_generated 表示训练集、验证集和生成图像的鉴别器输出，以及它们在 N 个连续 minibatch 上的均值用 E[·] 表示，则有</p> 
<p><img alt="" height="60" src="https://images2.imgbox.com/d6/ee/ka0JAOdP_o.png" width="737"></p> 
<p>对于 r_v 和 r_t，r = 0 表示没有过拟合，r = 1 表示完全过拟合。控制方式如下：将 p 初始化为零，并根据过拟合程度每四个 minibatch 调整一次 p 值。 如果过拟合太多/太少，将 p 递增/递减固定量。我们设置调整大小，以便 p 可以足够快地从 0 上升到 1。上升到 1 之后，把 p 置为 0。</p> 
<h3 id="12.3%20%E5%85%B6%E4%BB%96%E8%B4%A1%E7%8C%AE">12.3 其他贡献</h3> 
<p>作者对真实数据、生成数据以及生成器使用数据增强，这要求这些增强对生成器和鉴别器是可鉴别的。另一篇论文，可鉴别数据增强（Differentiable Augmentation，DA）就是对本贡献的进一步研究（消融实验）。</p> 
<p class="img-center"><img alt="" height="206" src="https://images2.imgbox.com/ba/7c/6w4MVe9f_o.png" width="216"></p> 
<p>以某个概率 p 执行数据增强可以理解为：增强模块生效的概率是 p。</p> 
<ul><li>当 p=0 时，训练时未使用数据增强。不能解决数据有限的问题。</li><li>当 p=1 时，完全使用增强的数据进行训练。模型未见过真实数据，从而使增强泄漏到生成数据中，即生成器学习到了（可能只是一部分）增强的数据分布。（DA 的研究结果与此矛盾，存疑） </li></ul> 
<h3 id="S.4%20%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9%E5%88%86%E6%9E%90">12.4 相关内容分析</h3> 
<p>“随机化是针对每个增强和 minibatch.中的每个图像单独完成的。 鉴于流程中有许多增强，即使 p 值相当小，鉴别器也不太可能看到完全干净的图像。 尽管如此，只要 p 保持在实际安全门限以下，生成器就会被引导只生成干净的图像。”</p> 
<p>这是本文中的一段内容，DA 一文对可以生成干净图像做出了解释：使用可鉴别增强使生成器学到的是真实数据的分布，而不是增强数据的分布。</p> 
<h2 id="13.%C2%A0Differentiable%20augmentation%20for%20data-efficient%20gan%20training%C2%A0">13. <strong>Differentiable augmentation for data-efficient gan training</strong> </h2> 
<p><a href="https://blog.csdn.net/qq_44681809/article/details/130875633" title="（2020，DA）用于数据高效 GAN 训练的可鉴别数据增强（Differentiable Augmentation）_EDPJ的博客-CSDN博客">（2020，DA）用于数据高效 GAN 训练的可鉴别数据增强（Differentiable Augmentation）_EDPJ的博客-CSDN博客</a></p> 
<h3 id="13.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3">13.1 核心思想</h3> 
<p>本文是对自适应鉴别器增强（adaptive discriminator augmentation，ADA）一文中提出的、用于提升数据有限时 GAN 的性能的、可鉴别数据增强（Differentiable Augmentation，DiffAugment）的进一步研究（消融实验）。结果表明，需要同时对真实数据、生成数据以及生成器进行增强。</p> 
<p class="img-center"><img alt="" height="60" src="https://images2.imgbox.com/d8/df/XKLq6rkD_o.png" width="764"></p> 
<h3 id="S.2%20%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%C2%A0">13.2 网络结构 </h3> 
<p><img alt="" height="106" src="https://images2.imgbox.com/ae/e6/mz83esbu_o.png" width="756"></p> 
<p>DiffAugment 网络结构如上图所示。</p> 
<ul><li>固定 G，鉴别器学习对变换后真实数据有高鉴别分数，对生成数据有低鉴别分数。</li><li>固定 D，生成器学习生成经过变换后能骗过鉴别器的图像。</li></ul> 
<p>变换 T 必须是相同的（随机）函数，但在上图所示的三个位置不一定是相同的随机变换（例如，平移、剪切、颜色变换）。 </p> 
<h3 id="S.3%20%E5%88%86%E6%9E%90">13.3 分析</h3> 
<p>如果只增强真实数据，则生成器学习到的分布式是经过变换的真实图像分布。</p> 
<p>如果只增强鉴别器（增强真实数据和生成数据），则生成器学习到的是生成未经变换的的数据来骗过鉴别器。这会打破生成器与鉴别器的平衡，导致收敛性差，从而使性能恶化，因为它们正在优化完全不同的目标。</p> 
<p>因此，需要同时对真实数据、生成数据以及生成器进行增强。</p> 
<h2 id="14.%C2%A0Masked%20Generative%20Adversarial%20Networks%20are%20Data-Efficient%20Generation%20Learners%C2%A0">14. <strong>Masked Generative Adversarial Networks are Data-Efficient Generation Learners</strong> </h2> 
<p><a href="https://blog.csdn.net/qq_44681809/article/details/130888915" title="（2022，MaskedGAN）掩蔽的生成对抗网络是数据高效生成学习者_EDPJ的博客-CSDN博客">（2022，MaskedGAN）掩蔽的生成对抗网络是数据高效生成学习者_EDPJ的博客-CSDN博客</a></p> 
<h3 id="14.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3">14.1 核心思想</h3> 
<p>为了在训练数据有限时，稳健的进行图像生成学习器，本文提出了掩蔽的生成对抗网络 (MaskedGAN) 。 MaskedGAN 的思想很简单：随机屏蔽掉某些图像信息。</p> 
<p>作者开发了两种沿掩蔽策略：</p> 
<ul><li>移位空间掩蔽，它在空间维度上用随机移位掩蔽图像。</li><li>平衡谱掩蔽，它用自适应概率掩蔽某些图像谱带。</li><li>这两种方法是正交的（空间维度和谱维度），可以同时用于提升模型性能。</li></ul> 
<p class="img-center"><img alt="" height="83" src="https://images2.imgbox.com/72/49/9l7SvgpB_o.png" width="716"></p> 
<p>其中 M(·) = M_spectral ( M_spatial(·)) 是偏移空间掩蔽和平衡谱掩蔽的组合。 </p> 
<h3 id="S.2%20%E6%B5%81%E7%A8%8B">14.2 流程</h3> 
<p><strong>空间移位掩蔽</strong>：它通过随机移位在空间维度上掩蔽图像。 直观地，沿空间维度的随机掩蔽强制鉴别器学习所有图像位置，而不是仅仅关注易于区分的位置。</p> 
<p>相比于不需要高精度空间维度的 MAE（把图像块编码为特征向量，块内像素被认为是相同的，mask 只出现在图像的固定位置），GAN 需要高保真的重建图像，因此需要随机移位来学习所有图像位置。</p> 
<p><strong>平衡谱掩蔽</strong>：将图像分解为谱空间中的多个波段，并以自适应概率掩蔽部分谱带。 沿谱维度的随机掩蔽鼓励鉴别器学习难以区分的波段（例如，捕捉形状和结构的高频波段），而不是主要学习简单波段（例如，捕捉颜色和亮度的低波段）。</p> 
<p>图像内容通常沿谱维度分布不均匀：在低频段周围分布非常密集，但在高频段非常稀疏。因此，谱维度的均匀掩蔽是不平衡的并且不能很好地用于图像生成。所以需要<strong>自适应掩蔽</strong>策略。平衡谱掩蔽将图像分解为谱空间中的多个波段，并以自适应概率掩蔽一部分谱带，即如果一个谱带包含更多（或更少）的内容，则以更高（或更低）的概率被掩蔽。</p> 
<p>先使用傅立叶变换把图像分解为多波段 representation，每个波段内容在所有波段内容之和中的占比就是该波段内容被掩蔽的概率。</p> 
<h3 id="14.3%20%E5%88%86%E6%9E%90">14.3 分析</h3> 
<p><strong>应用于生成器和鉴别器</strong>。</p> 
<p>应用随机空间和谱掩蔽迫使鉴别器学习更全面的信息，而不是仅关注易于区分的图像位置和谱。</p> 
<p>这两种掩蔽策略用于生成器。 不同的是，在生成器端，两个操作通过稀疏化从鉴别器反向传播的训练信号来作用于输出。 这种设计通过保持判别器和生成器的相似学习速度来减慢生成器学习并稳定整体对抗性学习。 它确保 MaskedGAN 在特定条件下可以收敛到局部纳什均衡。</p> 
<p><strong>与 ADA 和 DA 中的 cutout 的区别</strong>。掩蔽也可以被看作是一种数据增强方式。与 cutout 的区别在于</p> 
<ul><li>首先，如果在没有 padding 的情况下进行 “<strong>cutout </strong>”，它会以较高的概率掩蔽中心位置/像素，同时以较低的概率掩蔽边缘位置/像素，从而导致图像掩蔽偏差和图像生成质量下降。</li><li>其次，如果用 padding 进行 “<strong>cutout </strong>”，它可以以相同的概率屏蔽每个位置，但是它的屏蔽比率不固定且不稳定，即，当 cutout mask 从中心位置移动到角落位置时，整体屏蔽比率从 p_mask（cutout 大小与整个图像之间的比率）下降到 0，这可能会使整体训练不稳定并导致性能下降。</li></ul> 
<h2 id="15.%C2%A0FreGAN_Exploiting%20Frequency%20Components%20for%20Training%20GANs%20under%20Limited%20Data%C2%A0">15. <strong>FreGAN_Exploiting Frequency Components for Training GANs under Limited Data</strong> </h2> 
<p><a href="https://blog.csdn.net/qq_44681809/article/details/130901414" title="（2022，FreGAN）利用频率分量在有限数据下训练 GAN_EDPJ的博客-CSDN博客">（2022，FreGAN）利用频率分量在有限数据下训练 GAN_EDPJ的博客-CSDN博客</a></p> 
<h3 id="15.1%20%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3">15.1 核心思想</h3> 
<p>为了提高数据有限情况下 GAN 的合成性能，作者考虑使用图像的频率信息。</p> 
<p>GAN 在拟合数据分布时，往往倾向于拟合低频信息（例如，颜色和亮度）而忽略高频信息（边缘等精细细节）。</p> 
<p>本文提出了一种称为 FreGAN 的频率感知模型，以提高 G 和 D 的频率感知能力。通过鼓励 G 生成更合理高频信号，从而提高合成性能。</p> 
<h3 id="15.2%20%E6%96%B9%E6%B3%95">15.2 方法</h3> 
<p><img alt="" height="797" src="https://images2.imgbox.com/c3/60/375GYEiq_o.png" width="1200"></p> 
<p>FreGAN 的总体框架如上图所示。先利用 Haar 小波变换将特征分解为不同的频率分量。 然后，使用高频鉴别器 (HFD) 和跳频连接 (FSC) 分别提高 D 和 G 的频率意识，并使用高频对齐 (HFA) 进一步引导 G 合成合理的频率信号。 </p> 
<p><strong>小波变换</strong>。与在图像级别使用的传统小波变换不同，作者对 D 和 G 的中间特征执行它。</p> 
<p><strong>高频鉴别器（HFD）</strong>。从频域区分真实图像和虚假图像，如公式 1 和 2 所示。（对于公式 2，我认为因为作者的失误，而少添加一个负号）</p> 
<p><img alt="" height="163" src="https://images2.imgbox.com/be/f5/TevZA6Mf_o.png" width="1200"></p> 
<p>随着网络的加深，D 可能会避开高频信息，因此需要在鉴别器的多层上执行多尺度 HFD，保证有限数据中频率信息被充分挖掘和利用，进一步提高 D 的频率感知能力。 </p> 
<p><strong>跳频连接（FSC）</strong>。GAN 从低频到高频拟合频率信号，随着网络的加深，高频信号可能会被忽略。为了防止高频信息的丢失并进一步鼓励生成器产生丰富的细节，提出了跳频连接（FSC）。</p> 
<p>具体操作是，把 G 每一层重建的频率表示（representation）送入的下一层。</p> 
<p><strong>高频对齐（HFA）</strong>。G 可以合成任意频率信号，为引导 G 利用频率信息，为损失函数添加如下正则化项，使用 D 中间特征频率表示的高频分量监督 G 生成合理的中间特征频率表示的高频分量。</p> 
<p class="img-center"><img alt="" height="36" src="https://images2.imgbox.com/c0/4b/1MATkOZW_o.png" width="772"></p> 
<p>与 HFD 类似，同样在多层上执行多尺度 HFA。 </p> 
<h3 id="S.3%20%E4%BC%98%E5%8C%96">15.3 优化</h3> 
<p>基础损失函数如公式 5 和 6 所示。</p> 
<p><img alt="" height="97" src="https://images2.imgbox.com/58/49/zbH6EFh8_o.png" width="1039"></p> 
<p>还使用重建损失来鼓励鉴别器提取更具代表性的特征。</p> 
<p><img alt="" height="41" src="https://images2.imgbox.com/b1/3c/LwV2M8PY_o.png" width="890"></p> 
<p>其中，f 是 D 的中间特征，G 和 T 表示对特征 f 和输入图像 x 的处理。 判别器和生成器分别通过以下方式优化，每个损失的系数设置为1。</p> 
<p class="img-center"><img alt="" height="37" src="https://images2.imgbox.com/30/47/RR1t3R7K_o.png" width="269"></p> 
<p class="img-center"><img alt="" height="41" src="https://images2.imgbox.com/d4/c3/IfYLmpue_o.png" width="246"></p> 
<h2 id="16.%C2%A0High-frequency%20Component%20Helps%20Explain%20the%20Generalization%20of%20Convolutional%20Neural%20Networks">16. <strong>High-frequency Component Helps Explain the Generalization of Convolutional Neural Networks</strong></h2> 
<p><a href="https://blog.csdn.net/qq_44681809/article/details/130908329" title="（2020，高频泛化）高频成分有助于解释卷积神经网络的泛化_EDPJ的博客-CSDN博客">（2020，高频泛化）高频成分有助于解释卷积神经网络的泛化_EDPJ的博客-CSDN博客</a></p> 
<h3>16.1 核心思想</h3> 
<p>本文研究图像频谱与 CNN 泛化能力之间的关系。作者认为，CNN 的非直觉泛化行为是人类与模型之间感知差异的直接结果：CNN 可以捕捉到人类无法察觉的高频分量。</p> 
<h3 id="S.2%20%E7%A0%94%E7%A9%B6%E6%96%B9%E6%B3%95">16.2 研究方法</h3> 
<p>结合使用傅里叶变换和滤波器，获得图像的低频分量（LFC）和高频分量（HFC）。然后，使用傅里叶逆变换对分量进行重建，使用这些重建图像作为测试集验证模型的精度。</p> 
<p>请注意，傅里叶变换或其逆变换可能会引入复数。在本文中，简单地丢弃了傅里叶逆变换结果的虚部，以确保生成的图像可以像往常一样输入 CNN。</p> 
<h3>16.3 分析</h3> 
<p>研究表明，CNN 更喜欢学习恰好与人类感知偏好一致的 LFC。虽然有解释表明神经网络倾向于更简单的功能，但作者推测这是因为，由于数据集是由人类组织和注释的，因此数据集中 低频-标签 组合比 高频-标签 组合更“普遍”。</p> 
<p>本文我赞同的一段话“最后，我们想提出一个问题：LFC 中的网络偏好与人类感知偏好之间的巧合，可能是许多技术在攀登最先进阶梯的过程中，发明的 “幸存偏差” 的简单结果。 换句话说，神经网络近 100 年的发展过程就像技术的 “自然选择”。 幸存下来的想法可能恰好符合人类的喜好，否则可能会因为爬梯不力而连发表都不能。”</p> 
<h3 id="S.4%20%E5%90%AF%E5%8F%91%E5%BC%8F">16.4 启发式</h3> 
<p>作者研究不同启发式对泛化性能的影响。这里只列出来影响显著的两个。</p> 
<p><strong>Batch 大小</strong>。较大的 batch 有较小的泛化差距，而此时，模型学到的高频成分并不多。观察到的关系是直观的，因为一旦模型表现得像人一样（注释数据的是人，人无法观察高频成分），就会实现最小的泛化差距。</p> 
<p><strong>BatchNorm</strong>。 在有 BatchNorm 的条件下，模型可以利用更多的高频成分。BatchNorm 的优势之一是通过归一化来对齐不同预测信号的分布差异。 例如，HFC 通常显示出比 LFC 更小的幅度，因此在没有 BatchNorm 的情况下训练的模型可能不容易拾取这些 HFC。</p> 
<h3 id="S.5%20%E7%A8%B3%E5%81%A5%E6%80%A7%E4%B8%8E%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%B9%B3%E6%BB%91%E5%BA%A6">16.5 稳健性与卷积核平滑度</h3> 
<p>稳健性强的模型往往具有平滑的卷积核。如果模型通过高频分量进行预测，那么对高频分量的微小扰动就会对模型结果产生巨大影响。</p> 
<p>如果卷积核是“光滑”的，即相邻权重之间没有剧烈波动，则相应的频域将看到可以忽略不计的高频信号。</p> 
<p>使用平滑的卷积核，虽然会影响模型精度，但是可以提升稳健性。</p> 
<h2 id="17.%C2%A0Prototype%20Memory%20and%20Attention%20Mechanisms%20for%20Few%20Shot%20Image%20Generation%C2%A0">17. <strong>Prototype Memory and Attention Mechanisms for Few Shot Image Generation</strong> </h2> 
<p><a href="https://blog.csdn.net/qq_44681809/article/details/130920347" title="（2022，MoCA）Few-shot 图像生成的原型记忆（Prototype Memory）和注意力机制_EDPJ的博客-CSDN博客">（2022，MoCA）Few-shot 图像生成的原型记忆（Prototype Memory）和注意力机制_EDPJ的博客-CSDN博客</a></p> 
<h3>17.1 核心思想</h3> 
<p>人类的大脑中存在记忆原型（概念 / 特征，例如：人脸）的神经元簇，例如，“祖母神经元” 记忆祖母，“父亲神经元” 记忆父亲。</p> 
<p>受该研究启发，本文提出了记忆概念注意力（Memory Concept Attention，MoCA），用于提高 few-shot 图像生成质量。MoCA 只是一个模块，可以插入到 GAN 框架中任何现存的生成器架构层中。</p> 
<p>这些记忆原型是通过动量在线聚类（momentum online clustering）学习的，并通过基于记忆的注意力来利用。</p> 
<h3>17.2 方法</h3> 
<p><img alt="" height="799" src="https://images2.imgbox.com/4f/34/41bYJ8MW_o.png" width="1070"></p> 
<p>基于原型的记忆调制模块如上图所示，模块的输入 A 是 GAN 分层结构的特征图。使用如下函数把输入转换到低维空间，三项地输出分别对应于 attention 中的 query、key、value。</p> 
<p><img alt="" height="35" src="https://images2.imgbox.com/54/c4/LzJ7NR92_o.png" width="728"></p> 
<p><strong>原型记忆学习</strong>。Key Φ(·) 通过动量更新距离其最近的语义簇的记忆库中的原型。</p> 
<p class="img-center"><img alt="" height="31" src="https://images2.imgbox.com/fa/46/DJOA7e6o_o.png" width="593"></p> 
<p><img alt="" height="441" src="https://images2.imgbox.com/61/30/dSyZmXsf_o.png" width="1164"></p> 
<p><strong>记忆概念注意力（MoCA）</strong>。Query 通过余弦相似度选择记忆的距离最近的语义单元（一组原型单元的聚类均值）。相似性分数经 softmax 归一化之后，作为权重对该最近的语义单元加权，获得加权的记忆单元矩阵</p> 
<p class="img-center"><img alt="" height="26" src="https://images2.imgbox.com/1c/a9/PN0SuJRX_o.png" width="179"></p> 
<p><strong>空间上下文注意力（self-attention）</strong>。计算 query θ(A) 和 key Φ(A) 之间的亲和力图（相关度），对其使用softmax 归一化后，作为权重对 key Φ(A) 进行加权，获得空间上下文调制张量</p> 
<p class="img-center"><img alt="" height="26" src="https://images2.imgbox.com/3e/e9/nKEgVl72_o.png" width="174"></p> 
<p><strong>整合两条路径</strong>。最后，我们通过逐元素加法整合记忆 <strong>H</strong>_m 和空间上下文调制 <strong>H</strong>_s</p> 
<p class="img-center"><img alt="" height="27" src="https://images2.imgbox.com/71/6c/S3JtYDkl_o.png" width="169"></p> 
<p>然后通过 1x1 卷积 O(·) 将 H 从 embedding 空间转换回原始特征空间。 一个可学习的参数 γ 作为权重，加权后加回到输入激活中，即</p> 
<p class="img-center"><img alt="" height="33" src="https://images2.imgbox.com/fa/a6/WHQpUneR_o.png" width="210"></p> 
<p>然后输出到生成器的下一层。 </p> 
<h3 id="S.3%20%E9%99%90%E5%88%B6">17.3 限制</h3> 
<p>当底层数据集的多样性较低且基础网络相当大时，MoCA 缓存的概念可能会在生成过程中造成更多干扰，从而导致性能下降。 </p> 
<h3 id="S.4%20%E6%A6%82%E5%BF%B5%E7%B0%87%E5%88%86%E6%9E%90">17.4 概念簇分析</h3> 
<p>不同簇的 pattern 通常具有语义意义并且非常不同，即不同的簇存储不同的概念。</p> 
<p>通过将图像分解为关于 top-3 簇（基于特征图中每个像素与原型的亲和力）的不同二进制掩码，作者发现，对于大多数图像，前 2 个簇的概念通常与前景和背景信息相关，而第三个簇包含与图像中的高频细节相关的概念。这表明，使用 MoCA，图像合成也可以被视为从记忆中检索不同概念并将它们组合在一起以创建逼真的图像的过程。</p> 
<h2 id="18.%C2%A0Towards%20faster%20and%20stabilized%20gan%20training%20for%20high-fidelity%20few-shot%20image%20synthesis">18. <strong>Towards faster and stabilized gan training for high-fidelity few-shot image synthesis</strong></h2> 
<p><a href="https://blog.csdn.net/qq_44681809/article/details/130953201" title="（2021，FastGAN）用于高保真 few-shot 图像合成的更快、更稳定的 GAN 训练_EDPJ的博客-CSDN博客">（2021，FastGAN）用于高保真 few-shot 图像合成的更快、更稳定的 GAN 训练_EDPJ的博客-CSDN博客</a></p> 
<h3>18.1 核心思想</h3> 
<p>为了减小 GAN 的 few-shot 图像合成的计算成本，作者提出一种轻量化 GAN 结构（FastGAN），包含：一个跳跃层通道激励模块（skip-layer channel-wise excitation module，SLE），一个作为特征编码器的自监督鉴别器。这两种技术都需要对普通 GAN 进行微小改动，从而通过理想的即插即用特性增强 GAN 的实用性。</p> 
<p>FsatGAN 使用更小的模型（更少的参数）获得与 StyleGAN2 相比优越的性能，且收敛也更快。</p> 
<h3 id="S.2%20%E6%A8%A1%E5%9D%97%E6%9E%B6%E6%9E%84">18.2 模块架构</h3> 
<p><img alt="" height="709" src="https://images2.imgbox.com/8d/fc/uI803hrL_o.png" width="1200"></p> 
<p><strong>跳跃层通道激励 SLE（G）</strong>。该模块定义为：</p> 
<p class="img-center"><img alt="" height="30" src="https://images2.imgbox.com/0a/7b/VvC5qxye_o.png" width="589"></p> 
<p>它利用低尺度特征（激活，activation）来修改高尺度特征图上的通道响应。其中，x 和 y 分别是模块的输入和输出特征图。函数 F 包含对 x_low 的操作，Wi 表示要学习的模块权重。<img alt="" height="409" src="https://images2.imgbox.com/a6/c6/qBmzRymF_o.png" width="1200"></p> 
<p><strong>自监督鉴别器（D）</strong>。模块结构如图 4 所示。将 D 视为编码器并用解码器对其进行训练。 这种自动编码训练迫使 D 提取图像特征，解码器用该特征良好重建。 解码器与 D 一起优化重建损失，它只在真实样本上训练：</p> 
<p></p> 
<p><img alt="" height="41" src="https://images2.imgbox.com/9d/ca/079BuzMg_o.png" width="920"></p> 
<p>其中 f 是来自 D 的中间特征图，函数 G 包含对 f 和解码器的处理，函数 T 表示对来自真实图像 I_real 的样本 x 的处理（例如，裁剪。但不限于此）。  </p> 
<p></p> 
<h3 id="18.3%C2%A0%C2%A0Loss">18.3  Loss</h3> 
<p><img alt="" height="60" src="https://images2.imgbox.com/8b/ef/35UOExPA_o.png" width="837"></p> 
<h3 id="S.4%20%E5%88%86%E6%9E%90">18.4 分析</h3> 
<p><strong>SLE 与 ResBlock 的区别。</strong></p> 
<ul><li>相比于 ResBlock 将 skip-connection 实现为来自不同 conv 层的激活之间的逐元素加法（要求激活的空间维度相同），SLE 使用逐通道乘法，从而消除了繁重的卷积计算。</li><li>相比于 skip-connection 仅在同一分辨率内使用。SLE 在更大范围（例如，8x8 和 128x128、16x16 和 256x256）的分辨率之间执行跳跃连接，因为不再需要相等的空间维度。</li></ul> 
<p><strong>SLE 与其他技术的关系</strong>。</p> 
<ul><li>SLE 中的通道乘法与实例规范化（Instance Normalization）吻合，后者广泛用于风格迁移（style-transfer）。</li><li>SLE 像 StyleGAN 一样，使 G 能够自动分离内容（低级特征）和风格（高级特征）。</li></ul> 
<p><strong>自监督鉴别器与自动编码（AE）的区别</strong>。</p> 
<ul><li>AE 在从 D 学习到的隐空间上训练 G 作为解码器，或者把 D 的对抗训练作为 AE 训练之外的补充损失。</li><li>本文的模型是一个纯 GAN，具有更简单的训练模式，自编码训练只对 D 进行正则化，不涉及G。 </li></ul> 
<h2 id="19.%C2%A0Generalized%20One-shot%20Domain%20Adaptation%20of%20Generative%20Adversarial%20Networks">19. <strong>Generalized One-shot Domain Adaptation of Generative Adversarial Networks</strong></h2> 
<p><a href="https://blog.csdn.net/qq_44681809/article/details/130970078" title="（2022，实体迁移）GAN 的通用 one-shot 域自适应_EDPJ的博客-CSDN博客">（2022，实体迁移）GAN 的通用 one-shot 域自适应_EDPJ的博客-CSDN博客</a></p> 
<h3>19.1 核心思想</h3> 
<p>作者认为从源域到目标域的适应可以分为两部分：全局风格（纹理和颜色等）的迁移，以及属于目标域的新实体（帽子、眼镜等配饰）的生成。采用二进制掩码（0/1 mask）的方法辅助实体生成。</p> 
<p>作者提出了一个新颖简洁的框架来解决风格和实体迁移的通用 one-shot 域自适应任务。</p> 
<p>作者通过 Sliced Wasserstein 距离（SWD）来限制参考和合成的内部分布之间的差距。单个图像的内部 patch 分布包含丰富的意义。</p> 
<p>作者使用变分拉普拉斯正则化实现跨域对应。</p> 
<h3 id="S.2%20%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84">19.2 网络架构</h3> 
<p><img alt="" height="650" src="https://images2.imgbox.com/29/6d/dayVVSLD_o.png" width="1200"></p> 
<p>本文使用的模型架构如上图所示，作者在生成器中添加了一个辅助网络（aux），用于对参考图像的实体进行二进制掩码（标记实体位置）操作。 </p> 
<h3 id="S.3%20LOSS">19.3 LOSS</h3> 
<p>本文使用的完整 loss 函数如公式 3 所示。 </p> 
<p><img alt="" height="40" src="https://images2.imgbox.com/0c/c3/Q5nkE8YM_o.png" width="678"></p> 
<p><strong>重建损失 L_rec</strong>。重建损失由公式 4 表示。该损失由参考图像与其在目标域的重建图像（通过隐空间 W）之间的负结构相似性度量（negative structural similarity metric）、感知损失 lpips、以及参考图像与其在目标域的上采样重建掩码之间的均方误差组成。</p> 
<ul><li>第一项是为了使经由相同的隐编码 w 重建的图像具有相似的结构，从而保证参考图像与其在源域的重建图像 x_rec 有相似的结构。</li><li>第二项用于缩小参考图像与重建图像的差异。</li><li>第三项用于正确标注实体位置。</li></ul> 
<p><img alt="" height="33" src="https://images2.imgbox.com/c1/e4/b41prM9v_o.png" width="792"></p> 
<p><strong>风格损失 L_style</strong>。风格损失由公式 6 表示。Wasserstein 距离 (SWD) 用于最小化合成图像和参考图像的内部分布的差异。Φ_style 是一组来自预训练 l_pips 网络的卷积层，用于提取空间特征。</p> 
<p><img alt="" height="130" src="https://images2.imgbox.com/11/08/4H7fTpfR_o.png" width="1200"></p> 
<p>生成器学习掩码图像 ^y_rec 的样式而不是 y_ref 的样式，以防止实体的样式泄漏到其他区域。</p> 
<p><strong>实体损失 L_ent</strong>。实体损失由公式 7 表示，它促使掩码后的重建图像与生成图像相似。</p> 
<p><img alt="" height="123" src="https://images2.imgbox.com/94/f2/CORQujRD_o.png" width="1200"></p> 
<p><strong>变分拉普拉斯正则化 L_VLapR</strong>。变分拉普拉斯由公式 9 表示。</p> 
<p class="img-center"><img alt="" height="129" src="https://images2.imgbox.com/b1/2a/ryzQG7la_o.png" width="918"></p> 
<p><img alt="" height="45" src="https://images2.imgbox.com/3c/c7/JkDZf4vY_o.png" width="864"></p> 
<p>Φ 用于提取空间特征。在等式 9a 中，如果 w_i 和 w_j 在隐空间中很接近，w_(i,j) 将是一个大标量，促使</p> 
<p class="img-center"><img alt="" height="36" src="https://images2.imgbox.com/18/6c/d7Q72k71_o.png" width="476"></p> 
<p>相同。 这意味着自适应前后相邻合成图像的相对距离在特征空间中是等距的。最小化 L_VlapR 期望源域生成器 G_s 和目标域生成器 G_t 跨隐空间 W 的合成具有平滑的语义差异。</p> 
<h2 id="20.%C2%A0Improving%20GANs%20with%20A%20Dynamic%20Discriminator">20. <strong>Improving GANs with A Dynamic Discriminator</strong></h2> 
<p><a href="https://blog.csdn.net/qq_44681809/article/details/130991826" title="（2022，DynamicD）使用动态鉴别器改进 GAN_EDPJ的博客-CSDN博客">（2022，DynamicD）使用动态鉴别器改进 GAN_EDPJ的博客-CSDN博客</a></p> 
<h3>20.1 核心思想</h3> 
<p>在 GAN 训练过程中，虽然真实数据分布保持不变，但由于生成器的更新，合成分布不断变化，从而使判别器的二分类任务产生相应的变化。</p> 
<p>对此，作者提出了动态鉴别器（DynamicD），如图 2 所示。</p> 
<ul><li>当数据充足时，逐渐增大鉴别器网络的宽度，从而避免欠拟合；</li><li>当数据不足时，逐渐减小鉴别器网络的宽度，从而避免过拟合。</li></ul> 
<p><img alt="" height="269" src="https://images2.imgbox.com/93/d7/2HnK05xC_o.png" width="911"></p> 
<h3 id="S.2%20%E5%85%B7%E4%BD%93%E6%93%8D%E4%BD%9C">20.2 具体操作</h3> 
<p><strong>数据充足</strong>。应逐渐增大网络容量。从一个相对小的网络（原始网络的子集，例如，一半）开始训练，每几次迭代添加 αM 个新初始化的神经元，扩展系数 α 可以从 0.5 到 0 变化，M 为初始（小）网络的宽度。最大的网络与原始网络一致。</p> 
<p><strong>数据不足</strong>。应逐渐减小网络容量。通过随机删除一些神经元，来减小网络宽度。收缩系数 β 从 1.0 开始，然后逐渐下降到 0.5。</p> 
<p>与增加容量不同，作者发现减少所有层会使训练不稳定，尤其是通常包含较少神经元的较低层。 因此，作者排除了这些低级层的缩减方案。</p> 
<p>这种递减方案不同于标准的 Dropout，因为该方法形成了一个 “权重级别” 的 dropout，它由训练 batch 中的所有实例共享，而 Dropout 更像是 “特征级别” 的每个实例正则化器。 </p> 
<p>缩减策略不仅缩小了网络宽度，而且在一定程度上通过随机删除引入了多个鉴别器。分析表明，来自各种鉴别器的 representations 可以相互补充，防止严重记忆某种模式，即大大减轻过拟合问题。</p> 
<h3>20.3 分析</h3> 
<p>相比于基线模型（例如，StyleGAN2），本方法可以稍微加快收敛速度，通常需要更少的时间来达到相同的 FID。直观地理解就是，无论缩减还是扩张，需要训练的总参数量都比原模型小。</p> 
<p></p> 
<h2 id="%E8%A1%A5%E5%85%85%E7%9F%A5%E8%AF%86">补充知识</h2> 
<h2 id="5.%20%E8%A1%A5%E5%85%85%EF%BC%9A%E8%B0%B1%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%88spectral%20normalization%EF%BC%89">1. 谱归一化（spectral normalization）</h2> 
<h3 id="5.1%20%E7%90%86%E8%AE%BA%E4%BE%9D%E6%8D%AE">1.1 理论依据</h3> 
<p>利普希茨连续性（Lipschitz continuity）是形容一个函数“好”的特性。以一维函数为例，如果该函数是利普希茨连续的，那我我们可以找到一个圆锥体，以函数图像上每个点为中心的圆锥体，都使函数图像位于该圆锥体之外。如下图所示，</p> 
<div> 
 <p><img alt="" height="300" src="https://images2.imgbox.com/56/f7/CJ6uR0D8_o.jpg" width="598">​</p> 
</div> 
<p>如果一个一维函数是可微的，那么它的利普希茨常数（Lipschitz constant）K是其导数的最大值。利普希茨连续性要求K是有界量，这就限制了鉴别器的梯度，从而解决了gradient descent过程中梯度爆炸的问题。</p> 
<p>一般可微函数的利普希茨常数是其最大奇异值（largest singular value）或谱范数（spectral norm）。</p> 
<p>谱归一化的前提是：对于任意多层的鉴别器（可能是线性映射与非线性分量的复合函数），可以找到利普希茨常数或者其上界。</p> 
<h3 id="5.2%20%E7%AE%97%E6%B3%95">1.2 算法</h3> 
<p>随机初始化两个向量<img alt="u,v" src="https://images2.imgbox.com/40/7d/G7ZGWV8p_o.png">，令网络权重<img alt="W \leftarrow W/\sigma (W)" src="https://images2.imgbox.com/5f/2e/fuzwjEuV_o.png">，其中，<img alt="\sigma (W)" src="https://images2.imgbox.com/73/6e/tQF4Jt59_o.png">是W的最大奇异值，计算它的操作叫做power iteration，操作如下：</p> 
<p><img alt="\begin{array}{l} \mathop u\nolimits_{t + 1} = W\mathop v\nolimits_t \\ \mathop v\nolimits_{t + 1} = W\mathop u\nolimits_t \\ \sigma (W) = \mathop u\nolimits^T Wv \end{array}" src="https://images2.imgbox.com/bc/c0/pEF8qTci_o.png"></p> 
<p>经过多次迭代，<img alt="u,v" src="https://images2.imgbox.com/d8/dd/p7TVFWDx_o.png">将收敛为W的特征向量。</p> 
<p>该算法相比于梯度下降，计算量很小。</p> 
<h3 id="1.3%20%E5%8F%82%E8%80%83%E2%80%8B">1.3 参考​</h3> 
<p><a href="https://zhuanlan.zhihu.com/p/68407048" rel="nofollow" title="Spectral Normalization for GAN - 知乎">Spectral Normalization for GAN - 知乎</a></p> 
<p><a href="https://christiancosgrove.com/blog/2018/01/04/spectral-normalization-explained.html" rel="nofollow" title="Spectral Normalization Explained">Spectral Normalization Explained</a></p> 
<p>Miyato T, Kataoka T, Koyama M, et al. Spectral normalization for generative adversarial networks[J]. arXiv preprint arXiv:1802.05957, 2018. 下载地址：<a href="https://arxiv.org/abs/1802.05957" rel="nofollow" title="[1802.05957] Spectral Normalization for Generative Adversarial Networks (arxiv.org)">[1802.05957] Spectral Normalization for Generative Adversarial Networks (arxiv.org)</a></p> 
<h2 id="2.%20%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%EF%BC%88Fusion%EF%BC%89">2. 特征融合（Fusion）</h2> 
<p>在深度学习的很多工作中（例如目标检测、图像分割），<strong>融合不同尺度的特征</strong>是提高性能的一个重要手段。<strong>低层</strong>特征<strong>分辨率更高</strong>，包含<strong>更多位置、细节信息</strong>，但是由于经过的<strong>卷积更少</strong>，其<strong>语义性更低，噪声更多</strong>。<strong>高层</strong>特征具有<strong>更强的语义信息，</strong>但是<strong>分辨率很低</strong>，<strong>对细节的感知能力较差</strong>。如何将两者高效融合，取其长处，弃之糟泊，是改善分割模型的关键。</p> 
<p>很多工作通过融合多层来提升检测和分割的性能，按照融合与预测的先后顺序，分类为<strong>早融合(Early fusion)和晚融合(Late fusion)。</strong></p> 
<ul><li><strong>早融合(Early fusion):</strong> 先融合多层的特征，然后在融合后的特征上训练预测器（<strong>只在完全融合之后，才统一进行检测</strong>）。这类方法也被称为<strong>skip connection，</strong>即采用<strong>concat</strong>、<strong>add</strong>操作。这一思路的代表是Inside-Outside Net(ION)和HyperNet。两个经典的特征融合方法： 
  <ul><li><strong>concat</strong>：系列特征融合，直接将两个特征进行连接。两个输入特征x和y的维数若为p和q，输出特征z的维数为p+q；</li><li><strong>add</strong>：并行策略，将这两个特征向量组合成复向量，对于输入特征x和y，z = x + iy，其中i是虚数单位。</li></ul></li><li><strong>晚融合(Late fusion)：</strong>通过结合不同层的检测结果改进检测性能（<strong>尚未完成最终的融合之前，在部分融合的层上就开始进行检测，会有多层的检测，最终将多个检测结果进行融合</strong>）。这一类研究思路的代表有两种： 
  <ul><li><strong>feature不融合，多尺度的feture分别进行预测，然后对预测结果进行融合，</strong>如Single Shot MultiBox Detector (SSD) , Multi-scale CNN(MS-CNN)</li><li><strong>feature进行金字塔融合，融合后进行预测</strong>，如Feature Pyramid Network(<strong>FPN</strong>)等。</li></ul></li></ul> 
<p>参考：<a href="https://zhuanlan.zhihu.com/p/141685352" rel="nofollow" title="盘点目标检测中的特征融合技巧（根据YOLO v4总结） - 知乎 (zhihu.com)">盘点目标检测中的特征融合技巧（根据YOLO v4总结） - 知乎 (zhihu.com)</a> </p> 
<h2 id="1.%20%E5%8F%8D%E5%8D%B7%E7%A7%AF">3. 反卷积</h2> 
<p>反卷积的英文：Deconvolution、Transposed Convolution 或 Fractional-strided Convolutions。</p> 
<p>反卷积是从低分辨率映射到大分辨率的过程，用于扩大图像尺寸。</p> 
<p>反卷积是一种特殊的正向卷积，而不是卷积的反过程。</p> 
<p>反卷积的具体操作：为输入大小为 n*n 的图像的相邻像素之间添加 n-1 个零元素，则获得大小为 （2n-1）*（2n-1）的图像。对该图像进行常规卷积操作即可。</p> 
<h2 id="2.%20%E6%89%A9%E5%B1%95%20latent%20space%20W%2B">4. 仿射变换（Affine）</h2> 
<p>以矩阵乘法(线性变换)和向量加法(平移变换)的形式表示的一种变换。</p> 
<p>可以用仿射变换来表示：旋转（线性变换）、尺度变化(线性变换)、平移(向量加法) </p> 
<h2 id="4.%20%E7%99%BD%E5%8C%96%EF%BC%88Whitening%EF%BC%89">5. 白化（Whitening）</h2> 
<p>Whitening 的目的是去掉数据之间的相关联度，是很多算法进行预处理的步骤。比如说当训练图片数据时，由于图片中相邻像素值有一定的关联，所以很多信息是冗余的。这时候去相关的操作就可以采用白化操作。数据的 whitening 必须满足两个条件：一是不同特征间相关性最小，接近0；二是所有特征的方差相等（不一定为1）。常见的白化操作有PCA whitening和ZCA whitening。</p> 
<p>PCA whitening 是指，将数据 x 经过 PCA 降维为 z 后，可以看出 z 中每一维是独立的，满足whitening 的第一个条件。此时，只需要将 z 中的每一维都除以标准差就得到了每一维的方差为1，也就是说方差相等。</p> 
<p>ZCA whitening 是指数据 x 先经过 PCA 变换为 z，但是并不降维，因为这里是把所有的成分都选进去了。此时，也同样满足 whtienning 的第一个条件，特征间相互独立。然后同样进行方差为 1 的操作，最后将得到的矩阵左乘一个特征向量矩阵 U 即可。</p> 
<p></p> 
<h2 id="%E5%BE%80%E6%9C%9F%E6%80%BB%E7%BB%93%EF%BC%9A">往期总结</h2> 
<p><a href="https://blog.csdn.net/qq_44681809/article/details/129753762?ydreferer=aHR0cHM6Ly9tcC5jc2RuLm5ldC9tcF9ibG9nL21hbmFnZS9hcnRpY2xlP3NwbT0zMDAxLjUyOTg%3D" title="近期学习论文总结 1（GAN，latent space 相关）_EDPJ的博客-CSDN博客">近期学习论文总结 1（GAN，latent space 相关）_EDPJ的博客-CSDN博客</a></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/bff877eff092373e1ddac8835e063a5e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">c&#43;&#43;基础编程学习（一）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9985a4dab6a6400912d6d7681b112949/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Vue.config.js 配置项</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>