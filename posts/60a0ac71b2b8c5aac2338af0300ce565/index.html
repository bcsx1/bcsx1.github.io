<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Spark&#43;Flink&#43;Iceberg打造湖仓一体架构实践探索 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Spark&#43;Flink&#43;Iceberg打造湖仓一体架构实践探索" />
<meta property="og:description" content="数据湖-大数据生态杀青 数据仓库的痛点 只能存储结构化数据，无法采集存储非结构化数据无法存储原始数据，所有的数据须经过ETL清洗过程离线数仓的数据表牵一发而动全身，数据调整工程量大实时数仓存储空间有限，无法采集和存储海量实时数据回溯效率低下，实时数据和离线数据计算接口难以统一Kafka 做实时数仓，以及日志传输。Kafka 本身存储成本很高，且数据保留时间有时效性，一旦消费积压，数据达到过期时间后，就会造成数据丢失且没有消费到将实时要求不高的业务数据入湖、比如说能接受 1-10 分钟的延迟。因为 Iceberg 0.11 也支持 SQL 实时读取，而且还能保存历史数据。这样既可以减轻线上 Kafka 的压力，还能确保数据不丢失的同时也能实时读取 数据湖三剑客对比 Hudi Hudi：Hadoop Upserts Deletes and Incrementals（原为 Hadoop Upserts anD Incrementals），强调了其主要支持 Upserts、Deletes 和 Incremental 数据处理，其主要提供的写入工具是 Spark HudiDataSource API 和自身提供的 HoodieDeltaStreamer在查询方面，Hudi 支持 Hive、Spark、Presto。在性能方面，Hudi 设计了 HoodieKey ，一个类似于主键的东西。对于查询性能，一般需求是根据查询谓词生成过滤条件下推至 datasource。Hudi 这方面没怎么做工作，其性能完全基于引擎自带的谓词下推和 partition prune 功能。 Delta Delta定位是流批一体的 Data Lake 存储层，支持 update/delete/merge。不强调主键，因此其 update/delete/merge 的实现均是基于 spark 的 join 功能。在数据写入方面，Delta 与 Spark 是强绑定的，这一点 Hudi 是不同的：Hudi 的数据写入不绑定 Spark（可以用 Spark，也可以使用 Hudi 自己的写入工具写入）在查询方面，开源 Delta 目前支持 Spark 与 Presto，但是，Spark 是不可或缺的，因为 delta log 的处理需要用到 Spark。这意味着如果要用 Presto 查询 Delta，查询时还要跑一个 Spark 作业 Iceberg Iceberg一个通用化设计的Table Format，高性能的分析与可靠的数据管理，Iceberg 没有类似的 HoodieKey 设计，其不强调主键。上文已经说到，没有主键，做 update/delete/merge 等操作就要通过 Join 来实现，而 Join 需要有一个 类似 SQL 的执行引擎。Iceberg 在查询性能方面做了大量的工作。值得一提的是它的 hidden partition 功能。Hidden partition 意思是说，对于用户输入的数据，用户可以选取其中某些列做适当的变换（Transform）形成一个新的列作为 partition 列。这个 partition 列仅仅为了将数据进行分区，并不直接体现在表的 schema 中。 总结 Delta、Hudi、Iceberg三个开源项目中，Delta和Hudi跟Spark的代码深度绑定，尤其是写入路径。这两个项目设计之初，都基本上把Spark作为他们的默认计算引擎了。而Apache Iceberg的方向非常坚定，宗旨就是要做一个通用化设计的Table Format。它完美的解耦了计算引擎和底下的存储系统，便于多样化计算引擎和文件格式，很好的完成了数据湖架构中的Table Format这一层的实现，因此也更容易 成为Table Format层的开源事实标准Apache Iceberg也在朝着流批一体的数据存储层发展，manifest和snapshot的设计，有效地隔离不同transaction的变更 ，非常方便批处理和增量计算。并且，Apache Flink已经是一个流批一体的计算引擎，二都可以完美匹配，合力打造流批一体的数据湖架构。 Iceberg术语 数据文件 ( data files )" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/60a0ac71b2b8c5aac2338af0300ce565/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-03-16T18:04:49+08:00" />
<meta property="article:modified_time" content="2022-03-16T18:04:49+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Spark&#43;Flink&#43;Iceberg打造湖仓一体架构实践探索</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h5><a id="httpsimgblogcsdnimgcn8b82532fdf704a6e8ee31079beb7fd96pngxossprocessimagewatermarktype_d3F5LXplbmhlaQshadow_50text_Q1NETiBA5p625p6E5biI6ICB54u8size_20color_FFFFFFt_70g_sex_16_0"></a>数据湖-大数据生态杀青<img src="https://images2.imgbox.com/0a/c5/VHJWaiil_o.png" alt="数据湖-大数据生态杀青"></h5> 
<h5><a id="_1"></a>数据仓库的痛点</h5> 
<ul><li>只能存储结构化数据，无法采集存储非结构化数据</li><li>无法存储原始数据，所有的数据须经过ETL清洗过程</li><li>离线数仓的数据表牵一发而动全身，数据调整工程量大</li><li>实时数仓存储空间有限，无法采集和存储海量实时数据</li><li>回溯效率低下，实时数据和离线数据计算接口难以统一</li><li>Kafka 做实时数仓，以及日志传输。Kafka 本身存储成本很高，且数据保留时间有时效性，一旦消费积压，数据达到过期时间后，就会造成数据丢失且没有消费到</li><li>将实时要求不高的业务数据入湖、比如说能接受 1-10 分钟的延迟。因为 Iceberg 0.11 也支持 SQL 实时读取，而且还能保存历史数据。这样既可以减轻线上 Kafka 的压力，还能确保数据不丢失的同时也能实时读取</li></ul> 
<h5><a id="_9"></a>数据湖三剑客对比</h5> 
<h6><a id="Hudi_10"></a>Hudi</h6> 
<ul><li>Hudi：Hadoop Upserts Deletes and Incrementals（原为 Hadoop Upserts anD Incrementals），强调了其主要支持 Upserts、Deletes 和 Incremental 数据处理，其主要提供的写入工具是 Spark HudiDataSource API 和自身提供的 HoodieDeltaStreamer</li><li>在查询方面，Hudi 支持 Hive、Spark、Presto。</li><li>在性能方面，Hudi 设计了 HoodieKey ，一个类似于主键的东西。对于查询性能，一般需求是根据查询谓词生成过滤条件下推至 datasource。Hudi 这方面没怎么做工作，其性能完全基于引擎自带的谓词下推和 partition prune 功能。</li></ul> 
<h6><a id="Delta_14"></a>Delta</h6> 
<ul><li>Delta定位是流批一体的 Data Lake 存储层，支持 update/delete/merge。不强调主键，因此其 update/delete/merge 的实现均是基于 spark 的 join 功能。在数据写入方面，Delta 与 Spark 是强绑定的，这一点 Hudi 是不同的：Hudi 的数据写入不绑定 Spark（可以用 Spark，也可以使用 Hudi 自己的写入工具写入）</li><li>在查询方面，开源 Delta 目前支持 Spark 与 Presto，但是，Spark 是不可或缺的，因为 delta log 的处理需要用到 Spark。这意味着如果要用 Presto 查询 Delta，查询时还要跑一个 Spark 作业</li></ul> 
<h6><a id="Iceberg_17"></a>Iceberg</h6> 
<ul><li>Iceberg一个通用化设计的Table Format，高性能的分析与可靠的数据管理，Iceberg 没有类似的 HoodieKey 设计，其不强调主键。上文已经说到，没有主键，做 update/delete/merge 等操作就要通过 Join 来实现，而 Join 需要有一个 类似 SQL 的执行引擎。</li><li>Iceberg 在查询性能方面做了大量的工作。值得一提的是它的 hidden partition 功能。Hidden partition 意思是说，对于用户输入的数据，用户可以选取其中某些列做适当的变换（Transform）形成一个新的列作为 partition 列。这个 partition 列仅仅为了将数据进行分区，并不直接体现在表的 schema 中。</li></ul> 
<h6><a id="_20"></a>总结</h6> 
<ul><li>Delta、Hudi、Iceberg三个开源项目中，Delta和Hudi跟Spark的代码深度绑定，尤其是写入路径。这两个项目设计之初，都基本上把Spark作为他们的默认计算引擎了。而Apache Iceberg的方向非常坚定，宗旨就是要做一个通用化设计的Table Format。它完美的解耦了计算引擎和底下的存储系统，便于多样化计算引擎和文件格式，很好的完成了数据湖架构中的Table Format这一层的实现，因此也更容易 成为Table Format层的开源事实标准</li><li>Apache Iceberg也在朝着流批一体的数据存储层发展，manifest和snapshot的设计，有效地隔离不同transaction的变更 ，非常方便批处理和增量计算。并且，Apache Flink已经是一个流批一体的计算引擎，二都可以完美匹配，合力打造流批一体的数据湖架构。</li></ul> 
<h6><a id="Iceberg_23"></a>Iceberg术语</h6> 
<ul><li>数据文件 ( data files )<br> Iceberg 表真实存储数据的文件，一般存储在data目录下，以".parquet"结尾。</li><li>清单文件 ( Manifest file ）<br> 每行都是每个数据文件的详细描述，包括数据文件的状态、文件路径、分区信息、列级别的统计信息（比如每列的最大最小值、空值数等）、通过该文件、可过滤掉无关数据、提高检索速度。</li><li>快照（ Snapshot ）<br> 快照代表一张表在某个时刻的状态。每个快照版本包含某个时刻的所有数据文件列表。Data files 是存储在不同的 manifest files 里面， manifest files 是存储在一个 Manifest list 文件里面，而一个 Manifest list 文件代表一个快照。</li></ul> 
<h5><a id="spark__Iceberg_30"></a>spark + Iceberg离线数仓</h5> 
<ul><li> <p>前期准备<br> spark 3.0.0_scala_2.12<br> Iceberg 0.13.1<br> 编译好的iceberg-spark3-runtime-0.13.1.jar拷贝到spark/jars</p> </li><li> <p>DWD加载ods原始数据</p> </li></ul> 
<pre><code class="prism language-scala"><span class="token operator">&gt;</span>controller
<span class="token keyword">val</span> sparkConf <span class="token operator">=</span> <span class="token keyword">new</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>set<span class="token punctuation">(</span><span class="token string">"spark.sql.catalog.hadoop_prod"</span><span class="token punctuation">,</span> <span class="token string">"org.apache.iceberg.spark.SparkCatalog"</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>set<span class="token punctuation">(</span><span class="token string">"spark.sql.catalog.hadoop_prod.type"</span><span class="token punctuation">,</span> <span class="token string">"hadoop"</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>set<span class="token punctuation">(</span><span class="token string">"spark.sql.catalog.hadoop_prod.warehouse"</span><span class="token punctuation">,</span> <span class="token string">"hdfs://hadoop01:9820/spark/warehouse"</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>set<span class="token punctuation">(</span><span class="token string">"spark.sql.catalog.catalog-name.type"</span><span class="token punctuation">,</span> <span class="token string">"hadoop"</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>set<span class="token punctuation">(</span><span class="token string">"spark.sql.catalog.catalog-name.default-namespace"</span><span class="token punctuation">,</span> <span class="token string">"db"</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>set<span class="token punctuation">(</span><span class="token string">"spark.sql.sources.partitionOverwriteMode"</span><span class="token punctuation">,</span> <span class="token string">"dynamic"</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>set<span class="token punctuation">(</span><span class="token string">"spark.sql.session.timeZone"</span><span class="token punctuation">,</span> <span class="token string">"GMT+8"</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"dwd_app"</span><span class="token punctuation">)</span>
    <span class="token keyword">val</span> sparkSession <span class="token operator">=</span> SparkSession<span class="token punctuation">.</span>builder<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>config<span class="token punctuation">(</span>sparkConf<span class="token punctuation">)</span><span class="token punctuation">.</span>getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>
    DwdIcebergService<span class="token punctuation">.</span>readOdsData<span class="token punctuation">(</span>sparkSession<span class="token punctuation">)</span>
<span class="token operator">&gt;</span> service 
  <span class="token comment">// 加载member 到dwd</span>
  <span class="token keyword">def</span> loadMember<span class="token punctuation">(</span>sparkSession<span class="token operator">:</span> SparkSession<span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span> <span class="token operator">=</span><span class="token punctuation">{<!-- --></span>
    sparkSession<span class="token punctuation">.</span>read<span class="token punctuation">.</span>json<span class="token punctuation">(</span><span class="token string">"/datasource/iceberg/member.log"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>drop<span class="token punctuation">(</span><span class="token string">"dn"</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>withColumn<span class="token punctuation">(</span><span class="token string">"uid"</span><span class="token punctuation">,</span> col<span class="token punctuation">(</span><span class="token string">"uid"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cast<span class="token punctuation">(</span><span class="token string">"int"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>withColumn<span class="token punctuation">(</span><span class="token string">"ad_id"</span><span class="token punctuation">,</span> col<span class="token punctuation">(</span><span class="token string">"ad_id"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cast<span class="token punctuation">(</span><span class="token string">"int"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>writeTo<span class="token punctuation">(</span><span class="token string">"hadoop_prod.db.dwd_member"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>overwritePartitions<span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token punctuation">}</span>
</code></pre> 
<ul><li>DWS数据宽表</li></ul> 
<pre><code class="prism language-scala">  <span class="token keyword">def</span> getDwsMemberData<span class="token punctuation">(</span>sparkSession<span class="token operator">:</span> SparkSession<span class="token punctuation">,</span> dt<span class="token operator">:</span> <span class="token builtin">String</span><span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">import</span> sparkSession<span class="token punctuation">.</span>implicits<span class="token punctuation">.</span>_
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    <span class="token keyword">val</span> result <span class="token operator">=</span> dwdMember<span class="token punctuation">.</span>join<span class="token punctuation">(</span>dwdMemberRegtype<span class="token punctuation">.</span>drop<span class="token punctuation">(</span><span class="token string">"dt"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> Seq<span class="token punctuation">(</span><span class="token string">"uid"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"left"</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>join<span class="token punctuation">(</span>dwdPcentermempaymoney<span class="token punctuation">.</span>drop<span class="token punctuation">(</span><span class="token string">"dt"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> Seq<span class="token punctuation">(</span><span class="token string">"uid"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"left"</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>join<span class="token punctuation">(</span>dwdBaseAd<span class="token punctuation">,</span> Seq<span class="token punctuation">(</span><span class="token string">"ad_id"</span><span class="token punctuation">,</span> <span class="token string">"dn"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"left"</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>join<span class="token punctuation">(</span>dwdBaseWebsite<span class="token punctuation">,</span> Seq<span class="token punctuation">(</span><span class="token string">"siteid"</span><span class="token punctuation">,</span> <span class="token string">"dn"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"left"</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>join<span class="token punctuation">(</span>dwdVipLevel<span class="token punctuation">,</span> Seq<span class="token punctuation">(</span><span class="token string">"vip_id"</span><span class="token punctuation">,</span> <span class="token string">"dn"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"left_outer"</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>select<span class="token punctuation">(</span><span class="token string">"..."</span><span class="token punctuation">)</span><span class="token punctuation">.</span>as<span class="token punctuation">[</span>DwsMemberResult<span class="token punctuation">]</span>

    <span class="token keyword">val</span> resultData <span class="token operator">=</span> result<span class="token punctuation">.</span>groupByKey<span class="token punctuation">(</span>item <span class="token keyword">=&gt;</span> item<span class="token punctuation">.</span>uid <span class="token operator">+</span> <span class="token string">"_"</span> <span class="token operator">+</span> item<span class="token punctuation">.</span>dn<span class="token punctuation">)</span>
      <span class="token punctuation">.</span>mapGroups <span class="token punctuation">{<!-- --></span> <span class="token keyword">case</span> <span class="token punctuation">(</span>key<span class="token punctuation">,</span> iters<span class="token punctuation">)</span> <span class="token keyword">=&gt;</span>
        <span class="token keyword">val</span> keys <span class="token operator">=</span> key<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"_"</span><span class="token punctuation">)</span>
        <span class="token keyword">val</span> uid <span class="token operator">=</span> Integer<span class="token punctuation">.</span>parseInt<span class="token punctuation">(</span>keys<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">val</span> dn <span class="token operator">=</span> keys<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">val</span> dwsMembers <span class="token operator">=</span> iters<span class="token punctuation">.</span>toList
        <span class="token keyword">val</span> paymoney <span class="token operator">=</span> dwsMembers<span class="token punctuation">.</span>filter<span class="token punctuation">(</span>_<span class="token punctuation">.</span>paymoney <span class="token operator">!=</span> <span class="token keyword">null</span><span class="token punctuation">)</span>
          <span class="token punctuation">.</span>map<span class="token punctuation">(</span>item <span class="token keyword">=&gt;</span> BigDecimal<span class="token punctuation">.</span>apply<span class="token punctuation">(</span>item<span class="token punctuation">.</span>paymoney<span class="token punctuation">)</span><span class="token punctuation">)</span>
          <span class="token punctuation">.</span>reduceOption<span class="token punctuation">(</span>_ <span class="token operator">+</span> _<span class="token punctuation">)</span>
          <span class="token punctuation">.</span>getOrElse<span class="token punctuation">(</span>BigDecimal<span class="token punctuation">.</span>apply<span class="token punctuation">(</span><span class="token number">0.00</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>toString
   <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    <span class="token comment">// 分区列不能为null，spark-sql内存表null为字符串</span>
    resultData<span class="token punctuation">.</span>where<span class="token punctuation">(</span>$<span class="token string">"dn"</span> <span class="token operator">=</span><span class="token operator">!=</span> <span class="token string">"null"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    resultData<span class="token punctuation">.</span>where<span class="token punctuation">(</span>$<span class="token string">"dn"</span> <span class="token operator">=</span><span class="token operator">!=</span> <span class="token string">"null"</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>write<span class="token punctuation">.</span>format<span class="token punctuation">(</span><span class="token string">"iceberg"</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>mode<span class="token punctuation">(</span><span class="token string">"overwrite"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">"hadoop_prod.db.dws_member"</span><span class="token punctuation">)</span>
  <span class="token punctuation">}</span>
</code></pre> 
<ul><li>ADS统计分析</li></ul> 
<pre><code class="prism language-scala"> <span class="token keyword">def</span> queryDetails<span class="token punctuation">(</span>sparkSession<span class="token operator">:</span> SparkSession<span class="token punctuation">,</span> dt<span class="token operator">:</span> <span class="token builtin">String</span><span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">import</span> sparkSession<span class="token punctuation">.</span>implicits<span class="token punctuation">.</span>_
    <span class="token keyword">val</span> result <span class="token operator">=</span> DwsIcebergDao<span class="token punctuation">.</span>queryDwsMemberData<span class="token punctuation">(</span>sparkSession<span class="token punctuation">)</span><span class="token punctuation">.</span>as<span class="token punctuation">[</span>QueryResult<span class="token punctuation">]</span><span class="token punctuation">.</span>where<span class="token punctuation">(</span>s<span class="token string">"dt='${dt}'"</span><span class="token punctuation">)</span>
    result<span class="token punctuation">.</span>cache<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment">//统计根据url统计人数  wordcount</span>
    result<span class="token punctuation">.</span>mapPartitions<span class="token punctuation">(</span>partition <span class="token keyword">=&gt;</span> <span class="token punctuation">{<!-- --></span>
      partition<span class="token punctuation">.</span>map<span class="token punctuation">(</span>item <span class="token keyword">=&gt;</span> <span class="token punctuation">(</span>item<span class="token punctuation">.</span>appregurl <span class="token operator">+</span> <span class="token string">"_"</span> <span class="token operator">+</span> item<span class="token punctuation">.</span>dn <span class="token operator">+</span> <span class="token string">"_"</span> <span class="token operator">+</span> item<span class="token punctuation">.</span>dt<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">.</span>groupByKey<span class="token punctuation">(</span>_<span class="token punctuation">.</span>_1<span class="token punctuation">)</span>
      <span class="token punctuation">.</span>mapValues<span class="token punctuation">(</span>item <span class="token keyword">=&gt;</span> item<span class="token punctuation">.</span>_2<span class="token punctuation">)</span><span class="token punctuation">.</span>reduceGroups<span class="token punctuation">(</span>_ <span class="token operator">+</span> _<span class="token punctuation">)</span>
      <span class="token punctuation">.</span>map<span class="token punctuation">(</span>item <span class="token keyword">=&gt;</span> <span class="token punctuation">{<!-- --></span>
        <span class="token keyword">val</span> keys <span class="token operator">=</span> item<span class="token punctuation">.</span>_1<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"_"</span><span class="token punctuation">)</span>
        <span class="token keyword">val</span> appregurl <span class="token operator">=</span> keys<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token keyword">val</span> dn <span class="token operator">=</span> keys<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">val</span> dt <span class="token operator">=</span> keys<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
        <span class="token punctuation">(</span>appregurl<span class="token punctuation">,</span> item<span class="token punctuation">.</span>_2<span class="token punctuation">,</span> dt<span class="token punctuation">,</span> dn<span class="token punctuation">)</span>
      <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">.</span>toDF<span class="token punctuation">(</span><span class="token string">"appregurl"</span><span class="token punctuation">,</span> <span class="token string">"num"</span><span class="token punctuation">,</span> <span class="token string">"dt"</span><span class="token punctuation">,</span> <span class="token string">"dn"</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>writeTo<span class="token punctuation">(</span><span class="token string">"hadoop_prod.db.ads_register_appregurlnum"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>overwritePartitions<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment">// 统计各memberlevel等级 支付金额前三的用户: mysql、oracle、hive、phoenix、iceberg对where里都不支持开窗函数，spark内存函数强大</span>
    result<span class="token punctuation">.</span>withColumn<span class="token punctuation">(</span><span class="token string">"rownum"</span><span class="token punctuation">,</span> row_number<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>over<span class="token punctuation">(</span>Window<span class="token punctuation">.</span>partitionBy<span class="token punctuation">(</span><span class="token string">"memberlevel"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>orderBy<span class="token punctuation">(</span>desc<span class="token punctuation">(</span><span class="token string">"paymoney"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>where<span class="token punctuation">(</span><span class="token string">"rownum&lt;4"</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>orderBy<span class="token punctuation">(</span><span class="token string">"memberlevel"</span><span class="token punctuation">,</span> <span class="token string">"rownum"</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>select<span class="token punctuation">(</span><span class="token string">"..."</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span>writeTo<span class="token punctuation">(</span><span class="token string">"hadoop_prod.db.ads_register_top3memberpay"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>overwritePartitions<span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token punctuation">}</span>
</code></pre> 
<ul><li>yarn 上测试<br> 最后是花了 18 分钟跑完 1000 万条数据，查询表数据观察是否有数据丢失。数据没有丢失<br> <img src="https://images2.imgbox.com/df/0e/gFmHjtz9_o.png" alt="yarn集群测试"></li></ul> 
<h6><a id="FlinkIceberg__125"></a>Flink+Iceberg 流批一体架构</h6> 
<ul><li>前期准备<br> flink 1.13.0_scala_2.12<br> iceberg 0.13.1<br> 拷贝编译好的iceberg-flink-runtime-1.13-0.13.1.jar到flink/lib<br> 启动flink集群，运行flink sql：bin/sql-client.sh embedded shell</li><li>flink cdc采集数据到kafka，流模式写入iceberg</li></ul> 
<pre><code class="prism language-java">        StreamExecutionEnvironment env <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span><span class="token function">getExecutionEnvironment</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        env<span class="token punctuation">.</span><span class="token function">enableCheckpointing</span><span class="token punctuation">(</span><span class="token number">6000</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
        kafakSource<span class="token punctuation">.</span><span class="token function">setStartFromLatest</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        DataStream<span class="token generics function"><span class="token punctuation">&lt;</span>RowData<span class="token punctuation">&gt;</span></span> result <span class="token operator">=</span> env<span class="token punctuation">.</span><span class="token function">addSource</span><span class="token punctuation">(</span>kafakSource<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>item <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token punctuation">{<!-- --></span>
          <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
            rowData<span class="token punctuation">.</span><span class="token function">setField</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> uid<span class="token punctuation">)</span><span class="token punctuation">;</span>
            rowData<span class="token punctuation">.</span><span class="token function">setField</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> courseid<span class="token punctuation">)</span><span class="token punctuation">;</span>
            rowData<span class="token punctuation">.</span><span class="token function">setField</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> deviceid<span class="token punctuation">)</span><span class="token punctuation">;</span>
            rowData<span class="token punctuation">.</span><span class="token function">setField</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> StringData<span class="token punctuation">.</span><span class="token function">fromString</span><span class="token punctuation">(</span>array<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">trim</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token keyword">return</span> rowData<span class="token punctuation">;</span>
        <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        result<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">"&gt;&gt;&gt;处理完数据："</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        TableLoader testtopicTable <span class="token operator">=</span> TableLoader<span class="token punctuation">.</span><span class="token function">fromHadoopTable</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop01:9820/flink/warehouse/iceberg_db/dwd_view_log"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        FlinkSink<span class="token punctuation">.</span><span class="token function">forRowData</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">tableLoader</span><span class="token punctuation">(</span>testtopicTable<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        env<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<ul><li>批模式初始化加载数据</li></ul> 
<pre><code class="prism language-java">   DataStream<span class="token generics function"><span class="token punctuation">&lt;</span>RowData<span class="token punctuation">&gt;</span></span> batch <span class="token operator">=</span> FlinkSource<span class="token punctuation">.</span><span class="token function">forRowData</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">env</span><span class="token punctuation">(</span>env<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">tableLoader</span><span class="token punctuation">(</span>tableLoader<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">streaming</span><span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<ul><li>流模式增量处理数据</li></ul> 
<pre><code class="prism language-java">DataStream<span class="token generics function"><span class="token punctuation">&lt;</span>RowData<span class="token punctuation">&gt;</span></span> stream <span class="token operator">=</span> FlinkSource<span class="token punctuation">.</span><span class="token function">forRowData</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">env</span><span class="token punctuation">(</span>env<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">tableLoader</span><span class="token punctuation">(</span>tableLoader<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">streaming</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<ul><li>DataStream与Table转换写入iceberg</li></ul> 
<pre><code class="prism language-java">Table table <span class="token operator">=</span> dwsIcbergDao<span class="token punctuation">.</span><span class="token function">queryDwsMemberData</span><span class="token punctuation">(</span>env<span class="token punctuation">,</span> tableEnv<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">where</span><span class="token punctuation">(</span>$<span class="token punctuation">(</span><span class="token string">"dt"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">isEqual</span><span class="token punctuation">(</span>dt<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        DataStream<span class="token generics function"><span class="token punctuation">&lt;</span>QueryResult<span class="token punctuation">&gt;</span></span> queryResultDataStream <span class="token operator">=</span> tableEnv<span class="token punctuation">.</span><span class="token function">toAppendStream</span><span class="token punctuation">(</span>table<span class="token punctuation">,</span> QueryResult<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        tableEnv<span class="token punctuation">.</span><span class="token function">createTemporaryView</span><span class="token punctuation">(</span><span class="token string">"tmpA"</span><span class="token punctuation">,</span> queryResultDataStream<span class="token punctuation">)</span><span class="token punctuation">;</span>
        String sql <span class="token operator">=</span> <span class="token string">"select *from(select uid,memberlevel,register,appregurl"</span> <span class="token operator">+</span>
                <span class="token string">",regsourcename,adname,sitename,vip_level,cast(paymoney as decimal(10,4)),row_number() over"</span> <span class="token operator">+</span>
                <span class="token string">" (partition by memberlevel order by cast(paymoney as decimal(10,4)) desc) as rownum,dn,dt from tmpA where dt='"</span> <span class="token operator">+</span> dt <span class="token operator">+</span> <span class="token string">"') "</span> <span class="token operator">+</span>
                <span class="token string">" where rownum&lt;4"</span><span class="token punctuation">;</span>
        Table table1 <span class="token operator">=</span> tableEnv<span class="token punctuation">.</span><span class="token function">sqlQuery</span><span class="token punctuation">(</span>sql<span class="token punctuation">)</span><span class="token punctuation">;</span>
        DataStream<span class="token generics function"><span class="token punctuation">&lt;</span>RowData<span class="token punctuation">&gt;</span></span> top3DS <span class="token operator">=</span> tableEnv<span class="token punctuation">.</span><span class="token function">toRetractStream</span><span class="token punctuation">(</span>table1<span class="token punctuation">,</span> RowData<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span>item <span class="token operator">-</span><span class="token operator">&gt;</span> item<span class="token punctuation">.</span>f0<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>item <span class="token operator">-</span><span class="token operator">&gt;</span> item<span class="token punctuation">.</span>f1<span class="token punctuation">)</span><span class="token punctuation">;</span>

        String sql2 <span class="token operator">=</span> <span class="token string">"select appregurl,count(uid),dn,dt from tmpA where dt='"</span> <span class="token operator">+</span> dt <span class="token operator">+</span> <span class="token string">"' group by appregurl,dn,dt"</span><span class="token punctuation">;</span>
        Table table2 <span class="token operator">=</span> tableEnv<span class="token punctuation">.</span><span class="token function">sqlQuery</span><span class="token punctuation">(</span>sql2<span class="token punctuation">)</span><span class="token punctuation">;</span>
        DataStream<span class="token generics function"><span class="token punctuation">&lt;</span>RowData<span class="token punctuation">&gt;</span></span> appregurlnumDS <span class="token operator">=</span> tableEnv<span class="token punctuation">.</span><span class="token function">toRetractStream</span><span class="token punctuation">(</span>table2<span class="token punctuation">,</span> RowData<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span>item <span class="token operator">-</span><span class="token operator">&gt;</span> item<span class="token punctuation">.</span>f0<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>item <span class="token operator">-</span><span class="token operator">&gt;</span> item<span class="token punctuation">.</span>f1<span class="token punctuation">)</span><span class="token punctuation">;</span>

        TableLoader top3Table <span class="token operator">=</span> TableLoader<span class="token punctuation">.</span><span class="token function">fromHadoopTable</span><span class="token punctuation">(</span>warehouseDir <span class="token operator">+</span> <span class="token string">"/ads_register_top3memberpay"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        TableLoader appregurlnumTable <span class="token operator">=</span> TableLoader<span class="token punctuation">.</span><span class="token function">fromHadoopTable</span><span class="token punctuation">(</span>warehouseDir <span class="token operator">+</span> <span class="token string">"/ads_register_appregurlnum"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

	FlinkSink<span class="token punctuation">.</span><span class="token function">forRowData</span><span class="token punctuation">(</span>top3DS<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">tableLoader</span><span class="token punctuation">(</span>top3Table<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">overwrite</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    FlinkSink<span class="token punctuation">.</span><span class="token function">forRowData</span><span class="token punctuation">(</span>appregurlnumDS<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">tableLoader</span><span class="token punctuation">(</span>appregurlnumTable<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">overwrite</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<h6><a id="_185"></a>优化实践</h6> 
<h6><a id="1__186"></a>1 小文件处理</h6> 
<ul><li>Iceberg 0.11 以前，通过定时触发 batch api 进行小文件合并，这样虽然能合并，但是需要维护一套 Actions 代码，而且也不是实时合并的。</li></ul> 
<pre><code class="prism language-java">Table table <span class="token operator">=</span> <span class="token function">findTable</span><span class="token punctuation">(</span>options<span class="token punctuation">,</span> conf<span class="token punctuation">)</span><span class="token punctuation">;</span>
Actions<span class="token punctuation">.</span><span class="token function">forTable</span><span class="token punctuation">(</span>table<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">rewriteDataFiles</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">targetSizeInBytes</span><span class="token punctuation">(</span><span class="token number">10</span> <span class="token operator">*</span> <span class="token number">1024</span><span class="token punctuation">)</span> <span class="token comment">// 10KB</span>
        <span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p>Iceberg 0.11 新特性，支持了流式小文件合并。通过分区/存储桶键使用哈希混洗方式写数据、从源头直接合并文件，这样的好处在于，一个 task 会处理某个分区的数据，提交自己的 Datafile 文件，比如一个 task 只处理对应分区的数据。这样避免了多个 task 处理提交很多小文件的问题，且不需要额外的维护代码，只需在建表的时候指定属性 write.distribution-mode，该参数与其它引擎是通用的，比如 Spark 等。</p> 
<pre><code class="prism language-java">CREATE TABLE city_table <span class="token punctuation">(</span> 
     province BIGINT<span class="token punctuation">,</span>
     city STRING
<span class="token punctuation">)</span> PARTITIONED BY <span class="token punctuation">(</span>province<span class="token punctuation">,</span> city<span class="token punctuation">)</span> WITH <span class="token punctuation">(</span>
    <span class="token string">'write.distribution-mode'</span><span class="token operator">=</span><span class="token string">'hash'</span> 
<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<h6><a id="2__204"></a>2 排序功能</h6> 
<ul><li>在 Iceberg 0.11 之前，Flink 是不支持 Iceberg 排序功能的，所以之前只能结合 Spark 以批模式来支持排序功能，0.11 新增了排序特性的支持，Iceberg也支持flink的排序</li></ul> 
<pre><code class="prism language-java">insert into Iceberg_table select days from Kafka_tbl order by days<span class="token punctuation">,</span> province_id<span class="token punctuation">;</span>
</code></pre> 
<ul><li>利用 Iceberg 的排序特性，将天作为分区。按天、小时、分钟进行排序，那么 manifest 文件就会记录这个排序规则，从而在检索数据的时候，提高查询效率，既能实现 Hive 分区的检索优点，还能避免 Hive metadata 元数据过多带来的压力。</li></ul> 
<h6><a id="_212"></a>总结</h6> 
<ul><li>flink不支持隐藏分区，不支持创建带水位线的表</li><li>与 hudi 相比，缺少行级更新，只能对表的数据按分区进行 overwrite 全量覆盖</li><li>flink近实时入湖<br> ① Iceberg 提交 Transaction 时是以文件粒度来提交。这就没法以秒为单位提交 Transaction，否则会造成文件数量膨胀；<br> ② 没有在线服务节点。对于实时的高吞吐低延迟写入，无法得到纯实时的响应；<br> ③ Flink 写入以 checkpoint 为单位，物理数据写入 Iceberg 后并不能直接查询，当触发了 checkpoint 才会写 metadata 文件，这时数据由不可见变为可见。checkpoint 每次执行都会有一定时间。</li></ul> 
<pre><code class="prism language-java"><span class="token number">2022</span><span class="token operator">-</span><span class="token number">03</span><span class="token operator">-</span><span class="token number">16</span> <span class="token number">16</span><span class="token operator">:</span><span class="token number">09</span><span class="token operator">:</span><span class="token number">24</span><span class="token punctuation">,</span><span class="token number">486</span>   INFO <span class="token operator">--</span><span class="token operator">-</span> <span class="token punctuation">[</span>                        jobmanager<span class="token operator">-</span>future<span class="token operator">-</span>thread<span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span>  org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>runtime<span class="token punctuation">.</span>checkpoint<span class="token punctuation">.</span>CheckpointCoordinator                       <span class="token punctuation">(</span>line<span class="token operator">:</span> <span class="token number">1250</span><span class="token punctuation">)</span>  <span class="token operator">:</span>  Completed checkpoint <span class="token number">60</span> <span class="token keyword">for</span> job c7a6d8df0b422bb4c27a35b21a9142de <span class="token punctuation">(</span><span class="token number">9169</span> bytes in <span class="token number">5</span> ms<span class="token punctuation">)</span><span class="token punctuation">.</span>
<span class="token number">2022</span><span class="token operator">-</span><span class="token number">03</span><span class="token operator">-</span><span class="token number">16</span> <span class="token number">16</span><span class="token operator">:</span><span class="token number">09</span><span class="token operator">:</span><span class="token number">30</span><span class="token punctuation">,</span><span class="token number">481</span>   INFO <span class="token operator">--</span><span class="token operator">-</span> <span class="token punctuation">[</span>                                  Checkpoint Timer<span class="token punctuation">]</span>  org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>runtime<span class="token punctuation">.</span>checkpoint<span class="token punctuation">.</span>CheckpointCoordinator                       <span class="token punctuation">(</span>line<span class="token operator">:</span>  <span class="token number">741</span><span class="token punctuation">)</span>  <span class="token operator">:</span>  Triggering checkpoint <span class="token number">61</span> <span class="token punctuation">(</span>type<span class="token operator">=</span>CHECKPOINT<span class="token punctuation">)</span> @ <span class="token number">1647418170480</span> <span class="token keyword">for</span> job c7a6d8df0b422bb4c27a35b21a9142de<span class="token punctuation">.</span>
<span class="token number">2022</span><span class="token operator">-</span><span class="token number">03</span><span class="token operator">-</span><span class="token number">16</span> <span class="token number">16</span><span class="token operator">:</span><span class="token number">09</span><span class="token operator">:</span><span class="token number">30</span><span class="token punctuation">,</span><span class="token number">483</span>   INFO <span class="token operator">--</span><span class="token operator">-</span> <span class="token punctuation">[</span>IcebergFilesCommitter <span class="token operator">-</span><span class="token operator">&gt;</span> Sink<span class="token operator">:</span> IcebergSink hdfs<span class="token operator">:</span><span class="token operator">/</span><span class="token operator">/</span>hadoop01<span class="token operator">:</span><span class="token number">9820</span><span class="token operator">/</span>flink<span class="token operator">/</span>warehouse<span class="token operator">/</span>iceberg_db<span class="token operator">/</span>dwd_view_log <span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span><span class="token punctuation">)</span>#<span class="token number">0</span><span class="token punctuation">]</span>  org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>iceberg<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>sink<span class="token punctuation">.</span>IcebergFilesCommitter                             <span class="token punctuation">(</span>line<span class="token operator">:</span>  <span class="token number">162</span><span class="token punctuation">)</span>  <span class="token operator">:</span>  Start to flush snapshot state to state backend<span class="token punctuation">,</span> table<span class="token operator">:</span> hdfs<span class="token operator">:</span><span class="token operator">/</span><span class="token operator">/</span>hadoop01<span class="token operator">:</span><span class="token number">9820</span><span class="token operator">/</span>flink<span class="token operator">/</span>warehouse<span class="token operator">/</span>iceberg_db<span class="token operator">/</span>dwd_view_log<span class="token punctuation">,</span> checkpointId<span class="token operator">:</span> <span class="token number">61</span>
<span class="token number">2022</span><span class="token operator">-</span><span class="token number">03</span><span class="token operator">-</span><span class="token number">16</span> <span class="token number">16</span><span class="token operator">:</span><span class="token number">09</span><span class="token operator">:</span><span class="token number">30</span><span class="token punctuation">,</span><span class="token number">483</span>   INFO <span class="token operator">--</span><span class="token operator">-</span> <span class="token punctuation">[</span>                        jobmanager<span class="token operator">-</span>future<span class="token operator">-</span>thread<span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">]</span>  org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>runtime<span class="token punctuation">.</span>checkpoint<span class="token punctuation">.</span>CheckpointCoordinator                       <span class="token punctuation">(</span>line<span class="token operator">:</span> <span class="token number">1250</span><span class="token punctuation">)</span>  <span class="token operator">:</span>  Completed checkpoint <span class="token number">61</span> <span class="token keyword">for</span> job c7a6d8df0b422bb4c27a35b21a9142de <span class="token punctuation">(</span><span class="token number">9169</span> bytes in <span class="token number">3</span> ms<span class="token punctuation">)</span><span class="token punctuation">.</span>
</code></pre>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4e0151290fb806821bb71b2b0faa8e55/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">SSH命令批量操作服务器</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/df8b66f7d528c13597f60ce4d3e58e3d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">python题库--简单</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>