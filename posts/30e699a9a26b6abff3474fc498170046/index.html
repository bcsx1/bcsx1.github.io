<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>hugging faceå‚æ•°é«˜æ•ˆå¾®è°ƒpeftæºç è§£æ - ç¼–ç¨‹éšæƒ³</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="hugging faceå‚æ•°é«˜æ•ˆå¾®è°ƒpeftæºç è§£æ" />
<meta property="og:description" content="å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT) - çŸ¥ä¹
è®©å¤©ä¸‹æ²¡æœ‰éš¾Tuningçš„å¤§æ¨¡å‹-PEFTæŠ€æœ¯ç®€ä»‹ - çŸ¥ä¹
å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆä¸‰ï¼‰-P-Tuningã€P-Tuning v2 - çŸ¥ä¹
ä½ ä¼¼ä¹æ¥åˆ°äº†æ²¡æœ‰çŸ¥è¯†å­˜åœ¨çš„è’åŸ - çŸ¥ä¹
å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆå…­ï¼‰-MAM Adapterã€UniPELT - çŸ¥ä¹
GitHub - huggingface/peft: ğŸ¤— PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.
ChatGPTç­‰å¤§æ¨¡å‹é«˜æ•ˆè°ƒå‚å¤§æ³•â€”â€”PEFTåº“çš„ç®—æ³•ç®€ä»‹ - çŸ¥ä¹
PEFT
PEFTï¼šParameter Efficient Fine-TuningæŠ€æœ¯æ—¨åœ¨é€šè¿‡æœ€å°åŒ–å¾®è°ƒå‚æ•°çš„æ•°é‡å’Œè®¡ç®—å¤æ‚åº¦ï¼Œæ¥æé«˜é¢„è®­ç»ƒæ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œä»è€Œç¼“è§£å¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„è®­ç»ƒæˆæœ¬ã€‚Â Prefix-Tuningï¼ˆè½¯æç¤º/è¿ç»­æç¤ºï¼‰
åœ¨æ¯ä¸€å±‚çš„tokenä¹‹å‰æ„é€ ä¸€æ®µä»»åŠ¡ç›¸å…³çš„tokensä½œä¸ºPrefixï¼Œè®­ç»ƒæ—¶åªæ›´æ–°Prefixéƒ¨åˆ†çš„å‚æ•°ï¼Œè€ŒTransformerä¸­çš„å…¶ä»–éƒ¨åˆ†å‚æ•°å›ºå®šä¸€ä¸ªPrefixæ¨¡å—å°±æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„idåˆ°embeddingæ˜ å°„è¡¨ï¼Œå¯ä»¥åœ¨å¤šå±‚åˆ†åˆ«æ·»åŠ Prefixæ¨¡å—ä¸ºäº†é˜²æ­¢ç›´æ¥æ›´æ–°Prefixçš„å‚æ•°å¯¼è‡´è®­ç»ƒä¸ç¨³å®šçš„æƒ…å†µï¼Œåœ¨Prefixå±‚å‰é¢åŠ äº†MLPç»“æ„ä½œç”¨é˜¶æ®µï¼šæ‰€æœ‰transformer blockçš„Attentionæ³¨æ„åŠ›è®¡ç®— Prompt-Tuningï¼ˆè½¯æç¤º/è¿ç»­æç¤ºï¼‰
å¯çœ‹åšæ˜¯Prefix-Tuningçš„ç®€åŒ–ç‰ˆæœ¬ï¼Œåªåœ¨è¾“å…¥å±‚åŠ å…¥prompt tokensï¼Œå¹¶ä¸éœ€è¦åŠ å…¥MLPè¿›è¡Œè°ƒæ•´æå‡º Prompt Ensembling æ–¹æ³•æ¥é›†æˆé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„å¤šç§ promptsåœ¨åªé¢å¤–å¯¹å¢åŠ çš„3.6%å‚æ•°è§„æ¨¡ï¼ˆç›¸æ¯”åŸæ¥é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°é‡ï¼‰çš„æƒ…å†µä¸‹å–å¾—å’ŒFull-finetuningæ¥è¿‘çš„æ•ˆæœä½œç”¨é˜¶æ®µï¼šç¬¬ä¸€å±‚transformer blockçš„Attentionæ³¨æ„åŠ›è®¡ç®— P-Tuningï¼ˆè½¯æç¤º/è¿ç»­æç¤ºï¼‰
P-Tuningåªæ˜¯åœ¨è¾“å…¥çš„æ—¶å€™åŠ å…¥Embeddingï¼Œå¹¶é€šè¿‡LSTM&#43;MLPå¯¹prompt embeddingåºåˆ—è¿›è¡Œç¼–ç æ ¹æ®äººå·¥è®¾è®¡æ¨¡æ¿åŠ¨æ€ç¡®å®šprompt tokençš„æ·»åŠ ä½ç½®ï¼Œå¯ä»¥æ”¾åœ¨å¼€å§‹ï¼Œä¹Ÿå¯ä»¥æ”¾åœ¨ä¸­é—´ä½œç”¨é˜¶æ®µï¼šç¬¬ä¸€å±‚transformer blockçš„Attentionæ³¨æ„åŠ›è®¡ç®— P-Tuning V2ï¼ˆè½¯æç¤º/è¿ç»­æç¤ºï¼‰
å¯çœ‹åšæ˜¯Prefix-Tuningçš„ä¼˜åŒ–ç‰ˆæœ¬ã€‚åœ¨æ¨¡å‹çš„æ¯ä¸€å±‚éƒ½æ·»åŠ è¿ç»­çš„ promptsP-Tuning v2åœ¨ä¸åŒè§„æ¨¡å’Œä»»åŠ¡ä¸­éƒ½å¯ä¸å¾®è°ƒæ•ˆæœç›¸åª²ç¾ç§»é™¤é‡å‚æ•°åŒ–çš„ç¼–ç å™¨ï¼ˆå¦‚ï¼šPrefix Tuningä¸­çš„MLPã€P-Tuningä¸­çš„LSTMï¼‰ã€é’ˆå¯¹ä¸åŒä»»åŠ¡é‡‡ç”¨ä¸åŒçš„æç¤ºé•¿åº¦ã€å¼•å…¥å¤šä»»åŠ¡å­¦ä¹ ç­‰ä½œç”¨é˜¶æ®µï¼šæ‰€æœ‰transformer blockçš„Attentionæ³¨æ„åŠ›è®¡ç®— Adapterï¼ˆå˜ä½“ï¼šAdapterFusionã€AdapterDropï¼‰
åœ¨Transformer Blockä¸­åŠ å…¥ä¸¤å±‚MLPï¼Œå›ºå®šä½åŸæ¥é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°ä¸å˜ï¼Œåªå¯¹æ–°å¢çš„Adapterç»“æ„è¿›è¡Œå¾®è°ƒåœ¨åªé¢å¤–å¯¹å¢åŠ çš„3.6%å‚æ•°è§„æ¨¡ï¼ˆç›¸æ¯”åŸæ¥é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°é‡ï¼‰çš„æƒ…å†µä¸‹å–å¾—å’ŒFull-finetuningæ¥è¿‘çš„æ•ˆæœä½œç”¨é˜¶æ®µï¼šTransformer Blockä¸»ä½“ LoRAï¼ˆå˜ä½“ï¼šAdaLoRAã€QLoRAï¼‰
åœ¨è®­ç»ƒé˜¶æ®µï¼Œé¢„è®­ç»ƒå‚æ•°Wå›ºå®šï¼Œåªæ›´æ–°Aå’ŒBå‚æ•°ï¼ŒAå’ŒBæ¨¡æ‹ŸWçš„å˜åŒ–é‡ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œç”¨Aã€Bå‚æ•°ä¸åŸé¢„è®­ç»ƒå‚æ•°ç›¸åŠ æ›¿æ¢åŸæœ‰é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°ã€‚æ¨ç†è¿‡ç¨‹æ²¡æœ‰é¢å¤–çš„å‚æ•°é‡å’Œè®¡ç®—é‡ç›¸å½“äºæ˜¯ç”¨LoRAå»æ¨¡æ‹ŸFull-finetuneçš„è¿‡ç¨‹ï¼Œå‡ ä¹ä¸ä¼šå¸¦æ¥æ•ˆæœæŸå¤±ç”±äºæ²¡æœ‰ä½¿ç”¨Promptæ–¹å¼ï¼Œé¿å…äº†Promptæ–¹æ³•çš„ä¸€ç³»åˆ—é—®é¢˜ï¼ˆPromptçš„é—®é¢˜ï¼šéš¾è®­ç»ƒã€åºåˆ—é•¿åº¦å—é™ã€æ•ˆæœä¸å¦‚finetuneï¼‰ï¼Œåœ¨æ¨ç†é˜¶æ®µä¹Ÿæ²¡æœ‰å¼•å…¥é¢å¤–çš„å‚æ•°ä½œç”¨é˜¶æ®µï¼šTransformer Blockä¸»ä½“queryã€value BitFit
ä¸éœ€è¦å¯¹é¢„è®­ç»ƒæ¨¡å‹åšä»»ä½•æ”¹åŠ¨ï¼Œåªå¾®è°ƒè®­ç»ƒæŒ‡å®šç¥ç»ç½‘ç»œä¸­çš„åç½®Biaså‚æ•°é‡åªæœ‰ä¸åˆ°2%ï¼Œä½†æ˜¯æ•ˆæœå¯ä»¥æ¥è¿‘å…¨é‡å‚æ•°ä½œç”¨é˜¶æ®µï¼šæ¨¡å‹Backboneä¸»ä½“ PEFTæ–¹æ³•æ€»ç»“
å¢åŠ é¢å¤–å‚æ•°ï¼Œå¦‚ï¼šPrefix Tuningã€Prompt Tuningã€Adapter TuningåŠå…¶å˜ä½“é€‰å–ä¸€éƒ¨åˆ†å‚æ•°æ›´æ–°ï¼Œå¦‚ï¼šBitFitå¼•å…¥é‡å‚æ•°åŒ–ï¼Œå¦‚ï¼šLoRAã€AdaLoRAã€QLoRAæ··åˆé«˜æ•ˆå¾®è°ƒï¼Œå¦‚ï¼šMAM Adapterã€UniPELT ç”±äºpeftæ›´åœ¨å¿«é€Ÿå¼€å‘è¿­ä»£ä¸­ï¼Œä»£ç å˜åŠ¨å¯èƒ½ä¼šæ¯”è¾ƒå¤§ï¼Œæœ¬æ–‡ç›¸å…³ä»£ç æ¥è‡ªäºpeftçš„0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/30e699a9a26b6abff3474fc498170046/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-19T09:08:33+08:00" />
<meta property="article:modified_time" content="2023-07-19T09:08:33+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="ç¼–ç¨‹éšæƒ³" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">ç¼–ç¨‹éšæƒ³</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">hugging faceå‚æ•°é«˜æ•ˆå¾®è°ƒpeftæºç è§£æ</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p style="margin-left:.0001pt;text-align:justify;"><a href="https://zhuanlan.zhihu.com/p/621700272" rel="nofollow" title="å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT) - çŸ¥ä¹">å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT) - çŸ¥ä¹</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/618894319" rel="nofollow" title="è®©å¤©ä¸‹æ²¡æœ‰éš¾Tuningçš„å¤§æ¨¡å‹-PEFTæŠ€æœ¯ç®€ä»‹ - çŸ¥ä¹">è®©å¤©ä¸‹æ²¡æœ‰éš¾Tuningçš„å¤§æ¨¡å‹-PEFTæŠ€æœ¯ç®€ä»‹ - çŸ¥ä¹</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/635848732" rel="nofollow" title="å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆä¸‰ï¼‰-P-Tuningã€P-Tuning v2 - çŸ¥ä¹">å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆä¸‰ï¼‰-P-Tuningã€P-Tuning v2 - çŸ¥ä¹</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/636999010" rel="nofollow" title="ä½ ä¼¼ä¹æ¥åˆ°äº†æ²¡æœ‰çŸ¥è¯†å­˜åœ¨çš„è’åŸ - çŸ¥ä¹">ä½ ä¼¼ä¹æ¥åˆ°äº†æ²¡æœ‰çŸ¥è¯†å­˜åœ¨çš„è’åŸ - çŸ¥ä¹</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/636362246" rel="nofollow" title="å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆå…­ï¼‰-MAM Adapterã€UniPELT - çŸ¥ä¹">å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆå…­ï¼‰-MAM Adapterã€UniPELT - çŸ¥ä¹</a></p> 
<p><a href="https://github.com/huggingface/peft" title="GitHub - huggingface/peft: ğŸ¤— PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.">GitHub - huggingface/peft: ğŸ¤— PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/613863520" rel="nofollow" title="ChatGPTç­‰å¤§æ¨¡å‹é«˜æ•ˆè°ƒå‚å¤§æ³•â€”â€”PEFTåº“çš„ç®—æ³•ç®€ä»‹ - çŸ¥ä¹">ChatGPTç­‰å¤§æ¨¡å‹é«˜æ•ˆè°ƒå‚å¤§æ³•â€”â€”PEFTåº“çš„ç®—æ³•ç®€ä»‹ - çŸ¥ä¹</a></p> 
<p><a href="https://huggingface.co/docs/peft/index" rel="nofollow" title="PEFT">PEFT</a></p> 
<p></p> 
<p>PEFTï¼šParameter Efficient Fine-TuningæŠ€æœ¯æ—¨åœ¨é€šè¿‡æœ€å°åŒ–å¾®è°ƒå‚æ•°çš„æ•°é‡å’Œè®¡ç®—å¤æ‚åº¦ï¼Œæ¥æé«˜é¢„è®­ç»ƒæ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œä»è€Œç¼“è§£å¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„è®­ç»ƒæˆæœ¬ã€‚Â </p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><strong>Prefix-Tuningï¼ˆè½¯æç¤º/è¿ç»­æç¤ºï¼‰</strong></strong></p> 
<ol><li style="text-align:justify;">åœ¨æ¯ä¸€å±‚çš„tokenä¹‹å‰æ„é€ ä¸€æ®µä»»åŠ¡ç›¸å…³çš„tokensä½œä¸ºPrefixï¼Œè®­ç»ƒæ—¶åªæ›´æ–°Prefixéƒ¨åˆ†çš„å‚æ•°ï¼Œè€ŒTransformerä¸­çš„å…¶ä»–éƒ¨åˆ†å‚æ•°å›ºå®š</li><li style="text-align:justify;">ä¸€ä¸ªPrefixæ¨¡å—å°±æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„idåˆ°embeddingæ˜ å°„è¡¨ï¼Œå¯ä»¥åœ¨å¤šå±‚åˆ†åˆ«æ·»åŠ Prefixæ¨¡å—</li><li style="text-align:justify;">ä¸ºäº†é˜²æ­¢ç›´æ¥æ›´æ–°Prefixçš„å‚æ•°å¯¼è‡´è®­ç»ƒä¸ç¨³å®šçš„æƒ…å†µï¼Œåœ¨Prefixå±‚å‰é¢åŠ äº†MLPç»“æ„</li><li style="text-align:justify;"><strong><strong>ä½œç”¨é˜¶æ®µï¼š</strong></strong>æ‰€æœ‰transformer blockçš„Attentionæ³¨æ„åŠ›è®¡ç®—</li></ol> 
<p class="img-center"><img alt="" height="303" src="https://images2.imgbox.com/eb/41/ipotiOOj_o.png" width="535"></p> 
<p style="margin-left:.0001pt;text-align:center;"></p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><strong>Prompt-Tuningï¼ˆè½¯æç¤º/è¿ç»­æç¤ºï¼‰</strong></strong></p> 
<ol><li style="text-align:justify;">å¯çœ‹åšæ˜¯Prefix-Tuningçš„ç®€åŒ–ç‰ˆæœ¬ï¼Œåªåœ¨è¾“å…¥å±‚åŠ å…¥prompt tokensï¼Œå¹¶ä¸éœ€è¦åŠ å…¥MLPè¿›è¡Œè°ƒæ•´</li><li style="text-align:justify;">æå‡º Prompt Ensembling æ–¹æ³•æ¥é›†æˆé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„å¤šç§ prompts</li><li style="text-align:justify;">åœ¨åªé¢å¤–å¯¹å¢åŠ çš„3.6%å‚æ•°è§„æ¨¡ï¼ˆç›¸æ¯”åŸæ¥é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°é‡ï¼‰çš„æƒ…å†µä¸‹å–å¾—å’ŒFull-finetuningæ¥è¿‘çš„æ•ˆæœ</li><li style="text-align:justify;"><strong><strong>ä½œç”¨é˜¶æ®µï¼š</strong></strong>ç¬¬ä¸€å±‚transformer blockçš„Attentionæ³¨æ„åŠ›è®¡ç®—</li></ol> 
<p class="img-center"><img alt="" height="331" src="https://images2.imgbox.com/23/e1/tZrfmCOi_o.png" width="623"></p> 
<p style="margin-left:.0001pt;text-align:center;"></p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><strong>P-Tuningï¼ˆè½¯æç¤º/è¿ç»­æç¤ºï¼‰</strong></strong></p> 
<ol><li style="text-align:justify;">P-Tuningåªæ˜¯åœ¨è¾“å…¥çš„æ—¶å€™åŠ å…¥Embeddingï¼Œå¹¶é€šè¿‡LSTM+MLPå¯¹prompt embeddingåºåˆ—è¿›è¡Œç¼–ç </li><li style="text-align:justify;">æ ¹æ®äººå·¥è®¾è®¡æ¨¡æ¿åŠ¨æ€ç¡®å®šprompt tokençš„æ·»åŠ ä½ç½®ï¼Œå¯ä»¥æ”¾åœ¨å¼€å§‹ï¼Œä¹Ÿå¯ä»¥æ”¾åœ¨ä¸­é—´</li><li style="text-align:justify;"><strong><strong>ä½œç”¨é˜¶æ®µï¼š</strong></strong>ç¬¬ä¸€å±‚transformer blockçš„Attentionæ³¨æ„åŠ›è®¡ç®—</li></ol> 
<p class="img-center"><img alt="" height="191" src="https://images2.imgbox.com/4a/83/XWpRW1wJ_o.png" width="827"></p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><strong>P-Tuning V2ï¼ˆè½¯æç¤º/è¿ç»­æç¤ºï¼‰</strong></strong></p> 
<ol><li style="text-align:justify;">å¯çœ‹åšæ˜¯Prefix-Tuningçš„ä¼˜åŒ–ç‰ˆæœ¬ã€‚åœ¨æ¨¡å‹çš„æ¯ä¸€å±‚éƒ½æ·»åŠ è¿ç»­çš„ prompts</li><li style="text-align:justify;">P-Tuning v2åœ¨ä¸åŒè§„æ¨¡å’Œä»»åŠ¡ä¸­éƒ½å¯ä¸å¾®è°ƒæ•ˆæœç›¸åª²ç¾</li><li style="text-align:justify;">ç§»é™¤é‡å‚æ•°åŒ–çš„ç¼–ç å™¨ï¼ˆå¦‚ï¼šPrefix Tuningä¸­çš„MLPã€P-Tuningä¸­çš„LSTMï¼‰ã€é’ˆå¯¹ä¸åŒä»»åŠ¡é‡‡ç”¨ä¸åŒçš„æç¤ºé•¿åº¦ã€å¼•å…¥å¤šä»»åŠ¡å­¦ä¹ ç­‰</li><li style="text-align:justify;"><strong><strong>ä½œç”¨é˜¶æ®µï¼š</strong></strong>æ‰€æœ‰transformer blockçš„Attentionæ³¨æ„åŠ›è®¡ç®—</li></ol> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p class="img-center"><img alt="" height="237" src="https://images2.imgbox.com/39/af/EnRSYq9f_o.png" width="827"></p> 
<p></p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><strong>Adapterï¼ˆå˜ä½“ï¼šAdapterFusionã€AdapterDropï¼‰</strong></strong></p> 
<ol><li style="text-align:justify;">åœ¨Transformer Blockä¸­åŠ å…¥ä¸¤å±‚MLPï¼Œå›ºå®šä½åŸæ¥é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°ä¸å˜ï¼Œåªå¯¹æ–°å¢çš„Adapterç»“æ„è¿›è¡Œå¾®è°ƒ</li><li style="text-align:justify;">åœ¨åªé¢å¤–å¯¹å¢åŠ çš„3.6%å‚æ•°è§„æ¨¡ï¼ˆç›¸æ¯”åŸæ¥é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°é‡ï¼‰çš„æƒ…å†µä¸‹å–å¾—å’ŒFull-finetuningæ¥è¿‘çš„æ•ˆæœ</li><li style="text-align:justify;"><strong>ä½œç”¨é˜¶æ®µï¼š</strong>Transformer Blockä¸»ä½“</li></ol> 
<p class="img-center"><img alt="" height="361" src="https://images2.imgbox.com/f7/26/NMHxl80d_o.png" width="407"></p> 
<p style="margin-left:.0001pt;text-align:center;"></p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><strong>L</strong></strong><strong><strong>oRAï¼ˆå˜ä½“ï¼šAdaLoRAã€QLoRAï¼‰</strong></strong></p> 
<ol><li style="text-align:justify;">åœ¨è®­ç»ƒé˜¶æ®µï¼Œé¢„è®­ç»ƒå‚æ•°Wå›ºå®šï¼Œåªæ›´æ–°Aå’ŒBå‚æ•°ï¼ŒAå’ŒBæ¨¡æ‹ŸWçš„å˜åŒ–é‡ã€‚</li><li style="text-align:justify;">åœ¨æ¨ç†é˜¶æ®µï¼Œç”¨Aã€Bå‚æ•°ä¸åŸé¢„è®­ç»ƒå‚æ•°ç›¸åŠ æ›¿æ¢åŸæœ‰é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°ã€‚æ¨ç†è¿‡ç¨‹æ²¡æœ‰é¢å¤–çš„å‚æ•°é‡å’Œè®¡ç®—é‡</li><li style="text-align:justify;">ç›¸å½“äºæ˜¯ç”¨LoRAå»æ¨¡æ‹ŸFull-finetuneçš„è¿‡ç¨‹ï¼Œå‡ ä¹ä¸ä¼šå¸¦æ¥æ•ˆæœæŸå¤±</li><li style="text-align:justify;">ç”±äºæ²¡æœ‰ä½¿ç”¨Promptæ–¹å¼ï¼Œé¿å…äº†Promptæ–¹æ³•çš„ä¸€ç³»åˆ—é—®é¢˜ï¼ˆPromptçš„é—®é¢˜ï¼šéš¾è®­ç»ƒã€åºåˆ—é•¿åº¦å—é™ã€æ•ˆæœä¸å¦‚finetuneï¼‰ï¼Œåœ¨æ¨ç†é˜¶æ®µä¹Ÿæ²¡æœ‰å¼•å…¥é¢å¤–çš„å‚æ•°</li><li style="text-align:justify;"><strong>ä½œç”¨é˜¶æ®µï¼š</strong>Transformer Blockä¸»ä½“queryã€value</li></ol> 
<p style="margin-left:.0001pt;text-align:left;"></p> 
<p class="img-center"><img alt="" height="278" src="https://images2.imgbox.com/d1/95/8b4GiVVj_o.png" width="504"></p> 
<p></p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><strong>BitFit</strong></strong></p> 
<ol><li style="text-align:justify;">ä¸éœ€è¦å¯¹é¢„è®­ç»ƒæ¨¡å‹åšä»»ä½•æ”¹åŠ¨ï¼Œåªå¾®è°ƒè®­ç»ƒæŒ‡å®šç¥ç»ç½‘ç»œä¸­çš„åç½®Bias</li><li style="text-align:justify;">å‚æ•°é‡åªæœ‰ä¸åˆ°2%ï¼Œä½†æ˜¯æ•ˆæœå¯ä»¥æ¥è¿‘å…¨é‡å‚æ•°</li><li style="text-align:justify;">ä½œç”¨é˜¶æ®µï¼šæ¨¡å‹Backboneä¸»ä½“</li></ol> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><strong>PEFTæ–¹æ³•æ€»ç»“</strong></strong></p> 
<ol><li style="text-align:justify;">å¢åŠ é¢å¤–å‚æ•°ï¼Œå¦‚ï¼šPrefix Tuningã€Prompt Tuningã€Adapter TuningåŠå…¶å˜ä½“</li><li style="text-align:justify;">é€‰å–ä¸€éƒ¨åˆ†å‚æ•°æ›´æ–°ï¼Œå¦‚ï¼šBitFit</li><li style="text-align:justify;">å¼•å…¥é‡å‚æ•°åŒ–ï¼Œå¦‚ï¼šLoRAã€AdaLoRAã€QLoRA</li><li style="text-align:justify;">æ··åˆé«˜æ•ˆå¾®è°ƒï¼Œå¦‚ï¼šMAM Adapterã€UniPELT</li></ol> 
<p style="margin-left:.0001pt;text-align:left;"></p> 
<p class="img-center"><img alt="" height="557" src="https://images2.imgbox.com/7a/73/mwPNgBGS_o.png" width="640"></p> 
<p></p> 
<hr> 
<p style="margin-left:.0001pt;text-align:justify;"><strong>ç”±äºpeftæ›´åœ¨å¿«é€Ÿå¼€å‘è¿­ä»£ä¸­ï¼Œä»£ç å˜åŠ¨å¯èƒ½ä¼šæ¯”è¾ƒå¤§ï¼Œæœ¬æ–‡ç›¸å…³ä»£ç æ¥è‡ªäºpeftçš„0.3.0ç‰ˆæœ¬ã€‚</strong></p> 
<p></p> 
<p>Â Â Â Â Â Â Â Â ä¸‹é¢çœ‹ä¸€ä¸‹<a class="link-info" href="https://github.com/huggingface/peft" title="hugging face peft">hugging face peft</a>å¼€æºä»£ç ä¸­å¯¹äºä¸Šè¿°å‡ ç§æ–¹æ³•çš„å®ç°ã€‚tunersç›®å½•ä¸‹å®ç°äº†PrefixTuningã€PromptTuningã€PTuningã€Adapterã€LoRAã€AdaLoRAè¿™äº›æ–¹æ³•é…ç½®æ–‡ä»¶çš„æ„é€ ã€è§£æï¼Œæ–°å¢è®­ç»ƒå‚æ•°æ¨¡å‹çš„æ„é€ ï¼Œå„ç§PEFTæ–¹æ³•é…ç½®æ–‡ä»¶ç±»ä¹‹é—´çš„ç»§æ‰¿å…³ç³»ï¼Œå¦‚ä¸‹ï¼š</p> 
<pre><code>PeftConfig -&gt; PromptLearningConfig -&gt; (PrefixTuningConfigã€PromptEncoderConfigã€PromptTuningConfig)

PeftConfig -&gt; LoraConfig -&gt; AdaLoraConfig

PushToHubMixin -&gt; PeftConfigMixin -&gt; PeftConfig</code></pre> 
<p style="margin-left:.0001pt;text-align:left;">Â  Â  Â  Â  ä¸‹é¢å…ˆçœ‹ä¸€ä¸‹PrefixTuningã€PromptTuningã€PTuningV1æ¨¡å—çš„è¾“å…¥ã€è¾“å‡ºæƒ…å†µï¼š</p> 
<p style="margin-left:.0001pt;text-align:left;"><strong>Prefix-Tuning</strong></p> 
<p>PrefixEncoder(<br> Â  (embedding): Embedding(20, 18432)<br> )<br> åœ¨æ¯ä¸€å±‚transformer blockçš„keyå’Œvalueçš„å‰é¢éƒ½åŠ ä¸Švirtual embeddingï¼Œ2 * layers * hidden = 2 * 12 * 768 = 18432ï¼Œå…¶ä¸­2å°±å¯¹åº”keyå’ŒvalueÂ </p> 
<pre><code class="language-python">from peft import PrefixEncoder, PrefixTuningConfig
import torch

config = PrefixTuningConfig(
    peft_type="PREFIX_TUNING",
    task_type="SEQ_2_SEQ_LM",
    num_virtual_tokens=20,
    token_dim=768,
    num_transformer_submodules=1,
    num_attention_heads=12,
    num_layers=12,
    encoder_hidden_size=768,
    prefix_projection=False
)
print(config)

# åˆå§‹åŒ–PrefixEncoder
prefix_encoder = PrefixEncoder(config)
print(prefix_encoder)
</code></pre> 
<p style="margin-left:.0001pt;text-align:left;"><strong>Prompt-Tuning</strong></p> 
<p>PromptEmbedding(<br> Â  (embedding): Embedding(20, 768)<br> )<br> åªåœ¨è¾“å…¥å±‚çš„åŸå§‹åºåˆ—ä¸Šé¢æ·»åŠ prompt embeddingÂ </p> 
<pre><code class="language-python">from peft import PromptTuningConfig, PromptEmbedding
import torch

word_embedding = torch.nn.Embedding(num_embeddings=100, embedding_dim=768)
config = PromptTuningConfig(
    peft_type="PROMPT_TUNING",
    task_type="SEQ_2_SEQ_LM",
    num_virtual_tokens=20,
    token_dim=768,
    num_transformer_submodules=1,
    num_attention_heads=12,
    num_layers=12,
    # encoder_hidden_size=768
)
print(config)

# åˆå§‹åŒ–PromptEncoder
prompt_encoder = PromptEmbedding(config, word_embedding)
print(prompt_encoder)
</code></pre> 
<p><strong>PTuningV1</strong></p> 
<p>PromptEncoder(<br> Â  (embedding): Embedding(20, 768)<br> Â  (mlp_head): Sequential(<br> Â  Â  (0): Linear(in_features=768, out_features=768, bias=True)<br> Â  Â  (1): ReLU()<br> Â  Â  (2): Linear(in_features=768, out_features=768, bias=True)<br> Â  Â  (3): ReLU()<br> Â  Â  (4): Linear(in_features=768, out_features=768, bias=True)<br> Â  )<br> )<br> åªåœ¨è¾“å…¥å±‚çš„åŸå§‹åºåˆ—ä¸Šé¢æ·»åŠ prompt embeddingï¼Œå¹¶ä¸”ä½¿ç”¨äº†MLPåŠ å¼ºå­¦ä¹ èƒ½åŠ›Â </p> 
<pre><code class="language-python">from peft import PromptEncoderConfig, PromptEncoder
import torch

config = PromptEncoderConfig(
    peft_type="PREFIX_TUNING",
    task_type="SEQ_2_SEQ_LM",
    num_virtual_tokens=20,
    token_dim=768,
    num_transformer_submodules=1,
    num_attention_heads=12,
    num_layers=12,
    encoder_hidden_size=768,
    # prefix_projection=False
)
print(config)

# åˆå§‹åŒ–PrefixEncoder
p_encoder = PromptEncoder(config)
print(p_encoder)
</code></pre> 
<p><strong>Â PTuningV2</strong></p> 
<p>PrefixEncoder(<br> Â  (embedding): Embedding(20, 768)<br> Â  (transform): Sequential(<br> Â  Â  (0): Linear(in_features=768, out_features=768, bias=True)<br> Â  Â  (1): Tanh()<br> Â  Â  (2): Linear(in_features=768, out_features=18432, bias=True)<br> Â  )<br> )<br> ä¸PrefixTuningä¸€æ ·ï¼ˆå®é™…ä¸Šä¹Ÿæ˜¯ä½¿ç”¨åŒä¸€ä¸ªç±»æ¥å®ç°çš„ï¼‰ï¼Œåœ¨æ¯ä¸€å±‚transformer blockçš„keyå’Œvalueçš„å‰é¢éƒ½åŠ ä¸Švirtual embeddingï¼Œ2 * layers * hidden = 2 * 12 * 768 = 18432ï¼Œ2å¯¹åº”çš„å°±æ˜¯keyå’ŒvalueÂ </p> 
<pre><code class="language-python">from peft import PrefixEncoder, PrefixTuningConfig
import torch

config = PrefixTuningConfig(
    peft_type="PREFIX_TUNING",
    task_type="SEQ_2_SEQ_LM",
    num_virtual_tokens=20,
    token_dim=768,
    num_transformer_submodules=1,
    num_attention_heads=12,
    num_layers=12,
    encoder_hidden_size=768,
    prefix_projection=True
)
print(config)

# åˆå§‹åŒ–PrefixEncoder
prefix_encoder = PrefixEncoder(config)
print(prefix_encoder)
</code></pre> 
<p>Â Â Â Â Â Â Â Â PrefixTuningå’ŒPTuningV2åœ¨å®ç°ä¸ŠåŸºæœ¬ä¸Šæ˜¯ä¸€æ ·çš„ï¼Œå…¶å®å°±æ˜¯ä¸€æ ·çš„ã€‚ä¸‹é¢æ˜¯peftä½œè€…å›å¤çš„å…³äºPrefixTuningå’ŒPTuningV2åœ¨å®ç°ä¸Šçš„å…³ç³»ã€‚</p> 
<p class="img-center"><img alt="" height="838" src="https://images2.imgbox.com/90/86/hOi9ePQm_o.png" width="1138"></p> 
<p><strong>LoRA</strong></p> 
<pre><code class="language-python">from peft import LoraConfig, LoraModel, get_peft_model
from peft.tuners.lora import LoraLayer
import os
from transformers import AutoModelForSequenceClassification
from torch import nn
from typing import Any, List, Optional, Union

os.environ["HF_HOME"] = "./hf_downloads"


model_name_or_path = "bert-base-chinese"
tokenizer_name_or_path = "bert-base-chinese"

lora_config = LoraConfig(
    peft_type="LORA",
    task_type="SEQ_2_SEQ_LM",
    inference_mode=False,
    # å¦‚æœr=0ï¼Œbias='all'ï¼Œå°±å˜æˆäº†äº†BitFitå¾®è°ƒæ–¹æ³•
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    fan_in_fan_out=False,
    # bias -&gt; none æ‰€æœ‰å±‚çš„biaséƒ½ä¸å¾®è°ƒ
    # bias -&gt; all æ‰€æœ‰å±‚çš„biaséƒ½å¾®è°ƒ
    # bias -&gt; lora_only åªæœ‰LoRAç›¸å…³å±‚çš„biasè¿›è¡Œå¾®è°ƒ
    # bias å¯¹åº”BitFité«˜æ•ˆå¾®è°ƒæ–¹æ³•
    bias='lora_only'
)

model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)
model = get_peft_model(model, lora_config)
print(model)</code></pre> 
<p>Â Â Â Â Â Â Â Â Â æ‰“å°å‡ºæ¥çš„modelç»“æ„ä¸­åŒ…å«LoRAåœ¨queryå’Œvalueéƒ¨åˆ†æ·»åŠ çš„æƒé‡å‚æ•°ï¼š</p> 
<pre><code class="language-python">PeftModelForSeq2SeqLM(
  (base_model): LoraModel(
    (model): BertForSequenceClassification(
      (bert): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(21128, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0-11): 12 x BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(
                    in_features=768, out_features=768, bias=True
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=768, out_features=8, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=8, out_features=768, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(
                    in_features=768, out_features=768, bias=True
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=768, out_features=8, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=8, out_features=768, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (classifier): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)</code></pre> 
<p>Â Â Â Â Â Â Â Â ä¸‹é¢çœ‹ä¸€ä¸‹æ¨¡å‹é‡Œé¢çš„å¯è®­ç»ƒå‚æ•°æœ‰å“ªäº›ï¼Œå› ä¸ºåœ¨LoraConfigä¸­è®¾ç½®äº†biaså‚æ•°ä¸ºlora_onlyï¼Œæ‰€ä»¥æ‰€æœ‰loraå±‚çš„biaså‚æ•°ä¹Ÿä¼šè¢«è®¾ç½®ä¸ºå¯è®­ç»ƒï¼š</p> 
<pre><code class="language-python">for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)</code></pre> 
<pre><code class="language-python">base_model.model.bert.encoder.layer.0.attention.self.query.bias
base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight
base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight
base_model.model.bert.encoder.layer.0.attention.self.value.bias
base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight
base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight
base_model.model.bert.encoder.layer.1.attention.self.query.bias
base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight
base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight
base_model.model.bert.encoder.layer.1.attention.self.value.bias
base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight
base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight
base_model.model.bert.encoder.layer.2.attention.self.query.bias
base_model.model.bert.encoder.layer.2.attention.self.query.lora_A.default.weight
base_model.model.bert.encoder.layer.2.attention.self.query.lora_B.default.weight
base_model.model.bert.encoder.layer.2.attention.self.value.bias
base_model.model.bert.encoder.layer.2.attention.self.value.lora_A.default.weight
base_model.model.bert.encoder.layer.2.attention.self.value.lora_B.default.weight
base_model.model.bert.encoder.layer.3.attention.self.query.bias
base_model.model.bert.encoder.layer.3.attention.self.query.lora_A.default.weight
base_model.model.bert.encoder.layer.3.attention.self.query.lora_B.default.weight
base_model.model.bert.encoder.layer.3.attention.self.value.bias
base_model.model.bert.encoder.layer.3.attention.self.value.lora_A.default.weight
base_model.model.bert.encoder.layer.3.attention.self.value.lora_B.default.weight
base_model.model.bert.encoder.layer.4.attention.self.query.bias
base_model.model.bert.encoder.layer.4.attention.self.query.lora_A.default.weight
base_model.model.bert.encoder.layer.4.attention.self.query.lora_B.default.weight
base_model.model.bert.encoder.layer.4.attention.self.value.bias
base_model.model.bert.encoder.layer.4.attention.self.value.lora_A.default.weight
base_model.model.bert.encoder.layer.4.attention.self.value.lora_B.default.weight
base_model.model.bert.encoder.layer.5.attention.self.query.bias
base_model.model.bert.encoder.layer.5.attention.self.query.lora_A.default.weight
base_model.model.bert.encoder.layer.5.attention.self.query.lora_B.default.weight
base_model.model.bert.encoder.layer.5.attention.self.value.bias
base_model.model.bert.encoder.layer.5.attention.self.value.lora_A.default.weight
base_model.model.bert.encoder.layer.5.attention.self.value.lora_B.default.weight
base_model.model.bert.encoder.layer.6.attention.self.query.bias
base_model.model.bert.encoder.layer.6.attention.self.query.lora_A.default.weight
base_model.model.bert.encoder.layer.6.attention.self.query.lora_B.default.weight
base_model.model.bert.encoder.layer.6.attention.self.value.bias
base_model.model.bert.encoder.layer.6.attention.self.value.lora_A.default.weight
base_model.model.bert.encoder.layer.6.attention.self.value.lora_B.default.weight
base_model.model.bert.encoder.layer.7.attention.self.query.bias
base_model.model.bert.encoder.layer.7.attention.self.query.lora_A.default.weight
base_model.model.bert.encoder.layer.7.attention.self.query.lora_B.default.weight
base_model.model.bert.encoder.layer.7.attention.self.value.bias
base_model.model.bert.encoder.layer.7.attention.self.value.lora_A.default.weight
base_model.model.bert.encoder.layer.7.attention.self.value.lora_B.default.weight
base_model.model.bert.encoder.layer.8.attention.self.query.bias
base_model.model.bert.encoder.layer.8.attention.self.query.lora_A.default.weight
base_model.model.bert.encoder.layer.8.attention.self.query.lora_B.default.weight
base_model.model.bert.encoder.layer.8.attention.self.value.bias
base_model.model.bert.encoder.layer.8.attention.self.value.lora_A.default.weight
base_model.model.bert.encoder.layer.8.attention.self.value.lora_B.default.weight
base_model.model.bert.encoder.layer.9.attention.self.query.bias
base_model.model.bert.encoder.layer.9.attention.self.query.lora_A.default.weight
base_model.model.bert.encoder.layer.9.attention.self.query.lora_B.default.weight
base_model.model.bert.encoder.layer.9.attention.self.value.bias
base_model.model.bert.encoder.layer.9.attention.self.value.lora_A.default.weight
base_model.model.bert.encoder.layer.9.attention.self.value.lora_B.default.weight
base_model.model.bert.encoder.layer.10.attention.self.query.bias
base_model.model.bert.encoder.layer.10.attention.self.query.lora_A.default.weight
base_model.model.bert.encoder.layer.10.attention.self.query.lora_B.default.weight
base_model.model.bert.encoder.layer.10.attention.self.value.bias
base_model.model.bert.encoder.layer.10.attention.self.value.lora_A.default.weight
base_model.model.bert.encoder.layer.10.attention.self.value.lora_B.default.weight
base_model.model.bert.encoder.layer.11.attention.self.query.bias
base_model.model.bert.encoder.layer.11.attention.self.query.lora_A.default.weight
base_model.model.bert.encoder.layer.11.attention.self.query.lora_B.default.weight
base_model.model.bert.encoder.layer.11.attention.self.value.bias
base_model.model.bert.encoder.layer.11.attention.self.value.lora_A.default.weight
base_model.model.bert.encoder.layer.11.attention.self.value.lora_B.default.weight</code></pre> 
<p>Â Â Â Â Â Â Â Â ä¸‹é¢å°†LoRAå‚æ•°åˆå¹¶åˆ°é¢„è®­ç»ƒå‚æ•°ä¸­ï¼Œåˆå¹¶ä¹‹åçš„æ¨¡å‹å’ŒåŸå§‹é¢„è®­ç»ƒæ¨¡å‹ä¸€æ¨¡ä¸€æ ·ï¼š</p> 
<pre><code class="language-python">merged_model = model.merge_and_unload()
print(merged_model)</code></pre> 
<pre><code class="language-python">BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=2, bias=True)
)</code></pre> 
<hr> 
<p></p> 
<p style="margin-left:.0001pt;text-align:left;">Â Â Â Â Â Â Â Â ä¸‹é¢ä»¥peft_model.pyæ–‡ä»¶ä¸­PeftModelForSequenceClassificationçš„forwardå‡½æ•°å®ç°ä¸ºä¾‹ï¼Œçœ‹ä¸€ä¸‹åœ¨æ¨ç†é˜¶æ®µå¦‚ä½•å¯¹äºPrefixTuningã€PromptTuningã€PTuningV1ã€PTuningV2ã€Adapterã€LoRAè¿›è¡Œæ“ä½œã€‚</p> 
<p style="text-align:left;">Â  Â  Â  Â  1ã€é¦–å…ˆé€šè¿‡é…ç½®æ–‡ä»¶çš„æ‰€ç»§æ‰¿çš„çˆ¶ç±»ç±»å‹æ¥åˆ¤æ–­PEFTæ–¹æ³•æ˜¯å¦å±äºPromptç›¸å…³çš„ï¼Œå¦‚æœä¸æ˜¯ï¼Œå°±è¡¨ç¤ºä½¿ç”¨çš„æ˜¯Adapterã€LoRAç­‰æ–¹æ³•ï¼Œç›´æ¥æ‰§è¡Œæ¨ç†ã€‚LoRAçš„å…·ä½“æ¨ç†è®¡ç®—è¿‡ç¨‹åé¢å†è¡¥å……</p> 
<p class="img-center"><img alt="" height="300" src="https://images2.imgbox.com/2d/e7/z5UPJ2jQ_o.png" width="692"></p> 
<p style="margin-left:.0001pt;text-align:left;"></p> 
<p style="text-align:left;">Â  Â  Â  Â  2ã€å¦‚æœé€šè¿‡é…ç½®æ–‡ä»¶çš„æ‰€ç»§æ‰¿çš„çˆ¶ç±»ç±»å‹åˆ¤æ–­PEFTæ–¹æ³•å±äºPromptç›¸å…³çš„ï¼Œå› ä¸ºè¦åœ¨transformer blockçš„åºåˆ—å¼€å§‹ä½ç½®æ·»åŠ è™šæ‹Ÿtokençš„embeddingï¼Œæ‰€ä»¥ä¹Ÿè¦è¡¥å…¨attention mask</p> 
<p class="img-center"><img alt="" height="362" src="https://images2.imgbox.com/9f/57/DK4ULr1w_o.png" width="692"></p> 
<p style="text-align:left;">Â  Â  Â  Â  3ã€é€šè¿‡é…ç½®æ–‡ä»¶çš„ç±»å‹æ¥åˆ¤æ–­PEFTæ–¹æ³•åˆ°åº•æ˜¯PrefixTuning/PTuningV2ï¼Œè¿˜æ˜¯PromptTuning/PTuningV1ã€‚å¦‚æœæ˜¯PromptTuning/PTuningV1ï¼Œåˆ™å°†è™šæ‹Ÿtokençš„embeddingç›´æ¥concatåˆ°åŸå§‹è¾“å…¥åºåˆ—çš„å‰é¢ï¼Œé€å…¥base modelæ¨¡å‹è¿›è¡Œæ¨ç†ã€‚å¦‚æœæ˜¯PrefixTuning/PTuningV2ï¼Œç”±äºæ¶‰åŠåˆ°ç»™æ¯ä¸€ä¸ªtransformer blockçš„keyå’Œvalueæ·»åŠ è™šæ‹Ÿtokençš„embeddingï¼Œè¿˜éœ€è¦ä½¿ç”¨_prefix_tuning_forwardå‡½æ•°è¿›è¡Œé¢å¤–çš„å¤„ç†ã€‚</p> 
<p style="margin-left:.0001pt;text-align:left;"></p> 
<p class="img-center"><img alt="" height="410" src="https://images2.imgbox.com/7e/92/FzFnqTtU_o.png" width="692"></p> 
<p></p> 
<p style="margin-left:.0001pt;text-align:left;"><strong><strong>PromptTuning/PTuningV1æºç </strong></strong></p> 
<p style="margin-left:.0001pt;text-align:left;">Â Â Â Â Â Â Â Â PromptTuningå’ŒPTuningV1çš„å…±åŒç‚¹éƒ½æ˜¯ä½¿ç”¨äº†æµ…å±‚çš„Promptï¼Œåªåœ¨è¾“å…¥å±‚ä½¿ç”¨ã€‚æ‰€ä»¥PromptTuningå’ŒPTuningV1åœ¨ä½¿ç”¨æ—¶çš„æ–¹å¼åŸºæœ¬ç›¸åŒã€‚</p> 
<p style="margin-left:.0001pt;text-align:left;">Â Â Â Â Â Â Â Â PromptTuningçš„æºç è¯¦è§src/peft/tuners/prompt_tuning.pyï¼Œé‡Œé¢åŒ…å«äº†ç›¸å…³çš„é…ç½®é¡¹ä»¥åŠEncoderçš„åˆ›å»ºè¿‡ç¨‹ã€‚</p> 
<p style="margin-left:.0001pt;text-align:left;">Â Â Â Â Â Â Â Â PTuningV1çš„æºç è¯¦è§src/peft/tuners/p_tuning.pyï¼Œé‡Œé¢åŒ…å«äº†ç›¸å…³çš„é…ç½®é¡¹ä»¥åŠEncoderçš„åˆ›å»ºè¿‡ç¨‹ã€‚</p> 
<p style="margin-left:.0001pt;text-align:left;">Â Â Â Â Â Â Â Â PromptTuning/PTuningV1çš„æ¨ç†è¿‡ç¨‹å°±æ˜¯å°†è™šæ‹Ÿtokençš„embeddingç›´æ¥concatåˆ°åŸå§‹è¾“å…¥åºåˆ—çš„å‰é¢ï¼Œå¹¶å¯¹attention maskè¿›è¡Œæ‰©å……ï¼Œé€å…¥base modelæ¨¡å‹è¿›è¡Œæ¨ç†ã€‚å¯¹åº”ä¸Šé¢è®²åˆ°çš„2å’Œ3ã€‚</p> 
<p style="margin-left:.0001pt;text-align:left;"></p> 
<p style="margin-left:.0001pt;text-align:left;"><strong><strong>PrefixTuning/PTuningV2æºç </strong></strong></p> 
<p style="margin-left:.0001pt;text-align:left;">Â Â Â Â Â Â Â Â PrefixTuningå’ŒPTuningV2çš„å…±åŒç‚¹éƒ½æ˜¯ä½¿ç”¨äº†æ·±å±‚çš„Promptï¼Œåœ¨æ¯å±‚Transformer Blockéƒ½ä½¿ç”¨ï¼Œæ‰€ä»¥PrefixTuningå’ŒPTuningV2çš„å®ç°æ”¾åœ¨ä¸€èµ·ã€‚</p> 
<p style="margin-left:.0001pt;text-align:left;">Â Â Â Â Â Â Â Â ä¸Šé¢è¯´åˆ°å½“æ‰§è¡Œçš„PEFTç±»å‹æ˜¯PrefixTuning/PTuningV2æ—¶ï¼Œç”±äºè¦ç»™æ¯ä¸ªTransformer Blockçš„Keyå’ŒValueå‰é¢éƒ½åŠ ä¸Šå¯ä»¥å­¦ä¹ çš„virtual token embeddingï¼Œéœ€è¦ä½¿ç”¨_prefix_tuning_forwardå‡½æ•°è¿›è¡Œé¢å¤–çš„å¤„ç†ã€‚</p> 
<p style="margin-left:.0001pt;text-align:left;">Â Â Â Â Â Â Â Â åœ¨çœ‹_prefix_tuning_forwardå‡½æ•°ä¹‹å‰å…ˆäº†è§£ä¸€äº›ç›¸å…³çŸ¥è¯†ã€‚è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡å¯ä»¥åˆ†ä¸ºAuto-Encodingï¼ˆä¹Ÿå«NLUã€è‡ªç„¶è¯­è¨€ç†è§£ã€Masked Language Modelï¼‰å’ŒAuto-Regressiveï¼ˆä¹Ÿå«NLGã€è‡ªç„¶è¯­è¨€ç”Ÿæˆã€Language Modelï¼‰ã€‚å¯¹äºAuto-Encodingç±»å‹çš„ä»»åŠ¡ï¼Œåœ¨æ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹é˜¶æ®µï¼Œself-attentionéƒ½å¯ä»¥å¹¶è¡Œè®¡ç®—ã€‚å¯¹äºAuto-Regressiveç±»å‹çš„ä»»åŠ¡ï¼Œåœ¨æ¨¡å‹è®­ç»ƒé˜¶æ®µé€šè¿‡ä½¿ç”¨attention maskä¹Ÿèƒ½å¤Ÿè¿›è¡Œå¹¶è¡Œè®¡ç®—ï¼Œä½†æ˜¯åœ¨æ¨¡å‹é¢„æµ‹é˜¶æ®µï¼Œç”±äºæ˜¯æ—¶åºç”Ÿæˆä»»åŠ¡ï¼Œåªèƒ½ä¸€æ­¥ä¸€æ­¥çš„ç”Ÿæˆï¼Œæ²¡åŠæ³•å¹¶è¡Œã€‚æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹åœ¨ç”Ÿæˆé˜¶æ®µæ¯ä¸ªæ—¶é—´æ­¥æœ‰å“ªäº›è®¡ç®—ï¼š</p> 
<p style="margin-left:.0001pt;text-align:left;">Â Â Â Â Â Â Â Â <strong>æ—¶é—´æ­¥tï¼š</strong></p> 
<p style="margin-left:.0001pt;text-align:left;">Â Â Â Â Â Â Â Â Queryï¼šæ¥è‡ªå‰ä¸€ä¸ªæ—¶é—´æ­¥t-1æ—¶åˆ»çš„è¾“å‡ºï¼Œä½¿ç”¨queryçŸ©é˜µï¼ˆå°±æ˜¯ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼‰è½¬æ¢ä¹‹åå¾—åˆ°</p> 
<p style="margin-left:.0001pt;text-align:left;">Â Â Â Â Â Â Â Â Keyï¼šæ¥è‡ªå‰t-1ä¸ªæ—¶é—´æ­¥[1,â€¦â€¦,t-1]çš„è¾“å‡ºï¼Œä½¿ç”¨keyçŸ©é˜µï¼ˆå°±æ˜¯ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼‰è½¬æ¢ä¹‹åå¾—åˆ°</p> 
<p style="margin-left:.0001pt;text-align:left;">Â Â Â Â Â Â Â Â Valueï¼šæ¥è‡ªå‰t-1ä¸ªæ—¶é—´æ­¥[1,â€¦â€¦,t-1]çš„è¾“å‡ºï¼Œä½¿ç”¨valueçŸ©é˜µï¼ˆå°±æ˜¯ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼‰è½¬æ¢ä¹‹åå¾—åˆ°</p> 
<p style="margin-left:.0001pt;text-align:left;">Â Â Â Â Â Â Â Â <strong>æ—¶é—´æ­¥t + 1ï¼š</strong></p> 
<p style="margin-left:.0001pt;text-align:left;">Â Â Â Â Â Â Â Â Queryï¼šæ¥è‡ªå‰ä¸€ä¸ªæ—¶é—´æ­¥tæ—¶åˆ»çš„è¾“å‡ºï¼Œä½¿ç”¨queryçŸ©é˜µï¼ˆå°±æ˜¯ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼‰è½¬æ¢ä¹‹åå¾—åˆ°</p> 
<p style="margin-left:.0001pt;text-align:left;">Â Â Â Â Â Â Â Â Keyï¼šæ¥è‡ªå‰tä¸ªæ—¶é—´æ­¥[1,â€¦â€¦,t-1,t]çš„è¾“å‡ºï¼Œä½¿ç”¨keyçŸ©é˜µï¼ˆå°±æ˜¯ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼‰è½¬æ¢ä¹‹åå¾—åˆ°</p> 
<p style="margin-left:.0001pt;text-align:left;">Â Â Â Â Â Â Â Â Valueï¼šæ¥è‡ªå‰tä¸ªæ—¶é—´æ­¥[1,â€¦â€¦,t-1,t]çš„è¾“å‡ºï¼Œä½¿ç”¨valueçŸ©é˜µï¼ˆå°±æ˜¯ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼‰è½¬æ¢ä¹‹åå¾—åˆ°</p> 
<p style="margin-left:.0001pt;text-align:left;">Â Â Â Â Â Â Â Â å¯ä»¥çœ‹åˆ°ï¼Œæ¯ä¸ªæ—¶é—´æ­¥çš„Queryéƒ½å’Œä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºç›¸å…³ï¼Œæ¯ä¸€æ­¥éƒ½éœ€è¦é‡æ–°è®¡ç®—Queryï¼Œä½†æ˜¯keyå’Œvalueæ¥è‡ªå‰t-1ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºç›¸å…³ï¼Œæ‰€ä»¥t+1æ—¶åˆ»çš„keyå’Œvalueä¸tæ—¶åˆ»çš„keyå’Œvalueåœ¨[1,â€¦â€¦,t-1]æ—¶åˆ»ä¸Šçš„è®¡ç®—ç»“æœæ˜¯ç›¸åŒçš„ï¼Œä¹Ÿå°±æ˜¯è¿™äº›ç»“æœæ˜¯å¯ä»¥å¤ç”¨çš„ï¼Œåœ¨æ¯ä¸ªæ—¶åˆ»å¯ä»¥å¤ç”¨å‰ä¸€æ—¶åˆ»è®¡ç®—çš„keyå’Œvalueï¼Œç„¶åè¿½åŠ ä¸Šå½“å‰æ—¶åˆ»æ–°å¢çš„keyå’Œvalueï¼Œæ„æˆå®Œæ•´çš„keyå’Œvalueã€‚</p> 
<p style="margin-left:.0001pt;text-align:left;">Â Â Â Â Â Â Â Â åœ¨hugging faceå®ç°çš„self-attentionæ¨¡å—ä¸­ï¼Œä¸ºäº†å¤ç”¨decodeç”Ÿæˆé˜¶æ®µçš„keyå’Œvalueï¼Œä¼šä¼ å…¥ä¸€ä¸ªpast_key_valueså‚æ•°ï¼Œå¦‚æœpast_key_valuesä¸æ˜¯Noneï¼Œè¡¨ç¤ºå‰é¢æ—¶é—´æ­¥å·²ç»æœ‰è®¡ç®—ç»“æœäº†ï¼Œç›´æ¥å¤ç”¨ä¸Šä¸€æ­¥çš„ç»“æœï¼Œç„¶åå°†å½“å‰æ—¶é—´æ­¥çš„keyå’Œvalueæ‹¼æ¥ä¸Šå»ï¼Œæ›´æ–°åçš„past_key_valueså°†ç»§ç»­ä¼ é€’åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥ã€‚</p> 
<p style="margin-left:.0001pt;text-align:left;">Â Â Â Â Â Â Â Â æœ‰äº†ä¸Šé¢çš„èƒŒæ™¯çŸ¥è¯†ï¼Œå¯¹äº_prefix_tuning_forwardå‡½æ•°ä¸­å…³äºPrefixTuning/PTuningV2æ–¹æ³•çš„å®ç°å°±å¾ˆå¥½ç†è§£äº†ï¼Œå°±æ˜¯å°†ç”Ÿæˆçš„virtual token embeddingé€šè¿‡past_key_valueså‚æ•°å¸¦å…¥åˆ°transformer blockçš„æ¯ä¸€å±‚ï¼Œæ”¾åœ¨æ¯ä¸€å±‚keyå’Œvalueçš„å‰é¢ã€‚</p> 
<p style="margin-left:.0001pt;text-align:left;"></p> 
<p class="img-center"><img alt="" height="825" src="https://images2.imgbox.com/d8/22/I8em2q1Y_o.png" width="1067"></p> 
<p></p> 
<p><strong>LoRAæºç </strong></p> 
<p style="margin-left:.0001pt;text-align:left;">Â Â Â Â Â Â Â Â LoRAç®—æ³•çš„ä¸»è¦æµç¨‹å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ æ­¥ï¼š</p> 
<ol><li style="text-align:left;">æ ¹æ®LoraConfigä¸­çš„target_moduleå‚æ•°åœ¨base modelä¸­æ‰¾åˆ°éœ€è¦è¿›è¡ŒLoraæ“ä½œçš„module</li><li style="text-align:left;">ä¸ºæ‰¾åˆ°çš„target moduleæ·»åŠ Loraç›¸å…³å‚æ•°ï¼Œå°†è¿›è¡ŒLoraæ“ä½œä¹‹åçš„moduleå°è£…æˆLoraLayerå±‚ï¼Œç„¶åä½¿ç”¨Loraåçš„æ–°moduleæ›¿æ¢æ‰base modelä¸­è€çš„moduleã€‚</li><li style="text-align:left;">åœ¨è®­ç»ƒé˜¶æ®µï¼Œå°†æ–°æ·»åŠ çš„Loraå‚æ•°å’Œbiaså‚æ•°ï¼ˆå¦‚æœbiasè®¾ç½®ä¸ºallæˆ–è€…lora_onlyçš„è¯ï¼‰è®¾ç½®ä¸ºå¯è®­ç»ƒçš„ï¼Œrequires_grad=Trueï¼Œå…¶ä½™é¢„è®­ç»ƒå‚æ•°éƒ½å†»ç»“</li><li style="text-align:left;">åœ¨æ¨ç†é˜¶æ®µï¼Œå°†æ‰€æœ‰å‚æ•°è®¾ç½®ä¸ºrequires_grad=Falseï¼Œå¹¶å°†åšäº†Loraæ“ä½œçš„å±‚è¿›è¡Œå‚æ•°èåˆï¼Œå°†æ–°æ·»åŠ çš„Loraå‚æ•°èåˆåˆ°é¢„è®­ç»ƒå‚æ•°ä¸­</li></ol> 
<p style="margin-left:.0001pt;text-align:justify;">Â Â Â Â Â Â Â Â ä¸Šè¿°è¿™äº›æµç¨‹åœ¨add_adapterå‡½æ•°ä¸­è¿›è¡Œä¸²è”èµ·æ¥ï¼š</p> 
<pre><code class="language-python">    def add_adapter(self, adapter_name, config=None):
        if config is not None:
            # åŸºäºmodel configæ›´æ–°lora configï¼Œä¸»è¦è¡¥å……çš„æ˜¯target_moduleå‚æ•°
            model_config = self.model.config.to_dict() if hasattr(self.model.config, "to_dict") else self.model.config
            config = self._prepare_lora_config(config, model_config)
            self.peft_config[adapter_name] = config
        # æ ¹æ®target moduleå‚æ•°ï¼Œåœ¨base modelä¸­å¯»æ‰¾å¹¶æ›´æ–°ç›®æ ‡æ¨¡å—ï¼Œå°†loraç›¸å…³å†…å®¹æ›´æ–°åˆ°æ¨¡å‹æ¶æ„ä¸­
        self._find_and_replace(adapter_name)
        if len(self.peft_config) &gt; 1 and self.peft_config[adapter_name].bias != "none":
            raise ValueError(
                "LoraModel supports only 1 adapter with bias. When using multiple adapters, set bias to 'none' for all adapters."
            )
        # è®¾ç½®åªæœ‰loraç›¸å…³çš„å‚æ•°å¯ä»¥å¾®è°ƒï¼Œå†»ç»“å…¶ä»–å‚æ•°
        mark_only_lora_as_trainable(self.model, self.peft_config[adapter_name].bias)
        if self.peft_config[adapter_name].inference_mode:
            # åœ¨æ¨ç†æ¨¡å¼ä¸‹å†»ç»“æ‰€æœ‰å‚æ•°
            _freeze_adapter(self.model, adapter_name)</code></pre> 
<p style="margin-left:.0001pt;text-align:left;">åŸºäºä¸Šè¿°æµç¨‹ï¼ŒLoraæºç ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å¤§çš„æ¨¡å—ï¼ˆç±»ï¼‰ï¼š</p> 
<p style="text-align:left;">1ã€LoraConfigï¼šLoraé…ç½®æ–‡ä»¶</p> 
<p style="text-align:left;">Â  Â  Â  Â  a)ã€LoraConfigä¸­è®°å½•Loraç›¸å…³çš„é…ç½®å‚æ•°ï¼Œå…¶ä¸­é‡ç‚¹çœ‹ä¸€ä¸‹target_moduleã€biasã€modules_to_saveè¿™å‡ ä¸ªå‚æ•°ã€‚</p> 
<p style="text-align:left;">Â  Â  Â  Â  b)ã€target_moduleå‚æ•°ç”¨æ¥æŒ‡å‡ºbase modelä¸­å«ä»€ä¹ˆåå­—çš„å‚æ•°éœ€è¦æ‰§è¡ŒLoraæ“ä½œï¼Œåœ¨other.pyä¸­TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPINGåˆ—å‡ºäº†æ¯ä¸ªæ¨¡å‹ä¸­éœ€è¦è¿›è¡ŒLoraæ“ä½œçš„å‚æ•°åç§°ã€‚</p> 
<p style="text-align:left;">Â  Â  Â  Â  c)ã€biaså¯¹åº”çš„å°±æ˜¯BitFitå‚æ•°å¾®è°ƒæ–¹æ³•ï¼Œå°±æ˜¯åœ¨å¾®è°ƒæ—¶æ˜¯å¦å¾®è°ƒbiaså‚æ•°ï¼š</p> 
<pre><code># bias -&gt; none æ‰€æœ‰å±‚çš„biaséƒ½ä¸å¾®è°ƒ
# bias -&gt; all æ‰€æœ‰å±‚çš„biaséƒ½å¾®è°ƒ
# bias -&gt; lora_only åªæœ‰LoRAç›¸å…³å±‚çš„biasè¿›è¡Œå¾®è°ƒ</code></pre> 
<p style="margin-left:.0001pt;text-align:left;">Â Â Â Â Â Â Â Â modules_to_saveç”¨æ¥å­˜æ”¾å“ªäº›æ¨¡å‹å‚æ•°æ˜¯é‡æ–°è®­ç»ƒè¿‡çš„ï¼Œä¿å­˜æ¨¡å‹çš„æ—¶å€™å¯ä»¥åªä¿å­˜è¿™äº›å‚æ•°ï¼Œå…¶ä»–çš„éƒ½æ˜¯é¢„è®­ç»ƒå‚æ•°ï¼Œå¯ä»¥ä¸ä¿å­˜ã€‚</p> 
<p style="text-align:left;">2ã€LoraLayerï¼šç”¨äºè®°å½•Loraç›¸å…³å‚æ•°ï¼Œåˆå§‹åŒ–Loraéƒ¨åˆ†çš„æ¨¡å‹ã€‚åœ¨LoraLayeréƒ¨åˆ†ç›®å‰ä»…æ”¯æŒä¸¤ç§å­æ¨¡å‹çš„Loraæ“ä½œï¼Œåˆ†åˆ«æ˜¯nn.Embeddingå’Œnn.Linearï¼ˆConv1Dç­‰ä»·äºnn.Linearï¼‰ï¼Œä¸‹é¢çœ‹ä¸€ä¸‹è¿™éƒ¨åˆ†ä¸»è¦çš„ä¸‰ä¸ªç±»ï¼š</p> 
<p style="margin-left:.0001pt;text-align:justify;">Â  Â  Â  Â  a)ã€LoraLayerï¼šLoraLayerä¸­å®ç°äº†åœ¨Linearå±‚ä¸­æ·»åŠ Loraæ“ä½œçš„æ–¹æ³•update_layerå’Œåœ¨Embeddingå±‚ä¸­æ·»åŠ Loraæ“ä½œçš„update_layer_embeddingæ–¹æ³•ï¼Œè¿™ä¸¤ä¸ªæ–¹æ³•ä¸­æ ¹æ®åŸå§‹å‚æ•°å½¢çŠ¶åˆå§‹åŒ–äº†Loraå‚æ•°ï¼Œåˆ†åˆ«åœ¨å­ç±»Linearå’Œå­ç±»Embeddingä¸­è°ƒç”¨ã€‚</p> 
<p class="img-center"><img alt="" height="356" src="https://images2.imgbox.com/c3/c8/a9VBAMth_o.png" width="692"></p> 
<p class="img-center"><img alt="" height="411" src="https://images2.imgbox.com/13/3a/q7lw5I5i_o.png" width="692"></p> 
<p></p> 
<p style="margin-left:.0001pt;text-align:justify;">Â  Â  Â  Â  b)ã€Linearï¼šLinearå±‚ç»§æ‰¿nn.Linearå’ŒLoraLayerï¼Œå®ç°äº†åœ¨nn.Linearå±‚å’ŒConv1Då±‚ï¼ˆhugging faceçš„transformersä¸­å®ç°ï¼‰ä¸­ä½¿ç”¨Loraã€‚ä½¿ç”¨nn.Linearåˆ›å»ºå’ŒåŸå§‹å‚æ•°ç›¸åŒå½¢çŠ¶çš„å‚æ•°ï¼Œåé¢ç”¨æ¥å­˜å‚¨é¢„è®­ç»ƒçš„å‚æ•°éƒ¨åˆ†ï¼Œä½¿ç”¨LoraLayerçš„update_layerå‡½æ•°åˆ›å»ºLoraæ–°å¢çš„å‚æ•°éƒ¨åˆ†ï¼Œè¿™æ ·å°±å°†ä¸€ä¸ªLinearå­æ¨¡å—æ”¹é€ æˆäº†æ·»åŠ äº†Loraçš„å­æ¨¡å—ã€‚Linearå±‚ä¸­è¿˜å®ç°äº†mergeå’Œunmergeå‡½æ•°ç”¨æ¥å°†Loraå‚æ•°åˆå…¥åŸå§‹Linearå±‚çš„é¢„è®­ç»ƒå‚æ•°ä¸­ï¼Œæˆ–è€…å°†Loraå‚æ•°ä»åŸå§‹Linearå±‚çš„å‚æ•°ä¸­åˆ†ç¦»å‡ºæ¥ã€‚</p> 
<p class="img-center"><img alt="" height="458" src="https://images2.imgbox.com/08/1e/PyliZIQG_o.png" width="692"></p> 
<p></p> 
<p style="margin-left:.0001pt;text-align:justify;">Â  Â  Â  Â  c)ã€Embeddingï¼šEmbeddingå±‚ç»§æ‰¿nn.Embeddingå’ŒLoraLayerï¼Œä¸Linearçš„å®ç°å‡ ä¹ä¸€æ ·ï¼ŒEmbeddingä¹Ÿæ˜¯åˆ›å»ºäº†å’ŒåŸå§‹Embeddingå‚æ•°ç›¸åŒå½¢çŠ¶çš„å‚æ•°ï¼Œåé¢ç”¨æ¥å­˜å‚¨é¢„è®­ç»ƒçš„å‚æ•°éƒ¨åˆ†ï¼Œä½¿ç”¨update_layer_embeddingå‡½æ•°åˆ›å»ºLoraæ–°å¢çš„å‚æ•°éƒ¨åˆ†ï¼Œè¿™æ ·å°±å°†ä¸€ä¸ªEmbeddingå­æ¨¡å—æ”¹é€ æˆäº†æ·»åŠ äº†Loraçš„å­æ¨¡å—ã€‚Embeddingå±‚ä¸­ä¹Ÿå®ç°äº†mergeå’Œunmergeå‡½æ•°ç”¨æ¥å°†Loraå‚æ•°åˆå…¥åŸå§‹Embeddingå±‚çš„é¢„è®­ç»ƒå‚æ•°ä¸­ï¼Œæˆ–è€…å°†Loraå‚æ•°ä»åŸå§‹Embeddingå±‚çš„å‚æ•°ä¸­åˆ†ç¦»å‡ºæ¥ã€‚</p> 
<p class="img-center"><img alt="" height="477" src="https://images2.imgbox.com/08/8f/4iu37NCp_o.png" width="692"></p> 
<p>3ã€LoraModelï¼šåŸºäºä¸Šè¿°ç»„ä»¶ï¼ŒLoraModelå¯ä»¥æ‹†è§£ä¸ºä»¥ä¸‹éƒ¨åˆ†ï¼š</p> 
<p style="text-align:left;">Â  Â  Â  Â  a)ã€éå†base modelçš„æ‰€æœ‰å­moduleï¼Œæ ¹æ®LoraConfigä¸­çš„target_moduleå‚æ•°æ‰¾åˆ°éœ€è¦è¿›è¡ŒLoraæ“ä½œçš„moduleï¼Œåœ¨LoraModelçš„_find_and_replaceå‡½æ•°ä¸­å®ç°</p> 
<p class="img-center"><img alt="" height="285" src="https://images2.imgbox.com/5c/bd/kUj0TVSy_o.png" width="692"></p> 
<p></p> 
<p style="text-align:left;">Â  Â  Â  Â  b)ã€åŸºäºä¸Šä¸€æ­¥æ‰¾åˆ°çš„target moduleï¼Œæ‰¾åˆ°å…¶çˆ¶æ¨¡å—parentï¼Œå¹¶ä¸ºæ‰¾åˆ°çš„target moduleæ·»åŠ Loraç›¸å…³å‚æ•°ï¼Œå°†è¿›è¡ŒLoraæ“ä½œä¹‹åçš„moduleå°è£…æˆLoraLayerå±‚</p> 
<p class="img-center"><img alt="" height="50" src="https://images2.imgbox.com/47/54/uOCUnL7v_o.png" width="692"></p> 
<p class="img-center"><img alt="" height="534" src="https://images2.imgbox.com/06/20/soUzT6eC_o.png" width="692"></p> 
<p></p> 
<p style="margin-left:.0001pt;text-align:left;"><strong><strong>å¤‡æ³¨ï¼š</strong></strong>å½“å‰PEFTç‰ˆæœ¬ä¸­æ”¯æŒè¿›è¡ŒLoraçš„å±‚åªæœ‰nn.Linearã€nn.Embeddingå’ŒConv1Dï¼ˆæ¥è‡ªhugging faceçš„transformersåº“ï¼Œå®é™…ä¸Šå°±æ˜¯ä¸€ä¸ªweightåšäº†è½¬ç½®çš„nn.Linearå±‚ï¼‰ã€‚æ ¹æ®é‡å‚æ•°åŒ–Reparameterizationçš„æ€æƒ³ï¼ˆé‡å‚æ•°åŒ–è¯¦è§RepVGGï¼‰ï¼Œå·ç§¯æ“ä½œä¹Ÿå¯ä»¥ä½¿ç”¨Loraï¼Œpeft-mainåˆ†æ”¯é‡Œé¢å·²ç»æ­£åœ¨å¼€å‘è¿™ä¸ªåŠŸèƒ½äº†ã€‚</p> 
<p style="text-align:left;">Â  Â  Â  Â  c)ã€æ ¹æ®æ‰¾åˆ°çš„target moduleçš„çˆ¶æ¨¡å—ï¼Œä½¿ç”¨Loraåçš„æ–°moduleæ›¿æ¢æ‰base modelä¸­å¯¹åº”çš„è€çš„moduleï¼Œåœ¨_replace_moduleå‡½æ•°ä¸­å®ç°</p> 
<p class="img-center"><img alt="" height="462" src="https://images2.imgbox.com/33/d9/D1IAv9VK_o.png" width="692"></p> 
<p></p> 
<p style="text-align:left;">Â  Â  Â  Â  d)ã€åœ¨è®­ç»ƒé˜¶æ®µï¼Œå°†æ–°æ·»åŠ çš„Loraå‚æ•°å’Œbiaså‚æ•°ï¼ˆå¦‚æœbiasè®¾ç½®ä¸ºallæˆ–è€…lora_onlyçš„è¯ï¼‰è®¾ç½®ä¸ºå¯è®­ç»ƒçš„ï¼Œrequires_grad=Trueï¼Œå…¶ä½™é¢„è®­ç»ƒå‚æ•°éƒ½å†»ç»“</p> 
<p class="img-center"><img alt="" height="418" src="https://images2.imgbox.com/de/9c/2ZlNTFtL_o.png" width="692"></p> 
<p></p> 
<p style="text-align:left;">Â  Â  Â  Â  e)ã€åœ¨æ¨ç†é˜¶æ®µï¼Œå°†æ‰€æœ‰å‚æ•°è®¾ç½®ä¸ºrequires_grad=Falseï¼Œå¹¶å°†åšäº†Loraæ“ä½œçš„å±‚è¿›è¡Œå‚æ•°èåˆï¼Œå°†æ–°æ·»åŠ çš„Loraå‚æ•°èåˆåˆ°é¢„è®­ç»ƒå‚æ•°ä¸­</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4e7c144d676d4c4927b266d5a431244a/" rel="prev">
			<span class="pager__subtitle">Â«&thinsp;Previous</span>
			<p class="pager__title">MySQL-å¤šè¡¨è®¾è®¡-ä¸€å¯¹å¤š</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4112a8339148fc034a3d043da55bfd00/" rel="next">
			<span class="pager__subtitle">Next&thinsp;Â»</span>
			<p class="pager__title">ä¼ä¸šå¾®ä¿¡åŒå¼€å®æ“è®°å½•</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 ç¼–ç¨‹éšæƒ³.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>