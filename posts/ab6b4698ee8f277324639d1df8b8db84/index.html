<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Redis 主从集群 —— 超详细操作演示！ - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Redis 主从集群 —— 超详细操作演示！" />
<meta property="og:description" content="五、Redis 主从集群 五、Redis 主从集群5.1 主从集群搭建5.1.1、伪集群搭建与配置5.1.2、分级管理5.1.3、容灾冷处理 5.2 主从复制原理5.2.1、主从复制原理5.2.2、数据同步演变过程 5.3 哨兵机制实现5.3.1 简介5.3.2 Redis高可用集群搭建5.3.3 Redis高可用集群的启动5.3.4 Sentinel 优化配置 5.4 哨兵机制原理5.4.1 三个定时任务5.4.2 Redis 节点下线判断5.4.3 Sentinel Leader 选举5.4.4 master 选择算法5.4.5 故障转移过程5.4.6 节点上线 5.5 CAP 定理5.5.1 概念5.5.2 定理5.5.3 BASE理论5.5.4 CAP的应用 5.6 Raft 算法5.6.1 基础5.6.2 角色、任期及角色转变5.6.3 leader 选举5.6.4 数据同步5.6.5 脑裂5.6.6 Leader 宕机处理5.6.7 Raft 算法动画演示 数据库系列文章：
关系型数据库:
MySQL —— 基础语法大全MySQL —— 进阶 非关系型数据库:
Redis 的安装与配置Redis 基本命令（上）Redis 基本命令（下）Redis 持久化 五、Redis 主从集群 为了避免 Redis 的 单点故障 问题， 我们可以搭建一个 Redis 集群，将数据备份到集群中的其它节点上。若一个 Redis 节点宕机，则由集群中的其它节点顶上。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/ab6b4698ee8f277324639d1df8b8db84/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-24T14:56:00+08:00" />
<meta property="article:modified_time" content="2023-12-24T14:56:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Redis 主从集群 —— 超详细操作演示！</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>五、Redis 主从集群</h4> 
 <ul><li><ul><li><a href="#Redis__14" rel="nofollow">五、Redis 主从集群</a></li><li><ul><li><a href="#51__18" rel="nofollow">5.1 主从集群搭建</a></li><li><ul><li><ul><li><a href="#511_20" rel="nofollow">5.1.1、伪集群搭建与配置</a></li><li><a href="#512_99" rel="nofollow">5.1.2、分级管理</a></li><li><a href="#513_111" rel="nofollow">5.1.3、容灾冷处理</a></li></ul> 
    </li></ul> 
    </li><li><a href="#52__121" rel="nofollow">5.2 主从复制原理</a></li><li><ul><li><ul><li><a href="#521_122" rel="nofollow">5.2.1、主从复制原理</a></li><li><a href="#522_163" rel="nofollow">5.2.2、数据同步演变过程</a></li></ul> 
    </li></ul> 
    </li><li><a href="#53__239" rel="nofollow">5.3 哨兵机制实现</a></li><li><ul><li><ul><li><a href="#531__240" rel="nofollow">5.3.1 简介</a></li><li><a href="#532_Redis_248" rel="nofollow">5.3.2 Redis高可用集群搭建</a></li><li><a href="#533_Redis_318" rel="nofollow">5.3.3 Redis高可用集群的启动</a></li><li><a href="#534_Sentinel__371" rel="nofollow">5.3.4 Sentinel 优化配置</a></li></ul> 
    </li></ul> 
    </li><li><a href="#54__416" rel="nofollow">5.4 哨兵机制原理</a></li><li><ul><li><ul><li><a href="#541__417" rel="nofollow">5.4.1 三个定时任务</a></li><li><a href="#542_Redis__439" rel="nofollow">5.4.2 Redis 节点下线判断</a></li><li><a href="#543_Sentinel_Leader__450" rel="nofollow">5.4.3 Sentinel Leader 选举</a></li><li><a href="#544_master__462" rel="nofollow">5.4.4 master 选择算法</a></li><li><a href="#545__472" rel="nofollow">5.4.5 故障转移过程</a></li><li><a href="#546__485" rel="nofollow">5.4.6 节点上线</a></li></ul> 
    </li></ul> 
    </li><li><a href="#55_CAP__502" rel="nofollow">5.5 CAP 定理</a></li><li><ul><li><ul><li><a href="#551__503" rel="nofollow">5.5.1 概念</a></li><li><a href="#552__508" rel="nofollow">5.5.2 定理</a></li><li><a href="#553_BASE_510" rel="nofollow">5.5.3 BASE理论</a></li><li><a href="#554_CAP_535" rel="nofollow">5.5.4 CAP的应用</a></li></ul> 
    </li></ul> 
    </li><li><a href="#56_Raft__559" rel="nofollow">5.6 Raft 算法</a></li><li><ul><li><ul><li><a href="#561__560" rel="nofollow">5.6.1 基础</a></li><li><a href="#562__565" rel="nofollow">5.6.2 角色、任期及角色转变</a></li><li><a href="#563_leader__572" rel="nofollow">5.6.3 leader 选举</a></li><li><a href="#564__602" rel="nofollow">5.6.4 数据同步</a></li><li><a href="#565__627" rel="nofollow">5.6.5 脑裂</a></li><li><a href="#566_Leader__669" rel="nofollow">5.6.6 Leader 宕机处理</a></li><li><a href="#567_Raft__694" rel="nofollow">5.6.7 Raft 算法动画演示</a></li></ul> 
    </li></ul> 
   </li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<blockquote> 
 <p><font color="red"><strong>数据库系列文章：</strong></font><br> <br> <strong>关系型数据库</strong>:</p> 
 <ul><li><a href="https://blog.csdn.net/weixin_43412762/article/details/132051493">MySQL —— 基础语法大全</a></li><li><a href="https://blog.csdn.net/weixin_43412762/article/details/132631240">MySQL —— 进阶</a></li></ul> 
 <p><br> <strong>非关系型数据库</strong>:</p> 
 <ul><li><a href="https://blog.csdn.net/weixin_43412762/article/details/133688635">Redis 的安装与配置</a></li><li><a href="https://blog.csdn.net/weixin_43412762/article/details/133934585">Redis 基本命令（上）</a></li><li><a href="https://blog.csdn.net/weixin_43412762/article/details/134104962">Redis 基本命令（下）</a></li><li><a href="https://blog.csdn.net/weixin_43412762/article/details/134795648">Redis 持久化</a></li></ul> 
</blockquote> 
<h3><a id="Redis__14"></a>五、Redis 主从集群</h3> 
<p>    为了<strong>避免</strong> Redis 的 <mark><strong>单点故障</strong></mark> 问题， 我们可以搭建一个 Redis 集群，将<strong>数据备份</strong>到集群中的其它节点上。若一个 Redis 节点宕机，则由集群中的其它节点顶上。</p> 
<h4><a id="51__18"></a>5.1 主从集群搭建</h4> 
<p>    Redis的 <strong>主从集群</strong> 是一个“ <mark>一主多从</mark> ”的 <font color="red"><strong>读写分离</strong></font> 集群。 集群中 的 <code>Master</code> 节点负责处理客户端的<strong>读写请</strong>求，而 <code>Slave</code> 节点仅能处理客户端的<strong>读请求</strong>。只所以要将集群搭建为 <strong>读写分离</strong> 模式，主要原因是，对于数据库集群，<strong>写操作压力</strong>一般都较小，压力大多数来自于<strong>读操作请求</strong>。<mark>所以，<strong>只有一个节点</strong>负责处理<strong>写操作请求</strong>即可</mark>。</p> 
<h6><a id="511_20"></a>5.1.1、伪集群搭建与配置</h6> 
<p>    在采用 <font color="blue"><strong>单线程</strong> IO 模型</font> 时，为了<mark>提高处理器的<strong>利用率</strong></mark>，一般会在一个主机中安装多台 Redis，构建一个 Redis <strong>主从伪集群</strong> 。当然，搭建伪集群的另一个场景是，在学习 Redis ，而学习用主机内存不足以创建多个虚拟机。</p> 
<p>    下面要搭建的<strong>读写分离伪集群</strong>包含一个 <code>Master</code> 与两个 <code>Slave</code> 。 它们的端口号分别是： <code>6380</code> 、<code>6381</code> 、 <code>6382</code> 。</p> 
<p><strong>⭐️（1）复制 redis.conf</strong></p> 
<p>    在redis 安装目录中 <code>mkdir</code> 一个目录，名称随意。这里命名为 <code>cluster</code> 。然后将 <code>redis.conf</code> 文件复制到 <code>cluster</code> 目录中。该文件后面会被其它配置文件包含，所以该文件中需要设置每个 Redis 节点相同的<strong>公共的属性</strong>。</p> 
<p><img src="https://images2.imgbox.com/23/58/Un1D998b_o.png" alt="在这里插入图片描述"></p> 
<p><strong>⭐️（2）修改 redis.conf</strong></p> 
<p>    在 <code>redis.conf</code> 中做如下几项修改：</p> 
<p><strong>A、 masterauth</strong></p> 
<p><img src="https://images2.imgbox.com/86/97/in5bMeov_o.png" alt="在这里插入图片描述"></p> 
<p>    因为我们要搭建 <strong>主从集群</strong>，且每个主机都有可能会是 <code>Master</code> ，所以最好 <mark><em>不要设置密码验证</em></mark> 属性 <code>requirepass</code> 。如果真需要设置，一定要每个主机的密码都 <mark><em>设置为相同</em></mark> 的。此时每个配置文件中都要设置两个完全相同的属性： <code>requirepass</code> 与 <code>masterauth</code> 。</p> 
<ul><li>其中 <code>requirepass</code> 用于指定<strong>当前主机</strong>的访问密码；</li><li>而 <code>masterauth</code> 用于指定当前 <code>slave</code> 访问 <code>master</code> 时向 <code>master</code> 提交的访问密码，用于让 <code>master</code> <strong>验证自己身份是否合法</strong>。</li></ul> 
<p><img src="https://images2.imgbox.com/8d/44/wP1pWaRI_o.png" alt="在这里插入图片描述"></p> 
<p><strong>B、 repl-disable-tcp-nodelay</strong></p> 
<p><img src="https://images2.imgbox.com/b1/39/JHPdtdY9_o.png" alt="在这里插入图片描述"></p> 
<p>    该属性用于设置 <strong>是否禁用</strong> TCP 特性 <code>tcp-nodelay</code> 。设置为 <code>yes</code> 则<strong>禁用</strong> <code>tcp-nodelay</code> ，此时 <code>master</code> 与 <code>slave</code> 间的通信会<strong>产生延迟</strong>，但使用的 TCP 包数量会较少，<strong>占用的网络带宽会较小</strong>。相反，如果设置为 <code>no</code> ，则<strong>网络延迟会变小</strong>，但使用的 TCP 包数量会较多，相应<strong>占用的网络带宽会大</strong>。</p> 
<blockquote> 
 <p><code>tcp-nodelay</code>： 为了充分<strong>复用网络带宽</strong>， TCP 总是希望<strong>发送尽可能大的数据块</strong>。为了达到该目的， TCP 中使用了一个名为 <code>Nagle</code> 的算法。</p> 
 <ul><li>Nagle 算法的工作原理是，网络在接收到要发送的数据后，并不直接发送，而是<mark>等待着<strong>数据量足够大</strong>（由 TCP 网络特性决定）时再一次性发送出去</mark>。这样，网络上传输的<strong>有效数据比例</strong>就得到了大大提升，无效数据传递量极大减少，于是就节省了网络带宽，缓解了网络压力。</li></ul> 
 <p><code>tcp-nodelay</code> 则是 TCP 协议中 <code>Nagle</code> 算法的<strong>开头</strong>。</p> 
</blockquote> 
<p><strong>⭐️（3）新建 redis6380.conf</strong></p> 
<p><img src="https://images2.imgbox.com/38/1e/WO1LwzZK_o.png" alt="在这里插入图片描述"></p> 
<p>    新建一个redis 配置文件 <code>redis6380.conf</code> ，该配置文件中的 Redis 端口号为 <code>6380</code> 。</p> 
<p><strong>⭐️（4）再复制出两个 conf 文件</strong></p> 
<p>    再使用 <code>redis6380.conf</code> 复制出两个 <code>conf</code> 文件： <code>redis6381.conf</code> 与 <code>redis6382.conf</code> 。然后修改其中的内容。</p> 
<p><img src="https://images2.imgbox.com/04/be/q2Tf0k7e_o.png" alt="在这里插入图片描述"><br>     修改 <code>redis6381.conf</code> 的内容如下：</p> 
<p><img src="https://images2.imgbox.com/3d/01/RVN6MoHE_o.png" alt="在这里插入图片描述"></p> 
<p>    修改 <code>redis6382.conf</code> 的内容如下：</p> 
<p><img src="https://images2.imgbox.com/ea/58/zhk8Tp6P_o.png" alt="在这里插入图片描述"></p> 
<p><strong>⭐️（5）启动三台 redis</strong></p> 
<p>    分别使用 <code>redis6380.conf</code> 、 <code>redis6381.conf</code> 与 <code>redis6382.conf</code> 三个配置文件启动三台 Redis。</p> 
<p><img src="https://images2.imgbox.com/e6/73/ioN4HZLN_o.png" alt="在这里插入图片描述"></p> 
<p><strong>⭐️（6）设置主从关系</strong></p> 
<p>    再打开三个会话框，分别使用客户端连接三台<br> Redis 。 然后通过 <code>slaveof</code> 命令，指定 <code>6380</code> 的 Redis 为 <code>Master</code> 。</p> 
<p><img src="https://images2.imgbox.com/41/90/dl201hbf_o.png" alt="在这里插入图片描述"></p> 
<p><strong>⭐️（7）查看状态信息</strong></p> 
<p>    通过 <code>info replication</code> 命令可查看当前连接的 Redis 的状态信息。</p> 
<p><img src="https://images2.imgbox.com/87/56/kB3Mzozw_o.png" alt="在这里插入图片描述"></p> 
<h6><a id="512_99"></a>5.1.2、分级管理</h6> 
<p>    若 Redis <strong>主从集群</strong> 中的 <code>Slave</code> 较多时，它们的<strong>数据同步过程</strong>会对 <code>Master</code> 形成较大的性能压力。此时可以对这些 <code>Slave</code> 进行<strong>分级管理</strong>。</p> 
<p><img src="https://images2.imgbox.com/54/83/xct8Oam1_o.png" alt="在这里插入图片描述"><br>     设置方式很简单，只需要让<strong>低级别</strong> <code>Slave</code> 指定其 <code>slaveof</code> 的主机为其上一级 <code>Slave</code> 即可。不过，上一级 <code>Slave</code> 的状态仍为 <code>Slave</code> ，只不过，其是更上一级的 <code>Slave</code> 。</p> 
<p>    例如，指定 <code>6382</code> 主机为 <code>6381</code> 主机的 <code>Slave</code> ，而 <code>6381</code> 主机仍为真正的 <code>Master</code> 的 <code>Slave</code> 。</p> 
<p><img src="https://images2.imgbox.com/bd/b7/lsgspD4i_o.png" alt="在这里插入图片描述"></p> 
<ul><li>此时会发现， <code>Master</code> 的 <code>Slave</code> 只有 <code>6381</code> 一个主机。</li></ul> 
<h6><a id="513_111"></a>5.1.3、容灾冷处理</h6> 
<p>    在 <code>Master/Slave</code> 的 Redis 集群中，若 <code>Master</code> <strong>出现宕机</strong>怎么办呢？有两种处理方式：</p> 
<ul><li>一种是通过 <strong>手工 角色调整</strong>，使 <code>Slave</code> 晋升为 <code>Master</code> 的 <strong>冷处理</strong>；</li><li>一种是使用<strong>哨兵模式</strong>，实现 Redis集群的高可用 <code>HA</code> ，即<strong>热处理</strong>。</li></ul> 
<p>    无论 <code>Master</code> 是否宕机， <code>Slave</code> 都可通过 <code>slaveof no one</code> 将自己由 <code>Slave</code> 晋升为 <code>Master</code> 。如果其原本就有下一级的 <code>Slave</code> ，那么，其就直接变为了这些 <code>Slave</code> 的真正的 <code>Master</code> 了。而原来的 <code>Master</code> 也会失去这个原来的 <code>Slave</code> 。</p> 
<p><img src="https://images2.imgbox.com/d7/77/5UUAgc1E_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="52__121"></a>5.2 主从复制原理</h4> 
<h6><a id="521_122"></a>5.2.1、主从复制原理</h6> 
<p>    当一个 Redis 节点 (<code>slave</code> 节点） 接收到 类似 <code>slaveof 127.0.0.1 6380</code> 的指令后直至其可以从 <code>master</code> 持续复制数据，大体经历了如下几个过程：</p> 
<p><img src="https://images2.imgbox.com/51/d7/u17j6ABW_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/cf/91/OUzDPOY5_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/cc/9c/gPuGNsmu_o.png" alt="在这里插入图片描述"><br> <strong>⭐️（1） 保存master 地址</strong></p> 
<p>    当 <code>slave</code> 接收到 <code>slaveof</code> 指令后，<code>slave</code> 会立即将新的 <code>master</code> 的<strong>地址保存下来</strong>。</p> 
<p><strong>⭐️（2） 建立连接</strong></p> 
<p>     <code>slave</code> 中维护着一个<strong>定时任务</strong>，该定时任务会尝试着与该 <code>master</code> <strong>建立</strong> <code>socket</code> <strong>连接</strong>。如果连接无法建立，则其会不断<strong>定时重试</strong>，<mark>直到 <strong>连接成功</strong> 或 <strong>接收到</strong> <code>slaveof no one</code> 指令</mark>。</p> 
<p><strong>⭐️（3） slave 发送 ping 命令</strong></p> 
<p>    连接建立成功后，<code>slave</code> 会发送 <code>ping</code> 命令进行<strong>首次通信</strong>。如果 <code>slave</code> 没有收到 <code>master</code> 的回复，则 <code>slave</code> 会<strong>主动断开连接</strong>，<mark>下次的定时任务会<strong>重新尝试连接</strong></mark>。</p> 
<p><strong>⭐️（4） 对 slave 身份验证</strong></p> 
<p>    如果 <code>master</code> 收到了 <code>slave</code> 的 <code>ping</code> 命令，并不会立即对其进行回复，而是会<strong>先进行身份验证</strong>。如果验证失败，则会发送消息拒绝连接；<mark>如果验证成功，则向 <code>slave</code> <strong>发送连接成功响应</strong></mark>。</p> 
<p><strong>⭐️（5） master 持久化</strong></p> 
<p>    首次通信成功后，<code>slave</code> 会向 <code>master</code> 发送数据<strong>同步请求</strong>。当 <code>master</code> 接收到请求后，会 <code>fork</code> 出一个<strong>子进程</strong>，让子进程以 <font color="red"><strong>异步方式</strong></font> 立即进行<strong>持久化</strong>。</p> 
<p><strong>⭐️（6） 数据发送</strong></p> 
<p>    持久化完毕后 <code>master</code> 会再 <code>fork</code> 出一个<strong>子进程</strong>，让该子进程以 <font color="red"><strong>异步方式</strong></font> 将数据发送给 <code>slave</code>。<code>slave</code> 会将接收到的数据 <mark><strong>不断写入</strong>到本地的持久化文件中</mark>。<br>     在 <code>slave</code> <strong>数据同步</strong>过程中，<code>master</code> 的<strong>主进程</strong>仍在不断地接受着客户端的<strong>写操作</strong>，且不仅将新的数据写入到了<code>master</code> 内存，同时也写入到了 <font color="blue"><strong>同步缓存</strong></font> 。当 <code>master</code> 的持久化文件中的数据发送完毕后，<code>master</code> 会再将 <strong>同步缓存中新的数据</strong> 发送给 <code>slave</code>，由 <code>slave</code> 将其写入到本地持久化文件中。数据同步完成。</p> 
<p><strong>⭐️（7） slave 恢复内存数据</strong></p> 
<p>    当 <code>slave</code> 与 <code>master</code> 的<strong>数据同步</strong>完成后， <code>slave</code> 就会<strong>读取本地的持久化文件</strong>，将其恢复到<strong>本地内存</strong>，然后就可以对外提供读服务了。</p> 
<p><strong>⭐️（8） 持续增量复制</strong></p> 
<p>    在 <code>slave</code> 对外提供服务过程中， <code>master</code> 会持续不断的将<strong>新的数据</strong>以 <font color="red"><strong>增量方式</strong></font> 发送给 <code>slave</code> 以保证主从数据的一致性。</p> 
<h6><a id="522_163"></a>5.2.2、数据同步演变过程</h6> 
<p><strong>⭐️（1） sync 同步</strong></p> 
<p>    Redis <code>2.8</code> 版本之前，<strong>首次通信成功</strong>后， <code>slave</code> 会向 <code>master</code> 发送 <code>sync</code> <strong>数据同步请求</strong>。然后 <code>master</code> 就会将其<strong>所有数据</strong>全部发送给 <code>slave</code> ，由 <code>slave</code> 保存到其<strong>本地的持久化文件中</strong>。这个过程称为 <font color="red"><strong>全量复制</strong></font>。</p> 
<blockquote> 
 <p>    但这里 <mark>存在一个问题</mark>：在 <font color="red"><strong>全量复制</strong></font> 过程中可能会出现由于<strong>网络抖动</strong>而导致复制过程中断。当网络恢复后， <code>slave</code> 与 <code>master</code> 重新连接成功，此时 <code>slave</code> 会重新发送 <code>sync</code> 请求，然后会 <mark><strong>从头开始</strong> <strong>全量复制</strong></mark>。</p> 
</blockquote> 
<blockquote> 
 <p>    由于全量复制过程<strong>非常耗时</strong>，所以期间出现网络抖动的概率很高。而中断后的从头开始不仅需要消耗大量的<strong>系统资源</strong>、<strong>网络带宽</strong>，而且<mark>可能会出现长时间无法完成<strong>全量复制</strong>的情况</mark>。</p> 
</blockquote> 
<p><strong>⭐️（2） psync 同步</strong></p> 
<p>    Redis <code>2.8</code> 版本之后，<strong>全量复制</strong> 采用了 <code>psync</code> （<code>Partial Sync</code> ，<code>不完全同步</code>） <strong>同步策略</strong>。 当全量复制过程出现由于<strong>网络抖动</strong>而导致复制过程中断时，当重新连接成功后，复制过程可以 “<font color="red"><strong>断点续传</strong></font>" 。即从断开位置开始继续复制 ，而不用从头再来。这就大大提升了性能。</p> 
<p>    为了实现 <code>psync</code> ，整个系统做了 <strong>三个大的变化</strong>：</p> 
<p><strong>A、 复制偏移量</strong></p> 
<p>    系统为每个要传送数据进行了编号，该编号从 <code>0</code> 开始，<strong>每个字节</strong>一个编号。该编号称为复制偏移量。参与复制的主从节点都会维护该复制偏移量。</p> 
<p>    <code>master</code> 每发送过一个字节数据后就会进行累计。统计信息通过 <code>info replicatio</code>n 的 <code>master_repl_offset</code> 可查看到。同时， <code>slave</code> 会定时向 <code>master</code> 上报其自身已完成的 <strong>复制偏移量</strong> 给 <code>master</code> ，所以 <code>master</code> 也会保存 <code>slave</code> 的复制偏移量 <code>offset</code> 。</p> 
<p>    <code>slave</code> 在接收到 <code>master</code> 的数据后，也会累计接收到的<strong>偏移量</strong>。统计信息通过 <code>info replication</code> 的 <code>slave_repl_offset</code> 可查看到。</p> 
<p><strong>B、 主节点复制 ID</strong></p> 
<p>    当 <code>master</code> 启动后就会动态生成一个长度为 40 位的 16 进制字符串作为当前 <code>master</code> 的 <strong>复制 ID</strong> ，该 ID 是在进行<strong>数据同步</strong>时 <code>slave</code> <strong>识别</strong> <code>master</code> 使用的。通过 <code>info replication</code> 的<code>master_replid</code> 属性可查看到该 ID 。</p> 
<p><strong>C、 复制积压缓冲区</strong></p> 
<p>    当 <code>master</code> 有连接的 <code>slave</code> 时，在 <code>master</code> 中就会创建并维护一个队列 <code>backlog</code> ，默认大小为 <code>1MB</code> ，该队列称为 <font color="red"><strong>复制积压缓冲区</strong></font> 。 <code>master</code> 接收到了 <strong>写操作数据</strong> 不仅会写入到 <code>master</code> 主存，写入到 <code>master</code> 中为每个 <code>slave</code> 配置的<strong>发送缓存</strong>，而且还会写入到 <strong>复制积压缓冲区</strong>。其作用就是用于<mark>保存最近操作的数据，以备“ <strong>断点续传</strong> ”时做 <strong>数据补偿</strong>，防止数据丢失</mark>。</p> 
<p><strong>D、 psync 同步过程</strong><br> <img src="https://images2.imgbox.com/68/d0/Ly2iWFEw_o.png" alt="在这里插入图片描述"></p> 
<p>    <code>psync</code> 是一个由 <code>slave</code> 提交的命令，其格式为 <code>psync &lt;master_replid&gt; &lt;repl_offset&gt;</code> ，表示当前 <code>slave</code> 要从指定的 <code>master</code> 中的 <code>repl_offset+1</code> 处<strong>开始复制</strong>。 <code>repl_offset</code> 表示当前 <code>slave</code> 已经完成复制的数据的 <code>offset</code> 。该命令保证了 “ <strong>断点续传</strong> ”的实现。</p> 
<p>    在<strong>第一次</strong>开始复制时， <code>slave</code> 并不知道 <code>master</code> 的动态 ID ，并且一定是<strong>从头开始复制</strong>，所以其提交的 <code>psync</code> 命令为 <code>PSYNC ? -1</code> 。即 <code>master_replid</code> 为问号( ？)，<code>repl_offset</code> 为 <code>-1</code> 。</p> 
<p>    如果复制过程中断后 <code>slave</code> 与 <code>master</code> 成功连接，则 <code>slave</code> 再次提交 <code>psyn</code> 命令。此时的 <code>psyn</code> 命令的 <code>repl_offset</code> 参数为其前面已经完成复制的数据的偏移量。</p> 
<p>    其实，并不是 <code>slave</code> 提交了 <code>psyn</code> 命令后就可以立即从 <code>master</code> 处开始复制，而是需要 <code>master</code> 给出响应结果后，根据响应结果来执行。 <code>master</code> 根据 <code>slave</code> 提交的请求及 <code>master</code> 自身情况会 给出不同的响应结果。响应结果有三种可能：</p> 
<ul><li><code>FULLRESYNC</code> <code>&lt;master_replid&gt; &lt;repl_offset&gt;</code>：告知 <code>slave</code> 当前 <code>master</code> 的动态 ID 及可以开始<strong>全量复制</strong>了，这里的 <code>repl_offset</code> 一般为 <code>0</code></li><li><code>CONTINUE</code> ：告知 <code>slave</code> 可以按照你提交的 <code>repl_offset</code> 后面位置开始“<strong>续传</strong>”了</li><li><code>ERR</code> ：告知 <code>slave</code> ，当前 <code>master</code> 的版本低于 Redis 2.8 ，不支持 <code>psyn</code> ，你可以开始全量复制了</li></ul> 
<p><strong>E、 psync 存在的问题</strong></p> 
<ul><li>在 <code>psync</code> 数据同步过程中，若 <code>slave</code> <mark><strong>重启</strong></mark>，在 <code>slave</code> 内存中保存的 <code>master</code> 的 <code>动态 ID</code> 与续传 <code>offset</code> 都会消失，“<strong>断点续传</strong>” 将无法进行，从而只能进行<strong>全量复制</strong>，<mark>导致资源浪费</mark>。</li><li>在 <code>psync</code> 数据同步过程中， <code>master</code> <mark><strong>宕机</strong></mark> 后 <code>slave</code> 会发生“<strong>易主</strong>”，从而导致 <code>slave</code> 需要从新 <code>master</code> 进行<strong>全量复制</strong>，<mark>形成资源浪费</mark>。</li></ul> 
<p><strong>⭐️（3） psync 同步的改进</strong></p> 
<p>    Redis 4.0 对 <code>psync</code> 进行了改进，提出了“<strong>同源增量同步</strong>”策略。</p> 
<p><strong>A、 解决 slave 重启问题</strong></p> 
<p>    针对“ <code>slave</code> <mark><strong>重启</strong></mark> 时 <code>master</code> <strong>动态 ID 丢失问题</strong>”，改进后的 <code>psync</code> 将 <code>master</code> 的 <code>动态 ID</code> 直接写入到了 <code>slave</code> 的<strong>持久化文件</strong>中。</p> 
<p>    slave重启后直接从<strong>本地持久化文件</strong>中读取 <code>master</code> 的 <code>动态 ID</code> ，然后向 <code>master</code> 提交 <strong>获取复制偏移量的请求</strong>。 <code>master</code> 会根据提交请求的 <code>slave</code> 地址，查找到保存在 <code>master</code> 中的<strong>复制偏移量</strong>，然后向 <code>slave</code> 回复 <code>FULLRE SYNC</code> <code>&lt;master_replid&gt; &lt;repl_offset&gt;</code>，以告知 <code>slave</code> 其马上要开始发送的位置。然后 <code>master</code> 开始“<strong>断点续传</strong>”。</p> 
<p><strong>B、 解决 slave 易主问题</strong></p> 
<p>    <code>slave</code> 易主后需要和新 <code>master</code> 进行<strong>全量复制</strong>，本质原因是 <strong>新</strong> <code>master</code> 不认识 <code>slave</code> 提交的 <code>psync</code> 请求中“原 <code>master</code> 的 <code>动态 ID</code> ”。如果 <code>slave</code> 发送 <code>PSYNC</code> <code>&lt; 原 master_replid&gt; &lt;repl_offset&gt;</code> 命令，<strong>新</strong> <code>master</code> 能够识别出该 <code>slave</code> 要从 <strong>原</strong> <code>master</code> 复制数据，而自己的数据也都是从该 <code>master</code> 复制来的。那么 <strong>新</strong> <code>master</code> 就会明白，其与该 <code>slave</code> “<strong>师出同门</strong>”，应该接收其“<strong>断点续传</strong>”同步请求。</p> 
<p>    而 <strong>新</strong> <code>master</code> 中恰好保存的有“<strong>原</strong> <code>master</code> 的动态 ID ”。由于改进后的 <code>psync</code> 中每个 <code>slave</code> 都在本地保存了当前 <code>master</code> 的<strong>动态 ID</strong> ，所以当 <code>slave</code> 晋升为<strong>新的</strong> <code>master</code> 后，其本地仍保存有之前 <code>master</code> 的动态 ID 。而这一点也恰恰为解决“ <code>slave</code> 易主”问题提供了条件。通过 <code>master</code> 的 <code>info replicaton</code> 中的 <code>master_replid2</code> 可查看到。如果尚未发生过易主，则该值为 40 个 0 。</p> 
<p><strong>⭐️（4） 无盘操作</strong></p> 
<p>    Redis 6.0 对<strong>同步过程</strong>又进行了改进，提出了“<strong>无盘全量同步</strong>”与“<strong>无盘加载</strong>”策略，<mark>避免了耗时的 IO 操作</mark>。</p> 
<ul><li><strong>无盘全量同步</strong>： <code>master</code> 的主进程 <code>fork</code> 出的子进程直接将内存中的数据发送给 <code>slave</code> ，无需经过磁盘。</li><li><strong>无盘加载</strong>： <code>slave</code> 在接收到 <code>master</code> 发送来的数据后不需要将其写入到磁盘文件，而是直接写入到内存，这样 <code>slave</code> 就可快速完成数据恢复。</li></ul> 
<p><strong>⭐️（5） 共享复制积压缓冲区</strong></p> 
<p>    Redis 7.0 版本对 <strong>复制积压缓冲区</strong> 进行了改进，让各个 <code>slave</code> 的发送缓冲区 <strong>共享复制积压缓冲区</strong>。这使得复制积压缓冲区的作用，除了可以保障数据的安全性外，还作为所有 <code>slave</code> 的发送缓冲区，充分利用了<strong>复制积压缓冲区</strong>。</p> 
<h4><a id="53__239"></a>5.3 哨兵机制实现</h4> 
<h6><a id="531__240"></a>5.3.1 简介</h6> 
<p>    对于 <code>Master</code> <strong>宕机</strong>后的<strong>冷处理</strong>方式是无法实现高可用的。 Redis 从 2.6 版本开始提供了<strong>高可用</strong>的解决方案 <code>Sentinel</code> <strong>哨兵机制</strong>。在<strong>集群中</strong>再引入一个节点，该节点充当 <code>Sentinel</code> 哨兵，<mark>用于监视 <strong>Master</strong> 的运行状态</mark>，并在 Master 宕机后<strong>自动指定</strong>一个 <code>Slave</code> 作为新的 <code>Master</code> 。整个过程无需人工参与，完全由哨兵自动完成。</p> 
<p>    不过，此时的 <code>Sentinel</code> 哨兵又成为了一个<strong>单点故障点</strong>：若哨兵发生宕机，整个集群将瘫痪。所以为了解决 <code>Sentinel</code> 的单点问题，又要为 <code>Sentinel</code> 创建一个<strong>集群</strong>，即 <code>Sentinel</code> <strong>哨兵集群</strong>。一个哨兵的宕机，将不会影响到 Redis 集群的运行。</p> 
<p>    那么这些 <code>Sentinel</code> 哨兵是如何工作的呢？ <code>Sentinel</code> 是如何知道其监视的 Master 状态的呢？</p> 
<ul><li>每个 <code>Sentinel</code> 都会定时会向 Master <strong>发送心跳</strong>， 如果 Master 在<strong>有效时间内</strong>向它们都进行了<strong>响应</strong> ，则说明 Master 是“ <mark>活着的</mark>”。</li><li>如果 <code>Sentinel</code> 中有 <code>quorum</code> 个哨兵<strong>没有收到响应</strong>， 那么就认为 Master 已经<strong>宕机</strong>，然后 会有 一个<code>Sentinel</code> 做 <code>Failover</code> <strong>故障转移</strong>。即将原来的某一个 Slave <strong>晋升</strong>为 Master 。</li></ul> 
<h6><a id="532_Redis_248"></a>5.3.2 Redis高可用集群搭建</h6> 
<p>    在“<strong>不差钱</strong>”的情况下，可以让 <code>Sentinel</code> 占用<strong>独立的主机</strong>，即在 <strong>Redis</strong> 主机上只启动 <strong>Redis 进程</strong>，在 <code>Sentinel</code> 主机上只启动 <code>Sentinel 进程</code>。下面要搭建一个“ <mark><strong>一主二从三哨兵</strong></mark> ”的高可用伪集群，即这些角色全部安装运行在一台主机上。“<mark>一主二从</mark>”使用前面的主从集群，下面仅搭建一个 <code>Sentinel</code> 伪集群。<br> <img src="https://images2.imgbox.com/45/12/tCnME2SH_o.png" alt=""></p> 
<p><strong>⭐️（1） 复制 sentinel.conf</strong></p> 
<p>    将 Redis 安装目录中的 <code>sentinel.conf</code> 文件复制到 <code>cluster</code> 目录中。该配置文件中用于存放一些 <code>sentinel</code> 集群中的一些公共配置。</p> 
<p><img src="https://images2.imgbox.com/86/22/RKgToWf0_o.png" alt="在这里插入图片描述"></p> 
<p><strong>⭐️（2） 修改 sentinel.conf</strong></p> 
<p>    修改 <code>cluster/sentinel.conf</code> 配置文件。</p> 
<p><strong>A、 sentinel monitor</strong></p> 
<p>    该配置用于指定 <strong>Sentinel</strong> 要监控的 <code>master</code> 是谁 <code>&lt;ip&gt;&lt;redis-port&gt;</code>，并为 <code>master</code> 起了一个名字 <code>&lt;master-name&gt;</code> 。该名字在后面很多配置中都会使用。同时指定 <strong>Sentinel</strong> 集群中决定该 <code>master</code> “<mark><strong>客观</strong>下线状态</mark>” 判断的法定 <code>sentinel</code> 数量 <code>&lt;quorum&gt;</code>。</p> 
<ul><li><code>&lt;quorum&gt;</code> 的另一个用途与 <code>sentinel</code> 的 <strong>Leader</strong> 选举有关。<mark>要求中至少要有 <code>max(quorum, sentinelNum/2+1)</code> 个 <code>sentinel</code> 参与，选举才能进行。(<em><strong>面试会问</strong></em>)</mark></li></ul> 
<p><img src="https://images2.imgbox.com/00/d9/41UURuf9_o.png" alt="在这里插入图片描述"></p> 
<p>    这里将该配置<strong>注释掉</strong>，因为要在后面的其它配置文件中设置，如 果不注释就会出现配置冲突。</p> 
<p><strong>B、 sentinel auth-pass</strong></p> 
<p><img src="https://images2.imgbox.com/3e/86/OyI3RhCi_o.png" alt="在这里插入图片描述"></p> 
<p>    如果 Redis 主从集群中的主机设置了<strong>访问密码</strong>，那么该属性就需要指定 <code>master</code> 的主机名与访问密码。以方便 <code>sentinel</code> 监控 <code>master</code>。</p> 
<p><strong>⭐️（3） 新建 sentinel26380.conf</strong></p> 
<p>    在Redis 安装目录下的 <code>cluster</code> 目录中新建 <code>sentinel26380.conf</code> 文件作为 <strong>Sentinel 的配置文件</strong>，并在其中键入如下内容：</p> 
<pre><code class="prism language-c">include sentinel<span class="token punctuation">.</span>conf
pidfile <span class="token operator">/</span>var<span class="token operator">/</span>run<span class="token operator">/</span>sentinel_26380<span class="token punctuation">.</span>pid
port <span class="token number">26380</span>

# 使用自己主机的ip
sentinel monitor mymaster <span class="token number">192.168</span><span class="token number">.216</span><span class="token number">.128</span> <span class="token number">6380</span> <span class="token number">2</span>

<span class="token macro property"><span class="token directive-hash">#</span> <span class="token directive keyword">logfile</span> <span class="token expression">access26380<span class="token punctuation">.</span>log</span></span>
</code></pre> 
<p><img src="https://images2.imgbox.com/ef/73/O3wXmfQ8_o.png" alt="在这里插入图片描述"></p> 
<p>    <code>sentinel monitor</code> 属性用于指定当前监控的 <code>master</code> 的 <code>IP</code> 与 <code>Port</code> ，同时为集群中 <code>master</code> 指定一个名称 <code>mymaster</code> ，以方便其它属性使用。</p> 
<blockquote> 
 <p>    最后的 <code>2</code> 是参数 <code>quorum</code> 的值， <code>quorum </code>有两个用途。</p> 
 <ul><li>一个是只有当 <code>quorum</code> 个 <code>sentinel</code> 都认为当前 <code>master</code> 宕机了才能开启<strong>故障转移</strong>。</li><li>另一个用途与 <code>sentinel</code> 的 <strong>Leader</strong> <strong>选举</strong>有关。要求中至少要有 <code>max(quorum, sentinelNum/2+1)</code> 个 <code>sentinel</code> 参与，选举才能进行。</li></ul> 
</blockquote> 
<p><strong>⭐️（4） 再复制两个 conf 文件</strong></p> 
<p>    再使用 <code>sentinel26380.conf</code> 复制出两个 <code>conf</code> 文件：<code>sentinel26381.conf</code> 与 <code>sentinel26382.conf</code> 。然后修改其中的内容。</p> 
<p><img src="https://images2.imgbox.com/bb/de/kZ0dbHOJ_o.png" alt="在这里插入图片描述"></p> 
<p>    修改 <code>sentinel26381.conf</code> 。</p> 
<p><img src="https://images2.imgbox.com/44/9d/GBDqCyLg_o.png" alt="在这里插入图片描述"></p> 
<p>    修改 <code>sentinel26382.conf</code> 。</p> 
<p><img src="https://images2.imgbox.com/8f/6c/3bq5NunD_o.png" alt="在这里插入图片描述"></p> 
<h6><a id="533_Redis_318"></a>5.3.3 Redis高可用集群的启动</h6> 
<p><strong>⭐️（1） 启动并关联 Redis 集群</strong></p> 
<p>    首先要启动三台Redis ，然后再通过 <code>slaveof</code> 关联它们。</p> 
<p><img src="https://images2.imgbox.com/4a/83/AviBkhHX_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/59/43/OZHXHGap_o.png" alt="在这里插入图片描述"></p> 
<p><strong>⭐️（2） 启动 Sentinel 集群</strong></p> 
<p><strong>A、 启动命令</strong></p> 
<p>    在 <code>/usr/local/bin</code> 目录 下有一个命令 <code>redis-sentinel</code> 用于启动 <strong>Sentinel</strong> 。不过，我们发现一个奇怪的现象： <code>/usr/local/bin</code> 目录中的 <code>redis-sentinel</code> 命令是 <code>redis-server</code> 命令的<strong>软链接</strong>，这是为什么呢？</p> 
<p><img src="https://images2.imgbox.com/34/d3/PcD9HHXN_o.png" alt="在这里插入图片描述"></p> 
<p>    查看 Redis 安装目录中的 <code>src</code> 目录中的 <code>redis-server</code> 与<code>redis-sentinel</code> 命令，我们发现这两个命令的大小一模一样。其实，这两个命令<mark>本质上是同一个命令</mark>。</p> 
<p><img src="https://images2.imgbox.com/6d/ac/2JoJPX4Z_o.png" alt="在这里插入图片描述"></p> 
<p>    只所以可以启动不同的进程，主要是因为在启动时所加载的配置文件的不同。所以在<strong>启动 Sentinel</strong> 时，需要指定 <code>sentinel.conf</code> 配置文件。</p> 
<p><strong>B、 两种启动方式</strong></p> 
<p>    由于 <code>redis-server</code> 与 <code>redis-sentinel</code> 命令<mark>本质上是<strong>同一个命令</strong></mark>，所以使用这两个命令均可启动 <code>Sentinel</code> 。</p> 
<ul><li><strong>方式一</strong>，使用 <code>redis-sentinel</code> 命令： <code>redis-sentinel sentinel26380.conf</code></li><li><strong>方式二</strong>，使用 <code>redis-server</code> 命令： <code>redis-server sentinel26380.conf --sentinel</code></li></ul> 
<p><strong>C、 启动三台 Sentinel</strong></p> 
<p><img src="https://images2.imgbox.com/64/95/npBzPIWG_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/5b/63/AO4pf2yB_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/01/64/kBAWngJ1_o.png" alt="在这里插入图片描述"></p> 
<p><strong>⭐️（3） 查看 Sentinel 信息</strong></p> 
<p>    运行中的 <strong>Sentinel</strong> 就是一个 <mark><strong>特殊 Redis</strong></mark> ，其也可以通过客户端连接，然后通过 <code>info sentinel</code> 来查看当前连接的 Sentinel 的信息。</p> 
<p><img src="https://images2.imgbox.com/cd/07/Vdvfml3H_o.png" alt="在这里插入图片描述"></p> 
<p><strong>⭐️（4） 查看 sentinel 配置文件</strong></p> 
<p>    打开任意 <code>sentinel</code> 的配置文件，发现其配置内容中新<strong>增加了很多配置</strong>。</p> 
<p><img src="https://images2.imgbox.com/27/7b/4YsEVUWc_o.png" alt="在这里插入图片描述"></p> 
<h6><a id="534_Sentinel__371"></a>5.3.4 Sentinel 优化配置</h6> 
<p>    在公共的 <code>sentinel.conf</code> 文件 中，还可以通过修改一些其它属性的值来达到对 <code>Sentinel</code> 的<strong>配置优化</strong>。</p> 
<p><strong>⭐️（1） sentinel down-after-milliseconds</strong></p> 
<p><img src="https://images2.imgbox.com/ad/51/vpjW2IM3_o.png" alt="在这里插入图片描述"></p> 
<p>    每个 <strong>Sentinel</strong> 会通过定期发送 <code>ping</code> 命令来判断 <code>master</code> 、 <code>slave</code> 及其它 <strong>Sentinel</strong> 是否存活。如果 <strong>Sentinel</strong> 在该属性<em>指定的时间内没有收到它们的响应</em>，那么该 <strong>Sentinel</strong> 就会 <mark><strong>主观</strong>认为</mark> 该<strong>主机宕机</strong>。默认为 <code>30 秒</code>。</p> 
<p><strong>⭐️（2） sentinel parallel-syncs</strong></p> 
<p><img src="https://images2.imgbox.com/88/67/zvAK8sH1_o.png" alt="在这里插入图片描述"></p> 
<p>    该属性用于指定，在<strong>故障转移</strong>期间，即老的 <code>master</code> 出现问题，新的 <code>master</code> 刚晋升后，允许多少个 <code>slave</code> 同时从新 <code>master</code> 进行<strong>数据同步</strong>。默认值为 <code>1</code> 表示所有 <code>slave</code> 逐个从新 <code>master</code> 进行数据同步。</p> 
<blockquote> 
 <p>注意：为 <code>1</code> 时，同步时间较长，可能造成在<mark>数据不一致</mark>（<strong>可读</strong>）；但过大会造成新 <code>master</code> 同步压力过大，且redis集群<mark>不对外提供任何服务</mark>（<strong>读写都不行</strong>）！</p> 
</blockquote> 
<p><strong>⭐️（3） sentinel failover-timeout</strong></p> 
<p><img src="https://images2.imgbox.com/4a/c0/dVUaOz26_o.png" alt="在这里插入图片描述"></p> 
<p>    指定<strong>故障转移</strong>的<strong>超时时间</strong>，默认时间为 <code>3 分钟</code>。该超时时间的用途很多：</p> 
<ul><li>由于<strong>第一次故障转移</strong>失败，在同一个 <code>master</code> 上进行<strong>第二次故障转移</strong>尝试的时间为该 <code>failover-timeout</code> 的<strong>两倍</strong>。</li><li>新 <code>master</code> <strong>晋升完毕</strong>， <code>slave</code> 从老 <code>master</code> <strong>强制转到</strong>新 <code>master</code> 进行数据同步的 <font color="red"><strong>时间阈值</strong></font>。</li><li><strong>取消</strong> 正在进行的<strong>故障转换</strong>所需的 <font color="red"><strong>时间阈值</strong></font>。</li><li>新 <code>master</code> <strong>晋升完毕</strong>，所有 <code>replicas</code> 的配置文件更新为新 <code>master</code> 的 <font color="red"><strong>时间阈值</strong></font>。</li></ul> 
<p><strong>⭐️（4） sentinel deny-scripts-reconfig</strong></p> 
<p><img src="https://images2.imgbox.com/b4/b2/M4ZGNcJq_o.png" alt="在这里插入图片描述"></p> 
<p>    指定是否可以通过命令 <code>sentinel set</code> <strong>动态修改</strong> <code>notification-script</code> 与 <code>client-reconfig-script</code> 两个脚本。<mark>默认是<strong>不能的</strong></mark>。这两个脚本如果允许<strong>动态修改</strong>，可能会<mark>引发安全问题</mark>。</p> 
<p><strong>⭐️（5） 动态修改配置</strong></p> 
<p><img src="https://images2.imgbox.com/77/7c/4BsjIQjG_o.png" alt="在这里插入图片描述"></p> 
<p>    通过 <code>redis-cli</code> 连接上 <code>Sentinel</code> 后，通过 <code>sentinel set</code> 命令可<strong>动态修改配置信息</strong>。 例如，下面的命令<strong>动态修改</strong>了 <code>sentinel monitor</code> 中的 <code>quorum</code> 的值。</p> 
<p>    下表是 <code>sentinel set</code> 命令支持的参数：</p> 
<p><img src="https://images2.imgbox.com/ca/89/AplPEILR_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="54__416"></a>5.4 哨兵机制原理</h4> 
<h6><a id="541__417"></a>5.4.1 三个定时任务</h6> 
<p>    <strong>Sentinel</strong> <mark>维护着三个<strong>定时任务</strong></mark> 以<strong>监测</strong> <code>Redis</code> <strong>节点</strong>及其它 <code>Sentinel</code> <strong>节点</strong>的<strong>状态</strong>。</p> 
<p><strong>⭐️（1） info 任务</strong></p> 
<p>    每个 <code>Sentinel</code> 节点每 <code>10 秒</code> 就会向 <code>Redis</code> 集群中的<mark>每个节点</mark>发送 <code>info</code> 命令，以获得最新的 <code>Redis</code> <strong>拓扑结构</strong>。</p> 
<p><strong>⭐️（2） 心跳任务</strong></p> 
<p>    <strong>每个</strong> <code>Sentinel</code> 节点每 <code>1 秒</code> 就会向<strong>所有</strong> <code>Redis</code> 节点及<strong>其它</strong> <code>Sentinel</code> 节点发送一条 <code>ping</code> 命令，以检测这些节点的<mark>存活状态</mark>。该任务是判断节点在线状态的重要依据。</p> 
<p><strong>⭐️（3） 发布/订阅任务</strong></p> 
<p>    每个 <code>Sentinel</code> 节点在<strong>启动时</strong>都会向所有 Redis 节点订阅 <code>__sentinel__:hello</code> <strong>主题</strong>的信息，当 <code>Redis</code> 节点中该主题的信息发生了变化，就会立即通知到所有<strong>订阅者</strong>。</p> 
<p>    <strong>启动后</strong>，每个 <code>Sentinel</code> 节点每 <code>2 秒</code> 就会向每个 Redis 节点发布一 条 <code>__sentinel__:hello</code> <strong>主题的信息</strong>，该信息是<mark>当前 <code>Sentinel</code> 对每个 <code>Redis</code> 节点在线状态的判断结果及当前 <code>Sentinel</code> 节点信息</mark>。</p> 
<p>    当 <code>Sentinel</code> 节点接收到 <code> __sentinel__:hello</code> 主题信息后，就会<strong>读取并解析</strong>这些信息，然后主要完成以下<strong>三项工作</strong>：</p> 
<ul><li>如果发现有新的 <code>Sentinel</code> 节点加入，则<strong>记录</strong>下新加入 <code>Sentinel</code> 节点信息，并与其<strong>建立连接</strong>。</li><li>如果发现有 <code>Sentinel Leader</code> 选举的<strong>选票信息</strong>，则执行 <strong>Leader</strong> 选举过程。</li><li>汇总其它 <code>Sentinel</code> 节点对当前 <code>Redis</code> 节点<strong>在线状态的判断结果</strong>，作为 <code>Redis</code> 节点<strong>客观下线</strong>的判断依据。</li></ul> 
<h6><a id="542_Redis__439"></a>5.4.2 Redis 节点下线判断</h6> 
<p>    对于每个 <strong>Redis</strong> 节点在线状态的监控是由 <code>Sentinel</code> 完成的。</p> 
<p><strong>⭐️（1） 主观下线</strong></p> 
<p>    每个 <code>Sentinel</code> 节点每秒就会向每个 <strong>Redis</strong> 节点发送 <code>ping</code> 心跳检测，如果 <code>Sentinel</code> 在 <code>down-after-milliseconds</code> 时间内没有收到某 <strong>Redis</strong> 节点的回复，则 <code>Sentinel</code> 节点就会对该 <strong>Redis</strong> 节点做出“<strong><mark>下线状态</mark></strong>”的判断。这个判断仅仅是当前 <code>Sentinel</code> 节点的“ <mark>一家之言</mark> ”，所以称为<strong>主观下线</strong>。</p> 
<p><strong>⭐️（2） 客观下线</strong></p> 
<p>    当 <code>Sentinel</code> <strong>主观下线</strong>的节点是 <code>master</code> 时，该 <code>Sentinel</code> 节点（<font color="red"> <strong>主动</strong> </font>）会向每个其它 <code>Sentinel</code> 节点发送 <code>sentinel is-master-down-by-addr</code> 命令，以询问其对 <code>master</code> 在线状态的判断结果。这些 <code>Sentinel</code> 节点在收到命令后会向这个发问 <code>Sentinel</code> 节点响应 <code>0</code> （在线）或 <code>1</code> （下线）。当 <code>Sentinel</code> 收到 <mark><strong>超过</strong> <code>quorum</code> 个下线判断后</mark>，就会对 <code>master</code> 做出<strong>客观下线</strong>判断。</p> 
<h6><a id="543_Sentinel_Leader__450"></a>5.4.3 Sentinel Leader 选举</h6> 
<p>    当 <code>Sentinel</code> 节点对 <code>master</code> 做出<strong>客观下线</strong>判断后会由 <code>Sentinel Leader</code> 来 <mark><strong>完成后续</strong>的<strong>故障转移</strong></mark>，即 <code>Sentinel</code> 集群中的节点也并非是对等节点，是存在 <strong>Leader</strong> 与 <strong>Follower</strong> 的。</p> 
<p>     <code>Sentinel</code> 集群的 <strong>Leader</strong> 选举是通过 <code>Raft 算法</code> 实现的。 <code>Raft 算法</code> 比较复杂，后面会详细学习。这里仅简单介绍一下大致思路。</p> 
<p>    每个选举参与者都具有当选 <strong>Leader</strong> 的资格，当其完成了“<strong>客观下线</strong>”判断后，就会立即“<strong>毛遂自荐</strong>”推选自己做 <strong>Leader</strong> ，然后将<strong>自己的提案</strong>发送给所有参与者。其它参与者在收到提案后，<mark>只要自己手中的选票没有投出去，其就会立即通过该提案并将同意结果反馈给提案者（<font color="red"> <strong>先到先得</strong> </font>）</mark>，后续再过来的提案会由于该参与者没有了选票而被拒绝。当提案者收到了 <mark><strong>同意反馈</strong></mark> 数量<strong>大于等于</strong> <code>max(quorum, sentinelNum/2+1)</code> 时，该提案者当选 <strong>Leader</strong> 。</p> 
<p>说明：</p> 
<ul><li>在<strong>网络没有问题</strong>的前提下，基本就是谁先做出了“<strong>客观下线</strong>”判断，谁就会首先发起 <code>Sentinel Leader</code> 的选举，谁就会得到大多数参与者的支持，谁就会当选 <strong>Leader</strong> 。</li><li><code>Sentinel Leader</code> 选举会在次<strong>故障转移发生之前</strong>进行。</li><li><strong>故障转移</strong>结束后 <code>Sentinel</code> 不再维护这种 <code>Leader-Follower</code> 关系，即 <strong>Leader</strong> 不再存在。</li></ul> 
<h6><a id="544_master__462"></a>5.4.4 master 选择算法</h6> 
<p>    在进行<strong>故障转移</strong>时， <code>Sentinel Leader</code> 需要从所有 <code>Redis</code> 的 <code>Slave</code> 节点中选择出新的 <code>Master</code> 。其选择算法为：</p> 
<ol><li><strong>过滤掉</strong>所有 <s>主观下线的</s> ，或<s>心跳没有响应 <code>Sentinel</code> 的</s> ，或 <s><code>replica-priority</code> 值为 <code>0</code></s> 的 <code>Redis</code> 节点</li><li>在<strong>剩余</strong> <code>Redis</code> 节点中选择出 <code>replica-priority</code> <strong>最小的的节点列表</strong>。如果只有一个节点，则直接返回，否则，继续</li><li>从<strong>优先级相同</strong>的节点列表中选择<strong>复制偏移量最大的节点</strong>。如果只有一个节点，则直接返回，否则，继续</li><li>从<strong>复制偏移值量相同</strong>的节点列表中选择<strong>动态 ID 最小的节点</strong>返回</li></ol> 
<p><img src="https://images2.imgbox.com/9a/55/2JGx3p0N_o.jpg" alt="在这里插入图片描述"></p> 
<h6><a id="545__472"></a>5.4.5 故障转移过程</h6> 
<p>    <code>Sentinel Leader</code> 负责整个<strong>故障转移</strong>过程，经历了如上步骤：</p> 
<ol><li><code>Sentinel Leader</code> 根据 <code>master</code> 选择算法选择出一个 <code>slave</code> 节点作为新的 <code>master</code></li><li><code>Sentinel Leader</code> 向新 <code>master</code> 节点发送 <code>slaveof no one</code> 指令，使其<strong>晋升</strong>为 <code>master</code></li><li><code>Sentinel Leader</code> 向新 <code>master</code> 发送 <code>info replication</code> 指令，获取到 <code>master</code> 的<strong>动态 ID</strong></li><li><code>Sentinel Leader</code> 向其余 <strong>Redis</strong> 节点发送消息，以告知它们新 <code>master</code> 的<strong>动态 ID</strong> (<font color="blue"><em>广播</em></font>)</li><li><code>Sentinel Leader</code> 向其余 <strong>Redis</strong> 节点发送 <code>slaveof &lt;mastIp&gt; &lt;masterPort&gt;</code> 指令，使它们成为新 <code>master</code> 的 <code>slave</code></li><li><code>Sentinel Leader</code> 从所有 <code>slave</code> 节点中每次<strong>选择</strong>出 <code>parallel-syncs</code> 个 <code>slave</code> 从新 <code>master</code> <strong>同步数据</strong>，<strong>直至</strong>所有 <code>slave</code> <strong>全部同步完毕</strong></li><li>故障转移完毕</li></ol> 
<p><img src="https://images2.imgbox.com/14/8d/T8OFcKG0_o.jpg" alt="在这里插入图片描述"></p> 
<h6><a id="546__485"></a>5.4.6 节点上线</h6> 
<p>    <strong>不同的节点类型</strong>，其上线的方式也是不同的。</p> 
<p><strong>⭐️（1） 原 Redis 节点上线</strong></p> 
<p>    无论是原下线的 <code>master</code> 节点还是原下线的 <code>slave</code> 节点，只要是原 <code>Redis</code> 集群中的<strong>节点上线</strong>，只需启动 <code>Redis</code> 即可。因为每个 <code>Sentinel</code> 中都保存有原来其监控的所有 <code>Redis</code> 节点列表，<code>Sentinel</code> 会<strong>定时查看</strong>这些 <code>Redis</code> 节点 <mark>是否恢复</mark>。如果查看到其已经恢复，则会命其从当前 <code>master</code> 进行<strong>数据同步</strong>。</p> 
<p>    不过，如果是原 <code>master</code> 上线，在新 <code>master</code> 晋升后 <code>Sentinel Leader</code> 会立即先将原 <code>master</code> 节点<strong>更新为</strong> <code>slave</code> ，然后才会<strong>定时查看</strong>其是否恢复。</p> 
<p><strong>⭐️（2） 新 Redis 节点上线</strong></p> 
<p>    如果需要在 <code>Redis</code> 集群中添加一个新的节点，其未曾出现在 <code>Redis</code> 集群中，则上线操作 <mark><strong>只能手工完成</strong></mark>。即添加者在添加之前必须知道当前 <code>master</code> 是谁，然后在新节点启动后运行 <code>slaveof</code> 命令<strong>加入集群</strong>。</p> 
<p><strong>⭐️（3） Sentinel 节点上线</strong></p> 
<p>    如果要添加的是 <code>Sentinel</code> 节点，无论其<strong>是否曾经出现</strong>在 <code>Sentinel</code> 集群中，<mark><strong>都需要手工完成</strong></mark>。即添加者在添加之前必须知道当前 <code>master</code> 是谁，然后在<strong>配置文件</strong>中修改 <code>sentinel monitor</code> 属性，指定要监控的 <code>master</code> 。然后启动 <code>Sentinel</code> 即可。</p> 
<h4><a id="55_CAP__502"></a>5.5 CAP 定理</h4> 
<h6><a id="551__503"></a>5.5.1 概念</h6> 
<p>    <strong>CAP 定理</strong> 指的是在一个<strong>分布式系统</strong>中，<strong>一致性</strong> <code>Consistency</code> 、<strong>可用性</strong> <code>Availability</code> 、<strong>分区容错性</strong> <code>Partition tolerance</code> ，三者不可兼得 。</p> 
<ul><li><strong>一致性</strong>（ <code>C</code> ）：<strong>分布式系统</strong>中多个主机之间是否能够保持<strong>数据一致</strong>的特性。即，当系统数据发生更新操作后，各个主机中的数据仍然处于一致的状态。</li><li><strong>可用性</strong>（ <code>A</code> ）：系统提供的服务必须一直处于<strong>可用的状态</strong>，即对于用户的每一个请求，系统总是可以在 <font color="red"><strong>有限的时间内</strong></font> 对用户 <font color="red"><strong>做出响应</strong></font>。</li><li><strong>分区容错性</strong>（ <code>P</code> ）：分布式系统在遇到任何<strong>网络分区故障</strong>时，仍能够保证对外提供满足<strong>一致性</strong>和<strong>可用性</strong>的服务。</li></ul> 
<h6><a id="552__508"></a>5.5.2 定理</h6> 
<p>    <strong>CAP定理</strong>的内容是：对于<strong>分布式系统</strong>，网络环境相对是不可控的，出现网络分区是不可避免的，因此系统 <mark><strong>必须具备分区容错性</strong></mark>。但系统 <mark><strong>不能</strong>同时保证</mark> <strong>一致性</strong> 与 <strong>可用性</strong>。即要么 <code>CP</code> 要么 <code>AP</code> 。</p> 
<h6><a id="553_BASE_510"></a>5.5.3 BASE理论</h6> 
<p>    <strong>BASE</strong> 是 <code>Basically Available</code> （<strong>基本可用</strong>）、 <code>Soft state</code> （<strong>软状态</strong>） 和 <code>Eventually consistent</code> （<strong>最终一致性</strong>）三个短语的简写， BASE 是对 CAP 中 <strong>一致性</strong> 和 <strong>可用性</strong> <mark>权衡的结果</mark>，其来源于对大规模互联网<strong>系统分布式</strong>实践的结论，是基于 CAP 定理逐步演化而来的。</p> 
<p>    <strong>BASE</strong> 理论的核心思想是：<mark>即使无法做到 <font color="blue"><strong>强一致性</strong></font>，但每个系 都可以根据自身的业务特点，采用<strong>适当的方式</strong>来使系统达到<strong>最终一致性</strong></mark>。</p> 
<p><strong>（1） 基本可用</strong></p> 
<p>    <strong>基本可用</strong>是指分布式系统在出现不可预知故障的时候，<mark><strong>允许</strong>损失<strong>部分可用性</strong></mark>。</p> 
<p><strong>（2） 软状态</strong></p> 
<p>    <strong>软状态</strong>，是指允许系统数据存在的中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统主机 间进行数据同步的过程存在一定延时。<strong>软状态</strong>，其实就是一种<strong>灰度状态</strong>，<strong>过渡状态</strong>。</p> 
<p><strong>（3） 最终一致性</strong></p> 
<p>    <strong>最终一致性</strong>强调的是系统中所有的数据副本，在经过一段时间的同步后，<mark>最终能够达到一个一致的状态</mark>。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而 <mark><strong>不需要</strong></mark> 保证系统数据的<strong>实时一致性</strong>。</p> 
<blockquote> 
 <p><strong>实时一致性</strong>：要求实际内容一旦发生更新，客户端立刻可以访问到最新的数据。所以，在集群环境下该特性是无法实现的，只存在于单机环境中。<br> <strong>最终一致性</strong>：数据内容发生变更后，经过一小段时间后，客户端可以访问到最新的数据。<br> <br> <mark><strong>实时一致性</strong>与<strong>最终一致性</strong>两个概念是从客户端访问到一致性内容的 <font color="red"><strong>时间角度</strong></font> 来说的；单从客户端访问到的 <font color="red"><strong>内容角度</strong></font> 来说（不说时间问题），还有两个概念</mark>：<br> <br> <strong>强一致性</strong>：也称为严格一致性。要求客户端访问到的一定是更新过的新数据。<br> <strong>弱一致性</strong>：允许客户端从集群不同节点访问到的数据是不一致的。</p> 
</blockquote> 
<h6><a id="554_CAP_535"></a>5.5.4 CAP的应用</h6> 
<p>    下面将生产中常见到的一些<strong>中间件</strong>与<strong>服务器集群</strong>的 <code>CAP</code> 特性进行分析。</p> 
<p><strong>（1）Zookeeper 与 CAP</strong></p> 
<p>    <code>Zookeeper</code> 遵循的是 <code>CP</code> 模式 ，即 <mark><strong>保证了</strong></mark> <font color="red"><strong>一致性</strong></font>，但牺牲了可用性。</p> 
<p>    当 <strong>Leader</strong> 节点中的数据发生了变化后，在 <strong>Follower</strong> 还没有同步完成之前，整个 <strong>Zookeeper</strong> 集群是<mark>不对外提供服务</mark>的。如果此时有客户端来访问数据，则客户端会因访问超时而发生重试。(<em>不过，由于 <strong>Leader</strong> 的选举非常快，所以这种重试对于用户来说几乎是感知不到的</em>)。所以说， <strong>Zookeeper</strong> 保证了一致性，但牺牲了可用性。</p> 
<p><strong>（2） Consul 与 CAP</strong></p> 
<p>    <strong>Consul</strong> 遵循的是 <code>CP</code> 模式 ，即 <mark><strong>保证了</strong></mark> <font color="red"><strong>一致性</strong></font>，但牺牲了可用性。</p> 
<p><strong>（3） Redis 与 CAP</strong></p> 
<p>    <strong>Redis</strong> 遵循的是 <code>AP</code> 模式 ，即 <mark><strong>保证了</strong></mark> <font color="red"><strong>可用性</strong></font>，但牺牲了一致性。</p> 
<p><strong>（4） Eureka 与 CAP</strong></p> 
<p>    <strong>Eureka</strong> 遵循的是 <code>AP</code> 模式 ，即 <mark><strong>保证了</strong></mark> <font color="red"><strong>可用性</strong></font>，但牺牲了一致性。</p> 
<p><strong>（5） Nacos 与 CAP</strong></p> 
<p>    <strong>Nacos</strong> 在做<strong>注册中心</strong>时，默认是 <code>AP</code> 的。但其也支持 <code>CP</code> 模式，但需要用户<strong>提交请求</strong>进行转换。</p> 
<h4><a id="56_Raft__559"></a>5.6 Raft 算法</h4> 
<h6><a id="561__560"></a>5.6.1 基础</h6> 
<p>    Raft 算法是一种通过对 <strong>日志复制管理</strong> 来达到集群节点 <strong>一致性</strong> 的算法。 这个日志复制管理发生在集群节点中的 <code>Leader</code> 与 <code>Followers</code> 之间。 Raft 通过选举出的 <code>Leader</code> 节点负责管理<strong>日志复制</strong>过程，以实现各个节点间数据的<strong>一致性</strong>。</p> 
<blockquote> 
 <p><strong>论文</strong>：<a href="https://www.usenix.org/system/files/conference/atc14/atc14-paper-ongaro.pdf" rel="nofollow">https://www.usenix.org/system/files/conference/atc14/atc14-paper-ongaro.pdf</a></p> 
</blockquote> 
<h6><a id="562__565"></a>5.6.2 角色、任期及角色转变</h6> 
<p><img src="https://images2.imgbox.com/5c/46/6PNBIRIU_o.png" alt="在这里插入图片描述"></p> 
<p>在 <strong>Raft</strong> 中，节点有三种角色：</p> 
<ul><li><code>Leader</code> 唯一负责处理客户端 <font color="red"><strong>写请求</strong></font> 的节点；也可以处理客户端 <font color="red"><strong>读请求</strong></font> ；同时负责<strong>日志复制</strong>工作 (<mark>读写分离</mark>)</li><li><code>Candidate Leader</code> 选举的<strong>候选人</strong>，其可能会成为 <code>Leader</code> 。是一个选举中的<strong>过程角色</strong></li><li><code>Follower</code> ：可以处理客户端 <font color="red"><strong>读请求</strong></font> ；负责<strong>同步</strong>来自于 <code>Leader</code> 的日志；当接收到其它 <code>Cadidate</code> 的投票请求后可以进行<strong>投票</strong>；当发现 <code>Leader</code> 挂了，其会转变为 <code>Candidate</code> 发起 <code>Leader</code> 选举。</li></ul> 
<h6><a id="563_leader__572"></a>5.6.3 leader 选举</h6> 
<p>    通过 <code>Raft</code> 算法首先要实现集群中 <code>Leader</code> 的<strong>选举</strong>。</p> 
<p><strong>（1） 我要选举</strong></p> 
<p>    若 <code>follower</code> 在<strong>心跳超时范围内</strong>没有接收到来自于 <code>leader</code> 的<strong>心跳</strong>，则认为 <code>leader</code> 挂了。此时其首先会使其本地 <code>term</code> 增一。然后 <code>follower</code> 会完成以下步骤：</p> 
<ul><li>此时若接收到了其它 <code>candidate</code> 的<strong>投票请求</strong>，则会将选票投给这个 <code>candidate</code></li><li>由 <code>follower</code> 转变为 <code>candidate</code></li><li>若之前<strong>尚未投票</strong>，则<mark>向自己投一票</mark></li><li>向其它节点发出<strong>投票请求</strong>，然后等待响应</li></ul> 
<p><strong>（2） 我要投票</strong></p> 
<p>    <code>follower</code> 在接收到投票请求后，其会根据以下情况来判断是否投票：</p> 
<ul><li>发来投票请求的 <code>candidate</code> 的 <code>term</code> <mark><strong>不能小于</strong></mark> 我的 <code>term</code></li><li>在我当前 <code>term</code> 内，我的选票还没有投出去</li><li>若接收到多个 <code>candidate</code> 的请求， 我将采取 <code>first-come-first-served</code> 方式投票</li></ul> 
<p><strong>（3） 等待响应</strong></p> 
<p>    当一个 <code>Candidate</code> <strong>发出投票请求后</strong>会等待其它节点的响应结果。这个响应结果可能有三种情况：</p> 
<ul><li>收到<strong>过半选票</strong>，成为新的 <code>leader</code> 。然后会将消息<strong>广播</strong>给所有其它节点，以告诉大家我是新的 <code>Leader</code> 了</li><li>接收到别的 <code>candidate</code> 发来的新 <code>leader</code> 通知，比较了新 <code>leader</code> 的 <code>term</code> 并不比自己的 <code>term</code> 小，则自己转变为 <code>follower</code></li><li>经过一段时间后，没有收到过半选票，也没有收到新 <code>leader</code> 通知，则<strong>重新发出选举</strong></li></ul> 
<p><strong>（4） 选举时机</strong></p> 
<p>    在很多时候，当 <code>Leader</code> 真的挂了， <code>Follower</code> 几乎同时会感知到，所以它们几乎同时会变为 <code>candidate</code> 发起新的选举。此时就可能会出现较多 <code>candidate</code> 票数相同的情况，即无法选举出 <code>Leader</code> 。</p> 
<p>    为了<mark>防止</mark>这种情况的发生，<strong>Raft 算法</strong>其采用了 <code>randomized election timeouts</code> 策略来解决这个问题。 其会为这些 <code>Follower</code> 随机分配一个选举发起时间 <code>election timeout</code> ，这个 <code>timeout</code> 在<code>150-300ms</code> 范围内。<strong>只有到达了</strong> <code>election timeout</code> 时间的 <code>Follower</code> 才能转变为 <code>candidate</code> 否则等待。那么 <code>election timeout</code> 较小的 <code>Follower</code> 则会转变为 <code>candidate</code> 然后先发起选举，一般情况下其会<strong>优先获取到过半选票</strong>成为新的 <code>leader</code> 。</p> 
<h6><a id="564__602"></a>5.6.4 数据同步</h6> 
<p>    在 <code>Leader</code> 选举出来的情况下，通过 <font color="red"><strong>日志复制管理</strong></font> 实现集群中各节点<strong>数据的同步</strong>。</p> 
<p><strong>（1） 状态机</strong></p> 
<p>    <strong>Raft 算法</strong>一致性的实现，是基于<strong>日志复制状态机</strong>的。 状态机的最大特征是，不同 <code>Server</code> 中的状态机若<strong>当前状态</strong>相同，然后接受了<mark>相同的输入</mark>，则一定会得到<mark>相同的输出</mark>。</p> 
<p><img src="https://images2.imgbox.com/00/f2/juTqsCUv_o.png" alt="在这里插入图片描述"></p> 
<p><strong>（2） 处理流程</strong></p> 
<p><img src="https://images2.imgbox.com/23/48/g50TZ9FA_o.png" alt="在这里插入图片描述"></p> 
<p>    当 <code>leader</code> 接收到 <code>client</code> 的<strong>写操作请求</strong>后，大体会经历以下流程：</p> 
<ul><li><code>leader</code> 在接收到 <code>client</code> 的写操作请求后， <code>leader</code> 会将数据与 <code>term</code> 封装为一个 <code>box</code> ，并随着下一次心跳发送给所有 <code>followers</code> ，以征求大家对该 <code>box</code> 的意见。同时在本地将数据<strong>封装为日志</strong></li><li><code>follower</code> 在接收到来自 <code>leader</code> 的 <code>box</code> 后首先会比较该 <code>box</code> 的 term 与本地记录的曾接受过的 <code>box</code> 的最大 <code>term</code> ，只要不比自己的小就接受该 <code>box</code> ，并向 <code>leader</code> 回复同意。同时会将该 <code>box</code> 中的数据封装为日志。</li><li>当 <code>leader</code> 接收到<strong>过半同意响应</strong>后，会将日志 <code>commit</code> 到自己的状态机，状态机会输出一个结果，同时日志状态变为了 committed</li><li>同时 <code>leader</code> 还会通知所有 <code>follower</code> 将日志 <code>commit</code> 到它们本地的状态机，日志状态变为了 <code>committed</code></li><li>在 <code>commit</code> 通知发出的同时， <code>leader</code> 也会向 <code>client</code> 发出成功处理的响应</li></ul> 
<p><strong>（3） AP 支持</strong></p> 
<p><img src="https://images2.imgbox.com/45/9d/IaM59HMR_o.png" alt="在这里插入图片描述"></p> 
<p>    <strong>Log</strong> 由 <code>term index</code> 、 <code>log index</code> 及 <code>command</code> 构成。为了<strong>保证可用性</strong>，各个节点中的<mark>日志可以不完全相同</mark>，但 <code>leader</code> 会<strong>不断给</strong> <code>follower</code> 发送 <code>box</code> ，以使各个节点的 <code>log</code> <strong>最终达到相同</strong>。即 <code>raft</code> 算法不是强一致性的，而是 <font color="red"><strong>最终一致</strong></font> 的。</p> 
<h6><a id="565__627"></a>5.6.5 脑裂</h6> 
<p>    <strong>Raft集群</strong>存在 <mark>脑裂问题</mark>。在多机房部署中，由于网络连接问题，很容易形成多个分区。而<strong>多分区的形成，很容易产生脑裂，从而导致数据不一致</strong>。</p> 
<p>    由于<strong>三机房部署的 <mark><font color="red">容灾能力最强</font></mark></strong> ，所以生产环境下，三机房部署是最为常见的。下面以三机房部署为例进行分析，根据<strong>机房断网</strong>情况，可以分为五种情况：</p> 
<p><strong>（1） 情况一 —— 不确定</strong></p> 
<p><img src="https://images2.imgbox.com/2b/a1/R0R0hWMP_o.png" alt="在这里插入图片描述"></p> 
<p>    这种情况下， <strong>B 机房中</strong>的主机是 <mark>感知不到</mark> <code>Leader</code> 的存在的，所以 B 机房中的主机会发起新一轮的 <code>Leader</code> 选举。由于 B 机房与 C 机房是相连的，虽然 C 机房中的 <strong>Follower</strong> 能够感知到 A 机房中的 <code>Leader</code> ，但由于其接收到了更大 term 的投票请求，所以 C 机房的 <strong>Follower</strong> 也就放弃了 A 机房中的 <code>Leader</code> ，参与了新 <code>Leader</code> 的选举。</p> 
<p>    若新 <code>Leader</code> 出现在 B 机房， A 机房是感知不到新 <code>Leader</code> 的诞生的，其不会自动下课，所以会 <mark><font color="red"><strong>形成脑裂</strong></font></mark>。但由于 A 机房 <code>Leader</code> 处理的<strong>写操作请求 无法</strong>获取到过半响应，所以<mark>无法完成写操作</mark>。但 B 机房 <code>Leader</code> 的写操作处理是可以获取到<strong>过半响应</strong>的，所以可以<mark>完成写操</mark>作。故， A 机房与 B 、 C 机房中出现<strong>脑裂</strong>，且<mark>形成了数据的不一致</mark>。</p> 
<p>    若新 <code>Leader</code> 出现在 C 机房， A 机房中的 <code>Leader</code> 则会<strong>自动下课</strong>，所以 <mark>不会形成脑裂</mark>。</p> 
<p><strong>（2） 情况二 —— 形成脑裂</strong></p> 
<p><img src="https://images2.imgbox.com/94/4b/iTxvgKCF_o.png" alt="在这里插入图片描述"></p> 
<p>    这种情况与情况一基本是一样的。不同的是，一定会 <mark><font color="red"><strong>形成脑裂</strong></font></mark>，无论新 <code>Leader</code> 在 B 还是 C 机房。</p> 
<p><strong>（3） 情况三 —— 无脑裂</strong></p> 
<p><img src="https://images2.imgbox.com/8d/e1/dKQzjyWt_o.png" alt="在这里插入图片描述"></p> 
<p>    A 、 C 可以正常对外提供服务，但 B 无法选举出新的 <code>Leader</code> 。由于 B 中的主机全部变为了 <mark><strong>选举状态</strong>，所以无法提供任何服务，没有形成脑裂。</mark></p> 
<p><strong>（4） 情况四 —— 无脑裂</strong></p> 
<p><img src="https://images2.imgbox.com/31/66/6IB5NrDU_o.png" alt="在这里插入图片描述"></p> 
<p>    A 、 B 、 C 均可以对外提供服务，不受影响。</p> 
<p><strong>（5） 情况五 —— 无脑裂</strong></p> 
<p><img src="https://images2.imgbox.com/75/1d/ksjzMHxT_o.png" alt="在这里插入图片描述"></p> 
<p>    A 机房无法处理<strong>写操作请求</strong>，但可以对外<strong>提供读服务</strong>。</p> 
<p>    B 、 C 机房由于失去了 <code>Leader</code> ，均会发起选举，但由于均无法获取过半支持，所以均无法选举出新的 <code>Leader</code> 。</p> 
<h6><a id="566_Leader__669"></a>5.6.6 Leader 宕机处理</h6> 
<p><strong>（1） 请求到达前 Leader 挂了</strong></p> 
<p>    <code>client</code> 发送 <strong>写操作请求</strong> 到达 <code>Leader</code> 之前 <code>Leader</code> 就挂了，因为 <strong>请求</strong> 还没有到达集群，所以这个请求对于集群来说就没有存在过， 对集群 <strong>数据的一致性</strong> 没有任何影响 。 <code>Leader</code> 挂了之后，会选举产生新的 <code>Leader</code> 。</p> 
<p>    由于 <code>Stale Leader</code> 并未向 <code>client</code> 发送成功处理响应，所以 <code>client</code> 会**<mark>重新发送</mark>该写操作请求**。</p> 
<p><strong>（2） 未开始同步数据前 Leader 挂了</strong></p> 
<p>    <code>client</code> 发送 <strong>写操作请求</strong> 给 <code>Leader</code> 请求 到达 <code>Leader</code> 后， <code>Leader</code> 还没有开始向 <code>Followers</code> 发出数据 <code>Leader</code> 就挂了 。 这时集群会选举产生新的 <code>Leader</code> 。 <code>Stale Leader</code> 重启后会作为 <code>Follower</code> 重新 加入集群，并同步新 <code>Leader</code> 中的数据以保证数据一致性。 之前接收到 <code>client</code> 的数据 <mark>被丢弃</mark>。</p> 
<p>    由于 <code>Stale Leader</code> 并未向 <code>client</code> 发送成功处理响应，所以 <code>client</code> 会 <strong><mark>重新发送</mark>该写操作请求</strong>。</p> 
<p><strong>（3） 同步完部分后 Leader 挂了</strong></p> 
<p>    <code>client</code> 发送 <strong>写操作请求</strong> 给 <code>Leader</code>， <code>Leader</code>接收完数据后向所有 <code>Follower</code> 发送数据。 在部分 <code>Follower</code> 接收到数据后 <code>Leader</code> 挂了 。由于 <code>Leader</code> 挂了，就会发起新的 <code>Leader</code> 选举。</p> 
<ul><li>若 <code>Leader</code> 产生于 <strong>已完成数据接收</strong> 的 <code>Follower</code> ，其会继续将前面接收到的<strong>写操作请求转换为日志</strong>，并写入到 <strong>本地状态机</strong>，并向所有 <code>Flollower</code> 发出询问。在获取过半同意响应后会向所有 <code>Followers</code> 发送 <code>commit</code> 指令，同时向 <code>client</code> 进行响应。</li><li>若 <code>Leader</code>产生于<strong>尚未完成数据接收</strong>的 <code>Follower</code> ，那么原来已完成接收的 <code>Follower</code> 则会放弃曾接收到的数据。由于 <code>client</code> 没有接收到响应，所以 <code>client</code> <strong>会<mark>重新发送</mark>该写操作请求</strong>。</li></ul> 
<p><strong>（4） commit 通知发出后 Leader 挂了</strong></p> 
<p>    <code>client</code> <strong>发送写操作请求</strong> 给 <code>Leader</code>， <code>Leader</code>也成功向所有 <code>Followers</code> 发出的 <code>commit</code> 指令，并向 <code>client </code>发出响应后， <code>Leader</code>挂了。</p> 
<p>    由于 <code>Stale Leader</code> 已经向 <code>client</code> 发送成功接收响应，且 <code>commit</code> 通知已经发出，说明这个<strong>写操作请求</strong>已经被 <code>server</code> 成功处理。</p> 
<h6><a id="567_Raft__694"></a>5.6.7 Raft 算法动画演示</h6> 
<p>    在网络上有一个关于 <strong>Raft 算法</strong> 的动画，其 <mark>非常清晰全面</mark> 地演示了 <strong>Raft 算法</strong>的工作原理。该动画的地址为：<a href="http://thesecretlivesofdata.com/raft/" rel="nofollow">http://thesecretlivesofdata.com/raft/</a></p> 
<p><img src="https://images2.imgbox.com/53/ba/C33y86t2_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>Raft 算法动画演示包括以下内容：</p> 
 <ul><li>What is Distributed Consensus? (什么是分布式一致性?)</li><li>Protocol Overview (协议概述)</li><li>Leader Election (Leader 选举)</li><li>Log Replication（日志复制）</li><li>Other Resources（参考文献）</li></ul> 
</blockquote> 
<p>注：仅供学习参考，如有不足欢迎指正！</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1e8b2ac980aaf73f6c5720aab30e5753/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Python学习10</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/cc66a29e446e62a9d0c529f0b7e9d6c0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">算法笔记(模拟最大三数乘积问题)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>