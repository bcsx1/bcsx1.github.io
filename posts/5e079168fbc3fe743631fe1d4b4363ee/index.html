<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Jetson Orin安装riva以及llamaspeak，使用 Riva ASR/TTS 与 Llama 进行实时交谈，大语言模型成功运行笔记 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Jetson Orin安装riva以及llamaspeak，使用 Riva ASR/TTS 与 Llama 进行实时交谈，大语言模型成功运行笔记" />
<meta property="og:description" content="NVIDIA 的综合语音 AI 工具包 RIVA 可以处理这种情况。此外，RIVA 可以构建应用程序，在本地设备（如 NVIDIA Jetson）上处理所有这些内容。
RIVA 是一个综合性库，包括：
自动语音识别 （ASR）文本转语音合成 （TTS）神经机器翻译 （NMT）（语言到语言的翻译，例如英语到西班牙语）自然语言处理 （NLP） 服务的集合，例如命名实体识别 （NER）、标点符号和意图分类。 RIVA 在运行 JetPack 5 及更高版本的 Jetson Orin 和 Xavier 系列处理器上运行。在视频中，我们使用的是Jetson Orin模组和国产载板,usb免驱声卡和麦克风耳机。
riva和ngc的安装和测试 安装 通常，我们不涵盖演练安装。然而，这已经足够具有挑战性了，值得写这篇文章。RIVA 目前处于 Jetsons 的测试阶段（表示为 ARM64 或嵌入在 NVIDIA 文档中的多个位置）。您可能会发现，随着时间的流逝，某些方向会发生变化。
话虽如此，如果您是初学者，这可能有点困难。我们假设您正在关注视频。所以我尽可能的多写一些步骤。
RIVA 快速入门指南 您需要遵循 RIVA 快速入门指南。您应该能够按照操作，从“嵌入式”部分开始。您需要访问 NVIDIA NGC。NVIDIA NGC 是 NVIDIA AI 的仓库。NGC 需要一个免费的开发者帐户。NVIDIA有几个关于设置帐户和获取开发者密钥的视频：
注册 NGC 并生成 API 密钥NGC 配置集演练 NGC CLI 入门 — ngc-cli documentation (nvidia.com)
ARM64 Linux Install The NGC CLI binary for ARM64 is supported on Ubuntu 18." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/5e079168fbc3fe743631fe1d4b4363ee/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-29T19:10:03+08:00" />
<meta property="article:modified_time" content="2023-12-29T19:10:03+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Jetson Orin安装riva以及llamaspeak，使用 Riva ASR/TTS 与 Llama 进行实时交谈，大语言模型成功运行笔记</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>NVIDIA 的综合语音 AI 工具包 RIVA 可以处理这种情况。此外，RIVA 可以构建应用程序，在本地设备（如 NVIDIA Jetson）上处理所有这些内容。</p> 
<p>RIVA 是一个综合性库，包括：</p> 
<ul><li>自动语音识别 （ASR）</li><li>文本转语音合成 （TTS）</li><li>神经机器翻译 （NMT）（语言到语言的翻译，例如英语到西班牙语）</li><li>自然语言处理 （NLP） 服务的集合，例如命名实体识别 （NER）、标点符号和意图分类。</li></ul> 
<p></p> 
<p>RIVA 在运行 JetPack 5 及更高版本的 Jetson Orin 和 Xavier 系列处理器上运行。在视频中，我们使用的是Jetson Orin模组和国产载板,usb免驱声卡和麦克风耳机。</p> 
<h2>riva和ngc的安装和测试</h2> 
<h3>安装</h3> 
<p>通常，我们不涵盖演练安装。然而，这已经足够具有挑战性了，值得写这篇文章。RIVA 目前处于 Jetsons 的测试阶段（表示为 ARM64 或嵌入在 NVIDIA 文档中的多个位置）。您可能会发现，随着时间的流逝，某些方向会发生变化。</p> 
<p>话虽如此，如果您是初学者，这可能有点困难。我们假设您正在关注视频。所以我尽可能的多写一些步骤。</p> 
<h4>RIVA 快速入门指南</h4> 
<p>您需要遵循 <a href="https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html" rel="nofollow" title="RIVA 快速入门指南">RIVA 快速入门指南</a>。您应该能够按照操作，从<strong>“嵌入式</strong>”部分开始。您需要访问 <a href="https://docs.nvidia.com/ngc/gpu-cloud/ngc-user-guide/index.html#registering-activating-ngc-account" rel="nofollow" title="NVIDIA NGC">NVIDIA NGC</a>。NVIDIA NGC 是 NVIDIA AI 的仓库。NGC 需要一个免费的开发者帐户。NVIDIA有几个关于设置帐户和获取开发者密钥的视频：</p> 
<ul><li><a href="https://youtu.be/yBNt4qSnn0k" rel="nofollow" title="注册 NGC 并生成 API 密钥">注册 NGC 并生成 API 密钥</a></li><li><a href="https://youtu.be/LN5NzKx3qf0" rel="nofollow" title="NGC 配置集演练">NGC 配置集演练</a></li></ul> 
<p><img alt="" height="535" src="https://images2.imgbox.com/32/b4/Zk5jrWbq_o.png" width="1200"></p> 
<p><a href="https://docs.ngc.nvidia.com/cli/cmd.html" rel="nofollow" title="NGC CLI 入门 — ngc-cli documentation (nvidia.com)">NGC CLI 入门 — ngc-cli documentation (nvidia.com)</a></p> 
<p><img alt="" height="636" src="https://images2.imgbox.com/93/4d/w62BfpZo_o.png" width="1200"></p> 
<p><img alt="" height="671" src="https://images2.imgbox.com/74/89/U5MwTZwA_o.png" width="1200"></p> 
<p></p> 
<pre><code>ARM64 Linux Install
The NGC CLI binary for ARM64 is supported on Ubuntu 18.04 and later distributions.

Click Download CLI to download the zip file that contains the binary, then transfer the zip file to a directory where you have permissions and then unzip and execute the binary. You can also download, unzip, and install from the command line by moving to a directory where you have execute permissions and then running the following command:

wget --content-disposition https://api.ngc.nvidia.com/v2/resources/nvidia/ngc-apps/ngc_cli/versions/3.35.0/files/ngccli_arm64.zip -O ngccli_arm64.zip &amp;&amp; unzip ngccli_arm64.zip
Check the binary's md5 hash to ensure the file wasn't corrupted during download:

find ngc-cli/ -type f -exec md5sum {} + | LC_ALL=C sort | md5sum -c ngc-cli.md5
Check the binary's SHA256 hash to ensure the file wasn't corrupted during download. Run the following command

sha256sum ngccli_arm64.zip
Compare with the following value, which can also be found in the Release Notes of the Resource:

9f67759be0397d3b25eca09aeb2d3b4b7077e0c924a3198351ea75965bdec22f
After verifying value, make the NGC CLI binary executable and add your current directory to path:

chmod u+x ngc-cli/ngc
$ echo "export PATH=\"\$PATH:$(pwd)/ngc-cli\"" &gt;&gt; ~/.bash_profile &amp;&amp; source ~/.bash_profile
You must configure NGC CLI for your use so that you can run the commands.

Enter the following command, including your API key when prompted:

ngc config set
ARM64 Uninstall:
Warning: If you choose to have a custom path for your installation, or move the CLI Binary, these instructions may not be safe.

For CLI versions 3.0.0 and up:

Check .dirname `which ngc`
If this directory can be deleted, move to the next step. If not, move to step 3.

Delete the NGC CLI directory:
Enter the following command.

dirname `which ngc` | xargs rm -r
Delete the NGC CLI Binary. Enter the following command.
which ngc | xargs rm
This does not delete all files downloaded from the initial NGC CLI installation. Please go to the original installation folder and delete it.</code></pre> 
<p></p> 
<p><img alt="" height="694" src="https://images2.imgbox.com/b1/ec/Co81HsvY_o.png" width="1200"></p> 
<p>具体而言，本快速入门指南使您能够在本地工作站上部署预训练模型并运行示例客户端。</p> 
<p>Riva Speech AI Skills 支持两种架构，Linux x86_64 和 Linux ARM64。在本文档中，它们被称为<strong>数据中心</strong> （x86_64） 和<strong>嵌入式</strong> （ARM64）。</p> 
<p>有关更多信息和问题，请访问 <a href="https://forums.developer.nvidia.com/c/ai-data-science/deep-learning/riva/475" rel="nofollow" title="NVIDIA Riva 开发者论坛">NVIDIA Riva 开发者论坛</a>。</p> 
<div id="prerequisites" style="text-align:left;"> 
 <h3 style="margin-left:0;"><span style="color:#333333;"><span style="background-color:#ffffff;"><span style="color:#000000;">先决条件</span></span></span></h3> 
 <p><span style="color:#333333;"><span style="background-color:#ffffff;">在使用 Riva Speech AI 之前，请确保您满足以下先决条件：</span></span></p> 
 <div id="data-center"> 
  <h4 style="margin-left:0;"><span style="color:#333333;"><span style="background-color:#ffffff;"><span style="color:#000000;">数据中心</span></span></span></h4> 
  <ol><li> <p><span style="color:#333333;"><span style="background-color:#ffffff;">您有权访问并登录 NVIDIA NGC。有关分步说明，请参阅 <a class="reference external" href="https://docs.nvidia.com/ngc/ngc-overview/index.html#registering-activating-ngc-account" rel="nofollow" title="NGC 入门指南">NGC 入门指南</a>。</span></span></p> </li><li> <p><span style="color:#333333;"><span style="background-color:#ffffff;">您可以访问 NVIDIA Volta™、NVIDIA Turing™ 或基于 NVIDIA Ampere 架构的 A100 GPU。有关详细信息，请参阅<a class="reference internal" href="https://docs.nvidia.com/deeplearning/riva/user-guide/docs/support-matrix.html#support-matrix" rel="nofollow" title="支持矩阵">支持矩阵</a>。</span></span></p> </li><li> <p><span style="color:#333333;"><span style="background-color:#ffffff;">您已安装 Docker 并支持 NVIDIA GPU。有关详细信息，请参阅<a class="reference internal" href="https://docs.nvidia.com/deeplearning/riva/user-guide/docs/support-matrix.html#support-matrix" rel="nofollow" title="支持矩阵">支持矩阵</a>。</span></span></p> </li></ol> 
 </div> 
 <div id="embedded"> 
  <h4 style="margin-left:0;"><span style="color:#333333;"><span style="background-color:#ffffff;"><span style="color:#000000;">嵌入式</span></span></span></h4> 
  <ol><li> <p><span style="color:#333333;"><span style="background-color:#ffffff;">您有权访问并登录 NVIDIA NGC。有关分步说明，请参阅 <a class="reference external" href="https://docs.nvidia.com/ngc/ngc-overview/index.html#registering-activating-ngc-account" rel="nofollow" title="NGC 入门指南">NGC 入门指南</a>。</span></span></p> </li><li> <p><span style="color:#333333;"><span style="background-color:#ffffff;">您可以访问 NVIDIA Jetson Orin、NVIDIA Jetson AGX Xavier 或 NVIDIA Jetson NX Xavier™。有关详细信息，请参阅<a class="reference internal" href="https://docs.nvidia.com/deeplearning/riva/user-guide/docs/support-matrix.html#support-matrix" rel="nofollow" title="支持矩阵">支持矩阵</a>。</span></span></p> </li><li> <p><span style="color:#333333;"><span style="background-color:#ffffff;">您已在 Jetson 平台上安装了 NVIDIA JetPack™ 版本 5.1 或 5.1.1。有关详细信息，请参阅<a class="reference internal" href="https://docs.nvidia.com/deeplearning/riva/user-guide/docs/support-matrix.html#support-matrix" rel="nofollow" title="支持矩阵">支持矩阵</a>。</span></span></p> </li><li> <p><span style="color:#333333;"><span style="background-color:#ffffff;">Jetson 上有 ~15 GB 的可用磁盘空间，这是默认容器和模型所要求的。如果要部署任何 Riva 模型中间表示 （RMIR） 模型，则所需的额外磁盘空间为 ~14 GB 加上 RMIR 模型的大小。</span></span></p> </li><li> <p><span style="color:#333333;"><span style="background-color:#ffffff;">您已在 Jetson 平台上启用以下电源模式。这些模式激活所有 CPU 内核，并以最大频率为 CPU/GPU 提供时钟，以实现最佳性能。</span></span></p> 
    <div style="margin-left:0;"> 
     <div> 
      <pre id="codecell0" style="margin-left:0;"><span style="color:#333333;"><span style="background-color:#ffffff;">sudo nvpmodel -m <span style="color:#0000cf;"><strong>0</strong></span> <span style="color:#ce5c00;"><strong>(</strong></span>Jetson Orin AGX, mode MAXN<span style="color:#ce5c00;"><strong>)</strong></span>
sudo nvpmodel -m <span style="color:#0000cf;"><strong>0</strong></span> <span style="color:#ce5c00;"><strong>(</strong></span>Jetson Xavier AGX, mode MAXN<span style="color:#ce5c00;"><strong>)</strong></span>
sudo nvpmodel -m <span style="color:#0000cf;"><strong>2</strong></span> <span style="color:#ce5c00;"><strong>(</strong></span>Jetson Xavier NX, mode MODE_15W_6CORE<span style="color:#ce5c00;"><strong>)</strong></span>
</span></span></pre> 
     </div> 
    </div> </li><li> <p><span style="color:#333333;"><span style="background-color:#ffffff;">您已通过在文件中添加以下行将默认运行时设置为在 Jetson 平台上。编辑文件后，使用重新启动 Docker 服务。<code>nvidia</code><code>/etc/docker/daemon.json</code><code>sudo systemctl restart docker</code></span></span></p> 
    <div style="margin-left:0;"> 
     <div> 
      <pre id="codecell1" style="margin-left:0;"><span style="color:#333333;"><span style="background-color:#ffffff;"><span style="color:#4e9a06;">"default-runtime"</span>: <span style="color:#4e9a06;">"nvidia"</span>
</span></span></pre> 
     </div> 
    </div> </li></ol> 
 </div> 
</div> 
<h3>启用最大性能</h3> 
<p>如果您想要访问NVIDIA Jetson AGX Orin的全部性能，可以启用最大性能模式：这将在增加电力消耗的情况下最大化应用程序性能。</p> 
<pre><code>sudo nvpmodel -m 0</code></pre> 
<p>将CPU、GPU和EMC时钟的静态最大频率设置为最大</p> 
<pre><code>sudo jetson_clocks</code></pre> 
<h3><strong>禁用桌面图形用户界面(GUI)</strong></h3> 
<p>我们可以禁用桌面环境以节省RAM上的内存。</p> 
<pre><code>sudo systemctl set-default multi-user.target</code></pre> 
<p>然后重启</p> 
<pre><code>sudo reboot</code></pre> 
<p>利用下列命令再启用GUI</p> 
<pre><code>sudo systemctl set-default graphical.target</code></pre> 
<p>然后重启Jetson板子</p> 
<pre><code>sudo reboot</code></pre> 
<h3><strong>性能监控工具 - jtop</strong></h3> 
<p>Jetson Stats是由Raffaello Bonghi开发的一个具有漂亮界面的实用工具。<br> 您可以使用以下命令进行安装：</p> 
<pre><code>sudo -H pip install -U jetson-stats</code></pre> 
<p>然后运行</p> 
<pre><code>jtop</code></pre> 
<p>运行截屏：<img alt="" height="869" src="https://images2.imgbox.com/15/b9/QOqTGaio_o.png" width="1200"></p> 
<p></p> 
<div id="models-available-for-deployment" style="text-align:left;"> 
 <h3 style="margin-left:0;"><span style="color:#333333;"><span style="background-color:#ffffff;"><span style="color:#000000;">可供部署的模型</span></span></span></h3> 
 <p><span style="color:#333333;"><span style="background-color:#ffffff;">有两个按钮式部署选项可用于部署 Riva 语音 AI，它们使用 NGC 目录中提供的<a class="reference external" href="https://catalog.ngc.nvidia.com/models?query=label:%22Riva%22" rel="nofollow" title="预训练模型">预训练模型</a>：</span></span></p> 
 <p><span style="color:#333333;"><span style="background-color:#ffffff;"><strong>本地 Docker：</strong>您可以使用快速入门脚本设置本地工作站并使用 Docker 部署 Riva 服务。继续阅读本指南以使用快速入门脚本。</span></span></p> 
 <p><span style="color:#333333;"><span style="background-color:#ffffff;"><strong>Kubernetes 接口：</strong><a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/helm-charts/riva-api" rel="nofollow" title="Riva Helm Chart">Riva Helm Chart</a> 旨在自动执行一键部署到 Kubernetes 集群的步骤。有关详细信息，请参阅 <a class="reference internal" href="https://docs.nvidia.com/deeplearning/riva/user-guide/docs/installation/deploy-kubernetes.html" rel="nofollow" title="Kubernetes 部署">Kubernetes 部署</a>。<em>嵌入不支持此选项。</em></span></span></p> 
 <p><span style="color:#333333;"><span style="background-color:#ffffff;">除了使用预训练模型外，Riva Speech AI 还可以使用 <a class="reference external" href="https://github.com/NVIDIA/NeMo" title="NVIDIA NeMo">NVIDIA NeMo</a> 与微调的自定义模型一起运行。有关使用 NVIDIA NeMo 创建模型存储库的高级选项的详细信息，请参阅<a class="reference internal" href="https://docs.nvidia.com/deeplearning/riva/user-guide/docs/model-overview.html#nemo-development" rel="nofollow" title="“使用 NeMo 进行模型开发">“使用 NeMo 进行模型开发</a>”部分。</span></span></p> 
 <h3 style="margin-left:0px;text-align:left;"><span style="color:#000000;"><span style="background-color:#ffffff;">使用快速启动脚本进行本地部署<a class="headerlink" href="https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html#local-deployment-using-quick-start-scripts" rel="nofollow" style="margin-left:.2em;" title="#">#</a></span></span></h3> 
 <p style="text-align:left;"><span style="color:#333333;"><span style="background-color:#ffffff;">Riva 包含快速入门脚本，可帮助您开始使用 Riva 语音 AI 技能。这些脚本用于在本地部署服务、测试和运行示例应用程序。</span></span></p> 
 <ol><li> <p>下载脚本。转到 Riva <a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/resources/riva_quickstart/files?version=2.14.0" rel="nofollow" title="Data center">Data center</a> 或 <a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/resources/riva_quickstart_arm64/files?version=2.14.0" rel="nofollow" title="Embedded">Embedded</a> 快速入门，具体取决于您使用的平台。选择<strong>“文件浏览器</strong>”选项卡以下载脚本，或使用 <a class="reference external" href="https://ngc.nvidia.com/setup/installers/cli" rel="nofollow" title="NGC CLI 工具">NGC CLI 工具</a>从命令行下载。</p> <p><strong>数据中心</strong></p> 
   <div style="margin-left:0;"> 
    <div> 
     <pre id="codecell2" style="margin-left:0;">ngc registry resource download-version nvidia/riva/riva_quickstart:2.14.0
</pre> 
    </div> 
   </div> <p><span style="color:#fe2c24;"><strong>嵌入式Jetson</strong></span></p> 
   <div style="margin-left:0;"> 
    <div> 
     <pre id="codecell3" style="margin-left:0;">ngc registry resource download-version nvidia/riva/riva_quickstart_arm64:2.14.0</pre> 
    </div> 
   </div> </li></ol> 
 <p>注意：本教程使用 riva_quickstart_arm64_v2.12.1，最新版镜像拉不起来一直，本人遇到的第一个坑，小白就要多被坑。</p> 
 <p>2.初始化并启动 Riva。初始化步骤下载并准备 Docker 映像和模型。启动脚本将启动服务器。</p> 
 <p>注意</p> 
 <p>在平均互联网连接上，此过程可能需要长达一个小时。在<strong>数据中心</strong>，每个模型在下载后都会针对目标 GPU 进行单独优化。在<strong>嵌入式</strong>平台上，会下载 NVIDIA Jetson 上 GPU 的预优化模型。</p> 
</div> 
<p style="text-align:left;"><span style="color:#333333;"><span style="background-color:#ffffff;"><em>自选：</em>使用首选配置修改目录中的文件。选项包括：<code>config.sh</code><code>quickstart</code></span></span></p> 
<ul><li> <p>要启用哪些服务</p> </li><li> <p>从NGC检索哪些型号</p> </li><li> <p>将它们存放在哪里</p> </li><li> <p>如果系统上安装了多个 GPU，则使用哪个 GPU（有关详细信息，请参阅<a class="reference internal" href="https://docs.nvidia.com/deeplearning/riva/user-guide/docs/installation/deploy-local.html#deploy-local" rel="nofollow" title="本地 （Docker）">本地 （Docker）</a>）</p> </li><li> <p>SSL/TLS 证书的位置</p> </li><li> <p>密钥文件（如果使用安全连接）</p> </li></ul> 
<p>切换到 riva_quickstart 目录并修改 config.sh 以满足您的需求。完成此操作后，您就可以初始化服务器并下载模型了。注意这里需要使用 <strong><em>sudo</em></strong>，这与文档不同：</p> 
<p> <img alt="" height="226" src="https://images2.imgbox.com/ce/a0/AkOW4fR0_o.png" width="785"></p> 
<pre><code> sudo bash riva_init.sh</code></pre> 
<p>要在 Docker 容器中启动 RIVA 服务器，有一个方便的脚本：</p> 
<pre><code>bash riva_start.sh</code></pre> 
<p><img alt="" height="195" src="https://images2.imgbox.com/3a/b7/tLgdArmm_o.png" width="1136"></p> 
<h3>安装 RIVA Python 客户端</h3> 
<p><a href="https://github.com/nvidia-riva/python-clients.git" title="RIVA Python 客户端位于 Github 上">RIVA Python 客户端位于 Github 上</a>。在开始安装之前，请确保已安装 pip。它在 Ubuntu 存储库中被命名为 python3-pip。</p> 
<p><em><strong>注意：</strong>您应该为自己的开发过程修改它。例如，您可能希望使用 Python 虚拟环境。</em></p> 
<p>您还需要安装 testresources 和 portaudio 库。然后将用户添加到关联的组：</p> 
<pre><code>pip3 install testresources

sudo apt install portaudio19-dev

pip3 install pyaudio

sudo adduser $USER audio

sudo adduser $USER pulse-access

newgrp pulse-access</code></pre> 
<p>然后安装 python-clients 存储库。按照 README 文件中的说明进行操作。这是我们在视频中遵循的一个序列，以供参考。在执行此操作之前，请确保您位于顶级目录中。 </p> 
<pre><code> git clone https://github.com/nvidia-riva/python-clients.git
$ cd python-clients
$ git submodule init
$ git submodule update --remote --recursive
$ pip install -r requirements.txt
$ python3 setup.py bdist_wheel
$ pip install --force-reinstall dist/*.whl
$ pip install nvidia-riva-client</code></pre> 
<p> 测试RIVA客户端是否正常使用：</p> 
<p><img alt="" height="136" src="https://images2.imgbox.com/37/f9/pl7PFSHI_o.png" width="1200"></p> 
<pre><code>python3 talk.py --play-audio --text 'Speech is now the component of many different applications. Sometimes speech is integrated into devices, like Apple’s SIRI or Google. Speech may be also be placed in devices like the Amazon Alexa. These devices work in much the same way. First there is a wake-up word processed locally, like “Hey Siri!”. Subsequent voice commands are round tripped to a server. The server processes the voice commands (Automatic Speech Recognition or ASR) then returns a response.' --voice English-US.Female-1</code></pre> 
<p><img alt="" height="371" src="https://images2.imgbox.com/33/b3/iUR9i51v_o.png" width="1200"></p> 
<p> usb声卡提前插好，热插拔不支持的话会无法识别，本人遇到的第二坑</p> 
<p></p> 
<p><a href="https://github.com/oobabooga/text-generation-webui/issues/4779" title='ERROR:Failed to load the extension "openai". Not loading the API on nVidia Jetson Orin AGX · Issue #4779 · oobabooga/text-generation-webui · GitHub'>ERROR:Failed to load the extension "openai". Not loading the API on nVidia Jetson Orin AGX · Issue #4779 · oobabooga/text-generation-webui · GitHub</a></p> 
<h3><strong>Llamaspeak - 使用NVIDIA Riva ASR和TTS进行实时语音对话</strong> </h3> 
<p>根据这里的指导（https://github.com/dusty-nv/jetson-containers/tree/master/packages/llm/llamaspeak）去安装 llamaspeak.</p> 
<p>自己找可能会找错，我就找错了。遇到的第三坑</p> 
<p><a href="https://github.com/dusty-nv/jetson-containers/tree/master/packages/llm/llamaspeak" title="https://github.com/dusty-nv/jetson-containers/tree/master/packages/llm/llamaspeak">https://github.com/dusty-nv/jetson-containers/tree/master/packages/llm/llamaspeak</a></p> 
<p>在进一步操作之前，请确保从Hugging Face下载了模型。Meta的LLaMA是当今最受欢迎的开源LLM（大型语言模型）之一。因此，我们可以下载LLaMA2的量化70B模型。<br> 一旦Riva服务器状态为运行中，请打开另一个终端并执行以下命令： </p> 
<p><img alt="" height="433" src="https://images2.imgbox.com/7e/fe/5NrJ1Lav_o.png" width="1051"></p> 
<p>手动指定要加载的模型，而无需使用 Web UI：</p> 
<pre><code>./run.sh --workdir /opt/text-generation-webui $(./autotag text-generation-webui:1.7) \
   python3 server.py --listen --verbose --api \
	--model-dir=/data/models/text-generation-webui \
	--model=llama-2-13b-chat.Q4_K_M.gguf \
	--loader=llamacpp \
	--n-gpu-layers=128 \
	--n_ctx=4096 \
	--n_batch=4096 \
	--threads=$(($(nproc) - 2))</code></pre> 
<p> 末端运行下面提示就成功了：</p> 
<pre><code>llm_load_print_meta: model params     = 68.98 B
llm_load_print_meta: model size       = 38.58 GiB (4.80 BPW)
llm_load_print_meta: general.name   = LLaMA v2
llm_load_print_meta: BOS token = 1 '&lt;s&gt;'
llm_load_print_meta: EOS token = 2 '&lt;/s&gt;'
llm_load_print_meta: UNK token = 0 '&lt;unk&gt;'
llm_load_print_meta: LF token  = 13 '&lt;0x0A&gt;'
llm_load_tensors: ggml ctx size =    0.26 MiB
llm_load_tensors: using CUDA for GPU acceleration
llm_load_tensors: mem required  =  140.89 MiB
llm_load_tensors: offloading 80 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 83/83 layers to GPU
llm_load_tensors: VRAM used: 39362.61 MiB
....................................................................................................
llama_new_context_with_model: n_ctx      = 4096
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init: offloading v cache to GPU
llama_kv_cache_init: offloading k cache to GPU
llama_kv_cache_init: VRAM kv self = 1280.00 MiB
llama_new_context_with_model: kv self size  = 1280.00 MiB
llama_build_graph: non-view tensors processed: 1844/1844
llama_new_context_with_model: compute buffer total size = 4547.09 MiB
llama_new_context_with_model: VRAM scratch buffer: 4544.03 MiB
llama_new_context_with_model: total VRAM used: 45186.64 MiB (model: 39362.61 MiB, context: 5824.03 MiB)
AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 |
2023-12-29 09:22:20 INFO:Loaded the model in 74.01 seconds.

Starting streaming server at ws://0.0.0.0:5005/api/v1/stream
2023-12-29 09:22:20 INFO:Loading the extension "gallery"...
Starting API at http://0.0.0.0:5000/api
Running on local URL:  http://0.0.0.0:7860

To create a public link, set `share=True` in `launch()`.
</code></pre> 
<p><img alt="" height="25" src="https://images2.imgbox.com/d1/82/qlE15qGA_o.png" width="1150"> 内存基本用光，最好关闭图形界面。</p> 
<h4><span style="color:#1f2328;"><span style="background-color:#ffffff;">启用 HTTPS/SSL</span></span></h4> 
<p><span style="color:#1f2328;"><span style="background-color:#ffffff;">浏览器需要使用 HTTPS 才能访问客户端的麦克风。因此，您需要创建自签名 SSL 证书和密钥：</span></span></p> 
<pre><code>$ cd /path/to/your/jetson-containers/data
$ openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -sha256 -days 365 -nodes -subj '/CN=localhost'</code></pre> 
<p>您需要将它们放在 <a href="https://github.com/dusty-nv/jetson-containers/blob/master/data" title="jetson-containers/data">jetson-containers/data</a> 目录中，因为它会自动挂载到 下的容器中，并使您的 SSL 证书在容器运行中保持持久性。当您第一次将浏览器导航到使用这些自签名证书的页面时，它将向您发出警告，因为它们不是来自受信任的颁发机构：<code>/data</code></p> 
<p>您可以选择覆盖此功能，在您更改证书或设备的主机名/IP 更改之前，它不会再次出现</p> 
<h4><span style="color:#1f2328;"><span style="background-color:#ffffff;">运行 Llamaspeak</span></span></h4> 
<p><span style="color:#1f2328;"><span style="background-color:#ffffff;">要使用其默认参数和您生成的 SSL 密钥运行 llamaspeak 聊天服务器，请按如下方式启动它：</span></span></p> 
<p>有关可更改的命令行选项，请参阅 <a href="https://github.com/dusty-nv/jetson-containers/blob/master/packages/llm/llamaspeak/chat.py" title="chat.py">chat.py</a>。例如，要启用或日志记录：<code>--verbose</code><code>--debug</code></p> 
<pre><code>./run.sh --workdir=/opt/llamaspeak \
  --env SSL_CERT=/data/cert.pem \
  --env SSL_KEY=/data/key.pem \
  $(./autotag llamaspeak) \
  python3 chat.py --verbose --debug</code></pre> 
<p>如果在从 Web 客户端获取音频或响应时遇到问题，请启用调试日志记录以检查消息流量。 </p> 
<p>加上debug后音频正常。后续有待查验，还安装了一个未知的音频库看起来并未起到效果也发出来把也许你们能用到。</p> 
<p><img alt="" height="379" src="https://images2.imgbox.com/12/a5/TXFfm0Fl_o.png" width="1200"></p> 
<p><img alt="" height="751" src="https://images2.imgbox.com/1a/67/IxuqC9py_o.png" width="1124"></p> 
<p>这里的错误nvidia说JETPACK5.1.1修复了,重新刷机更新系统尴尬不你们，遇到这个问题现在只能重启可能是内存不够引起的。 </p> 
<p>演示一下效果，新年快乐明年见；</p> 
<div class="csdn-video-box"> 
 <iframe id="rS6Ult4g-1703848197639" frameborder="0" src="https://live.csdn.net/v/embed/355319" allowfullscreen="true" data-mediaembed="csdn"></iframe> 
 <p>llamaspeak</p> 
</div> 
<p></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/81d0abb84ed19f69e99831aa5a9a1e0a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Video面试题</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d8d48fedb4f0e664084c3e9899a4e8d5/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">昇腾910平台安装驱动、固件、CANN toolkit、pytorch</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>