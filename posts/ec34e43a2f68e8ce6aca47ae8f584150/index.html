<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>计算机视觉基础学习-图像拼接 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="计算机视觉基础学习-图像拼接" />
<meta property="og:description" content="1、基础理解 首先本文介绍的图像拼接并非对尺寸相同的图片进行简单拼接，而是基于全景图的拼接
普通相机拍摄图像时，无法兼顾相机视场与视场中单个物体的分辨率问题，而全景相机普遍价格昂贵，
不适用于低成本的一般性场景。为了使用普通相机获取宽视角，甚至是 360°全景图像，人们提出了图像拼接技术。图像拼接技术是指将含有重叠部分的两幅或多幅图像，通过图像预处理、图像配准和图像融合技术，拼接成一幅包含各图像信息的高分辨率、宽视角图像的技术。
图像拼接是将同一场景的多个重叠图像拼接成较大的图像的一种方法
基本思想：图像拼接并非简单的将两张有共同区域的图像把相同的区域重合起来，由于两张图像拍摄的角度与位置不同，虽然有共同的区域，但拍摄时相机的内参与外参均不相同，所以简单的覆盖拼接是不合理的。因此，对于图像拼接需要以一张图像为基准对另外一张图像进行相应的变换（透视变换），然后将透视变换后的图像进行简单的平移后与基准图像的共同区域进行重合。
图像拼接技术就是将数张有重叠部分的图像（可能是不同时间、不同视角或者不同传感器获得的）拼成一幅无缝的全景图或高分辨率图像的技术。图像拼接在医学成像、计算机视觉、卫星数据、军事目标自动识别等领域具有重要意义。
图像配准（image alignment）和图像融合是图像拼接的两个关键技术。图像配准是图像融合的基础,而且图像配准算法的计算量一般非常大,因此图像拼接技术的发展很大程度上取决于图像配准技术的创新。早期的图像配准技术主要采用点匹配法,这类方法速度慢、精度低,而且常常需要人工选取初始匹配点,无法适应大数据量图像的融合。图像拼接的方法很多,不同的算法步骤会有一定差异,但大致的过程是相同的。
2、图像拼接基础步骤 图像拼接技术的基本流程为：图像采集与获取、图像预处理、图像配准、图像融合等。
图像获取——指使用各类图像采集设备进行图像采集和获取。方式有平移式获取、旋转式获取、手持式获取等图像预处理——是图像配准前的预备工作，消除影响图像配准精度的无关信息，提升图像配准效率。图像预处理包括图像去噪、几何校正、均匀颜色等。图像预处理的目的是提高配准精度、降低配准难度，包括调整灰度差异、去噪、几何修正以及将两幅图像投影到同一坐标系等基本操作图像配准——将多幅图像进行匹配、叠加的过程。图像配准的精度十分重要，因此算法既要保证配准精度，又不能计算量过大。应当能够估计待拼接图像之间可能存在的缩放、旋转、仿射变换、投影变换以及亮度和颜色等变化。是计算出两幅图像间的空间变换模型并进行空间变换，使两幅图像的重叠部分在空间上对准，是图像拼接的关键．图像之间的空间变换关系包括: 平移、旋转、尺度缩放、仿射变换、投影变换，其中投影变换更具有普遍性图像融合——是由于进行图像配准操作，可能由于算法误差累积、色彩差异等原因，导致圖像出现拼接缝隙，以及整幅图像的颜色亮度差异。因此在图像配准处理后需要进行图像融合处理，矫正差异，消除缝隙，才能使拼接的图像更加自然。图像融合的目的是得到无缝的高质量图像．在不损失原始图像信息的前提下，消除接缝与亮度差异，实现拼接边界的平滑过渡． 其中，图像配准是其中最关键的一步
3、图像配准技术 图像配准是计算出两幅图像间的空间变换模型并进行空间变换，使两幅图像的重叠部分在空间上对准，是图像拼接的关键．图像之间的空间变换关系包括: 平移、旋转、尺度缩放、仿射变换、投影变换，其中投影变换更具有普遍性。
图像拼接的关键是精确找出相邻两张图像中重叠部分的位置，然后确定两张图像的变换关系‚即图像配准。由于视角、 拍摄时间、分辨率、光照强度、传感器类型等的差异‚待拼接的 图像往往存在平移、旋转、尺度变化、透视形变、色差、扭曲、运 动目标遮挡等差别‚配准的目的就是找出一种最能描述待拼接 图像之间映射关系的变换模型。目前常用的一些空间变换模 型有平移变换、刚性变换、仿射变换以及投影变换等。
假设图像 f1( x，y) 、f2( x，y) 存在投影变换关系，则用以下齐次方程表示:
其中: m0、m1、m3 和 m4 共同表示旋转角度和缩放尺度; m2 和 m5 分别表示 x 方向与 y 方向上的平移量; m6 和 m7 分别表示 x 方向和 y 方向上的变形量．图像配准的关键是用上式确定空间变换模型 M 的参数．
根据各参数的意义及不同变换模型的特点‚对矩阵 Ｍ 作 相应简化就可以得到各变换模型的参数矩阵。
图像配准技术分类如下，图像配准可以分为基于频域和基于空间域的。
3.1 基于频域的 基于频域的图像配准方法主要利用傅里叶变换把图像先变换到频域再做处理。
这类方法需要在频域中进行计算，以便找到一对图像之间的最佳变换参数。这些算法利用相位相关的特性来配准图像。
基于傅里叶梅林变换(Fourier-MellinTransform,FMT)的图像配准算法就是一种比较典型的基于频域的图像配准算法，该算法的
优点是速度比较快。Kuglin和Mines提出的相位相关法1以及Castro和Morandi提出的扩展相位相关法都是基于频域的图像配准算法。
基于频域的算法可克服相关性噪声和频率噪声, 可以大大减小几何失真对匹配性能的影响‚计算速度快,对小平移量、旋转及变尺度图像的拼接较适合,但是在两张图像重叠部分不大的情况下结果较差
使用互功率谱来检测变换。
(a)和(b)源图像，它们之间有位移；
(c )和(d)是(a)和(b)的光谱；" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/ec34e43a2f68e8ce6aca47ae8f584150/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-09-09T16:49:15+08:00" />
<meta property="article:modified_time" content="2022-09-09T16:49:15+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">计算机视觉基础学习-图像拼接</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="1_0"></a>1、基础理解</h2> 
<p>首先本文介绍的图像拼接并非对尺寸相同的图片进行简单拼接，而是基于全景图的拼接</p> 
<p>普通相机拍摄图像时，无法兼顾相机视场与视场中单个物体的分辨率问题，而全景相机普遍价格昂贵，<br> 不适用于低成本的一般性场景。为了使用普通相机获取宽视角，甚至是 360°全景图像，人们提出了图像拼接技术。图像拼接技术是指将含有重叠部分的两幅或多幅图像，通过图像预处理、图像配准和图像融合技术，拼接成一幅包含各图像信息的高分辨率、宽视角图像的技术。</p> 
<p>图像拼接是将同一场景的多个重叠图像拼接成较大的图像的一种方法</p> 
<p>基本思想：图像拼接并非简单的将两张有共同区域的图像把相同的区域重合起来，由于两张图像拍摄的角度与位置不同，虽然有共同的区域，但拍摄时相机的内参与外参均不相同，所以简单的覆盖拼接是不合理的。因此，对于图像拼接需要以一张图像为基准对另外一张图像进行相应的变换（透视变换），然后将透视变换后的图像进行简单的平移后与基准图像的共同区域进行重合。</p> 
<p>图像拼接技术就是将数张有重叠部分的图像（可能是不同时间、不同视角或者不同传感器获得的）拼成一幅无缝的全景图或高分辨率图像的技术。图像拼接在医学成像、计算机视觉、卫星数据、军事目标自动识别等领域具有重要意义。</p> 
<p>图像配准（image alignment）和图像融合是图像拼接的两个关键技术。图像配准是图像融合的基础,而且图像配准算法的计算量一般非常大,因此图像拼接技术的发展很大程度上取决于图像配准技术的创新。早期的图像配准技术主要采用点匹配法,这类方法速度慢、精度低,而且常常需要人工选取初始匹配点,无法适应大数据量图像的融合。图像拼接的方法很多,不同的算法步骤会有一定差异,但大致的过程是相同的。</p> 
<p><img src="https://images2.imgbox.com/12/99/GJLDVDVB_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="2_17"></a>2、图像拼接基础步骤</h2> 
<p>图像拼接技术的基本流程为：图像采集与获取、图像预处理、图像配准、图像融合等。</p> 
<ol><li>图像获取——指使用各类图像采集设备进行图像采集和获取。方式有平移式获取、旋转式获取、手持式获取等</li><li>图像预处理——是图像配准前的预备工作，消除影响图像配准精度的无关信息，提升图像配准效率。图像预处理包括图像去噪、几何校正、均匀颜色等。图像预处理的目的是提高配准精度、降低配准难度，包括调整灰度差异、去噪、几何修正以及将两幅图像投影到同一坐标系等基本操作</li><li>图像配准——将多幅图像进行匹配、叠加的过程。图像配准的精度十分重要，因此算法既要保证配准精度，又不能计算量过大。应当能够估计待拼接图像之间可能存在的缩放、旋转、仿射变换、投影变换以及亮度和颜色等变化。是计算出两幅图像间的空间变换模型并进行空间变换，使两幅图像的重叠部分在空间上对准，是图像拼接的关键．图像之间的空间变换关系包括: 平移、旋转、尺度缩放、仿射变换、投影变换，其中投影变换更具有普遍性</li><li>图像融合——是由于进行图像配准操作，可能由于算法误差累积、色彩差异等原因，导致圖像出现拼接缝隙，以及整幅图像的颜色亮度差异。因此在图像配准处理后需要进行图像融合处理，矫正差异，消除缝隙，才能使拼接的图像更加自然。图像融合的目的是得到无缝的高质量图像．在不损失原始图像信息的前提下，消除接缝与亮度差异，实现拼接边界的平滑过渡．</li></ol> 
<p><img src="https://images2.imgbox.com/74/c7/lHbIG5eO_o.png" alt="在这里插入图片描述"></p> 
<p>其中，<em><strong>图像配准</strong></em>是其中最关键的一步</p> 
<h2><a id="3_29"></a>3、图像配准技术</h2> 
<p>图像配准是计算出两幅图像间的空间变换模型并进行空间变换，使两幅图像的重叠部分在空间上对准，是图像拼接的关键．图像之间的空间变换关系包括: 平移、旋转、尺度缩放、仿射变换、投影变换，其中投影变换更具有普遍性。<br> 图像拼接的关键是精确找出相邻两张图像中重叠部分的位置，然后确定两张图像的变换关系‚即图像配准。由于视角、 拍摄时间、分辨率、光照强度、传感器类型等的差异‚待拼接的 图像往往存在平移、旋转、尺度变化、透视形变、色差、扭曲、运 动目标遮挡等差别‚配准的目的就是找出一种最能描述待拼接 图像之间映射关系的变换模型。目前常用的一些空间变换模 型有平移变换、刚性变换、仿射变换以及投影变换等。<br> <img src="https://images2.imgbox.com/85/23/wCUZsdu3_o.png" alt="在这里插入图片描述"></p> 
<p>假设图像 f1( x，y) 、f2( x，y) 存在投影变换关系，则用以下齐次方程表示:<br> <img src="https://images2.imgbox.com/55/57/aPrcjTXs_o.png" alt="在这里插入图片描述"><br> 其中: m0、m1、m3 和 m4 共同表示旋转角度和缩放尺度; m2 和 m5 分别表示 x 方向与 y 方向上的平移量; m6 和 m7 分别表示 x 方向和 y 方向上的变形量．图像配准的关键是用上式确定空间变换模型 M 的参数．<br> <img src="https://images2.imgbox.com/18/a2/ixG1kIG8_o.png" alt="在这里插入图片描述"><br> 根据各参数的意义及不同变换模型的特点‚对矩阵 Ｍ 作 相应简化就可以得到各变换模型的参数矩阵。</p> 
<p>图像配准技术分类如下，图像配准可以分为基于频域和基于空间域的。<br> <img src="https://images2.imgbox.com/11/ba/NIsPXMFt_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="31__45"></a>3.1 基于频域的</h3> 
<p>基于频域的图像配准方法主要利用傅里叶变换把图像先变换到频域再做处理。<br> 这类方法需要在频域中进行计算，以便找到一对图像之间的最佳变换参数。这些算法利用相位相关的特性来配准图像。<br> 基于傅里叶梅林变换(Fourier-MellinTransform,FMT)的图像配准算法就是一种比较典型的基于频域的图像配准算法，该算法的<br> 优点是速度比较快。Kuglin和Mines提出的相位相关法1以及Castro和Morandi提出的扩展相位相关法都是基于频域的图像配准算法。<br> 基于频域的算法可克服相关性噪声和频率噪声, 可以大大减小几何失真对匹配性能的影响‚计算速度快,对小平移量、旋转及变尺度图像的拼接较适合,但是在两张图像重叠部分不大的情况下结果较差<br> <img src="https://images2.imgbox.com/c3/4e/U8kbdsQj_o.png" alt="在这里插入图片描述"><br> 使用互功率谱来检测变换。<br> (a)和(b)源图像，它们之间有位移；<br> (c )和(d)是(a)和(b)的光谱；<br> (e)指示源图像之间位移的脉冲函数<br> 相位谱是一个位于两图偏移处的δ脉冲函数‚因此可度量两图之间的相似程度。 用极坐标的方式表示两幅图像,则可用相同的方法计算出图像间的旋转角度</p> 
<h4><a id="311__59"></a>3.1.1 相位相关法</h4> 
<p>根据傅立叶变换的平移不变性，相位相关法将空间域上像素的平移转换为频率域上相位的平移</p> 
<h4><a id="312__61"></a>3.1.2 扩展相位法</h4> 
<p>扩展相位法解决了相位相关法不适用于平移和旋转的问题。又在相位相关法的基础上提出了将相位相关法和对数极坐标变换相结合的扩展相位相关法，通过对数极坐标变换将两幅图像间的旋转和尺度缩放关系转换为该坐标系下的平移关系。<br> 扩展相位相关法具有相位相关法的效率高、对光照变化不敏感、抗噪能力强、稳定性好等优点，同时又解决了旋转和尺度缩放问题，较相位相关法适用范围更广。该方法还可以与边缘检测相结合，进一步提高鲁棒性和计算效率。但是算法复杂，并且要求两幅图像之间有较高的重合度。</p> 
<h3><a id="32__64"></a>3.2 基于空间域的</h3> 
<p>这类算法使用像素的属性来执行配准，因此它们是最直接的图像拼接方法。大多数现有的图像拼接算法都属于这一类。基于空间域的图像拼接可以是基于区域的，也可以是基于特征的。</p> 
<h4><a id="321__67"></a>3.2.1 基于区域匹配</h4> 
<p>基于区域的图像拼接算法依赖于需要拼接的两幅图像中像素值的“窗口”之间的计算。基本的方法是相对于彼此移动图像的“窗口”,并观察像素匹配的程度。随后，获得变换参数并用于扭曲和缝合图像。基于区域的镶嵌算法通常被称为基于像素的镶嵌，因为它们使用像素-像素匹配，而不是基于要素的镶嵌中使用的要素-要素匹配。<br> 如以单个像素为单位平移的方法，确定当两图像重叠部分之间的相似性，测度达到最大时的变换模型，常见的相似性测度有平均绝对差( mean absolute deviation，MAD) 、误差平方和( sum of squared difference，SSD) 、归一化互相关 ( normalized cross correlation，NCC) 等<br> 两种最常用的基于区域的图像镶嵌算法是基于归一化互相关的镶嵌和基于互信息的镶嵌。这两种方法都提供了图像相似性的度量，因为这些度量的较大值来自匹配区域或“窗口”</p> 
<h5><a id="3211__71"></a>3.2.1.1 基于归一化互相关</h5> 
<p>他的方法为每次移位计算两幅图像中的“窗口”之间的相似性<br> 相关法是指对于存在平移、旋转和尺度缩放的图像，利用 图像间相似性最大化的原理实现配准，即通过优化相似性准则计算图像间的变换参数。相似性准则包括灰度差的平方、相关函数和归一化相关函数等。特征块匹配算法就是一种利用相 关性准则的图像配准法。由于相关法是一种寻优的全搜索算法，计算量相当大。</p> 
<h5><a id="3212__74"></a>3.2.1.2 基于互信息</h5> 
<p>与基于图像强度值计算相似性的NCC不同，互信息基于两幅图像之间共享的信息量来度量相似性。</p> 
<p>互信息是最常用的多模态图像相似性测度，在1995年由 Ｖｉｏｌａ和 Ｃｏｌｌｉｇｎｏｎ最先分别独立提出 。该方法不需要对两种 成像模式中图像强度间关系的性质作任何假设，也不需要对图 像进行任何预处理‚所以被广泛用于 ＣＴ、ＭＲＩ、ＰＥＴ等多模态图像配准。互信息用熵来定义，常用的是 Ｓｈａｎｎｏｎ熵互信息。</p> 
<h4><a id="322__78"></a>3.2.2 基于特征匹配</h4> 
<p><img src="https://images2.imgbox.com/bd/c0/uGgjkRP0_o.png" alt="在这里插入图片描述"></p> 
<p>基于特征的图像拼接是利用图像的明显的特征来估计图像之间的变换，而不是利用图像全部的信息。<br> 与基于区域的图像配准相比，基于特征的图像配准只使用图像的部分信息，如轮廓、角点等特征。</p> 
<p>图像的配准问题可以归结为求解对应点集。在待配准的图像中选取一些特征点，对准了这些特征点，两幅图像也就配准了。控制点法往往要借助人工选取初始匹配点，这大大降低了算法的速度和适用范围。因此有必要采用一些数学方法自动实现图像间对应控制点的 选取。</p> 
<p>基于特征的配准算法主要流程包含三个步骤:特征提取、特征匹配和变换矩阵的求解。通过对图像的像素点归纳抽象出高级语义的图像特征，以特征作为待配准图像的匹配依据，对重叠区域的图像进行特征搜索确定图像之间的匹配关系。图像的特征可以是一个区域、一条线或者是一个点，由于特征点具有几何变换的鲁棒性，故在图像拼接中，一般采用特征点作为配准的特征。</p> 
<ol><li>特征提取<br> 提取特征是为了对图像进行匹配，理论上应该根据待配准图像的特点考虑选择何种特征作为配准的依据，在特征的选择上主要需考虑三个方面:第一，选择待配准的两幅图像中都存在的特征，只有两幅图像中都能提取出来的特征才能进行图像的特征匹配。第二，必须能够同时在两幅图像中提取出足够数量的特征才是有效的。第三，选取的特征必须具有较好的独特性且便于下一步的特征匹配工作。</li><li>特征匹配<br> 在提取出待配准图像的特征集之后，接下来就要确定它们各自特征集中的特征点的匹配关系，即进行特征匹配。要确定特征集之间特征点的匹配对应关系，首先要对它们进行高层次的语义描述，对每个特征点抽象出一个特征描述符，通过特征描述符来寻找匹配的特征点。而在寻找的过程中，由于特征点集数据量的庞大，必须采用一定的搜索策略来进行匹配对的确定，通常采用的搜索算法有k-d树算法，BBF算法等。</li><li>变换矩阵的求解<br> 经过特征点的匹配，确定了相邻两幅图像之间的对应的特征点匹配对集，根据计算机视觉几何理论可知，相邻两幅图像之间的关系可由一个3x3的透视变换矩阵来刻画，接下来的主要任务就是根据他们之间的对应点匹配集求出透视变换矩阵。通过求解出来的透视变换矩阵，可以将处于不同像素坐标系下的待配准图像进行图像变换，使它们变换到统一的坐标系 下，完成图像的配准。透视变换矩阵的求解关系到配准的质量好坏，仅仅从待配准图像的初始匹配对中求解代表摄像机的复杂运动模型的变换矩阵是很难的，通常应先进行匹配对的提纯，然后采用迭代求精的方法确定精确的变换矩阵参数，常用的方法有M估计法，随机抽样一致性(RANSAC)算法,，MLESAC算法等。</li></ol> 
<p><img src="https://images2.imgbox.com/d6/b4/rMaDUjRX_o.png" alt="在这里插入图片描述"></p> 
<p><strong>基于特征的配准方法优点</strong><br> 一，适用范围广，即使是两幅位移较大的待配准图像也能够通过提取特征进行配准。<br> 二，配准的依据是图像的特征而不是图像的所有像素点，只利用的图像的一部分信息，可以大大地降低运算量，提高配准的效率。<br> 三，由于特征匹配是针对高层次语义上的特征描述符，故算法对于待配准图像之间存在噪声干扰和亮度差异具有很好的鲁棒性。<br> <strong>基于特征的配准方法缺点</strong><br> 只利用了图像的部分信息，如果特征提取的数量过少，在特征匹配阶段即使是很小的错误都会导致最终配准的失败。故基于特征的配准算法其核心关键在于选取鲁棒性高的特征并且精确地进行特征匹配。</p> 
<h5><a id="3221__108"></a>3.2.2.1 基于轮廓图像匹配</h5> 
<p>基于轮廓特征的算法是指对图像进行轮廓提取，然后再对提取的轮廓进行匹配，从而确定重叠位置。</p> 
<p>基于轮廓特征的配准算法首先对图像进行直方图均衡和去噪处理，再对图像进行轮廓提取，然后对提取的轮廓进行配准，进而确定重叠区域。</p> 
<p>基而于达轮到廓配特准征的的目配的准。算法适用于光照不一致、存在尺度关 系及旋转的图像。该方法需要准确提取出明显的轮廓特征，对于数据的缺失比较敏感，要求两幅图像的对应轮廓要比较完整。对于轮廓特征不明显或噪声干扰较大的图像不适用。</p> 
<h5><a id="3222__115"></a>3.2.2.2 基于底层特征匹配</h5> 
<p>这些方法不需要具有大的重叠区域的图像来镶嵌它们。这类镶嵌算法依赖于使用低层特征的稀疏集合的变换计算。常用的低层特征包括边缘、角点、像素、颜色、直方图等。无论选择哪种低级特征，它都必须是独特的，并且遍布整个图像，并且在两幅图像中都应该是可有效检测的。<br> 特征检测器算法应该使得即使在存在各种几何和辐射变化的情况下，从一组图像中检测到的共同特征的数量也足够高。此外，检测器必须具有高重复率，使得在图像对之间的重叠区域中检测到相同的特征。</p> 
<h6><a id="32221_Harris_118"></a>3.2.2.2.1 基于Harris角点检测器</h6> 
<p><em><strong>角点</strong></em>是指灰度图像中局部灰度梯度变化较大的点，或轮廓线上局部范围内的曲率极大值点。角点检测法基本思想是提取特征点，匹配两幅图像中的特征点作为特征点对，再通过随机抽样一致性( random sample consensus，ＲANSAC) 算法， 使用特征点对估计空间变换模型的参数。</p> 
<p>角点没有明确的数学定义。一般认为角点是二维图像亮度变化剧烈的点或图像边缘曲线上曲率极大值的点。这些点在保留图像图形重要特征的同时，可以有效地减少信息的数据量，使其信息的含量很高，有效地提高了计算的速度和配准的可靠性，使得实时处理成为可能。<br> <img src="https://images2.imgbox.com/1c/8b/xVsxahG0_o.png" alt="在这里插入图片描述"></p> 
<p>该算法是通过在图像中沿圆弧曲线扫描得到的角 点的信息（夹角、边缘方向）来进行拼接的。该算法最 常见的是 Harris 角点检测算法。</p> 
<p><img src="https://images2.imgbox.com/b5/e3/aHYapC3V_o.png" alt="在这里插入图片描述"><br> 可以看出Harris对于角点比较明显的图像设置正确的阈值可以成功的完成角点检测，对于较为复杂的图像阈值不易设置容易出现误检测点。但是在图像拼接时角点检测有着简单易实现的优点，在以建筑等为主要内容的简单棱角分明场景中交为适用，但不适合用于纹理复杂、形变较大、发生较大尺度变换的图像的拼接中。</p> 
<p>Harris 算法使用图像的梯度特征检测角点，具有光照不变性，还可以解决平移、旋转问题，但是像素邻域的窗函数对尺度变换敏感，不具备尺度不变性; Harris 需要人工指定阈值，适应性较差，但可以与自适应阈值算法相结合提高算法效率．</p> 
<h6><a id="32222_FAST_134"></a>3.2.2.2.2 基于FAST角点检测器</h6> 
<p>FAST 算法基本思想是如果某像素的灰度值与足够多的邻域像素灰度值的差值较大时，认为该点为<br> 特征点。<br> FAST 算法操作简单，实时性好，但对包含噪声的图像鲁棒性不好，检测结果受给定阈值 s 和 t 的影响较<br> 大，不具备尺度不变性和旋转不变性．研究表明，适当增加图像对比度并滤波去噪能显著提升 FAST 的稳定性。<br> 快速算法是一种角点检测算法，在计算上比大多数其他低级特征提取方法更有效和更快；因此，基于该算法的拼接方法特别适合于实时图像处理应用。</p> 
<p><strong>角点配准步骤</strong><br> 具体的配准步骤为：<br> ａ）利用角点检测算子检测图像中的角点；<br> ｂ）利用控制点匹配算法对检测到的角点进行匹配，找出角点匹配对；<br> ｃ）剔除伪匹配对‚得到正确匹配对‚根据这些匹配对计算出变换参数；<br> ｄ）进行拼接融合得到全景图像。 基于特征点的配准算法量较小，配准精度高，缺点是边缘信息少的图像、大旋转和大尺度缩放的图像和多光谱图像不能很好地进行配准。</p> 
<h6><a id="32223_SIFT_149"></a>3.2.2.2.3 基于SIFT特征检测器</h6> 
<p>SIFT 算法能提取具有尺度不变、旋转不变、光照不变的局部极值点作为特征点．其基本思想是使用降采样和高斯函数构建图像尺度空间，提取极值点作为潜在特征点并剔除干扰点，将特征点邻域像素的梯度作为特征向量来描述特征点，计算特征向量间的欧氏距离进行特征点配对，根据公式估计空间变换模型的参数。<br> SIFT （Scale Invariant and Feature Transform ） 算 法是 David G．Lowe 在1999年首先提出的用 目标识别的一个方法‚首先对两幅图像在尺度和灰度两个空间进行特征检测，利用拉普拉斯金字塔的性质消除尺度变化的影响，确定关键点的位置和所处的尺度，然后用关键点邻域梯度的主方向作为该点的方向特征，以实现算子对尺度和方向的无关性，生成关键点的 SIFT 特征向量，利用该向量进行匹配。<br> SIFT算法是一种低级特征检测算法，它从图像中检测独特的特征(也称为“关键点”)。SIFT描述符对于图像域中的平移、旋转和缩放变换是不变的，并且对于适度的透视变换和光照变化是鲁棒的。SIFT的操作基于五个主要步骤:尺度空间构建、尺度空间极值检测、关键点定位、方向分配和定义关键点描述符。</p> 
<p><strong>基于 ＳＩＦＴ的配准算法主要步骤如下：</strong><br> ａ）检测尺度空间极值点，初步确定关键点的位置和所在尺度。<br> ｂ）精确确定关键点的位置和尺度，同时剔除低对比度的关键点和不稳定的边缘响应点。<br> ｃ）分配关键点方向。利用关键点邻域像素的梯度方向分 布特性为每个关键点指定方向参数，保证 ＳＩＦＴ算子的旋转不 变性。<br> ｄ）生成关键点描述子。将坐标轴旋转为关键点的方向‚ 然后以关键点为中心取 8×8的窗口‚计算每个 4×4的小块上 八个方向的梯度方向直方图，每个梯度方向的累加值形成一个 种子点。实际计算过程中‚为了增强匹配的稳健性。<br> ｅ）生成两幅图像的 ＳＩＦＴ特征向量后，采用关键点特征向 量的欧式距离作为两幅图像中关键点的相似性判定准则。得到满足准则的 ＳＩＦＴ匹配点对。<br> ｆ）根据得到的 ＳＩＦＴ匹配点对计算出图像的变换参数。<br> ｇ）进行拼接融合得到全景图像。</p> 
<p>ＳＩＦＴ特征是图像的局部特征‚对旋转、尺度缩放、亮度变化 保持不变性‚对视角变化、仿射变换、噪声也具有一定的鲁棒性。</p> 
<h6><a id="32224_SURF_164"></a>3.2.2.2.4 基于SURF特征检测器</h6> 
<p>SURF算法是一种尺度和旋转不变的局部特征检测器。和SIFT一样，这个算法也是基于尺度空间理论。</p> 
<h3><a id="33__168"></a>3.3 图像拼接代码方法汇总</h3> 
<p><a href="https://www.cnblogs.com/skyfsm/p/7401523.html" rel="nofollow">基于角点、特征检测器的图像拼接</a><br> <a href="https://www.cnblogs.com/skyfsm/p/7411961.html" rel="nofollow">图像拼接与图像融合</a><br> <a href="https://blog.csdn.net/qq_34623621/article/details/120783508">opencv常用拼接方法</a><br> <a href="https://blog.csdn.net/weixin_46858140/article/details/116132096">RANSAC示例1</a><br> <a href="https://blog.csdn.net/t20187/article/details/116136126">RANSAC示例2</a><br> <a href="https://blog.csdn.net/Jiawen_/article/details/116135753">RANSAC示例3</a><br> <a href="https://blog.csdn.net/qq_44818951/article/details/116073137">APAP算法示例1</a><br> <a href="https://blog.csdn.net/qq_45749702/article/details/124136571">APAP算法示例2</a><br> <a href="https://blog.csdn.net/weixin_44438112/article/details/116094886">APAP算法示例3</a></p> 
<h2><a id="4_179"></a>4、图像融合</h2> 
<p>图像融合是图像拼接的另一个关键技术。图像融合是将两幅已配准图像中有用信息综合到一幅图像中并以可视化 法显示的技术。配准后的图像由于分辨率和视角的不同以及光照等因素的影响，有时甚至是多光谱图像之间进行的拼接，在图像拼接的重叠部分有时会产生模糊、鬼影或噪声点，边界 处也可能形成明显的拼缝。为了改善拼接图像的视觉效果和客观质量，需要对拼接后的图像进行融合</p> 
<h2><a id="_181"></a>参考资料</h2> 
<p><a href="https://blog.csdn.net/t20187/article/details/116136126">图像拼接1-RANSAC</a><br> <a href="https://blog.csdn.net/weixin_44438112/article/details/116094886">Python 图像拼接-APAP</a><br> <a href="https://blog.csdn.net/weixin_46858140/article/details/116132096">图像拼接-RANSAC</a><br> <a href="https://www.fx361.com/page/2021/1128/9144473.shtml" rel="nofollow">图像拼接关键技术</a><br> <a href="https://blog.csdn.net/weixin_37079656/article/details/118443884">图像拼接简述</a><br> <a href="https://blog.csdn.net/robitmind/article/details/121944376">图像拼接算法分类</a><br> <a href="https://www.sciencedirect.com/science/article/pii/S1047320315002059" rel="nofollow">A survey on image mosaicing techniques</a><br> <a href="https://blog.csdn.net/qq_42383076/article/details/120367528">局部特征提取方法汇总</a></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/17b00cdd4ff982032f7a1d0fe127223b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Redis缓存入门</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f04479fd7dfe34ba909872255e8c6f43/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【ArcGIS小技巧】ArcMap启动太慢？清理一下ArcToolbox的缓存可以加速</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>