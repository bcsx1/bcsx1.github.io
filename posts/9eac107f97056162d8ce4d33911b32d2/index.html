<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Cross-Modality Domain Adaptation - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Cross-Modality Domain Adaptation" />
<meta property="og:description" content="Cross-Modality Domain Adaptation for Medical Image Segmentation Unsupervised 3D Semantic Segmentation Domain Adaptation
领域适应（DA）最近引起了医学成像界的强烈兴趣。通过鼓励算法对未知情况或不同的输入数据域具有鲁棒性，域自适应提高了机器学习方法对各种临床环境的适用性。虽然已经提出了各种各样的 DA 技术用于图像分割，但这些技术中的大多数已经在私有数据集或小型公共可用数据集上得到验证。此外，这些数据集主要解决单类问题。为了解决这些限制，crossMoDA 挑战引入了第一个用于无监督跨模态域自适应的大型多类数据集。
Domain Adaptation (DA) has recently raised strong interests in the medical imaging community. By encouraging algorithms to be robust to unseen situations or different input data domains, Domain Adaptation improves the applicability of machine learning approaches to various clinical settings. While a large variety of DA techniques has been proposed for image segmentation, most of these techniques have been validated either on private datasets or on small publicly available datasets." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/9eac107f97056162d8ce4d33911b32d2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-01-05T10:46:42+08:00" />
<meta property="article:modified_time" content="2022-01-05T10:46:42+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Cross-Modality Domain Adaptation</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="CrossModality_Domain_Adaptation_for_Medical_Image_Segmentationhttpscrossmodachallengeml_0"></a><a href="https://crossmoda-challenge.ml/" rel="nofollow">Cross-Modality Domain Adaptation for Medical Image Segmentation</a></h2> 
<p>Unsupervised 3D Semantic Segmentation Domain Adaptation</p> 
<p>领域适应（DA）最近引起了医学成像界的强烈兴趣。通过鼓励算法对未知情况或不同的输入数据域具有鲁棒性，域自适应提高了机器学习方法对各种临床环境的适用性。虽然已经提出了各种各样的 DA 技术用于图像分割，但这些技术中的大多数已经在私有数据集或小型公共可用数据集上得到验证。此外，这些数据集主要解决单类问题。为了解决这些限制，crossMoDA 挑战引入了第一个用于无监督跨模态域自适应的大型多类数据集。</p> 
<blockquote> 
 <p>Domain Adaptation (DA) has recently raised strong interests in the medical imaging community. By encouraging algorithms to be robust to unseen situations or different input data domains, Domain Adaptation improves the applicability of machine learning approaches to various clinical settings. While a large variety of DA techniques has been proposed for image segmentation, most of these techniques have been validated either on private datasets or on small publicly available datasets. Moreover, these datasets mostly address single-class problems. To tackle these limitations, the crossMoDA challenge introduces the first large and multi-class dataset for unsupervised cross-modality Domain Adaptation.</p> 
</blockquote> 
<h4><a id="Aim_8"></a>Aim</h4> 
<p>挑战的目标是分割两个关键的大脑结构：前庭神经鞘瘤（VS）和耳蜗。虽然 ceT1加权的磁共振成像（MRI）扫描通常用于 VS 分割，但最近的研究表明，高分辨率T2（hrT2）成像可能是 ceT1 的可靠、安全和低成本替代方法。基于这些原因，我们提出了一种无监督的跨模态挑战（从 ceT1 到 hrT2），旨在自动在 hrT2 扫描上执行 VS 和耳蜗分割。训练源和目标集分别是带注释的 ceT1 和未带注释的 hrT2 扫描。<br> <img src="https://images2.imgbox.com/5e/bf/tJxnrfxh_o.png" alt="image-20220104215718127"><br> <img src="https://images2.imgbox.com/db/a8/xOlQNkMn_o.png" alt="image-20220104215738316"><br> <em>以下是列表中前三的方法：</em></p> 
<h4><a id="SelfTraining_Based_Unsupervised_CrossModality_Domain_Adaptation_for_Vestibular_Schwannoma_and_Cochlea_Segmentation_14"></a>Self-Training Based Unsupervised Cross-Modality Domain Adaptation for Vestibular Schwannoma and Cochlea Segmentation</h4> 
<p><em>Hyungseob Shin ; Hyeon Gyu Kim; Sewon Kim; Yohan Jun ; Taejoon Eo ; Dosik Hwang (Yonsei University)</em></p> 
<h5><a id="Challenge_Goal_18"></a>Challenge Goal</h5> 
<p><img src="https://images2.imgbox.com/83/e5/mFEf1g7v_o.png" alt="image-20220101202428209"></p> 
<p>训练集只提供了 CET1 的标签，HRT2 没有标签。需要通过域自适应的方式把源域 CET1 的图像强度分布转到与目标域 HRT2 一样或相似的图像强度分布上（这里隐含不改变 CET1 的图像结构），这样就可以用单模态分割网络（nnUNet）来分割所有原有的 HRT2 和用 CycleGAN 从 CET1 转换过来的 fake-HRT2。毕竟医学图像多模态的定义就是由不同模态在同一解剖区域成像敏感度不同，导致的图像强度分布不同，从而难以用单个网络来分割多模态医学图像。</p> 
<h5><a id="Method_24"></a>Method</h5> 
<p><img src="https://images2.imgbox.com/28/53/8CV9kfo6_o.png" alt="image-20220102165143181"></p> 
<p>在本文中，我们提出了一种新的无监督跨模态医学图像分割方法，并在前庭神经鞘瘤 (VS) 和耳蜗分割上对其进行了验证。 VS 的快速准确诊断在临床工作流程中非常重要，用于此的最典型 MR 协议包括对比增强 T1 加权 (ceT1) 和高分辨率 T2 加权 (hrT2) 扫描。为此，提出了从一对 ceT1 和 hrT2 图像中分割 VS 和耳蜗的研究 [1]。在这项研究中，我们的目标是对 VS 和耳蜗进行分割。hrT2 扫描使用基于自我训练的无监督方法，在有注释的 ceT1 和无注释的 hrT2 扫描上进行训练。我们的方法包括</p> 
<p>我们的方法包括 4 个主要步骤：</p> 
<ul><li>从 CET1 扫描到 HRT2 扫描的保留 VS 结构的域转换，然后是自训练方案，其中包括 ：</li><li>使用合成的 HRT2 进行有监督 VS 分割训练（因为合成的 T2 扫描原本带有注释），</li><li>用训练好的 nnUNet 在无注释的真实 HRT2 扫描上生成伪标签，以及</li><li>通过结合真假 HRT2 数据（即真实 T2 扫描及其伪标签、带有真实标签的 T2 扫描）进一步训练，提高 VS 和耳蜗分割的泛化性能。</li></ul> 
<p>考虑到我们的任务是在 hrT2 图像上预测 VS 和 Cochlea，我们首先使用 cycleGAN 将我们有标签的 ceT1 图像转换为 hrT2 图像，从而在之后用生成的 hrT2 图像训练分割模型。提出的图像强度转换（域适应）网络使用 2D CycleGAN。</p> 
<h4><a id="Unsupervised_Domain_Adaptation_in_Semantic_Segmentation_Based_on_Pixel_Alignment_and_SelfTraining_PAST_41"></a>Unsupervised Domain Adaptation in Semantic Segmentation Based on Pixel Alignment and Self-Training (PAST)</h4> 
<p><em>Hexin Dong，Fei Yu，Jie Zhao，Bin Dong，and Li Zhang</em></p> 
<p>为了解决这个问题，我们提出了一种结合像素级对齐和自训练（PAST）的有效且直观的 UDA 方法。首先，我们将有标签图像从 ceT1 域转移到 hrT2 域，以便图像可以对齐到相同的分布。其次，在转换的 ceT1 扫描和 hrT2 扫描生成的伪标签上进一步训练模型，它们在 hrT2 域上找到了更好的决策边界。实验结果表明，我们的方法大大减少了域偏移，并在验证集上以 0.8395 的骰子得分获得了第二名。</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-3QFTaRgE-1641350566118)(C:\Users\SkrBang\AppData\Roaming\Typora\typora-user-images\image-20220104232231918.png)]</p> 
<h4><a id="Using_OutoftheBox_Frameworks_for_Unpaired_Image_Translation_and_Image_Segmentation_for_the_crossMoDA_Challenge_51"></a>Using Out-of-the-Box Frameworks for Unpaired Image Translation and Image Segmentation for the crossMoDA Challenge</h4> 
<p><em>Jae Won Choi</em></p> 
<p>我们使用 <a href="https://github.com/taesungp/contrastive-unpaired-translation">CUT</a>，这是一种 patch-wised 对比学习和对抗性学习的 unpaired image-to-image translation 的 GAN 模型，以把 CET1 加权的 MR 图像（CET1 域）转到 HRT2 加权的 MR 图像（HRT2 域）的域。接着，对于 HRT2 域进行分割任务，我们利用 <a href="https://github.com/MIC-DKFZ/nnUNet">nnU-Net</a> <sup class="footnote-ref"><a href="#fn1" rel="nofollow" id="fnref1">1</a></sup>，这是一个在医学图像分割中表现出最先进性能（state-of-the-art）的框架。</p> 
<p><img src="https://images2.imgbox.com/46/e2/i9HAdBbX_o.png" alt="image-20220104215335526"></p> 
<p>我们沿 z 轴对 CET1 3D 图像进行切片以获取大小为 256 × 256 像素的 N 个图像，因为 CUT 仅支持 2D 图像进行训练。</p> 
<p>我们把训练集中的所有 CET1 图像在经过训练的域适应模型 CUT 上转换来获取假的 HRT2 图像。生成的假 HRT2 图像沿 z 轴堆叠，以重建训练集中 CET1 的 3D 数据。来自训练集中相应 CET1 图像的假 HRT2 图像和相应的标签用于训练分割模型。</p> 
<p>我们的结果表明，公开可用的通用深度学习框架可以在没有新网络或方法的情况下在医学成像中实现一定程度的性能。由于挑战的情况，这项研究涉及有限范围的超参数。对不同预处理和数据增强的进一步实验可能会提高性能。此外，有必要与其他开箱即用的框架进行比较。</p> 
<p>，这项研究涉及有限范围的超参数。对不同预处理和数据增强的进一步实验可能会提高性能。此外，有必要与其他开箱即用的框架进行比较。</p> 
<hr class="footnotes-sep"> 
<section class="footnotes"> 
 <ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>sensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H.: nnu-net: a selfconfiguring method for deep learning-based biomedical image segmentation. Nature methods 18(2), 203–211 (2021) <a href="#fnref1" rel="nofollow" class="footnote-backref">↩︎</a></p> </li></ol> 
</section>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b3e76b24886c24510fe859783fa3258b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">技术分享 | MySQL Binlog 通过 MySQL 客户端导入数据库效率低的原因</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9e03f62591140234207c8e9a086c7d96/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">leetcode 647 剑指 Offer II 020. 回文子字符串的个数python</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>