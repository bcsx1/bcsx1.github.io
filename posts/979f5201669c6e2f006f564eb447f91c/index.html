<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>《Deep Convolutional Inverse Graphics Network》(DCIGN) 翻译 - 编程随想</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="《Deep Convolutional Inverse Graphics Network》(DCIGN) 翻译" />
<meta property="og:description" content="本文对《Deep Convolutional Inverse Graphics Network》的主要内容进行翻译和记录。
1 引言 深度学习在从图像中自动学习层次化表征任务上取得了显著成效。但如何产生数据的最优表征方面的工作相对较少。
许多不同的关于表征学习的理论和应用研究中，均体现出对表征相同的要求：不变性(invariance)、有意义性(meaningfulness)、抽象性(abstraction)和解耦性(disentanglement)。Bengio指出解耦表征(disentangled representation)具有编码变化对真实世界中的变化是稀疏的特点，即数据的不同解释性分布往往相互独立地变化，连续的真实世界序列的变化往往一次只对应少量隐藏特征的变化。
逆图形视觉(“vision as inverse graphics”)模型对提供这些特征的图像进行表征。计算机图形学(computer graphics)包含从场景的紧凑描述(图形编码, graphics code)映射到图像的函数。该类图形编码通常是解耦的，从而可以通过对变换(如对象位置、姿势、照明、纹理和形状等)的细粒度控制来渲染场景。这些编码被设计为容易且可解释地表征真实数据序列，以便在软件中紧凑地表征常见变换。这一标准与Bengio的类似。图形编码可以方便地与理想表征的属性对齐。
最近的逆图形研究遵循了一个通用策略：定义具有隐藏参数的概率或确定性模型，通过推理或优化算法找到给定观测下最合适的潜在参数集。但这些方法无法自动生成可解释的图形编码，也难以再现三维图像。
本文提供了一种学习复杂变换(如平面外旋转、光照变化等)的可解释图形编码的方法。给定一组图像，本文使用一个混合编码器-解码器模型学习解耦不同变换的表征。为了实现这一点，本文采用一个具有多层卷积核反卷积操作的深度有向图模型，并基于随机梯度变分贝叶斯算法(Stochastic Gradient Variational Bayes, SGVB)进行训练。
本文提供一种训练策略以鼓励图形编码层的每组神经元表征特定的变换。为了学习解耦表征，我们将数据划分成包含激活变换和未激活变换的小批次进行训练，而非想监督学习一样提供目标值。目标函数确保图像重构质量。比如，点头的面部图像具有仰角激活，但其形状、纹理及其他仿射变换未激活。通过这样的数据训练迫使图形编码层中选定的神经元表征特定的激活变换。给定一张人脸图像，本文模型可以重新生成与输入不停姿势和光照的图像。
2 相关工作 具体参考文献这里不做赘述，请参考原文。
3 模型 如图1，深度卷积逆图像网络的基本架构包含两部分：一个捕捉给定数据x的图形编码Z的分布的编码器和一个学习给定Z产生近似的条件分布的解码器。Z是包含分解的隐藏变量(如姿势、光照、形状等)的解耦表征。
图1 模型结构。模型遵循变分自编码器的架构。编码器有若干层卷积和最大池化层组成，解码器有若干层上采样和卷积层组成。(a) 训练期间使用目标函数-log(P(x|zi))&#43;KL(Q(zi|x)||P(zi)计算并反向传播梯度. (b) 测试期间，通过调整编码器获得的编码以渲染成新的图像，类似与操作现成的3D渲染引擎。 记编码器输出为，用于参数化变分近似分布，Q是多变量正态分布。这样做 有两个好处：(1)样本相对Q的参数的梯度可以通过重参数技巧容易地获得；(2)三维扫描仪数据上训练的不同金泰新装模型具有相同的多变量正态隐藏分布。给定连接和的参数，分布参数和隐藏编码Z可以表示为
3.1 针对特定变换的训练
本工作的主要目标是学习数据的具有解耦和具有语义解释性的隐藏变量的表征。
一个对于场景信息的表征的自然选择是图像引擎中已经使用的表征设计。如果说人脸图像能拆分成姿势、光照、形状等变量来解构，那么自然可以选择这些变换来构造表征。图2给出了尝试学习的表征。
图2 表征向量结构。前三维特征分对应人脸图像的方向角、仰角和光照角度。 为实现这一目标，设计训练流程如下：将数据组织成只对应单个场景变量变化(方位角、仰角、光源方位)的小批量数据(mini-batch)。这些场景变化是现实世界中存在的，我们称其为外部变量(extrinsic variables), 分别由编码中的表征。
此外还生成外部变量固定，其他面部属性变化的小批量数据。这些批次数据包含相同观察条件和姿势下的不同人脸图像。这些内在属性，如身份、形状、表情等，用剩余的隐藏变量表征。
本方法基于SGVB训练，但对编码器输出及梯度训练进行了一些关键的调整，具体训练流程如下： 1. 从期望变化集合{方位角、仰角、光源方位、内在属性}中随机选择一个潜在变量记为
2. 随机选择一个对应变量变化的小批次数据；
3.将小批次中的每个样例输入网络并学习其隐藏表征；
4.计算本批次中所有表征向量的均值；
5.在将编码器输出输入解码器前，将编码向量中所有非选定变化的值()用所有向量的均值替代，即这些输出被钳制(clamped)；
6.根据SGVB计算重构误差并在解码器中反向传播；
7.关于的梯度替换为其与平均值的差值(解释详见3.2)，关于的梯度不变；
8.用修正过的梯度在编码器中反向传播
由于内在属性的表征远高于外在表征，因此需要更多的训练。因此在选择批次时变化时，方位角：仰角：光照角度：内在属性=1:1:1:10。该比例是广泛测试后得出的在本文两个数据集上均有效的比例。
该训练方式通过钳制一个神经元外的所有神经元的输出，迫使解码器仅使用该神经元的值的改变来重构该批次的变化。
该训练方式使得网络的隐藏变量与对应生成参数具有很强的等变性(equivariance)，如图6。从而编码器能够轻松获取真实生成参数。
3.2 不变性目标(Invariance Targeting)
通过一次只包含一种变换的数据进行训练，以鼓励特定的神经元包含特定的信息，这是等变形。同时我们希望明确阻止这些神经元对其他信息的学习，希望其对其他变换具有不变性(invariance)。由于每批小批量训练数据中只包含一种变化，因此不变性目标可以看做编码器的除指定神经元外的神经元(及对应其他变换的神经元)应当给出相同输出。
为了使DC-IGN实现上述特性，对于所有对应非激活变换的神经元，用其与均值的差值的&#34;误差梯度&#34;来训练。 可以简单地将次梯度视为
4 实验 本文基于Paysan等人获取的12000各批次的三维人脸数据训练，每个批次包含20张人脸，身份、姿势和照明存在随机变化。训练时使用rmsprop学习算法，元学习率为0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/979f5201669c6e2f006f564eb447f91c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-08T15:36:25+08:00" />
<meta property="article:modified_time" content="2022-07-08T15:36:25+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程随想" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程随想</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">《Deep Convolutional Inverse Graphics Network》(DCIGN) 翻译</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <blockquote> 
 <p>本文对《<a class="link-info" href="https://arxiv.org/abs/1503.03167" rel="nofollow" title="Deep Convolutional Inverse Graphics Network">Deep Convolutional Inverse Graphics Network</a>》的主要内容进行翻译和记录。</p> 
</blockquote> 
<h4>1 引言</h4> 
<p>        深度学习在从图像中自动学习层次化表征任务上取得了显著成效。但如何产生数据的最优表征方面的工作相对较少。<br>         许多不同的关于表征学习的理论和应用研究中，均体现出对表征相同的要求：<strong>不变性(invariance)、有意义性(meaningfulness)、抽象性(abstraction)和解耦性(disentanglement)</strong>。Bengio指出解耦表征(disentangled representation)具有编码变化对真实世界中的变化是稀疏的特点，即数据的不同解释性分布往往相互独立地变化，连续的真实世界序列的变化往往一次只对应少量隐藏特征的变化。<br>         逆图形视觉(“vision as inverse graphics”)模型对提供这些特征的图像进行表征。计算机图形学(computer graphics)包含从场景的紧凑描述(图形编码, graphics code)映射到图像的函数。该类图形编码通常是解耦的，从而可以通过对变换(如对象位置、姿势、照明、纹理和形状等)的细粒度控制来渲染场景。这些编码被设计为容易且可解释地表征真实数据序列，以便在软件中紧凑地表征常见变换。这一标准与Bengio的类似。图形编码可以方便地与理想表征的属性对齐。<br> 最近的逆图形研究遵循了一个通用策略：定义具有隐藏参数的概率或确定性模型，通过推理或优化算法找到给定观测下最合适的潜在参数集。但这些方法无法自动生成可解释的图形编码，也难以再现三维图像。<br>         本文提供了一种学习复杂变换(如平面外旋转、光照变化等)的可解释图形编码的方法。给定一组图像，本文使用一个混合编码器-解码器模型学习解耦不同变换的表征。为了实现这一点，本文采用一个具有多层卷积核反卷积操作的深度有向图模型，并基于<strong>随机梯度变分贝叶斯算法(Stochastic Gradient Variational Bayes, SGVB)</strong>进行训练。<br>         本文提供一种训练策略以鼓励图形编码层的每组神经元表征特定的变换。为了学习解耦表征，我们将数据划分成包含激活变换和未激活变换的小批次进行训练，而非想监督学习一样提供目标值。目标函数确保图像重构质量。比如，点头的面部图像具有仰角激活，但其形状、纹理及其他仿射变换未激活。通过这样的数据训练迫使图形编码层中选定的神经元表征特定的激活变换。给定一张人脸图像，本文模型可以重新生成与输入不停姿势和光照的图像。</p> 
<h4>2 相关工作</h4> 
<p>具体参考文献这里不做赘述，请参考原文。</p> 
<h4>3 模型</h4> 
<p>         如图1，深度卷积逆图像网络的基本架构包含两部分：一个捕捉给定数据x的图形编码Z的分布的编码器和一个学习给定Z产生近似<img alt="\tilde{x}" class="mathcode" src="https://images2.imgbox.com/74/e3/sQiVb00v_o.png">的条件分布的解码器。Z是包含分解的隐藏变量<img alt="z_i\in Z" class="mathcode" src="https://images2.imgbox.com/c1/cd/cJpjRBCv_o.png">(如姿势、光照、形状等)的解耦表征。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" height="397" src="https://images2.imgbox.com/f4/e1/V40U1utn_o.png" width="875"> 
  <figcaption>
    图1 
   <strong>模型结构</strong>。模型遵循变分自编码器的架构。编码器有若干层卷积和最大池化层组成，解码器有若干层上采样和卷积层组成。(a) 训练期间使用目标函数-log(P(x|zi))+KL(Q(zi|x)||P(zi)计算并反向传播梯度. (b) 测试期间，通过调整编码器获得的编码以渲染成新的图像，类似与操作现成的3D渲染引擎。 
  </figcaption> 
 </figure> 
</div> 
<p><br>         记编码器输出为<img alt="y_e=encoder(x)" class="mathcode" src="https://images2.imgbox.com/5d/c9/TOCOlHbT_o.png">，用于参数化变分近似分布<img alt="Q(z_i|y_e)" class="mathcode" src="https://images2.imgbox.com/39/4f/Q5lLrSgj_o.png">，Q是多变量正态分布。这样做 有两个好处：(1)样本相对Q的参数的梯度可以通过重参数技巧容易地获得；(2)三维扫描仪数据上训练的不同金泰新装模型具有相同的多变量正态隐藏分布。给定连接<img alt="y_e" class="mathcode" src="https://images2.imgbox.com/4d/3e/3Z6zOSNq_o.png">和<img alt="z_i" class="mathcode" src="https://images2.imgbox.com/c9/49/UzkjehOf_o.png">的参数<img alt="W_e" class="mathcode" src="https://images2.imgbox.com/36/23/mRFD5qhj_o.png">，分布参数<img alt="\theta=(\mu _{z_i},\sum\nolimits_{z_i})" class="mathcode" src="https://images2.imgbox.com/a1/4b/kvTgSfbw_o.png">和隐藏编码Z可以表示为</p> 
<p></p> 
<p class="img-center"><img alt="" height="116" src="https://images2.imgbox.com/c5/3a/VTKZ44T4_o.png" width="409"></p> 
<p><strong>3.1 针对特定变换的训练</strong></p> 
<p>        本工作的主要目标是学习数据的具有解耦和具有语义解释性的隐藏变量的表征。<br>         一个对于场景信息的表征的自然选择是图像引擎中已经使用的表征设计。如果说人脸图像能拆分成姿势、光照、形状等变量来解构，那么自然可以选择这些变换来构造表征。图2给出了尝试学习的表征。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" height="130" src="https://images2.imgbox.com/19/2b/psaXiQNl_o.png" width="701"> 
  <figcaption>
    图2 表征向量结构。前三维特征分对应人脸图像的方向角、仰角和光照角度。 
  </figcaption> 
 </figure> 
</div> 
<p><br>         为实现这一目标，设计训练流程如下：将数据组织成只对应单个场景变量变化(方位角、仰角、光源方位)的小批量数据(mini-batch)。这些场景变化是现实世界中存在的，我们称其为外部变量(extrinsic variables), 分别由编码中<img alt="z_1,z_2,z_3" class="mathcode" src="https://images2.imgbox.com/e6/43/mQbj9AOp_o.png">的表征。<br>         此外还生成外部变量固定，其他面部属性变化的小批量数据。这些批次数据包含相同观察条件和姿势下的不同人脸图像。这些内在属性，如身份、形状、表情等，用剩余的隐藏变量<img alt="z_{[4,200]}" class="mathcode" src="https://images2.imgbox.com/58/3d/CoF9qBlM_o.png">表征。<br>         本方法基于SGVB训练，但对编码器输出及梯度训练进行了一些关键的调整，具体训练流程如下： </p> 
<blockquote> 
 <p>1. 从期望变化集合{方位角、仰角、光源方位、内在属性}中随机选择一个潜在变量记为<img alt="z_{train}" class="mathcode" src="https://images2.imgbox.com/08/f3/Rvtq8QoG_o.png"><br> 2. 随机选择一个对应变量变化的小批次数据；<br> 3.将小批次中的每个样例输入网络并学习其隐藏表征；<br> 4.计算本批次中所有表征向量的均值；<br> 5.在将编码器输出输入解码器前，将编码向量中所有非选定变化的值(<img alt="z_i\neq z_{train}" class="mathcode" src="https://images2.imgbox.com/fb/93/fFYvPSxL_o.png">)用所有向量的均值替代，即这些输出被钳制(clamped)；<br> 6.根据SGVB计算重构误差并在解码器中反向传播；<br> 7.关于<img alt="z_i\neq z_{train}" class="mathcode" src="https://images2.imgbox.com/9f/bb/8ZIEHanw_o.png">的梯度替换为其与平均值的差值(解释详见3.2)，关于<img alt="z_{train}" class="mathcode" src="https://images2.imgbox.com/c2/d7/9SMJIFML_o.png">的梯度不变；<br> 8.用修正过的梯度在编码器中反向传播</p> 
</blockquote> 
<p>        由于内在属性的表征远高于外在表征，因此需要更多的训练。因此在选择批次时变化时，方位角：仰角：光照角度：内在属性=1:1:1:10。该比例是广泛测试后得出的在本文两个数据集上均有效的比例。<br>        该训练方式通过钳制一个神经元外的所有神经元的输出，迫使解码器仅使用该神经元的值的改变来重构该批次的变化。<br>         该训练方式使得网络的隐藏变量与对应生成参数具有很强的<strong>等变性(equivariance)</strong>，如图6。从而编码器能够轻松获取真实生成参数。</p> 
<p><strong>3.2 不变性目标(Invariance Targeting)</strong></p> 
<p>        通过一次只包含一种变换的数据进行训练，以鼓励特定的神经元包含特定的信息，这是等变形。同时我们希望明确阻止这些神经元对其他信息的学习，希望其对其他变换具有不变性(invariance)。<strong>由于每批小批量训练数据中只包含一种变化，因此不变性目标可以看做编码器的除指定神经元外的神经元(及对应其他变换的神经元)应当给出相同输出</strong>。<br>         为了使DC-IGN实现上述特性，对于所有对应非激活变换<img alt="z_{inactive}" class="mathcode" src="https://images2.imgbox.com/01/02/DSmd4nny_o.png">的神经元，用其与均值的差值的"误差梯度"来训练。 可以简单地将次梯度视为</p> 
<h4>4 实验</h4> 
<p>       本文基于Paysan等人获取的12000各批次的三维人脸数据训练，每个批次包含20张人脸，身份、姿势和照明存在随机变化。训练时使用rmsprop学习算法，元学习率为0.0005，动量衰减0.1，权重衰减0.01。<br>        此外为了研究提出算法在其他数据集上的适用性，也训练网络用于重构Aubry提取的三维椅子图像。该数据集中元素存在较大差异，包括椅子的款式、360度的视角等。</p> 
<p><strong> 4.1 三维人脸数据集</strong></p> 
<p>       解码器网路学习一个近似渲染引擎，如图3、4。给定一个静态测试图像，编码器生成描述描述场景变量的Z，可以独立控制这些变量用解码器生成新图像。图5定性说明了网络在平滑线性流形上表征姿势和光照的能力，说明本文算法能够解耦复杂变换。图5中给出了测试集随机子集的变换的推断和真实值。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" height="429" src="https://images2.imgbox.com/f0/90/BPAe4yJm_o.png" width="905"> 
  <figcaption>
    图3 控制姿势变量：结果定性地体现了解码器对于生成不同姿势图像的泛化能力。(a) 将对应仰角的编码平滑地从-15到15变化。(b) 将对应方位角的编码平滑地从-15到15变化。 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" height="439" src="https://images2.imgbox.com/04/64/i31GxPkn_o.png" width="789"> 
  <figcaption>
    图4 (a) 控制光照变量：结果定性地体现了解码器对于生成不同光照条件的图像的泛化能力。(b) 解耦表征和耦合表征的比较：上方：常规网络重构的原始图像和改变编码后的重构图像；下方：DC-IGN网络重构的原始图像和改变编码后的重构图像。 
  </figcaption> 
 </figure> 
</div> 
<p></p> 
<figure class="image"> 
 <img alt="" height="238" src="https://images2.imgbox.com/d9/71/VkU4Gncq_o.png" width="930"> 
 <figcaption>
   图5 解码器在行角度和光照条件下渲染图像的泛化性：通过改变光照、方位角、仰角生成新的数据集，并测试DC-IGN表征Z的不变性。(a)(b)(c)说明编码器网络合理地预测静态测试图像的变换。有趣的是，在(a)中编码器似乎学到了一个分界点来区分面部左右侧的方位角。 
 </figcaption> 
</figure> 
<p></p> 
<p><strong>4.1.1 和耦合表征(Entangled Representations)比较</strong><br>        比较解耦表征算法和耦合表征算法的重构性能。耦合表征算法采用与DC-IGN结构相同，基于常规SGVB训练，但没有采用本文训练方式的网络。如图3，给网络提供一个输入，尝试用解码器生成不同方位角的图像。在耦合表征算法中，为了确认哪些隐藏表征和方位角相关，将方位角变化的小批量数据输入编码器进行编码，并计算该批次编码的方差，方差最大的隐藏编码被认为和方位角关系最密切，记为<img alt="z_{azimuth}" class="mathcode" src="https://images2.imgbox.com/71/8d/PVftwgjJ_o.png">。之后改变两个模型各自得到的<img alt="z_{azimuth}" class="mathcode" src="https://images2.imgbox.com/6d/bd/sbbuX6sV_o.png">生成新的图像。图4表明采用解耦标称对新视角图像重建很关键。</p> 
<p><strong>4.2 椅子数据</strong></p> 
<p>       数据集包含1375把不同椅子的CAD三维模型渲染的静态图像。这些椅具不同款式、纹理、角度等。训练集使用1200把椅子的图像，测试集使用剩余150把椅子的图像，即网络从未见过测试集椅子。图像调整为150*150像素的灰度图。<br>       用<img alt="z_1" class="mathcode" src="https://images2.imgbox.com/a6/3d/q1YSMupR_o.png">表征椅子的平面旋转的方位角，其他变化表征为<img alt="z_{[2,200]}" class="mathcode" src="https://images2.imgbox.com/11/2d/c5QSlzzU_o.png">。DC-IGN实现了<img alt="2.7722\times 10^{-4}" class="mathcode" src="https://images2.imgbox.com/8d/26/b0bQKVCk_o.png">的重构均方误差。<br>       图6给出提出网络对未见过的椅子在不同角度的渲染。一些椅子生成时能够有平滑的过渡，生成不同的中间姿势。但对有一些椅子似乎只捕捉到关键帧(keyframes)的表征，旨在几个角度上又清晰的输出。</p> 
<figure class="image"> 
 <img alt="" height="476" src="https://images2.imgbox.com/c8/ff/5kAPktT7_o.png" width="714"> 
 <figcaption>
   图6 控制旋转变量：每行均通过编码器对最左侧图像编码，之后修改单个隐藏编码值并通过解码器生成右侧图像。网络在训练时从未见过这些椅子。(a) 一些积极的例子。需要注意DC-IGN会对未见过的椅子组件进行猜测。如第一行中网络猜测椅子具有扶手，尽管实际上其看不见。(b) 一些准确率较低的例子。 
 </figcaption> 
</figure> 
<p></p> 
<h4>5 讨论</h4> 
<p>       本文表明通过静态图像训练具有可解释图形编码表征的深度卷积逆图形网络是可能的。通过在变分自编码器中利用深度卷积和反卷积结构，本文模型可以使用随机变分目标函数的反向传播进行端到端的训练。本文提出一种训练方式迫使网络学习解耦的可解释的表征。基于三维人脸数据，展示了学习到的表征的不变性和等变性。<br>       为了将本文方法拓展到更复杂的场景，可能需要利用更深的结构以处理大量类别对象。设计基于时空的卷积结构来捕捉更复杂的动作变换也具有吸引力。此外，目前的SGVB公式仅适用于连续隐藏变量，可能有必要将公式拓展以处理离散分布或者拓展到循环设定。模型中的解码器也可以用特定域的解码器替代进行细粒度的基于模型的推理。</p> 
<p></p> 
<p><br>  </p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/fee4abe5650bd033572f9af18ad13382/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">ijkplayer音频播放分析</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6e8d9086708f92e3e63b1b8bc5528e8f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Openlayer以图片填充面，pattern填充</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程随想.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>