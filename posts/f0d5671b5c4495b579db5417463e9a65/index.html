<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ä¸»æˆåˆ†åˆ†æ ç‹¬ç«‹æˆåˆ†åˆ†æ_æ‚¨æ›¾ç»æƒ³äº†è§£çš„ä¸»æˆåˆ†åˆ†æèƒŒåçš„ç›´è§‰ - ç¼–ç¨‹éšæƒ³</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="ä¸»æˆåˆ†åˆ†æ ç‹¬ç«‹æˆåˆ†åˆ†æ_æ‚¨æ›¾ç»æƒ³äº†è§£çš„ä¸»æˆåˆ†åˆ†æèƒŒåçš„ç›´è§‰" />
<meta property="og:description" content="ä¸»æˆåˆ†åˆ†æ ç‹¬ç«‹æˆåˆ†åˆ†æ
Unsupervised machine learning has always been used to work very closely with supervised machine learning algorithms and one of the most popular unsupervised learning technique has been Principal Component analysis, why it is extremely popular is because of its ability to process our independent set of variables in such a manner where we end up with the set of variables which has more insightful information and very little noise." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/f0d5671b5c4495b579db5417463e9a65/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-09-13T23:28:50+08:00" />
<meta property="article:modified_time" content="2020-09-13T23:28:50+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="ç¼–ç¨‹éšæƒ³" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">ç¼–ç¨‹éšæƒ³</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ä¸»æˆåˆ†åˆ†æ ç‹¬ç«‹æˆåˆ†åˆ†æ_æ‚¨æ›¾ç»æƒ³äº†è§£çš„ä¸»æˆåˆ†åˆ†æèƒŒåçš„ç›´è§‰</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <article style="font-size: 16px;"> 
 <p>ä¸»æˆåˆ†åˆ†æ ç‹¬ç«‹æˆåˆ†åˆ†æ</p> 
 <div> 
  <section> 
   <div> 
    <div> 
     <blockquote> 
      <p>Unsupervised machine learning has always been used to work very closely with supervised machine learning algorithms and one of the most popular unsupervised learning technique has been Principal Component analysis, why it is extremely popular is because of its ability to process our independent set of variables in such a manner where we end up with the set of variables which has more insightful information and very little noise.</p> 
      <p> æ— ç›‘ç£æœºå™¨å­¦ä¹ ä¸€ç›´è¢«ç”¨æ¥ä¸æœ‰ç›‘ç£æœºå™¨å­¦ä¹ ç®—æ³•éå¸¸ç´§å¯†åœ°åˆä½œï¼Œæœ€æµè¡Œçš„æ— ç›‘ç£å­¦ä¹ æŠ€æœ¯ä¹‹ä¸€å°±æ˜¯ä¸»æˆåˆ†åˆ†æï¼Œä¹‹æ‰€ä»¥å¦‚æ­¤æµè¡Œï¼Œæ˜¯å› ä¸ºå®ƒèƒ½å¤Ÿå¤„ç†è¿™æ ·çš„ç‹¬ç«‹å˜é‡é›†ä¸€ç§ä»¥å˜é‡é›†ç»“æŸçš„æ–¹å¼ï¼Œè¯¥å˜é‡é›†å…·æœ‰æ›´æ·±å…¥çš„ä¿¡æ¯ï¼Œå¹¶ä¸”å™ªéŸ³å¾ˆå°ã€‚ </p> 
     </blockquote> 
     <p>Now with less noise and reduced dimensions, the data set becomes extremely lightweight, can be visualized better, and can be processed better by our ML models with very little overfitting. Thatâ€™s why PCA is the darling of most of the data engineers who have this role of analyzing the data to reduce the cost of data processing by our machines in the cloud both in terms of speed and storage.</p> 
     <p> ç°åœ¨ï¼Œå™ªå£°é™ä½äº†ï¼Œå°ºå¯¸å‡å°äº†ï¼Œæ•°æ®é›†å˜å¾—éå¸¸è½»ä¾¿ï¼Œå¯ä»¥æ›´å¥½åœ°å¯è§†åŒ–ï¼Œå¹¶ä¸”å¯ä»¥ç”¨æˆ‘ä»¬çš„MLæ¨¡å‹è¿›è¡Œå¾ˆå¥½çš„æ‹Ÿåˆï¼Œè€Œå‡ ä¹æ²¡æœ‰è¿‡åº¦æ‹Ÿåˆçš„æƒ…å†µã€‚ è¿™å°±æ˜¯ä¸ºä»€ä¹ˆPCAæ˜¯å¤§å¤šæ•°æ•°æ®å·¥ç¨‹å¸ˆçš„å® å„¿ï¼Œä»–ä»¬æ‹…è´Ÿç€åˆ†ææ•°æ®çš„è§’è‰²ï¼Œä»¥é™ä½æˆ‘ä»¬çš„è®¡ç®—æœºåœ¨äº‘è®¡ç®—æ–¹é¢çš„é€Ÿåº¦å’Œå­˜å‚¨æˆæœ¬ã€‚ </p> 
     <blockquote> 
      <p>â€œTime saved is money saved for the industry , which PCA handles very diligently â€œ</p> 
      <p> â€œèŠ‚çœçš„æ—¶é—´å°±æ˜¯ä¸ºè¯¥è¡Œä¸šèŠ‚çœçš„é’±ï¼ŒPCAéå¸¸åŠªåŠ›åœ°å¤„ç†äº†è¿™ä¸€é—®é¢˜â€ </p> 
     </blockquote> 
     <p>So with this wisdom at our disposal, itâ€™s time to uncover this extremely powerful machine learning tool called PCA.</p> 
     <p> å› æ­¤ï¼Œæœ‰äº†æˆ‘ä»¬çš„è¿™ç§æ™ºæ…§ï¼Œè¯¥æ˜¯æ—¶å€™å‘ç°è¿™ç§åŠŸèƒ½å¼ºå¤§çš„æœºå™¨å­¦ä¹ å·¥å…·PCAäº†ã€‚ </p> 
     <h2> ä»€ä¹ˆæ˜¯PCAï¼Ÿ <span style="font-weight: bold;">(</span>What Is PCA?<span style="font-weight: bold;">)</span></h2> 
     <p>It is an unsupervised ML tool to reduce the dimensionality of the large data set having large numbers of independent variables with collinearity/correlation among themselves.</p> 
     <p> è¿™æ˜¯ä¸€ç§æ— ç›‘ç£çš„MLå·¥å…·ï¼Œç”¨äºå‡å°‘å…·æœ‰å¤§é‡è‡ªå˜é‡ä¸”å½¼æ­¤ä¹‹é—´å…·æœ‰å…±çº¿æ€§/ç›¸å…³æ€§çš„å¤§å‹æ•°æ®é›†çš„ç»´æ•°ã€‚ </p> 
     <blockquote> 
      <p>PCA in others terms is used for Dimensionality reduction by reducing noise in the given independent variables.</p> 
      <p> æ¢å¥è¯è¯´ï¼ŒPCAç”¨äºé€šè¿‡å‡å°‘ç»™å®šè‡ªå˜é‡ä¸­çš„å™ªå£°æ¥é™ä½å°ºå¯¸ã€‚ </p> 
     </blockquote> 
     <p>One has to understand how dimensionality reduction works before one can really assess how valuable PCA can be in the field of Unsupervised learning, so letâ€™s get into the details of â€œ<strong>Dimensionality Reduction â€œ</strong></p> 
     <p> åœ¨çœŸæ­£è¯„ä¼°PCAåœ¨æ— ç›‘ç£å­¦ä¹ é¢†åŸŸä¸­çš„ä»·å€¼ä¹‹å‰ï¼Œå¿…é¡»å…ˆäº†è§£é™ç»´çš„å·¥ä½œåŸç†ï¼Œå› æ­¤è®©æˆ‘ä»¬æ·±å…¥äº†è§£â€œ <strong>é™ç»´â€</strong> </p> 
     <h2> ä»€ä¹ˆæ˜¯é™ç»´åŠå…¶å¦‚ä½•å·¥ä½œï¼Ÿ <span style="font-weight: bold;">(</span>What is Dimensionality Reduction &amp; How Does It Work?<span style="font-weight: bold;">)</span></h2> 
     <p>Dimensions here stand for all the column values present in our dataframe, and when it comes to reducing those columns we only use the independent features. So the technique of getting rid of those independent variables is called Dimensionality reduction.</p> 
     <p> ç»´åº¦ä»£è¡¨æ•°æ®æ¡†ä¸­æ‰€æœ‰åˆ—çš„å€¼ï¼Œå½“æ¶‰åŠå‡å°‘è¿™äº›åˆ—æ—¶ï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨ç‹¬ç«‹åŠŸèƒ½ã€‚ å› æ­¤ï¼Œæ¶ˆé™¤è¿™äº›è‡ªå˜é‡çš„æŠ€æœ¯ç§°ä¸ºé™ç»´ã€‚ </p> 
     <h2> å¦‚ä½•å®ç°é™ç»´ï¼Ÿ <span style="font-weight: bold;">(</span>How Dimensionality Reduction Is Achieved?<span style="font-weight: bold;">)</span></h2> 
     <p>Dimensionality reduction is achieved using two of the below-given techniques</p> 
     <p> ä½¿ç”¨ä»¥ä¸‹ä¸¤ç§æŠ€æœ¯å®ç°é™ç»´ </p> 
     <ul><li><p><strong>Feature Elimination</strong></p><p> <strong>åŠŸèƒ½æ¶ˆé™¤</strong> </p></li><li><p><strong>Feature Extraction</strong></p><p> <strong>ç‰¹å¾æå–</strong> </p></li></ul> 
     <h2> åŠŸèƒ½æ¶ˆé™¤ï¼š <span style="font-weight: bold;">(</span>Feature Elimination :<span style="font-weight: bold;">)</span></h2> 
     <p>Itâ€™s a simple but very harsh method of getting rid of those feature columns which doesnâ€™t look important through the analysis.</p> 
     <p> è¿™æ˜¯ä¸€ç§ç®€å•ä½†éå¸¸è‹›åˆ»çš„æ–¹æ³•ï¼Œç”¨äºæ¶ˆé™¤é‚£äº›åœ¨åˆ†æä¸­çœ‹èµ·æ¥å¹¶ä¸é‡è¦çš„è¦ç´ åˆ—ã€‚ </p> 
     <h3> åå¤„ï¼š <span style="font-weight: bold;">(</span>Disadvantage:<span style="font-weight: bold;">)</span></h3> 
     <ul><li>The very obvious disadvantage of this methodology is that we will simply lose all the valuable information that a particular feature has to offer, which can be of prime importance. So in professional setup preferably this is the least used mechanism to reduce dimensions of the data set.<p class="nodelete"></p> è¿™ç§æ–¹æ³•çš„ä¸€ä¸ªéå¸¸æ˜æ˜¾çš„ç¼ºç‚¹æ˜¯ï¼Œæˆ‘ä»¬å°†ç®€å•åœ°ä¸¢å¤±ç‰¹å®šåŠŸèƒ½å¿…é¡»æä¾›çš„æ‰€æœ‰æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œè¿™å¯èƒ½æ˜¯æœ€é‡è¦çš„ã€‚ å› æ­¤ï¼Œåœ¨ä¸“ä¸šè®¾ç½®ä¸­ï¼Œæœ€å¥½æ˜¯ä½¿ç”¨æœ€å°‘çš„æœºåˆ¶æ¥å‡å°‘æ•°æ®é›†çš„å°ºå¯¸ã€‚ </li></ul> 
     <h3> æ¶ˆé™¤ç‰¹å¾çš„ä¼˜åŠ¿ï¼š <span style="font-weight: bold;">(</span>Advantage of Feature Elimination :<span style="font-weight: bold;">)</span></h3> 
     <ul><li>It is easy to interpret<p class="nodelete"></p> è¿™å¾ˆå®¹æ˜“è§£é‡Š </li><li>It gives a high level of accuracy but at the cost of model overfitting<p class="nodelete"></p> å®ƒæä¾›äº†å¾ˆé«˜çš„ç²¾åº¦ï¼Œä½†ä»¥æ¨¡å‹è¿‡åº¦æ‹Ÿåˆä¸ºä»£ä»· </li></ul> 
     <h3> ç‰¹å¾æå– ï¼š <span style="font-weight: bold;">(</span>Feature Extraction :<span style="font-weight: bold;">)</span></h3> 
     <p>In Feature extraction, intuition is to capture or extract meaningful information from the existing set of features and create a new set of feature column which ensure all the valuable info is retained and all the noises are eliminated.</p> 
     <p> åœ¨ç‰¹å¾æå–ä¸­ï¼Œç›´è§‰æ˜¯ä»ç°æœ‰ç‰¹å¾é›†ä¸­æ•è·æˆ–æå–æœ‰æ„ä¹‰çš„ä¿¡æ¯ï¼Œå¹¶åˆ›å»ºä¸€ç»„æ–°çš„ç‰¹å¾åˆ—ï¼Œä»¥ç¡®ä¿ä¿ç•™æ‰€æœ‰æœ‰ä»·å€¼çš„ä¿¡æ¯å¹¶æ¶ˆé™¤æ‰€æœ‰å™ªå£°ã€‚ </p> 
     <p>Now that you understand the concept of dimensionality reduction, itâ€™s time to understand the role of PCA . When it comes to extracting meaningful information from our feature variable, PCA is our way to go.</p> 
     <p> ç°åœ¨æ‚¨å·²ç»äº†è§£äº†é™ç»´çš„æ¦‚å¿µï¼Œæ˜¯æ—¶å€™äº†è§£PCAçš„ä½œç”¨äº†ã€‚ ä»ç‰¹å¾å˜é‡ä¸­æå–æœ‰æ„ä¹‰çš„ä¿¡æ¯æ—¶ï¼ŒPCAæ˜¯æˆ‘ä»¬çš„æœ€ä½³é€‰æ‹©ã€‚ </p> 
     <blockquote> 
      <p>PCA is the tool to do feature extraction in careful and intelligent way</p> 
      <p> PCAæ˜¯ä»¥è°¨æ…å’Œæ™ºèƒ½çš„æ–¹å¼è¿›è¡Œç‰¹å¾æå–çš„å·¥å…· </p> 
     </blockquote> 
     <p>These extracted features are then generally used in our supervised or deep learning models to make the required predictions.</p> 
     <p> è¿™äº›æå–çš„ç‰¹å¾éšåé€šå¸¸ç”¨äºæˆ‘ä»¬çš„ç›‘ç£æˆ–æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼Œä»¥è¿›è¡Œæ‰€éœ€çš„é¢„æµ‹ã€‚ </p> 
     <h2> é™ç»´å¦‚ä½•å·¥ä½œï¼Ÿ <span style="font-weight: bold;">(</span>How Does Dimensionality Reduction Work?<span style="font-weight: bold;">)</span></h2> 
     <p>It works on identifying the highly correlated variables and try to combine them into a new set of features or by completely eliminating them, thereby reducing the <strong>multi-collinearity </strong>among those independent columns.</p> 
     <p> å®ƒè‡´åŠ›äºè¯†åˆ«é«˜åº¦ç›¸å…³çš„å˜é‡ï¼Œå¹¶å°è¯•å°†å®ƒä»¬ç»„åˆä¸ºä¸€ç»„æ–°åŠŸèƒ½æˆ–é€šè¿‡å®Œå…¨æ¶ˆé™¤å®ƒä»¬ï¼Œä»è€Œå‡å°‘è¿™äº›ç‹¬ç«‹åˆ—ä¹‹é—´çš„<strong>å¤šé‡å…±çº¿æ€§</strong> ã€‚ </p> 
     <h3> é™ç»´çš„ä¼˜åŠ¿ï¼š <span style="font-weight: bold;">(</span>Advantages Of Dimensionality Reduction :<span style="font-weight: bold;">)</span></h3> 
     <ul><li>This helps in enhanced model performance<p class="nodelete"></p> è¿™æœ‰åŠ©äºå¢å¼ºæ¨¡å‹æ€§èƒ½ </li><li>Decreases the computation time of the model eventually helping the model to converge faster.<p class="nodelete"></p> å‡å°‘æ¨¡å‹çš„è®¡ç®—æ—¶é—´ï¼Œæœ€ç»ˆå¸®åŠ©æ¨¡å‹æ›´å¿«æ”¶æ•›ã€‚ </li><li>With a reduced set of dimensions, now data visualization becomes very super easy<p class="nodelete"></p> é€šè¿‡å‡å°‘å°ºå¯¸é›†ï¼Œç°åœ¨æ•°æ®å¯è§†åŒ–å˜å¾—éå¸¸å®¹æ˜“ </li><li>The required storage memory reduces considerably, so the cost for the companies to store those data sets gets reduced significantly<p class="nodelete"></p> æ‰€éœ€çš„å­˜å‚¨å†…å­˜å¤§å¤§å‡å°‘ï¼Œå› æ­¤å…¬å¸å­˜å‚¨è¿™äº›æ•°æ®é›†çš„æˆæœ¬å¤§å¤§é™ä½äº† </li><li><p>It treats the anomaly of the â€œ<strong>curse of dimensionally </strong>â€œ</p><p> å®ƒå¤„ç†â€œ <strong>å°ºå¯¸è¯…å’’</strong> â€çš„å¼‚å¸¸ </p></li><li>Reduces the model overfitting<p class="nodelete"></p> å‡å°‘æ¨¡å‹è¿‡åº¦æ‹Ÿåˆ </li></ul> 
     <h3> â€œ PCAå¯å¸®åŠ©æ‚¨äº†è§£åœ¨ä¸å®Œå…¨æ¶ˆé™¤å¯ç”¨åŠŸèƒ½å˜é‡çš„æƒ…å†µä¸‹å¿…é¡»ä¿ç•™å“ªäº›æœ‰ä»·å€¼çš„ä¿¡æ¯ä»¥åŠéœ€è¦é‡Šæ”¾å“ªäº›å™ªå£°â€ <span style="font-weight: bold;">(</span>â€œPCA helps you understand what are the valuable information which one has to retain and what are the noises that one needs to let go without completely eliminating the feature variables available â€<span style="font-weight: bold;">)</span></h3> 
     <h2> PCAå¦‚ä½•å·¥ä½œï¼ŒPCAçš„ç›´è§‰æ˜¯ä»€ä¹ˆï¼Ÿ <span style="font-weight: bold;">(</span>How Does PCA Works, Whatâ€™s The Intuition Behind PCA?<span style="font-weight: bold;">)</span></h2> 
     <p>The main idea behind PCA is to project the independent features in a lower dimension in such a way that they end up explaining the maximum variance in the given data. The intuition here is not to eliminate but to synthesize a new set of features called principal from the existing set of variables in such a manner that they end up being projected in the direction where there is maximum variance.</p> 
     <p> PCAèƒŒåçš„ä¸»è¦æ€æƒ³æ˜¯ä»¥è¾ƒä½çš„ç»´åº¦æŠ•å½±ç‹¬ç«‹è¦ç´ ï¼Œä»¥ä½¿å®ƒä»¬æœ€ç»ˆè§£é‡Šç»™å®šæ•°æ®ä¸­çš„æœ€å¤§æ–¹å·®ã€‚ ç›´è§‰ä¸æ˜¯è¦æ¶ˆé™¤è€Œæ˜¯è¦ä»ç°æœ‰å˜é‡é›†ä¸­åˆæˆä¸€ç»„æ–°çš„ç§°ä¸ºä¸»ä½“çš„ç‰¹å¾ï¼Œä»¥ä½¿è¿™äº›ç‰¹å¾æœ€ç»ˆæœæœ€å¤§æ–¹å·®çš„æ–¹å‘æŠ•å½±ã€‚ </p> 
     <h3> è®©æˆ‘ä»¬äº†è§£ä¸»æˆåˆ†åˆ†æçš„å·¥ä½œåŸç†ï¼š <span style="font-weight: bold;">(</span>Letâ€™s understand the working of Principal Component Analysis :<span style="font-weight: bold;">)</span></h3> 
     <p>Letâ€™s visualize the process of PCA.</p> 
     <p> è®©æˆ‘ä»¬å¯è§†åŒ–PCAçš„è¿‡ç¨‹ã€‚ </p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/43/b6/DuYVqMW5_o.png" width="837" height="1280" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        PCA : Fig 1.0 
      </figcaption> 
      <figcaption>
        PCAï¼šå›¾1.0 
      </figcaption> 
     </figure> 
     <ul><li>Suppose we have X independent features shown in the image at the left.<p class="nodelete"></p> å‡è®¾æˆ‘ä»¬æœ‰Xä¸ªç‹¬ç«‹ç‰¹å¾ï¼Œå¦‚å·¦å›¾æ‰€ç¤ºã€‚ </li><li>These have 8 number of columns, where it has variables with collinearity. Represented as X1, X2, X3, X4, X5 etcâ€¦.<p class="nodelete"></p> å®ƒä»¬å…·æœ‰8åˆ—æ•°ï¼Œå…¶ä¸­å…·æœ‰å…·æœ‰å…±çº¿æ€§çš„å˜é‡ã€‚ è¡¨ç¤ºä¸ºX1ï¼ŒX2ï¼ŒX3ï¼ŒX4ï¼ŒX5ç­‰ã€‚ </li><li>So through PCA, we will extract a new feature variable called Z, which has the PCA components being represented as Z1, Z2, Z3, etcâ€¦<p class="nodelete"></p> å› æ­¤ï¼Œé€šè¿‡PCAï¼Œæˆ‘ä»¬å°†æå–ä¸€ä¸ªåä¸ºZçš„æ–°ç‰¹å¾å˜é‡ï¼Œå…¶ä¸­çš„PCAç»„ä»¶åˆ†åˆ«è¡¨ç¤ºä¸ºZ1ï¼ŒZ2ï¼ŒZ3ç­‰ã€‚ </li><li>So after we performed our PCA we get the linear equations of Principal components like Z1, as can be seen in the image at the left.<p class="nodelete"></p> å› æ­¤ï¼Œåœ¨æ‰§è¡ŒPCAä¹‹åï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸»åˆ†é‡çš„çº¿æ€§æ–¹ç¨‹å¼ï¼Œä¾‹å¦‚Z1ï¼Œå¦‚å·¦å›¾æ‰€ç¤ºã€‚ </li><li>Z1, Z2, Z3, etc are the linear equations that have the coefficient and the data set extracted from x1, x3, x3 independent variables.<p class="nodelete"></p> Z1ï¼ŒZ2ï¼ŒZ3ç­‰æ˜¯çº¿æ€§æ–¹ç¨‹ï¼Œå…·æœ‰ä»x1ï¼Œx3ï¼Œx3è‡ªå˜é‡æå–çš„ç³»æ•°å’Œæ•°æ®é›†ã€‚ </li><li>Here i have tried to give you the intuition of how PCA is performed at the very high level, in-fact there goes a lot of algebraic arithmetic process behind to get to those principal components, which we will cover going forward.<p class="nodelete"></p> åœ¨è¿™é‡Œï¼Œæˆ‘è¯•å›¾å‘æ‚¨ä»‹ç»PCAå¦‚ä½•åœ¨å¾ˆé«˜çš„æ°´å¹³ä¸Šæ‰§è¡Œï¼Œå®é™…ä¸Šï¼Œåé¢è¿˜æœ‰è®¸å¤šä»£æ•°è¿ç®—è¿‡ç¨‹å¯ä»¥åˆ°è¾¾è¿™äº›ä¸»è¦ç»„æˆéƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ç»§ç»­ä»‹ç»è¿™äº›å†…å®¹ã€‚ </li></ul> 
     <h2> å¯è§†åŒ–PCAï¼š <span style="font-weight: bold;">(</span>Visualizing the PCA :<span style="font-weight: bold;">)</span></h2> 
     <blockquote> 
      <p>PCA assumes that the directions with the largest variances are the most important or the most principal</p> 
      <p> PCAå‡è®¾æ–¹å·®æœ€å¤§çš„æ–¹å‘æ˜¯æœ€é‡è¦æˆ–æœ€ä¸»è¦çš„æ–¹å‘ </p> 
     </blockquote> 
     <h3> è®©æˆ‘ä»¬æŒ‰ç…§ä¸‹é¢çš„æ— èŠ±æœ2.0æ¥å¯è§†åŒ–PCAçš„åŠŸèƒ½ï¼š <span style="font-weight: bold;">(</span>Letâ€™s follow the fig 2.0 below and visualize how PCA functions :<span style="font-weight: bold;">)</span></h3> 
     <p>Suppose we have 2 Dimensional independent variable X1 &amp; X2. Have used only 2D data set to explain the PCA with the required visualization and simplicity. In the Plot1 section of fig 2.0, we can see that x1 and x2 variables have been represented with an ellipsoid kind of image where features X1 &amp; X2 are having a positive correlation.</p> 
     <p> å‡è®¾æˆ‘ä»¬æœ‰2ç»´è‡ªå˜é‡X1å’ŒX2ã€‚ ä»…ä½¿ç”¨2Dæ•°æ®é›†ä»¥æ‰€éœ€çš„å¯è§†åŒ–å’Œç®€å•æ€§æ¥è§£é‡ŠPCAã€‚ åœ¨å›¾2.0çš„Plot1éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°x1å’Œx2å˜é‡å·²ç»ç”¨æ¤­åœ†å½¢çš„å›¾åƒè¡¨ç¤ºï¼Œå…¶ä¸­ç‰¹å¾X1å’ŒX2å…·æœ‰æ­£ç›¸å…³ã€‚ </p> 
    </div> 
   </div> 
   <div> 
    <figure style="display:block;text-align:center;"> 
     <div> 
      <div> 
       <div style="text-align: center;"> 
        <img alt="Image for post" src="https://images2.imgbox.com/08/93/SxoAmMx9_o.png" width="1280" height="967" style="outline: none;"> 
       </div> 
      </div> 
     </div> 
    </figure> 
   </div> 
   <div> 
    <div> 
     <p>Here PC1 axis is the natural axis or we can say the first principal direction along which our independent variable samples will show the largest variance. Similarly, the PC2 axis is the second most important principle direction with considerable variance.</p> 
     <p> è¿™é‡ŒPC1è½´æ˜¯è‡ªç„¶è½´ï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥è¯´æˆ‘ä»¬çš„è‡ªå˜é‡æ ·æœ¬å°†æ˜¾ç¤ºæœ€å¤§æ–¹å·®çš„ç¬¬ä¸€ä¸ªä¸»æ–¹å‘ã€‚ åŒæ ·ï¼ŒPC2è½´æ˜¯ç¬¬äºŒé‡è¦çš„ä¸»è¦æ–¹å‘ï¼Œå¹¶ä¸”æœ‰å¾ˆå¤§çš„å·®å¼‚ã€‚ </p> 
     <p>In order to reduce the 2 dimension data, we need to transform or project the PC2 axis data set onto the PC1 axis, which has the largest variance to capture. which you can see in plot 2 , where we have projected each data sample to the PC2 axis. Thus we will be able to reduce the features from 2D to 1D. These projected samples form the principal components which have the <strong>Eigenvalues </strong>explaining the largest variance.</p> 
     <p> ä¸ºäº†å‡å°‘äºŒç»´æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦å°†PC2è½´æ•°æ®é›†è½¬æ¢æˆ–æŠ•å½±åˆ°PC1è½´ä¸Šï¼Œè¯¥PC1è½´å…·æœ‰æœ€å¤§çš„å˜åŒ–è¦æ•è·ã€‚ æ‚¨å¯ä»¥åœ¨å›¾2ä¸­çœ‹åˆ°è¯¥å›¾ï¼Œå…¶ä¸­æˆ‘ä»¬å·²å°†æ¯ä¸ªæ•°æ®æ ·æœ¬æŠ•å½±åˆ°PC2è½´ä¸Šã€‚ è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥å°†ç‰¹å¾ä»2Då‡å°‘åˆ°1Dã€‚ è¿™äº›æŠ•å½±æ ·æœ¬æ„æˆäº†ä¸»è¦<strong>ç‰¹å¾</strong> ï¼Œè¿™äº›<strong>ç‰¹å¾å…·æœ‰</strong>è§£é‡Šæœ€å¤§æ–¹å·®çš„<strong>ç‰¹å¾å€¼</strong> ã€‚ </p> 
     <p>Now that we have high-level intuition of how PCA works letâ€™s get into the details of the same.</p> 
     <p> ç°åœ¨ï¼Œæˆ‘ä»¬å¯¹PCAçš„å·¥ä½œåŸç†æœ‰äº†ä¸€ä¸ªé«˜å±‚æ¬¡çš„ç›´è§‚è®¤è¯†ï¼Œè®©æˆ‘ä»¬æ·±å…¥ç ”ç©¶ä¸€ä¸‹PCAçš„ç»†èŠ‚ã€‚ </p> 
     <h2> PCAï¼šå®æ–½æ­¥éª¤ï¼š <span style="font-weight: bold;">(</span>PCA: Implementation Steps:<span style="font-weight: bold;">)</span></h2> 
     <h2> ç­”ï¼šè§„èŒƒåŒ–æ•°æ®é›†ï¼š <span style="font-weight: bold;">(</span>A: Normalizing The Data Set:<span style="font-weight: bold;">)</span></h2> 
     <p>In the first step, we need to normalize the data that we have so that PCA works properly. This is done by subtracting the respective means from the numbers in the respective column. So if we have two dimensions X and Y, all X become ğ”- and all Y become ğ’š-. This produces a dataset whose mean is zero and standard deviation is 1. We can achieve this using sci-kit learn scaling/normalizing methods.</p> 
     <p> ç¬¬ä¸€æ­¥ï¼Œæˆ‘ä»¬éœ€è¦è§„èŒƒåŒ–æˆ‘ä»¬æ‹¥æœ‰çš„æ•°æ®ï¼Œä»¥ä½¿PCAæ­£å¸¸å·¥ä½œã€‚ è¿™æ˜¯é€šè¿‡ä»ç›¸åº”åˆ—ä¸­çš„æ•°å­—å‡å»ç›¸åº”çš„å‡å€¼æ¥å®Œæˆçš„ã€‚ å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸¤ä¸ªç»´åº¦Xå’ŒYï¼Œåˆ™æ‰€æœ‰Xéƒ½å˜ä¸ºğ”-ï¼Œæ‰€æœ‰Yéƒ½å˜ä¸ºğ’š-ã€‚ è¿™å°†äº§ç”Ÿä¸€ä¸ªå‡å€¼ä¸ºé›¶ä¸”æ ‡å‡†åå·®ä¸º1çš„æ•°æ®é›†ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨sci-kitå­¦ä¹ ç¼©æ”¾/å½’ä¸€åŒ–æ–¹æ³•æ¥å®ç°æ­¤ç›®çš„ã€‚ </p> 
     <p>Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable.</p> 
     <p> ä»æ•°å­¦ä¸Šè®²ï¼Œè¿™å¯ä»¥é€šè¿‡å‡å»å¹³å‡å€¼å¹¶é™¤ä»¥æ¯ä¸ªå˜é‡æ¯ä¸ªå€¼çš„æ ‡å‡†å·®æ¥å®ç°ã€‚ </p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div style="text-align: center;"> 
          <img alt="Image for post" src="https://images2.imgbox.com/07/9b/qcQVAdCt_o.png" width="263" height="54" style="outline: none;"> 
         </div> 
        </div> 
       </div> 
      </div> 
     </figure> 
     <p>Once the standardization is done, all the variables will be transformed to the same scale.</p> 
     <p> ä¸€æ—¦å®Œæˆæ ‡å‡†åŒ–ï¼Œæ‰€æœ‰å˜é‡å°†è½¬æ¢ä¸ºç›¸åŒçš„æ¯”ä¾‹ã€‚ </p> 
     <h2> Bï¼šæŸ¥æ‰¾å½’ä¸€åŒ–æ•°æ®é›†çš„åæ–¹å·® <span style="font-weight: bold;">(</span>B: Find The Covariance Of The Normalized Data Set<span style="font-weight: bold;">)</span></h2> 
     <p>For 2D vectors: Our 2Ã—2 Covariance matrix will look like,</p> 
     <p> å¯¹äº2Då‘é‡ï¼šæˆ‘ä»¬çš„2Ã—2åæ–¹å·®çŸ©é˜µå¦‚ä¸‹æ‰€ç¤ºï¼š </p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div style="text-align: center;"> 
          <img alt="Image for post" src="https://images2.imgbox.com/63/50/bOz4o8XU_o.jpg" width="427" height="59" style="outline: none;"> 
         </div> 
        </div> 
       </div> 
      </div> 
     </figure> 
     <p>Where,</p> 
     <p> å“ªé‡Œï¼Œ </p> 
     <p><strong>Var[X1] = Cov[X1,X1] and Var[X2] = Cov[X2,X2].</strong></p> 
     <p> <strong>Var [X1] = Cov [X1ï¼ŒX1]å’ŒVar [X2] = Cov [X2ï¼ŒX2]ã€‚</strong> </p> 
     <h2> Cï¼šæ‰¾åˆ°åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼ï¼š <span style="font-weight: bold;">(</span>C: Find the Eigenvectors and EigenValues of the Covariance matrix :<span style="font-weight: bold;">)</span></h2> 
     <p>Next, we calculate eigenvalues and eigenvectors for the covariance matrix. The matrix here is a square matrix A. <strong>Æ›</strong> is an eigenvalue for a matrix <strong>A</strong> if we get equation given below:</p> 
     <p> æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è®¡ç®—åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ã€‚ è¿™é‡Œçš„çŸ©é˜µæ˜¯ä¸€ä¸ªæ–¹é˜µ<strong>A.Æ›</strong>æ˜¯ä¸€ä¸ªçŸ©é˜µ<strong>A</strong>çš„ç‰¹å¾å€¼ï¼Œå¦‚æœæˆ‘ä»¬å¾—åˆ°æ–¹ç¨‹å¦‚ä¸‹ï¼š </p> 
     <h2> det(Æ›Iâ€” A)= 0ï¼Œ <span style="font-weight: bold;">(</span>det( Æ›I â€” A ) = 0,<span style="font-weight: bold;">)</span></h2> 
     <p>Here,</p> 
     <p> è¿™é‡Œï¼Œ </p> 
     <ul><li><p><strong>I</strong> is the identity matrix of the same dimension</p><p> <strong>æˆ‘</strong>æ˜¯åŒä¸€ä¸ªç»´åº¦çš„å•ä½çŸ©é˜µ </p></li><li><p><strong>A</strong> which is a required condition for the matrix subtraction</p><p> <strong>A</strong>æ˜¯çŸ©é˜µå‡æ³•çš„å¿…è¦æ¡ä»¶ </p></li><li><p><strong>detâ€™ </strong>is the determinant of the matrix.</p><p> <strong>det'</strong>æ˜¯çŸ©é˜µçš„è¡Œåˆ—å¼ã€‚ </p></li></ul> 
     <p>For each eigenvalue <strong>Æ›</strong>, a corresponding eigenvector <strong>v</strong>, can be found by solving:</p> 
     <p> å¯¹äºæ¯ä¸ªç‰¹å¾å€¼<strong>Æ›</strong> ï¼Œå¯ä»¥é€šè¿‡æ±‚è§£æ¥æ‰¾åˆ°å¯¹åº”çš„ç‰¹å¾å‘é‡<strong>v</strong> ï¼š </p> 
     <h2> (Æ›Iâ€” A)v = 0 <span style="font-weight: bold;">(</span>( Æ›I â€” A )v = 0<span style="font-weight: bold;">)</span></h2> 
     <h2> Dï¼šè®¡ç®—ç‰¹å¾å‘é‡ä»¥å½¢æˆä¸ªäººæˆåˆ†ç©ºé—´ï¼š <span style="font-weight: bold;">(</span>D: Calculating the feature vector to form Personal Component Space:<span style="font-weight: bold;">)</span></h2> 
     <p>We rank the eigenvalues from largest to smallest so that it gives us the components in order of significance. Here at this stage we try to reduce the dimension to find the eigenvectors which will be forming the feature vectors, which is a matrix of vectors, called the eigenvectors.</p> 
     <p> æˆ‘ä»¬ä»æœ€å¤§åˆ°æœ€å°å¯¹ç‰¹å¾å€¼è¿›è¡Œæ’åºï¼Œä»¥ä¾¿æŒ‰é‡è¦æ€§é¡ºåºä¸ºæˆ‘ä»¬æä¾›åˆ†é‡ã€‚ åœ¨æ­¤é˜¶æ®µï¼Œæˆ‘ä»¬åœ¨æ­¤é˜¶æ®µå°è¯•å‡å°ç»´æ•°ä»¥æ‰¾åˆ°å°†å½¢æˆç‰¹å¾å‘é‡çš„ç‰¹å¾å‘é‡ï¼Œè¯¥ç‰¹å¾å‘é‡æ˜¯å‘é‡çš„çŸ©é˜µï¼Œç§°ä¸ºç‰¹å¾å‘é‡ã€‚ </p> 
     <p>Feature Vector = (eig1, eig2, eg3,â€¦ ), depends upon the mathematical space we are dealing with<em>.</em></p> 
     <p> ç‰¹å¾å‘é‡=(eig1ï¼Œeig2ï¼Œeg3ï¼Œâ€¦)ï¼Œå–å†³äºæˆ‘ä»¬è¦å¤„ç†çš„æ•°å­¦ç©ºé—´<em>ã€‚</em> </p> 
     <h2> Eï¼šå½¢æˆä¸»è¦æˆåˆ†ï¼š <span style="font-weight: bold;">(</span>E: Forming Principal Components :<span style="font-weight: bold;">)</span></h2> 
     <p>After we go through previous steps we make use of the feature vector values, transpose it and multiply it with the transpose of the scaled features of the original which we performed at the initial stage of normalization.</p> 
     <p> åœ¨å®Œæˆå‰é¢çš„æ­¥éª¤ä¹‹åï¼Œæˆ‘ä»¬ä½¿ç”¨ç‰¹å¾å‘é‡å€¼ï¼Œå¯¹å…¶è¿›è¡Œè½¬ç½®å¹¶å°†å…¶ä¸æˆ‘ä»¬åœ¨å½’ä¸€åŒ–åˆå§‹é˜¶æ®µæ‰§è¡Œçš„åŸå§‹ç¼©æ”¾æ¯”ä¾‹ç‰¹å¾çš„è½¬ç½®ç›¸ä¹˜ã€‚ </p> 
     <p>We build a new reduced dataset from the K chosen principle components.</p> 
     <p> æˆ‘ä»¬ä»Kä¸ªé€‰å®šçš„ä¸»æˆåˆ†ä¸­æ„å»ºäº†ä¸€ä¸ªæ–°çš„ç®€åŒ–æ•°æ®é›†ã€‚ </p> 
     <p><em>reducedData= FeatureVectorT x ScaledDataT</em></p> 
     <p> <em>reduceData = FeatureVectorT x ScaledDataT</em> </p> 
     <p>Here, <em>reducedData </em>is the Matrix consisting of the principal components,</p> 
     <p> åœ¨è¿™é‡Œï¼Œ <em>reduceData</em>æ˜¯ç”±ä¸»è¦æˆåˆ†ç»„æˆçš„çŸ©é˜µï¼Œ </p> 
     <p><em>FeatureVector </em>is the matrix we formed using the eigenvectors we chose to keep, and</p> 
     <p> <em>FeatureVector</em>æ˜¯æˆ‘ä»¬ä½¿ç”¨é€‰æ‹©ä¿ç•™çš„ç‰¹å¾å‘é‡å½¢æˆçš„çŸ©é˜µï¼Œå¹¶ä¸” </p> 
     <p><em>ScaledData </em>is the scaled version of the original dataset. T stands for transpose we perform on the feature and scaled data.</p> 
     <p> <em>ScaledData</em>æ˜¯åŸå§‹æ•°æ®é›†çš„ç¼©æ”¾ç‰ˆæœ¬ã€‚ Tä»£è¡¨æˆ‘ä»¬å¯¹ç‰¹å¾å’Œç¼©æ”¾æ•°æ®æ‰§è¡Œçš„è½¬ç½®ã€‚ </p> 
     <h2> æ€»ç»“PCAï¼š <span style="font-weight: bold;">(</span>Summarizing PCA:<span style="font-weight: bold;">)</span></h2> 
     <p>PCA brings together:</p> 
     <p> PCAæ±‡é›†äº†ï¼š </p> 
     <ol><li>A measure of how each variable is associated with one another. It uses the Covariance matrix to do so.<p class="nodelete"></p> ä¸€ç§åº¦é‡æ¯ä¸ªå˜é‡å¦‚ä½•ç›¸äº’å…³è”çš„åº¦é‡ã€‚ å®ƒä½¿ç”¨åæ–¹å·®çŸ©é˜µæ‰§è¡Œæ­¤æ“ä½œã€‚ </li><li>The directions in which our data are dispersed. (Eigenvectors.)<p class="nodelete"></p> æ•°æ®åˆ†æ•£çš„æ–¹å‘ã€‚ (ç‰¹å¾å‘é‡) </li><li>The relative importance of these different directions. (Eigenvalues.)<p class="nodelete"></p> è¿™äº›ä¸åŒæ–¹å‘çš„ç›¸å¯¹é‡è¦æ€§ã€‚ (ç‰¹å¾å€¼)ã€‚ </li></ol> 
     <blockquote> 
      <p><em>PCA combines our predictors and allows us to drop the eigenvectors that are relatively unimportant</em></p> 
      <p> <em>PCAç»“åˆäº†æˆ‘ä»¬çš„é¢„æµ‹å˜é‡ï¼Œå¹¶å…è®¸æˆ‘ä»¬åˆ é™¤ç›¸å¯¹ä¸é‡è¦çš„ç‰¹å¾å‘é‡</em> </p> 
     </blockquote> 
     <h3> PCAçš„ç¼ºç‚¹ï¼š <span style="font-weight: bold;">(</span>The disadvantage of PCA :<span style="font-weight: bold;">)</span></h3> 
     <ul><li>These components are hard to interpret in prediction<p class="nodelete"></p> è¿™äº›æˆåˆ†å¾ˆéš¾åœ¨é¢„æµ‹ä¸­è§£é‡Š </li><li>Sensitive to the certain outliers<p class="nodelete"></p> å¯¹æŸäº›ç¦»ç¾¤å€¼æ•æ„Ÿ </li><li>PCA assumes that features are highly correlated, which may or may not be the case on a real-world scenario<p class="nodelete"></p> PCAå‡å®šåŠŸèƒ½é«˜åº¦ç›¸å…³ï¼Œåœ¨å®é™…æƒ…å†µä¸‹å¯èƒ½ä¼šæˆ–å¯èƒ½ä¸ä¼š </li></ul> 
     <h2> ä½¿ç”¨Pythonçš„åŠ¨æ‰‹PCAå®ç°ï¼š <span style="font-weight: bold;">(</span>Hands-On PCA Implementation Using Python :<span style="font-weight: bold;">)</span></h2> 
     <p>We will cover all the explained steps above in our hands-on python lab, where we will understand how to deal with the problem of the curse of dimensionality using PCA and will also see how to implement principal component analysis using python.</p> 
     <p> æˆ‘ä»¬å°†åœ¨åŠ¨æ‰‹æ“ä½œçš„pythonå®éªŒä¸­ä»‹ç»ä¸Šè¿°æ‰€æœ‰æ­¥éª¤ï¼Œåœ¨è¯¥å®éªŒä¸­ï¼Œæˆ‘ä»¬å°†äº†è§£å¦‚ä½•ä½¿ç”¨PCAå¤„ç†ç»´åº¦è¯…å’’çš„é—®é¢˜ï¼Œè¿˜å°†äº†è§£å¦‚ä½•ä½¿ç”¨pythonå®ç°ä¸»æˆåˆ†åˆ†æã€‚ </p> 
     <p>See you soon in Part 2 of our PCA series,</p> 
     <p> å³å°†åœ¨æˆ‘ä»¬çš„PCAç³»åˆ—ç¬¬2éƒ¨åˆ†ä¸­ä¸æ‚¨è§é¢ï¼Œ </p> 
     <h3> å€¼å¾—æ·±æ€çš„äº‹æƒ…ï¼Œåœ¨æˆ‘é€€å‡ºä¹‹å‰ï¼š <span style="font-weight: bold;">(</span>Food for thought Before I sign-off:<span style="font-weight: bold;">)</span></h3> 
     <p>â€œ The more you try to make sense of data the more informed &amp; responsible business leader you will become.â€</p> 
     <p> â€œæ‚¨è¶ŠåŠªåŠ›ç†è§£æ•°æ®ï¼Œæ‚¨å°†å˜å¾—è¶Šæœ‰è§è¯†å’Œè´Ÿè´£ä»»çš„ä¸šåŠ¡ä¸»ç®¡ã€‚â€ </p> 
     <p><strong>Thanks for being there all along â€¦â€¦</strong></p> 
     <p> <strong>æ„Ÿè°¢æ‚¨ä¸€ç›´ä»¥æ¥çš„å…‰ä¸´â€¦â€¦</strong> </p> 
     <p>.</p> 
     <p> ã€‚ </p> 
     <p>â€¦</p> 
     <p> â€¦ </p> 
    </div> 
   </div> 
  </section> 
 </div> 
 <blockquote> 
  <p>ç¿»è¯‘è‡ª: <a href="https://medium.com/swlh/intuition-behind-principal-component-analysis-you-ever-wanted-to-understand-af1b8c1ea801" rel="nofollow">https://medium.com/swlh/intuition-behind-principal-component-analysis-you-ever-wanted-to-understand-af1b8c1ea801</a></p> 
 </blockquote> 
 <p>ä¸»æˆåˆ†åˆ†æ ç‹¬ç«‹æˆåˆ†åˆ†æ</p> 
</article>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a4742dd3653cf05c09a08612f419ac13/" rel="prev">
			<span class="pager__subtitle">Â«&thinsp;Previous</span>
			<p class="pager__title">æ¨èç³»ç»Ÿé¢†åŸŸç›¸å…³çŸ¥è¯†æ€»ç»“</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7fcd779bae4478f408f0983920e026d9/" rel="next">
			<span class="pager__subtitle">Next&thinsp;Â»</span>
			<p class="pager__title">Android SharedPreferencesç¤ºä¾‹</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 ç¼–ç¨‹éšæƒ³.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151182"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>